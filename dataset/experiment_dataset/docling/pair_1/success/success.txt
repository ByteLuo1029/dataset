2025-04-14T15:48:22.3610887Z Current runner version: '2.323.0'
2025-04-14T15:48:22.3636402Z ##[group]Operating System
2025-04-14T15:48:22.3637185Z Ubuntu
2025-04-14T15:48:22.3637636Z 24.04.2
2025-04-14T15:48:22.3638206Z LTS
2025-04-14T15:48:22.3638643Z ##[endgroup]
2025-04-14T15:48:22.3639149Z ##[group]Runner Image
2025-04-14T15:48:22.3639742Z Image: ubuntu-24.04
2025-04-14T15:48:22.3640289Z Version: 20250406.1.0
2025-04-14T15:48:22.3641351Z Included Software: https://github.com/actions/runner-images/blob/ubuntu24/20250406.1/images/ubuntu/Ubuntu2404-Readme.md
2025-04-14T15:48:22.3642736Z Image Release: https://github.com/actions/runner-images/releases/tag/ubuntu24%2F20250406.1
2025-04-14T15:48:22.3643648Z ##[endgroup]
2025-04-14T15:48:22.3644448Z ##[group]Runner Image Provisioner
2025-04-14T15:48:22.3645111Z 2.0.422.1
2025-04-14T15:48:22.3645555Z ##[endgroup]
2025-04-14T15:48:22.3646614Z ##[group]GITHUB_TOKEN Permissions
2025-04-14T15:48:22.3648456Z Contents: read
2025-04-14T15:48:22.3649004Z Metadata: read
2025-04-14T15:48:22.3649758Z Packages: read
2025-04-14T15:48:22.3650337Z ##[endgroup]
2025-04-14T15:48:22.3652746Z Secret source: Actions
2025-04-14T15:48:22.3653536Z Prepare workflow directory
2025-04-14T15:48:22.4207045Z Prepare all required actions
2025-04-14T15:48:22.4265043Z Getting action download info
2025-04-14T15:48:22.7612946Z ##[group]Download immutable action package 'actions/checkout@v4'
2025-04-14T15:48:22.7614138Z Version: 4.2.2
2025-04-14T15:48:22.7615145Z Digest: sha256:ccb2698953eaebd21c7bf6268a94f9c26518a7e38e27e0b83c1fe1ad049819b1
2025-04-14T15:48:22.7616508Z Source commit SHA: 11bd71901bbe5b1630ceea73d27597364c9af683
2025-04-14T15:48:22.7617200Z ##[endgroup]
2025-04-14T15:48:22.8473768Z ##[group]Download immutable action package 'actions/cache@v4'
2025-04-14T15:48:22.8474869Z Version: 4.2.3
2025-04-14T15:48:22.8475666Z Digest: sha256:c8a3bb963e1f1826d8fcc8d1354f0dd29d8ac1db1d4f6f20247055ae11b81ed9
2025-04-14T15:48:22.8476577Z Source commit SHA: 5a3ec84eff668545956fd18022155c47e93e2684
2025-04-14T15:48:22.8477335Z ##[endgroup]
2025-04-14T15:48:23.0259151Z Complete job name: code-checks / run-checks (3.11)
2025-04-14T15:48:23.0946364Z ##[group]Run actions/checkout@v4
2025-04-14T15:48:23.0947260Z with:
2025-04-14T15:48:23.0947677Z   repository: docling-project/docling
2025-04-14T15:48:23.0948367Z   token: ***
2025-04-14T15:48:23.0948745Z   ssh-strict: true
2025-04-14T15:48:23.0949125Z   ssh-user: git
2025-04-14T15:48:23.0949521Z   persist-credentials: true
2025-04-14T15:48:23.0949956Z   clean: true
2025-04-14T15:48:23.0950344Z   sparse-checkout-cone-mode: true
2025-04-14T15:48:23.0950814Z   fetch-depth: 1
2025-04-14T15:48:23.0951188Z   fetch-tags: false
2025-04-14T15:48:23.0951580Z   show-progress: true
2025-04-14T15:48:23.0951962Z   lfs: false
2025-04-14T15:48:23.0952316Z   submodules: false
2025-04-14T15:48:23.0952698Z   set-safe-directory: true
2025-04-14T15:48:23.0953306Z env:
2025-04-14T15:48:23.0953672Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T15:48:23.0954299Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T15:48:23.0954715Z ##[endgroup]
2025-04-14T15:48:23.2701725Z Syncing repository: docling-project/docling
2025-04-14T15:48:23.2703819Z ##[group]Getting Git version info
2025-04-14T15:48:23.2704912Z Working directory is '/home/runner/work/docling/docling'
2025-04-14T15:48:23.2706043Z [command]/usr/bin/git version
2025-04-14T15:48:23.2730475Z git version 2.49.0
2025-04-14T15:48:23.2759109Z ##[endgroup]
2025-04-14T15:48:23.2780516Z Temporarily overriding HOME='/home/runner/work/_temp/57504461-2b7f-4c34-95f8-8477e02c04cf' before making global git config changes
2025-04-14T15:48:23.2786510Z Adding repository directory to the temporary git global config as a safe directory
2025-04-14T15:48:23.2787724Z [command]/usr/bin/git config --global --add safe.directory /home/runner/work/docling/docling
2025-04-14T15:48:23.2821564Z Deleting the contents of '/home/runner/work/docling/docling'
2025-04-14T15:48:23.2825689Z ##[group]Initializing the repository
2025-04-14T15:48:23.2829969Z [command]/usr/bin/git init /home/runner/work/docling/docling
2025-04-14T15:48:23.2895844Z hint: Using 'master' as the name for the initial branch. This default branch name
2025-04-14T15:48:23.2897181Z hint: is subject to change. To configure the initial branch name to use in all
2025-04-14T15:48:23.2899015Z hint: of your new repositories, which will suppress this warning, call:
2025-04-14T15:48:23.2900386Z hint:
2025-04-14T15:48:23.2901253Z hint: 	git config --global init.defaultBranch <name>
2025-04-14T15:48:23.2902323Z hint:
2025-04-14T15:48:23.2902949Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2025-04-14T15:48:23.2904107Z hint: 'development'. The just-created branch can be renamed via this command:
2025-04-14T15:48:23.2904877Z hint:
2025-04-14T15:48:23.2905293Z hint: 	git branch -m <name>
2025-04-14T15:48:23.2906062Z Initialized empty Git repository in /home/runner/work/docling/docling/.git/
2025-04-14T15:48:23.2912893Z [command]/usr/bin/git remote add origin https://github.com/docling-project/docling
2025-04-14T15:48:23.2946619Z ##[endgroup]
2025-04-14T15:48:23.2947508Z ##[group]Disabling automatic garbage collection
2025-04-14T15:48:23.2950878Z [command]/usr/bin/git config --local gc.auto 0
2025-04-14T15:48:23.2979837Z ##[endgroup]
2025-04-14T15:48:23.2980588Z ##[group]Setting up auth
2025-04-14T15:48:23.2987480Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2025-04-14T15:48:23.3020285Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2025-04-14T15:48:23.3312367Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2025-04-14T15:48:23.3347340Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2025-04-14T15:48:23.3599981Z [command]/usr/bin/git config --local http.https://github.com/.extraheader AUTHORIZATION: basic ***
2025-04-14T15:48:23.3649727Z ##[endgroup]
2025-04-14T15:48:23.3650613Z ##[group]Fetching the repository
2025-04-14T15:48:23.3659098Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +f5f4a75403931970fb53411f3ce01fc3f653cfd2:refs/remotes/origin/ci-sign-pypi-packages
2025-04-14T15:48:25.0292993Z From https://github.com/docling-project/docling
2025-04-14T15:48:25.0294943Z  * [new ref]         f5f4a75403931970fb53411f3ce01fc3f653cfd2 -> origin/ci-sign-pypi-packages
2025-04-14T15:48:25.0320697Z ##[endgroup]
2025-04-14T15:48:25.0321980Z ##[group]Determining the checkout info
2025-04-14T15:48:25.0323583Z ##[endgroup]
2025-04-14T15:48:25.0328055Z [command]/usr/bin/git sparse-checkout disable
2025-04-14T15:48:25.0369756Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2025-04-14T15:48:25.0399769Z ##[group]Checking out the ref
2025-04-14T15:48:25.0405099Z [command]/usr/bin/git checkout --progress --force -B ci-sign-pypi-packages refs/remotes/origin/ci-sign-pypi-packages
2025-04-14T15:48:25.2737344Z Switched to a new branch 'ci-sign-pypi-packages'
2025-04-14T15:48:25.2738952Z branch 'ci-sign-pypi-packages' set up to track 'origin/ci-sign-pypi-packages'.
2025-04-14T15:48:25.2762564Z ##[endgroup]
2025-04-14T15:48:25.2804888Z [command]/usr/bin/git log -1 --format=%H
2025-04-14T15:48:25.2830011Z f5f4a75403931970fb53411f3ce01fc3f653cfd2
2025-04-14T15:48:25.3015531Z ##[group]Run sudo apt-get update && sudo apt-get install -y tesseract-ocr tesseract-ocr-eng tesseract-ocr-fra tesseract-ocr-deu tesseract-ocr-spa tesseract-ocr-script-latn libleptonica-dev libtesseract-dev pkg-config
2025-04-14T15:48:25.3017015Z [36;1msudo apt-get update && sudo apt-get install -y tesseract-ocr tesseract-ocr-eng tesseract-ocr-fra tesseract-ocr-deu tesseract-ocr-spa tesseract-ocr-script-latn libleptonica-dev libtesseract-dev pkg-config[0m
2025-04-14T15:48:25.3069238Z shell: /usr/bin/bash -e {0}
2025-04-14T15:48:25.3069795Z env:
2025-04-14T15:48:25.3069975Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T15:48:25.3070203Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T15:48:25.3070396Z ##[endgroup]
2025-04-14T15:48:25.3852587Z Get:1 file:/etc/apt/apt-mirrors.txt Mirrorlist [142 B]
2025-04-14T15:48:25.4213170Z Hit:2 http://azure.archive.ubuntu.com/ubuntu noble InRelease
2025-04-14T15:48:25.4232715Z Get:3 http://azure.archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]
2025-04-14T15:48:25.4282691Z Hit:4 http://azure.archive.ubuntu.com/ubuntu noble-backports InRelease
2025-04-14T15:48:25.4302346Z Get:5 http://azure.archive.ubuntu.com/ubuntu noble-security InRelease [126 kB]
2025-04-14T15:48:25.5527617Z Get:6 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [991 kB]
2025-04-14T15:48:25.5668594Z Get:7 http://azure.archive.ubuntu.com/ubuntu noble-updates/main Translation-en [219 kB]
2025-04-14T15:48:25.5693491Z Get:8 http://azure.archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [1053 kB]
2025-04-14T15:48:25.5770132Z Get:9 http://azure.archive.ubuntu.com/ubuntu noble-updates/universe Translation-en [265 kB]
2025-04-14T15:48:25.5800043Z Get:10 http://azure.archive.ubuntu.com/ubuntu noble-updates/restricted Translation-en [182 kB]
2025-04-14T15:48:25.6365548Z Hit:11 https://packages.microsoft.com/repos/azure-cli noble InRelease
2025-04-14T15:48:25.6619602Z Get:12 http://azure.archive.ubuntu.com/ubuntu noble-security/main amd64 Packages [746 kB]
2025-04-14T15:48:25.6681840Z Get:13 http://azure.archive.ubuntu.com/ubuntu noble-security/main Translation-en [143 kB]
2025-04-14T15:48:25.6702196Z Get:14 http://azure.archive.ubuntu.com/ubuntu noble-security/universe amd64 Packages [829 kB]
2025-04-14T15:48:25.6757124Z Get:15 http://azure.archive.ubuntu.com/ubuntu noble-security/universe Translation-en [180 kB]
2025-04-14T15:48:25.6783445Z Get:16 http://azure.archive.ubuntu.com/ubuntu noble-security/restricted Translation-en [175 kB]
2025-04-14T15:48:25.7900704Z Get:17 https://packages.microsoft.com/ubuntu/24.04/prod noble InRelease [3600 B]
2025-04-14T15:48:25.9097833Z Get:18 https://packages.microsoft.com/ubuntu/24.04/prod noble/main arm64 Packages [17.1 kB]
2025-04-14T15:48:26.0042308Z Get:19 https://packages.microsoft.com/ubuntu/24.04/prod noble/main amd64 Packages [27.2 kB]
2025-04-14T15:48:30.7827040Z Fetched 5083 kB in 1s (7536 kB/s)
2025-04-14T15:48:31.4657807Z Reading package lists...
2025-04-14T15:48:31.4984282Z Reading package lists...
2025-04-14T15:48:31.7004941Z Building dependency tree...
2025-04-14T15:48:31.7012736Z Reading state information...
2025-04-14T15:48:31.8941484Z pkg-config is already the newest version (1.8.1-2build1).
2025-04-14T15:48:31.8942111Z The following additional packages will be installed:
2025-04-14T15:48:31.8943804Z   bzip2-doc comerr-dev libacl1-dev libarchive-dev libattr1-dev libbz2-dev
2025-04-14T15:48:31.8949112Z   libcurl4-openssl-dev libext2fs-dev libgif7 liblept5 liblzma-dev
2025-04-14T15:48:31.8955437Z   libtesseract5 nettle-dev tesseract-ocr-osd
2025-04-14T15:48:31.8966368Z Suggested packages:
2025-04-14T15:48:31.8967011Z   doc-base libcurl4-doc libidn-dev libkrb5-dev libldap2-dev librtmp-dev
2025-04-14T15:48:31.8967732Z   libssh2-1-dev liblzma-doc
2025-04-14T15:48:31.9198236Z The following NEW packages will be installed:
2025-04-14T15:48:31.9199780Z   bzip2-doc comerr-dev libacl1-dev libarchive-dev libattr1-dev libbz2-dev
2025-04-14T15:48:31.9201567Z   libcurl4-openssl-dev libext2fs-dev libgif7 liblept5 libleptonica-dev
2025-04-14T15:48:31.9207669Z   liblzma-dev libtesseract-dev libtesseract5 nettle-dev tesseract-ocr
2025-04-14T15:48:31.9208450Z   tesseract-ocr-deu tesseract-ocr-eng tesseract-ocr-fra tesseract-ocr-osd
2025-04-14T15:48:31.9209030Z   tesseract-ocr-script-latn tesseract-ocr-spa
2025-04-14T15:48:31.9387428Z 0 upgraded, 22 newly installed, 0 to remove and 100 not upgraded.
2025-04-14T15:48:31.9388007Z Need to get 52.9 MB of archives.
2025-04-14T15:48:31.9388447Z After this operation, 141 MB of additional disk space will be used.
2025-04-14T15:48:31.9389466Z Get:1 file:/etc/apt/apt-mirrors.txt Mirrorlist [142 B]
2025-04-14T15:48:32.0131634Z Get:2 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 bzip2-doc all 1.0.8-5.1build0.1 [499 kB]
2025-04-14T15:48:32.1166376Z Get:3 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 libbz2-dev amd64 1.0.8-5.1build0.1 [33.6 kB]
2025-04-14T15:48:32.1614949Z Get:4 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 liblzma-dev amd64 5.6.1+really5.4.5-1ubuntu0.2 [176 kB]
2025-04-14T15:48:32.2096223Z Get:5 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 libattr1-dev amd64 1:2.5.2-1build1.1 [23.1 kB]
2025-04-14T15:48:32.2607567Z Get:6 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 libacl1-dev amd64 2.3.2-1build1.1 [78.5 kB]
2025-04-14T15:48:32.3114513Z Get:7 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 comerr-dev amd64 2.1-1.47.0-2.4~exp1ubuntu4.1 [43.8 kB]
2025-04-14T15:48:32.3601982Z Get:8 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 libext2fs-dev amd64 1.47.0-2.4~exp1ubuntu4.1 [300 kB]
2025-04-14T15:48:32.4212707Z Get:9 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 nettle-dev amd64 3.9.1-2.2build1.1 [1154 kB]
2025-04-14T15:48:32.5266261Z Get:10 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 libarchive-dev amd64 3.7.2-2ubuntu0.3 [589 kB]
2025-04-14T15:48:32.6307343Z Get:11 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 libcurl4-openssl-dev amd64 8.5.0-2ubuntu10.6 [446 kB]
2025-04-14T15:48:32.7283187Z Get:12 http://azure.archive.ubuntu.com/ubuntu noble/main amd64 libgif7 amd64 5.2.2-1ubuntu1 [35.2 kB]
2025-04-14T15:48:32.7835984Z Get:13 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 liblept5 amd64 1.82.0-3build4 [1099 kB]
2025-04-14T15:48:32.9442523Z Get:14 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 libleptonica-dev amd64 1.82.0-3build4 [1565 kB]
2025-04-14T15:48:33.1248540Z Get:15 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 libtesseract5 amd64 5.3.4-1build5 [1291 kB]
2025-04-14T15:48:33.2911630Z Get:16 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 libtesseract-dev amd64 5.3.4-1build5 [1633 kB]
2025-04-14T15:48:33.4899243Z Get:17 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr-eng all 1:4.1.0-2 [1818 kB]
2025-04-14T15:48:33.7009499Z Get:18 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr-osd all 1:4.1.0-2 [3841 kB]
2025-04-14T15:48:33.9890860Z Get:19 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr amd64 5.3.4-1build5 [328 kB]
2025-04-14T15:48:34.0988828Z Get:20 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr-deu all 1:4.1.0-2 [818 kB]
2025-04-14T15:48:34.2199167Z Get:21 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr-fra all 1:4.1.0-2 [584 kB]
2025-04-14T15:48:34.3292046Z Get:22 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr-script-latn all 1:4.1.0-2 [35.5 MB]
2025-04-14T15:48:35.6487147Z Get:23 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr-spa all 1:4.1.0-2 [1065 kB]
2025-04-14T15:48:35.9620339Z Fetched 52.9 MB in 4s (14.0 MB/s)
2025-04-14T15:48:35.9830681Z Selecting previously unselected package bzip2-doc.
2025-04-14T15:48:36.0034700Z (Reading database ... 
2025-04-14T15:48:36.0035169Z (Reading database ... 5%
2025-04-14T15:48:36.0035579Z (Reading database ... 10%
2025-04-14T15:48:36.0036497Z (Reading database ... 15%
2025-04-14T15:48:36.0036784Z (Reading database ... 20%
2025-04-14T15:48:36.0037221Z (Reading database ... 25%
2025-04-14T15:48:36.0037654Z (Reading database ... 30%
2025-04-14T15:48:36.0038054Z (Reading database ... 35%
2025-04-14T15:48:36.0038445Z (Reading database ... 40%
2025-04-14T15:48:36.0038858Z (Reading database ... 45%
2025-04-14T15:48:36.0039263Z (Reading database ... 50%
2025-04-14T15:48:36.0091051Z (Reading database ... 55%
2025-04-14T15:48:36.0347353Z (Reading database ... 60%
2025-04-14T15:48:36.0494544Z (Reading database ... 65%
2025-04-14T15:48:36.0643572Z (Reading database ... 70%
2025-04-14T15:48:36.0859814Z (Reading database ... 75%
2025-04-14T15:48:36.1029256Z (Reading database ... 80%
2025-04-14T15:48:36.1367382Z (Reading database ... 85%
2025-04-14T15:48:36.1830395Z (Reading database ... 90%
2025-04-14T15:48:36.2100581Z (Reading database ... 95%
2025-04-14T15:48:36.2100894Z (Reading database ... 100%
2025-04-14T15:48:36.2101262Z (Reading database ... 221801 files and directories currently installed.)
2025-04-14T15:48:36.2146816Z Preparing to unpack .../00-bzip2-doc_1.0.8-5.1build0.1_all.deb ...
2025-04-14T15:48:36.2192725Z Unpacking bzip2-doc (1.0.8-5.1build0.1) ...
2025-04-14T15:48:36.2453163Z Selecting previously unselected package libbz2-dev:amd64.
2025-04-14T15:48:36.2589631Z Preparing to unpack .../01-libbz2-dev_1.0.8-5.1build0.1_amd64.deb ...
2025-04-14T15:48:36.2603291Z Unpacking libbz2-dev:amd64 (1.0.8-5.1build0.1) ...
2025-04-14T15:48:36.2822414Z Selecting previously unselected package liblzma-dev:amd64.
2025-04-14T15:48:36.2960449Z Preparing to unpack .../02-liblzma-dev_5.6.1+really5.4.5-1ubuntu0.2_amd64.deb ...
2025-04-14T15:48:36.2971317Z Unpacking liblzma-dev:amd64 (5.6.1+really5.4.5-1ubuntu0.2) ...
2025-04-14T15:48:36.3269467Z Selecting previously unselected package libattr1-dev:amd64.
2025-04-14T15:48:36.3406285Z Preparing to unpack .../03-libattr1-dev_1%3a2.5.2-1build1.1_amd64.deb ...
2025-04-14T15:48:36.3417981Z Unpacking libattr1-dev:amd64 (1:2.5.2-1build1.1) ...
2025-04-14T15:48:36.3682225Z Selecting previously unselected package libacl1-dev:amd64.
2025-04-14T15:48:36.3818457Z Preparing to unpack .../04-libacl1-dev_2.3.2-1build1.1_amd64.deb ...
2025-04-14T15:48:36.3830520Z Unpacking libacl1-dev:amd64 (2.3.2-1build1.1) ...
2025-04-14T15:48:36.4156913Z Selecting previously unselected package comerr-dev:amd64.
2025-04-14T15:48:36.4296744Z Preparing to unpack .../05-comerr-dev_2.1-1.47.0-2.4~exp1ubuntu4.1_amd64.deb ...
2025-04-14T15:48:36.4334354Z Unpacking comerr-dev:amd64 (2.1-1.47.0-2.4~exp1ubuntu4.1) ...
2025-04-14T15:48:36.4592694Z Selecting previously unselected package libext2fs-dev.
2025-04-14T15:48:36.4729276Z Preparing to unpack .../06-libext2fs-dev_1.47.0-2.4~exp1ubuntu4.1_amd64.deb ...
2025-04-14T15:48:36.4743353Z Unpacking libext2fs-dev (1.47.0-2.4~exp1ubuntu4.1) ...
2025-04-14T15:48:36.5040157Z Selecting previously unselected package nettle-dev:amd64.
2025-04-14T15:48:36.5178040Z Preparing to unpack .../07-nettle-dev_3.9.1-2.2build1.1_amd64.deb ...
2025-04-14T15:48:36.5191158Z Unpacking nettle-dev:amd64 (3.9.1-2.2build1.1) ...
2025-04-14T15:48:36.5671063Z Selecting previously unselected package libarchive-dev:amd64.
2025-04-14T15:48:36.5810961Z Preparing to unpack .../08-libarchive-dev_3.7.2-2ubuntu0.3_amd64.deb ...
2025-04-14T15:48:36.5821197Z Unpacking libarchive-dev:amd64 (3.7.2-2ubuntu0.3) ...
2025-04-14T15:48:36.6223598Z Selecting previously unselected package libcurl4-openssl-dev:amd64.
2025-04-14T15:48:36.6363627Z Preparing to unpack .../09-libcurl4-openssl-dev_8.5.0-2ubuntu10.6_amd64.deb ...
2025-04-14T15:48:36.6373756Z Unpacking libcurl4-openssl-dev:amd64 (8.5.0-2ubuntu10.6) ...
2025-04-14T15:48:36.6729624Z Selecting previously unselected package libgif7:amd64.
2025-04-14T15:48:36.6868096Z Preparing to unpack .../10-libgif7_5.2.2-1ubuntu1_amd64.deb ...
2025-04-14T15:48:36.6892345Z Unpacking libgif7:amd64 (5.2.2-1ubuntu1) ...
2025-04-14T15:48:36.7122179Z Selecting previously unselected package liblept5:amd64.
2025-04-14T15:48:36.7260491Z Preparing to unpack .../11-liblept5_1.82.0-3build4_amd64.deb ...
2025-04-14T15:48:36.7272710Z Unpacking liblept5:amd64 (1.82.0-3build4) ...
2025-04-14T15:48:36.7633500Z Selecting previously unselected package libleptonica-dev.
2025-04-14T15:48:36.7772757Z Preparing to unpack .../12-libleptonica-dev_1.82.0-3build4_amd64.deb ...
2025-04-14T15:48:36.7782180Z Unpacking libleptonica-dev (1.82.0-3build4) ...
2025-04-14T15:48:36.8308150Z Selecting previously unselected package libtesseract5:amd64.
2025-04-14T15:48:36.8447996Z Preparing to unpack .../13-libtesseract5_5.3.4-1build5_amd64.deb ...
2025-04-14T15:48:36.8457465Z Unpacking libtesseract5:amd64 (5.3.4-1build5) ...
2025-04-14T15:48:36.8862102Z Selecting previously unselected package libtesseract-dev:amd64.
2025-04-14T15:48:36.9033045Z Preparing to unpack .../14-libtesseract-dev_5.3.4-1build5_amd64.deb ...
2025-04-14T15:48:36.9043334Z Unpacking libtesseract-dev:amd64 (5.3.4-1build5) ...
2025-04-14T15:48:36.9582229Z Selecting previously unselected package tesseract-ocr-eng.
2025-04-14T15:48:36.9722592Z Preparing to unpack .../15-tesseract-ocr-eng_1%3a4.1.0-2_all.deb ...
2025-04-14T15:48:36.9731600Z Unpacking tesseract-ocr-eng (1:4.1.0-2) ...
2025-04-14T15:48:37.0157878Z Selecting previously unselected package tesseract-ocr-osd.
2025-04-14T15:48:37.0296085Z Preparing to unpack .../16-tesseract-ocr-osd_1%3a4.1.0-2_all.deb ...
2025-04-14T15:48:37.0304114Z Unpacking tesseract-ocr-osd (1:4.1.0-2) ...
2025-04-14T15:48:37.1010139Z Selecting previously unselected package tesseract-ocr.
2025-04-14T15:48:37.1150740Z Preparing to unpack .../17-tesseract-ocr_5.3.4-1build5_amd64.deb ...
2025-04-14T15:48:37.1159832Z Unpacking tesseract-ocr (5.3.4-1build5) ...
2025-04-14T15:48:37.1515401Z Selecting previously unselected package tesseract-ocr-deu.
2025-04-14T15:48:37.1654007Z Preparing to unpack .../18-tesseract-ocr-deu_1%3a4.1.0-2_all.deb ...
2025-04-14T15:48:37.1664613Z Unpacking tesseract-ocr-deu (1:4.1.0-2) ...
2025-04-14T15:48:37.1964139Z Selecting previously unselected package tesseract-ocr-fra.
2025-04-14T15:48:37.2100747Z Preparing to unpack .../19-tesseract-ocr-fra_1%3a4.1.0-2_all.deb ...
2025-04-14T15:48:37.2112337Z Unpacking tesseract-ocr-fra (1:4.1.0-2) ...
2025-04-14T15:48:37.2381533Z Selecting previously unselected package tesseract-ocr-script-latn.
2025-04-14T15:48:37.2520670Z Preparing to unpack .../20-tesseract-ocr-script-latn_1%3a4.1.0-2_all.deb ...
2025-04-14T15:48:37.2529763Z Unpacking tesseract-ocr-script-latn (1:4.1.0-2) ...
2025-04-14T15:48:37.7270916Z Selecting previously unselected package tesseract-ocr-spa.
2025-04-14T15:48:37.7407481Z Preparing to unpack .../21-tesseract-ocr-spa_1%3a4.1.0-2_all.deb ...
2025-04-14T15:48:37.7419960Z Unpacking tesseract-ocr-spa (1:4.1.0-2) ...
2025-04-14T15:48:37.7949959Z Setting up bzip2-doc (1.0.8-5.1build0.1) ...
2025-04-14T15:48:37.7976188Z Setting up libattr1-dev:amd64 (1:2.5.2-1build1.1) ...
2025-04-14T15:48:37.8000273Z Setting up nettle-dev:amd64 (3.9.1-2.2build1.1) ...
2025-04-14T15:48:37.8028633Z Setting up tesseract-ocr-script-latn (1:4.1.0-2) ...
2025-04-14T15:48:37.8056195Z Setting up tesseract-ocr-eng (1:4.1.0-2) ...
2025-04-14T15:48:37.8081840Z Setting up libcurl4-openssl-dev:amd64 (8.5.0-2ubuntu10.6) ...
2025-04-14T15:48:37.8110291Z Setting up tesseract-ocr-fra (1:4.1.0-2) ...
2025-04-14T15:48:37.8136006Z Setting up comerr-dev:amd64 (2.1-1.47.0-2.4~exp1ubuntu4.1) ...
2025-04-14T15:48:37.8174849Z Setting up liblzma-dev:amd64 (5.6.1+really5.4.5-1ubuntu0.2) ...
2025-04-14T15:48:37.8200073Z Setting up tesseract-ocr-deu (1:4.1.0-2) ...
2025-04-14T15:48:37.8228051Z Setting up libgif7:amd64 (5.2.2-1ubuntu1) ...
2025-04-14T15:48:37.8253677Z Setting up tesseract-ocr-spa (1:4.1.0-2) ...
2025-04-14T15:48:37.8281192Z Setting up tesseract-ocr-osd (1:4.1.0-2) ...
2025-04-14T15:48:37.8306823Z Setting up libext2fs-dev (1.47.0-2.4~exp1ubuntu4.1) ...
2025-04-14T15:48:37.8329589Z Setting up libacl1-dev:amd64 (2.3.2-1build1.1) ...
2025-04-14T15:48:37.8359640Z Setting up libbz2-dev:amd64 (1.0.8-5.1build0.1) ...
2025-04-14T15:48:37.8382037Z Setting up liblept5:amd64 (1.82.0-3build4) ...
2025-04-14T15:48:37.8411987Z Setting up libleptonica-dev (1.82.0-3build4) ...
2025-04-14T15:48:37.8439345Z Setting up libtesseract5:amd64 (5.3.4-1build5) ...
2025-04-14T15:48:37.8463532Z Setting up libarchive-dev:amd64 (3.7.2-2ubuntu0.3) ...
2025-04-14T15:48:37.8490236Z Setting up libtesseract-dev:amd64 (5.3.4-1build5) ...
2025-04-14T15:48:37.8515962Z Setting up tesseract-ocr (5.3.4-1build5) ...
2025-04-14T15:48:37.8545716Z Processing triggers for libc-bin (2.39-0ubuntu8.4) ...
2025-04-14T15:48:37.8831538Z Processing triggers for man-db (2.12.0-4build2) ...
2025-04-14T15:48:44.2302109Z Processing triggers for install-info (7.1-3build2) ...
2025-04-14T15:48:44.9310658Z 
2025-04-14T15:48:44.9311368Z Running kernel seems to be up-to-date.
2025-04-14T15:48:44.9311744Z 
2025-04-14T15:48:44.9311892Z No services need to be restarted.
2025-04-14T15:48:44.9312153Z 
2025-04-14T15:48:44.9312291Z No containers need to be restarted.
2025-04-14T15:48:44.9312555Z 
2025-04-14T15:48:44.9312769Z No user sessions are running outdated binaries.
2025-04-14T15:48:44.9313086Z 
2025-04-14T15:48:44.9313383Z No VM guests are running outdated hypervisor (qemu) binaries on this host.
2025-04-14T15:48:45.8814815Z ##[group]Run echo "TESSDATA_PREFIX=$(dpkg -L tesseract-ocr-eng | grep tessdata$)" >> "$GITHUB_ENV"
2025-04-14T15:48:45.8815462Z [36;1mecho "TESSDATA_PREFIX=$(dpkg -L tesseract-ocr-eng | grep tessdata$)" >> "$GITHUB_ENV"[0m
2025-04-14T15:48:45.8864167Z shell: /usr/bin/bash -e {0}
2025-04-14T15:48:45.8864387Z env:
2025-04-14T15:48:45.8864586Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T15:48:45.8864789Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T15:48:45.8864981Z ##[endgroup]
2025-04-14T15:48:45.9193033Z ##[group]Run actions/cache@v4
2025-04-14T15:48:45.9193276Z with:
2025-04-14T15:48:45.9193455Z   path: ~/.cache/huggingface
2025-04-14T15:48:45.9193673Z   key: huggingface-cache-py3.11
2025-04-14T15:48:45.9194598Z   enableCrossOsArchive: false
2025-04-14T15:48:45.9194824Z   fail-on-cache-miss: false
2025-04-14T15:48:45.9195027Z   lookup-only: false
2025-04-14T15:48:45.9195213Z   save-always: false
2025-04-14T15:48:45.9195390Z env:
2025-04-14T15:48:45.9195543Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T15:48:45.9195748Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T15:48:45.9195994Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-14T15:48:45.9196262Z ##[endgroup]
2025-04-14T15:48:46.2385973Z Cache hit for: huggingface-cache-py3.11
2025-04-14T15:48:47.3956003Z Received 83886080 of 1113780294 (7.5%), 80.0 MBs/sec
2025-04-14T15:48:48.3955479Z Received 247463936 of 1113780294 (22.2%), 118.0 MBs/sec
2025-04-14T15:48:49.3962685Z Received 377487360 of 1113780294 (33.9%), 120.0 MBs/sec
2025-04-14T15:48:50.3969504Z Received 528482304 of 1113780294 (47.4%), 126.0 MBs/sec
2025-04-14T15:48:51.4273998Z Received 671088640 of 1113780294 (60.3%), 127.2 MBs/sec
2025-04-14T15:48:52.4276368Z Received 805306368 of 1113780294 (72.3%), 127.3 MBs/sec
2025-04-14T15:48:53.4294312Z Received 947912704 of 1113780294 (85.1%), 128.5 MBs/sec
2025-04-14T15:48:54.4291738Z Received 1109585990 of 1113780294 (99.6%), 131.7 MBs/sec
2025-04-14T15:48:54.4852524Z Received 1113780294 of 1113780294 (100.0%), 131.3 MBs/sec
2025-04-14T15:48:54.4855489Z Cache Size: ~1062 MB (1113780294 B)
2025-04-14T15:48:54.4964519Z [command]/usr/bin/tar -xf /home/runner/work/_temp/5ece87bc-f5be-40cb-9ee2-a552135ab977/cache.tzst -P -C /home/runner/work/docling/docling --use-compress-program unzstd
2025-04-14T15:48:56.4772878Z Cache restored successfully
2025-04-14T15:48:56.7120658Z Cache restored from key: huggingface-cache-py3.11
2025-04-14T15:48:56.7315649Z Prepare all required actions
2025-04-14T15:48:56.7316093Z Getting action download info
2025-04-14T15:48:56.8789090Z ##[group]Download immutable action package 'actions/setup-python@v5'
2025-04-14T15:48:56.8789470Z Version: 5.5.0
2025-04-14T15:48:56.8789831Z Digest: sha256:1fac95d751afd58314aebd2ab01d4bf3b3109a441917bd2cac2a22898ec494f7
2025-04-14T15:48:56.8790331Z Source commit SHA: 8d9ed9ac5c53483de85588cdf95a591a75ab9f55
2025-04-14T15:48:56.8790654Z ##[endgroup]
2025-04-14T15:48:57.2321186Z ##[group]Run ./.github/actions/setup-poetry
2025-04-14T15:48:57.2321462Z with:
2025-04-14T15:48:57.2321628Z   python-version: 3.11
2025-04-14T15:48:57.2321816Z env:
2025-04-14T15:48:57.2321979Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T15:48:57.2322185Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T15:48:57.2322431Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-14T15:48:57.2322699Z ##[endgroup]
2025-04-14T15:48:57.2386761Z ##[group]Run pipx install poetry==1.8.5
2025-04-14T15:48:57.2387269Z [36;1mpipx install poetry==1.8.5[0m
2025-04-14T15:48:57.2435007Z shell: /usr/bin/bash --noprofile --norc -e -o pipefail {0}
2025-04-14T15:48:57.2435322Z env:
2025-04-14T15:48:57.2435497Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T15:48:57.2435720Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T15:48:57.2435963Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-14T15:48:57.2436246Z ##[endgroup]
2025-04-14T15:48:57.3846921Z creating virtual environment...
2025-04-14T15:48:57.4850492Z installing poetry from spec 'poetry==1.8.5'...
2025-04-14T15:49:04.7014667Z done! ✨ 🌟 ✨
2025-04-14T15:49:04.7015193Z   installed package poetry 1.8.5, installed using Python 3.12.3
2025-04-14T15:49:04.7015730Z   These apps are now globally available
2025-04-14T15:49:04.7016100Z     - poetry
2025-04-14T15:49:04.7258956Z ##[group]Run actions/setup-python@v5
2025-04-14T15:49:04.7259203Z with:
2025-04-14T15:49:04.7259379Z   python-version: 3.11
2025-04-14T15:49:04.7259573Z   cache: poetry
2025-04-14T15:49:04.7259796Z   check-latest: false
2025-04-14T15:49:04.7260085Z   token: ***
2025-04-14T15:49:04.7260274Z   update-environment: true
2025-04-14T15:49:04.7260483Z   allow-prereleases: false
2025-04-14T15:49:04.7260685Z   freethreaded: false
2025-04-14T15:49:04.7260866Z env:
2025-04-14T15:49:04.7261037Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T15:49:04.7261248Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T15:49:04.7261496Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-14T15:49:04.7261766Z ##[endgroup]
2025-04-14T15:49:04.8937051Z ##[group]Installed versions
2025-04-14T15:49:04.8998657Z Successfully set up CPython (3.11.11)
2025-04-14T15:49:04.8999293Z ##[endgroup]
2025-04-14T15:49:04.9985161Z [command]/opt/pipx_bin/poetry config --list
2025-04-14T15:49:05.3778497Z cache-dir = "/home/runner/.cache/pypoetry"
2025-04-14T15:49:05.3779222Z experimental.system-git-client = false
2025-04-14T15:49:05.3779816Z installer.max-workers = null
2025-04-14T15:49:05.3780799Z installer.modern-installation = true
2025-04-14T15:49:05.3781419Z installer.no-binary = null
2025-04-14T15:49:05.3781948Z installer.parallel = true
2025-04-14T15:49:05.3782352Z keyring.enabled = true
2025-04-14T15:49:05.3782661Z solver.lazy-wheel = true
2025-04-14T15:49:05.3782906Z virtualenvs.create = true
2025-04-14T15:49:05.3783165Z virtualenvs.in-project = null
2025-04-14T15:49:05.3783453Z virtualenvs.options.always-copy = false
2025-04-14T15:49:05.3783752Z virtualenvs.options.no-pip = false
2025-04-14T15:49:05.3784203Z virtualenvs.options.no-setuptools = false
2025-04-14T15:49:05.3784541Z virtualenvs.options.system-site-packages = false
2025-04-14T15:49:05.3785000Z virtualenvs.path = "{cache-dir}/virtualenvs"  # /home/runner/.cache/pypoetry/virtualenvs
2025-04-14T15:49:05.3785436Z virtualenvs.prefer-active-python = false
2025-04-14T15:49:05.3785819Z virtualenvs.prompt = "{project_name}-py{python_version}"
2025-04-14T15:49:05.3786137Z warnings.export = true
2025-04-14T15:49:05.6325951Z Cache hit for: setup-python-Linux-x64-python-3.11.11-poetry-v2-de9cc18c777c5ffb2a903b64fdaa7ce75e28e674116098f03bcf95bf424206d0
2025-04-14T15:49:06.7968979Z Received 100663296 of 2930192107 (3.4%), 95.4 MBs/sec
2025-04-14T15:49:07.7979547Z Received 260046848 of 2930192107 (8.9%), 123.6 MBs/sec
2025-04-14T15:49:08.8343696Z Received 402653184 of 2930192107 (13.7%), 126.1 MBs/sec
2025-04-14T15:49:09.8344276Z Received 532676608 of 2930192107 (18.2%), 125.6 MBs/sec
2025-04-14T15:49:10.9135121Z Received 671088640 of 2930192107 (22.9%), 125.0 MBs/sec
2025-04-14T15:49:11.9199493Z Received 822083584 of 2930192107 (28.1%), 127.9 MBs/sec
2025-04-14T15:49:12.9201460Z Received 1002438656 of 2930192107 (34.2%), 134.1 MBs/sec
2025-04-14T15:49:13.9215680Z Received 1153433600 of 2930192107 (39.4%), 135.3 MBs/sec
2025-04-14T15:49:14.9228089Z Received 1317011456 of 2930192107 (44.9%), 137.6 MBs/sec
2025-04-14T15:49:15.9224202Z Received 1447034880 of 2930192107 (49.4%), 136.2 MBs/sec
2025-04-14T15:49:16.9230310Z Received 1543503872 of 2930192107 (52.7%), 132.2 MBs/sec
2025-04-14T15:49:17.9232594Z Received 1610612736 of 2930192107 (55.0%), 126.6 MBs/sec
2025-04-14T15:49:18.9760258Z Received 1744830464 of 2930192107 (59.5%), 126.2 MBs/sec
2025-04-14T15:49:20.0181877Z Received 1879048192 of 2930192107 (64.1%), 126.0 MBs/sec
2025-04-14T15:49:21.0227058Z Received 2034237440 of 2930192107 (69.4%), 127.4 MBs/sec
2025-04-14T15:49:22.0257153Z Received 2185232384 of 2930192107 (74.6%), 128.4 MBs/sec
2025-04-14T15:49:23.0248268Z Received 2348810240 of 2930192107 (80.2%), 130.0 MBs/sec
2025-04-14T15:49:24.0251003Z Received 2516582400 of 2930192107 (85.9%), 131.6 MBs/sec
2025-04-14T15:49:25.0607674Z Received 2684354560 of 2930192107 (91.6%), 132.8 MBs/sec
2025-04-14T15:49:26.0623279Z Received 2839543808 of 2930192107 (96.9%), 133.6 MBs/sec
2025-04-14T15:49:26.6109821Z Received 2930192107 of 2930192107 (100.0%), 134.2 MBs/sec
2025-04-14T15:49:26.6111011Z Cache Size: ~2794 MB (2930192107 B)
2025-04-14T15:49:26.6150731Z [command]/usr/bin/tar -xf /home/runner/work/_temp/40e83ced-ac98-4ada-9b11-ab4ad4f7fad9/cache.tzst -P -C /home/runner/work/docling/docling --use-compress-program unzstd
2025-04-14T15:49:41.4536584Z Cache restored successfully
2025-04-14T15:49:42.9385610Z [command]/opt/pipx_bin/poetry env use /opt/hostedtoolcache/Python/3.11.11/x64/bin/python
2025-04-14T15:49:44.2197741Z Using virtualenv: /home/runner/.cache/pypoetry/virtualenvs/docling-oEGFWdgn-py3.11
2025-04-14T15:49:44.2588084Z Cache restored from key: setup-python-Linux-x64-python-3.11.11-poetry-v2-de9cc18c777c5ffb2a903b64fdaa7ce75e28e674116098f03bcf95bf424206d0
2025-04-14T15:49:44.2712509Z ##[group]Run poetry install --all-extras
2025-04-14T15:49:44.2712816Z [36;1mpoetry install --all-extras[0m
2025-04-14T15:49:44.2768985Z shell: /usr/bin/bash --noprofile --norc -e -o pipefail {0}
2025-04-14T15:49:44.2769319Z env:
2025-04-14T15:49:44.2769498Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T15:49:44.2769724Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T15:49:44.2769984Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-14T15:49:44.2770379Z   pythonLocation: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:49:44.2770792Z   PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.11.11/x64/lib/pkgconfig
2025-04-14T15:49:44.2771186Z   Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:49:44.2771541Z   Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:49:44.2771899Z   Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:49:44.2772257Z   LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.11.11/x64/lib
2025-04-14T15:49:44.2772550Z ##[endgroup]
2025-04-14T15:49:45.2146004Z Installing dependencies from lock file
2025-04-14T15:49:45.9429856Z 
2025-04-14T15:49:45.9430636Z No dependencies to install or update
2025-04-14T15:49:45.9486486Z 
2025-04-14T15:49:45.9487025Z Installing the current project: docling (2.30.0)
2025-04-14T15:49:46.0390190Z ##[group]Run poetry run pre-commit run --all-files
2025-04-14T15:49:46.0390619Z [36;1mpoetry run pre-commit run --all-files[0m
2025-04-14T15:49:46.0437660Z shell: /usr/bin/bash -e {0}
2025-04-14T15:49:46.0437924Z env:
2025-04-14T15:49:46.0438128Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T15:49:46.0438379Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T15:49:46.0438669Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-14T15:49:46.0439047Z   pythonLocation: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:49:46.0439486Z   PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.11.11/x64/lib/pkgconfig
2025-04-14T15:49:46.0439915Z   Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:49:46.0440294Z   Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:49:46.0440666Z   Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:49:46.0441033Z   LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.11.11/x64/lib
2025-04-14T15:49:46.0441381Z ##[endgroup]
2025-04-14T15:49:49.2214177Z Black....................................................................Passed
2025-04-14T15:49:50.7404725Z isort....................................................................Passed
2025-04-14T15:50:56.3267461Z MyPy.....................................................................Passed
2025-04-14T15:50:59.3469622Z nbQA Black...............................................................Passed
2025-04-14T15:51:01.1808294Z nbQA isort...............................................................Passed
2025-04-14T15:51:01.9422336Z Poetry check.............................................................Passed
2025-04-14T15:51:01.9626750Z ##[group]Run poetry install --all-extras
2025-04-14T15:51:01.9627294Z [36;1mpoetry install --all-extras[0m
2025-04-14T15:51:01.9693549Z shell: /usr/bin/bash -e {0}
2025-04-14T15:51:01.9694301Z env:
2025-04-14T15:51:01.9694596Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T15:51:01.9694994Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T15:51:01.9695475Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-14T15:51:01.9696107Z   pythonLocation: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:51:01.9696861Z   PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.11.11/x64/lib/pkgconfig
2025-04-14T15:51:01.9697584Z   Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:51:01.9698223Z   Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:51:01.9698855Z   Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:51:01.9699515Z   LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.11.11/x64/lib
2025-04-14T15:51:01.9700054Z ##[endgroup]
2025-04-14T15:51:02.9167067Z Installing dependencies from lock file
2025-04-14T15:51:03.6731306Z 
2025-04-14T15:51:03.6731946Z No dependencies to install or update
2025-04-14T15:51:03.6788138Z 
2025-04-14T15:51:03.6788764Z Installing the current project: docling (2.30.0)
2025-04-14T15:51:03.7807246Z ##[group]Run poetry run pytest -v tests
2025-04-14T15:51:03.7807615Z [36;1mpoetry run pytest -v tests[0m
2025-04-14T15:51:03.7856874Z shell: /usr/bin/bash -e {0}
2025-04-14T15:51:03.7857103Z env:
2025-04-14T15:51:03.7857279Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T15:51:03.7857519Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T15:51:03.7857783Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-14T15:51:03.7858134Z   pythonLocation: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:51:03.7858549Z   PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.11.11/x64/lib/pkgconfig
2025-04-14T15:51:03.7858942Z   Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:51:03.7859288Z   Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:51:03.7859642Z   Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T15:51:03.7859994Z   LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.11.11/x64/lib
2025-04-14T15:51:03.7860289Z ##[endgroup]
2025-04-14T15:51:04.7169020Z ============================= test session starts ==============================
2025-04-14T15:51:04.7170213Z platform linux -- Python 3.11.11, pytest-7.4.4, pluggy-1.5.0 -- /home/runner/.cache/pypoetry/virtualenvs/docling-oEGFWdgn-py3.11/bin/python
2025-04-14T15:51:04.7171245Z cachedir: .pytest_cache
2025-04-14T15:51:04.7171669Z rootdir: /home/runner/work/docling/docling
2025-04-14T15:51:04.7172307Z plugins: anyio-4.9.0, xdist-3.6.1
2025-04-14T15:51:10.5624998Z collecting ... collected 67 items
2025-04-14T15:51:10.5625436Z 
2025-04-14T15:51:10.5961484Z tests/test_backend_asciidoc.py::test_asciidocs_examples PASSED           [  1%]
2025-04-14T15:51:11.2335705Z tests/test_backend_csv.py::test_e2e_valid_csv_conversions PASSED         [  2%]
2025-04-14T15:51:11.2368365Z tests/test_backend_csv.py::test_e2e_invalid_csv_conversions PASSED       [  4%]
2025-04-14T15:51:11.5396127Z tests/test_backend_docling_json.py::test_convert_valid_docling_json PASSED [  5%]
2025-04-14T15:51:11.5407143Z tests/test_backend_docling_json.py::test_invalid_docling_json PASSED     [  7%]
2025-04-14T15:51:38.1339303Z tests/test_backend_docling_parse.py::test_text_cell_counts PASSED        [  8%]
2025-04-14T15:51:43.8665013Z tests/test_backend_docling_parse.py::test_get_text_from_rect PASSED      [ 10%]
2025-04-14T15:51:49.0057146Z tests/test_backend_docling_parse.py::test_crop_page_image PASSED         [ 11%]
2025-04-14T15:51:49.2248653Z tests/test_backend_docling_parse.py::test_num_pages PASSED               [ 13%]
2025-04-14T15:51:51.2985756Z tests/test_backend_docling_parse_v2.py::test_text_cell_counts PASSED     [ 14%]
2025-04-14T15:51:52.4479007Z tests/test_backend_docling_parse_v2.py::test_get_text_from_rect PASSED   [ 16%]
2025-04-14T15:51:53.5965071Z tests/test_backend_docling_parse_v2.py::test_crop_page_image PASSED      [ 17%]
2025-04-14T15:51:53.6039026Z tests/test_backend_docling_parse_v2.py::test_num_pages PASSED            [ 19%]
2025-04-14T15:51:53.6843454Z tests/test_backend_docling_parse_v4.py::test_text_cell_counts PASSED     [ 20%]
2025-04-14T15:51:56.4800145Z tests/test_backend_docling_parse_v4.py::test_get_text_from_rect PASSED   [ 22%]
2025-04-14T15:51:59.3318317Z tests/test_backend_docling_parse_v4.py::test_crop_page_image PASSED      [ 23%]
2025-04-14T15:51:59.3391238Z tests/test_backend_docling_parse_v4.py::test_num_pages PASSED            [ 25%]
2025-04-14T15:51:59.7444964Z tests/test_backend_html.py::test_heading_levels PASSED                   [ 26%]
2025-04-14T15:51:59.7452242Z tests/test_backend_html.py::test_ordered_lists SKIPPED (Temporarily ...) [ 28%]
2025-04-14T15:52:00.0024521Z tests/test_backend_html.py::test_e2e_html_conversions PASSED             [ 29%]
2025-04-14T15:52:00.0036208Z tests/test_backend_jats.py::test_e2e_pubmed_conversions PASSED           [ 31%]
2025-04-14T15:52:00.0045552Z tests/test_backend_jats.py::test_e2e_pubmed_conversions_stream PASSED    [ 32%]
2025-04-14T15:52:00.0054998Z tests/test_backend_jats.py::test_e2e_pubmed_conversions_no_stream PASSED [ 34%]
2025-04-14T15:52:02.1205705Z tests/test_backend_markdown.py::test_convert_valid PASSED                [ 35%]
2025-04-14T15:52:02.3134997Z tests/test_backend_msexcel.py::test_e2e_xlsx_conversions PASSED          [ 37%]
2025-04-14T15:52:02.3483405Z tests/test_backend_msexcel.py::test_pages PASSED                         [ 38%]
2025-04-14T15:52:02.4076252Z tests/test_backend_msword.py::test_heading_levels PASSED                 [ 40%]
2025-04-14T15:52:03.0104204Z tests/test_backend_msword.py::test_e2e_docx_conversions PASSED           [ 41%]
2025-04-14T15:52:03.0109592Z tests/test_backend_patent_uspto.py::test_patent_export SKIPPED (Slow...) [ 43%]
2025-04-14T15:52:05.2702877Z tests/test_backend_patent_uspto.py::test_patent_groundtruth PASSED       [ 44%]
2025-04-14T15:52:05.2770892Z tests/test_backend_patent_uspto.py::test_tables PASSED                   [ 46%]
2025-04-14T15:52:05.2778479Z tests/test_backend_patent_uspto.py::test_patent_uspto_ice PASSED         [ 47%]
2025-04-14T15:52:05.2785431Z tests/test_backend_patent_uspto.py::test_patent_uspto_grant_v2 PASSED    [ 49%]
2025-04-14T15:52:05.2792443Z tests/test_backend_patent_uspto.py::test_patent_uspto_app_v1 PASSED      [ 50%]
2025-04-14T15:52:05.2879044Z tests/test_backend_patent_uspto.py::test_patent_uspto_grant_aps PASSED   [ 52%]
2025-04-14T15:52:05.6624562Z tests/test_backend_pdfium.py::test_text_cell_counts PASSED               [ 53%]
2025-04-14T15:52:05.6910279Z tests/test_backend_pdfium.py::test_get_text_from_rect PASSED             [ 55%]
2025-04-14T15:52:05.7428456Z tests/test_backend_pdfium.py::test_crop_page_image PASSED                [ 56%]
2025-04-14T15:52:05.7471808Z tests/test_backend_pdfium.py::test_num_pages PASSED                      [ 58%]
2025-04-14T15:52:05.8935278Z tests/test_backend_pptx.py::test_e2e_pptx_conversions PASSED             [ 59%]
2025-04-14T15:52:05.9448212Z tests/test_cli.py::test_cli_help PASSED                                  [ 61%]
2025-04-14T15:52:05.9508445Z tests/test_cli.py::test_cli_version PASSED                               [ 62%]
2025-04-14T15:52:16.0907024Z tests/test_cli.py::test_cli_convert PASSED                               [ 64%]
2025-04-14T15:52:41.1450719Z tests/test_code_formula.py::test_code_and_formula_conversion PASSED      [ 65%]
2025-04-14T15:52:41.1458755Z tests/test_data_gen_flag.py::test_gen_test_data_flag PASSED              [ 67%]
2025-04-14T15:52:45.3119429Z tests/test_document_picture_classifier.py::test_picture_classifier PASSED [ 68%]
2025-04-14T15:57:13.6735654Z tests/test_e2e_conversion.py::test_e2e_pdfs_conversions PASSED           [ 70%]
2025-04-14T15:58:31.8853543Z tests/test_e2e_ocr_conversion.py::test_e2e_conversions PASSED            [ 71%]
2025-04-14T15:58:31.8928622Z tests/test_input_doc.py::test_in_doc_from_valid_path PASSED              [ 73%]
2025-04-14T15:58:31.9118900Z tests/test_input_doc.py::test_in_doc_from_invalid_path PASSED            [ 74%]
2025-04-14T15:58:31.9236090Z tests/test_input_doc.py::test_in_doc_from_valid_buf PASSED               [ 76%]
2025-04-14T15:58:31.9258526Z tests/test_input_doc.py::test_in_doc_from_invalid_buf PASSED             [ 77%]
2025-04-14T15:58:34.5239213Z tests/test_input_doc.py::test_image_in_pdf_backend PASSED                [ 79%]
2025-04-14T15:58:34.5352645Z tests/test_input_doc.py::test_in_doc_with_page_range PASSED              [ 80%]
2025-04-14T15:58:34.5428409Z tests/test_input_doc.py::test_guess_format PASSED                        [ 82%]
2025-04-14T15:58:40.7165206Z tests/test_interfaces.py::test_convert_path PASSED                       [ 83%]
2025-04-14T15:58:46.9544043Z tests/test_interfaces.py::test_convert_stream PASSED                     [ 85%]
2025-04-14T15:58:46.9575723Z tests/test_invalid_input.py::test_convert_unsupported_doc_format_wout_exception PASSED [ 86%]
2025-04-14T15:58:46.9603366Z tests/test_invalid_input.py::test_convert_unsupported_doc_format_with_exception PASSED [ 88%]
2025-04-14T15:58:46.9633032Z tests/test_invalid_input.py::test_convert_too_small_filesize_limit_wout_exception PASSED [ 89%]
2025-04-14T15:58:46.9659985Z tests/test_invalid_input.py::test_convert_too_small_filesize_limit_with_exception PASSED [ 91%]
2025-04-14T15:59:46.2895317Z tests/test_legacy_format_transform.py::test_compare_legacy_output PASSED [ 92%]
2025-04-14T15:59:46.2932948Z tests/test_options.py::test_accelerator_options PASSED                   [ 94%]
2025-04-14T16:02:42.5747203Z tests/test_options.py::test_e2e_conversions PASSED                       [ 95%]
2025-04-14T16:03:07.8686070Z tests/test_options.py::test_page_range PASSED                            [ 97%]
2025-04-14T16:03:11.9075506Z tests/test_options.py::test_ocr_coverage_threshold PASSED                [ 98%]
2025-04-14T16:03:24.6999272Z tests/test_options.py::test_parser_backends PASSED                       [100%]
2025-04-14T16:03:24.7001782Z 
2025-04-14T16:03:24.7002085Z =============================== warnings summary ===============================
2025-04-14T16:03:24.7003133Z ../../../.cache/pypoetry/virtualenvs/docling-oEGFWdgn-py3.11/lib/python3.11/site-packages/docling_core/types/doc/document.py:4067: 1 warning
2025-04-14T16:03:24.7003774Z tests/test_backend_asciidoc.py: 4 warnings
2025-04-14T16:03:24.7004324Z tests/test_backend_csv.py: 24 warnings
2025-04-14T16:03:24.7004597Z tests/test_backend_docling_json.py: 3 warnings
2025-04-14T16:03:24.7004886Z tests/test_backend_html.py: 28 warnings
2025-04-14T16:03:24.7005147Z tests/test_backend_markdown.py: 20 warnings
2025-04-14T16:03:24.7005431Z tests/test_backend_msexcel.py: 3 warnings
2025-04-14T16:03:24.7005700Z tests/test_backend_msword.py: 32 warnings
2025-04-14T16:03:24.7005977Z tests/test_backend_patent_uspto.py: 14 warnings
2025-04-14T16:03:24.7006247Z tests/test_backend_pptx.py: 6 warnings
2025-04-14T16:03:24.7006491Z tests/test_cli.py: 2 warnings
2025-04-14T16:03:24.7006719Z tests/test_code_formula.py: 1 warning
2025-04-14T16:03:24.7007007Z tests/test_document_picture_classifier.py: 1 warning
2025-04-14T16:03:24.7007302Z tests/test_e2e_conversion.py: 44 warnings
2025-04-14T16:03:24.7007577Z tests/test_e2e_ocr_conversion.py: 40 warnings
2025-04-14T16:03:24.7007838Z tests/test_interfaces.py: 8 warnings
2025-04-14T16:03:24.7008106Z tests/test_legacy_format_transform.py: 6 warnings
2025-04-14T16:03:24.7008383Z tests/test_options.py: 10 warnings
2025-04-14T16:03:24.7009081Z   /home/runner/.cache/pypoetry/virtualenvs/docling-oEGFWdgn-py3.11/lib/python3.11/site-packages/docling_core/types/doc/document.py:4067: DeprecationWarning: deprecated
2025-04-14T16:03:24.7010145Z     if not d.validate_tree(d.body) or not d.validate_tree(d.furniture):
2025-04-14T16:03:24.7010395Z 
2025-04-14T16:03:24.7010541Z tests/test_backend_asciidoc.py::test_asciidocs_examples
2025-04-14T16:03:24.7011863Z   /home/runner/.cache/pypoetry/virtualenvs/docling-oEGFWdgn-py3.11/lib/python3.11/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
2025-04-14T16:03:24.7013209Z     warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)
2025-04-14T16:03:24.7013432Z 
2025-04-14T16:03:24.7013574Z tests/test_backend_csv.py::test_e2e_valid_csv_conversions
2025-04-14T16:03:24.7014737Z   /home/runner/work/docling/docling/docling/backend/csv_backend.py:76: UserWarning: Inconsistent column lengths detected in CSV data. Expected 3 columns, but found rows with varying lengths. Ensure all rows have the same number of columns.
2025-04-14T16:03:24.7015613Z     warnings.warn(
2025-04-14T16:03:24.7015727Z 
2025-04-14T16:03:24.7015860Z tests/test_backend_csv.py::test_e2e_valid_csv_conversions
2025-04-14T16:03:24.7016772Z   /home/runner/work/docling/docling/docling/backend/csv_backend.py:76: UserWarning: Inconsistent column lengths detected in CSV data. Expected 4 columns, but found rows with varying lengths. Ensure all rows have the same number of columns.
2025-04-14T16:03:24.7017620Z     warnings.warn(
2025-04-14T16:03:24.7017732Z 
2025-04-14T16:03:24.7018004Z tests/test_cli.py::test_cli_convert
2025-04-14T16:03:24.7019069Z   /home/runner/work/docling/docling/docling/pipeline/standard_pdf_pipeline.py:226: DeprecationWarning: Field `generate_table_images` is deprecated. To obtain table images, set `PdfPipelineOptions.generate_page_images = True` before conversion and then use the `TableItem.get_image` function.
2025-04-14T16:03:24.7020160Z     and self.pipeline_options.generate_table_images
2025-04-14T16:03:24.7020357Z 
2025-04-14T16:03:24.7020450Z tests/test_code_formula.py: 1 warning
2025-04-14T16:03:24.7020702Z tests/test_e2e_conversion.py: 11 warnings
2025-04-14T16:03:24.7020971Z tests/test_e2e_ocr_conversion.py: 10 warnings
2025-04-14T16:03:24.7021229Z tests/test_interfaces.py: 2 warnings
2025-04-14T16:03:24.7021496Z tests/test_legacy_format_transform.py: 2 warnings
2025-04-14T16:03:24.7021770Z tests/test_options.py: 10 warnings
2025-04-14T16:03:24.7022826Z   /home/runner/work/docling/docling/docling/pipeline/standard_pdf_pipeline.py:215: DeprecationWarning: Field `generate_table_images` is deprecated. To obtain table images, set `PdfPipelineOptions.generate_page_images = True` before conversion and then use the `TableItem.get_image` function.
2025-04-14T16:03:24.7024020Z     or self.pipeline_options.generate_table_images
2025-04-14T16:03:24.7024224Z 
2025-04-14T16:03:24.7024319Z tests/test_e2e_conversion.py: 1 warning
2025-04-14T16:03:24.7024583Z tests/test_e2e_ocr_conversion.py: 10 warnings
2025-04-14T16:03:24.7024842Z tests/test_interfaces.py: 2 warnings
2025-04-14T16:03:24.7025105Z tests/test_legacy_format_transform.py: 1 warning
2025-04-14T16:03:24.7025382Z tests/test_options.py: 10 warnings
2025-04-14T16:03:24.7026430Z   /home/runner/work/docling/docling/docling/pipeline/standard_pdf_pipeline.py:61: DeprecationWarning: Field `generate_table_images` is deprecated. To obtain table images, set `PdfPipelineOptions.generate_page_images = True` before conversion and then use the `TableItem.get_image` function.
2025-04-14T16:03:24.7027507Z     or self.pipeline_options.generate_table_images
2025-04-14T16:03:24.7027698Z 
2025-04-14T16:03:24.7027793Z tests/test_e2e_conversion.py: 11 warnings
2025-04-14T16:03:24.7028059Z tests/test_e2e_ocr_conversion.py: 10 warnings
2025-04-14T16:03:24.7028316Z tests/test_interfaces.py: 2 warnings
2025-04-14T16:03:24.7028894Z   /home/runner/work/docling/docling/tests/verify_utils.py:311: DeprecationWarning: Use document instead.
2025-04-14T16:03:24.7029392Z     doc_pred: DsDocument = doc_result.legacy_document
2025-04-14T16:03:24.7029592Z 
2025-04-14T16:03:24.7029680Z tests/test_e2e_conversion.py: 11 warnings
2025-04-14T16:03:24.7029943Z tests/test_e2e_ocr_conversion.py: 10 warnings
2025-04-14T16:03:24.7030199Z tests/test_interfaces.py: 2 warnings
2025-04-14T16:03:24.7030682Z   /home/runner/work/docling/docling/tests/verify_utils.py:397: DeprecationWarning: Use export_to_doctags() instead.
2025-04-14T16:03:24.7031249Z     doc_pred_dt = doc_result.document.export_to_document_tokens()
2025-04-14T16:03:24.7031479Z 
2025-04-14T16:03:24.7031645Z tests/test_legacy_format_transform.py::test_compare_legacy_output
2025-04-14T16:03:24.7032037Z tests/test_legacy_format_transform.py::test_compare_legacy_output
2025-04-14T16:03:24.7032422Z tests/test_legacy_format_transform.py::test_compare_legacy_output
2025-04-14T16:03:24.7032803Z tests/test_legacy_format_transform.py::test_compare_legacy_output
2025-04-14T16:03:24.7033192Z tests/test_legacy_format_transform.py::test_compare_legacy_output
2025-04-14T16:03:24.7033577Z tests/test_legacy_format_transform.py::test_compare_legacy_output
2025-04-14T16:03:24.7034343Z   /home/runner/work/docling/docling/tests/test_legacy_format_transform.py:51: DeprecationWarning: Use document instead.
2025-04-14T16:03:24.7034915Z     conv_res.legacy_document.model_dump(
2025-04-14T16:03:24.7035089Z 
2025-04-14T16:03:24.7035284Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2025-04-14T16:03:24.7035709Z =========== 65 passed, 2 skipped, 363 warnings in 739.99s (0:12:19) ============
2025-04-14T16:03:25.8612463Z ##[group]Run for file in docs/examples/*.py; do
2025-04-14T16:03:25.8612840Z [36;1mfor file in docs/examples/*.py; do[0m
2025-04-14T16:03:25.8613112Z [36;1m  # Skip batch_convert.py[0m
2025-04-14T16:03:25.8614274Z [36;1m  if [[ "$(basename "$file")" =~ ^(batch_convert|minimal_vlm_pipeline|minimal|export_multimodal|custom_convert|develop_picture_enrichment|rapidocr_with_custom_models|offline_convert|pictures_description|pictures_description_api|vlm_pipeline_api_model).py ]]; then[0m
2025-04-14T16:03:25.8615255Z [36;1m      echo "Skipping $file"[0m
2025-04-14T16:03:25.8615490Z [36;1m      continue[0m
2025-04-14T16:03:25.8615680Z [36;1m  fi[0m
2025-04-14T16:03:25.8615843Z [36;1m[0m
2025-04-14T16:03:25.8616022Z [36;1m  echo "Running example $file"[0m
2025-04-14T16:03:25.8616286Z [36;1m  poetry run python "$file" || exit 1[0m
2025-04-14T16:03:25.8616538Z [36;1mdone[0m
2025-04-14T16:03:25.8673132Z shell: /usr/bin/bash -e {0}
2025-04-14T16:03:25.8673364Z env:
2025-04-14T16:03:25.8673569Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T16:03:25.8673796Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T16:03:25.8674240Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-14T16:03:25.8674608Z   pythonLocation: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T16:03:25.8675016Z   PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.11.11/x64/lib/pkgconfig
2025-04-14T16:03:25.8675412Z   Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T16:03:25.8675755Z   Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T16:03:25.8676102Z   Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T16:03:25.8676455Z   LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.11.11/x64/lib
2025-04-14T16:03:25.8676742Z ##[endgroup]
2025-04-14T16:03:25.8772809Z Skipping docs/examples/batch_convert.py
2025-04-14T16:03:25.8776168Z Skipping docs/examples/custom_convert.py
2025-04-14T16:03:25.8790061Z Running example docs/examples/develop_formula_understanding.py
2025-04-14T16:03:31.8849420Z INFO:docling.document_converter:Going to convert document batch...
2025-04-14T16:03:31.8850922Z INFO:docling.document_converter:Initializing pipeline for ExampleFormulaUnderstandingPipeline with options hash bf654671c275e83c17f423d7e07fb63c
2025-04-14T16:03:31.8953342Z INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'
2025-04-14T16:03:31.8955235Z INFO:docling.models.factories:Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']
2025-04-14T16:03:31.9301733Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-14T16:03:33.6105429Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-14T16:03:34.3584575Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-14T16:03:34.6969718Z INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'
2025-04-14T16:03:34.6970686Z INFO:docling.models.factories:Registered picture descriptions: ['vlm', 'api']
2025-04-14T16:03:34.6972156Z INFO:docling.pipeline.base_pipeline:Processing document 2203.01017v2.pdf
2025-04-14T16:07:20.0303684Z /usr/bin/xdg-open: 882: www-browser: not found
2025-04-14T16:07:20.0315625Z /usr/bin/xdg-open: 882: www-browser: not found
2025-04-14T16:07:20.0316681Z /usr/bin/xdg-open: 882: www-browser: not found
2025-04-14T16:07:20.0326661Z /usr/bin/xdg-open: 882: links2: not found
2025-04-14T16:07:20.0332589Z /usr/bin/xdg-open: 882: www-browser: not found
2025-04-14T16:07:20.0336336Z /usr/bin/xdg-open: 882: links2: not found
2025-04-14T16:07:20.0357249Z /usr/bin/xdg-open: 882: links2: not found
2025-04-14T16:07:20.0358108Z /usr/bin/xdg-open: 882: elinks: not found
2025-04-14T16:07:20.0358752Z /usr/bin/xdg-open: 882: links2: not found
2025-04-14T16:07:20.0359391Z /usr/bin/xdg-open: 882: elinks: not found
2025-04-14T16:07:20.0378896Z /usr/bin/xdg-open: 882: links: not found
2025-04-14T16:07:20.0379689Z /usr/bin/xdg-open: 882: elinks: not found
2025-04-14T16:07:20.0390898Z /usr/bin/xdg-open: 882: elinks: not found
2025-04-14T16:07:20.0395924Z /usr/bin/xdg-open: 882: links: not found
2025-04-14T16:07:20.0398648Z /usr/bin/xdg-open: 882: lynx: not found
2025-04-14T16:07:20.0407226Z /usr/bin/xdg-open: 882: links: not found
2025-04-14T16:07:20.0422878Z /usr/bin/xdg-open: 882: lynx: not found
2025-04-14T16:07:20.0442649Z /usr/bin/xdg-open: 882: links: not found
2025-04-14T16:07:20.0444308Z /usr/bin/xdg-open: 882: lynx: not found
2025-04-14T16:07:20.0444912Z /usr/bin/xdg-open: 882: w3m: not found
2025-04-14T16:07:20.0445830Z xdg-open: no method available for opening '/tmp/tmp8xkdgiq8.PNG'
2025-04-14T16:07:20.0446620Z /usr/bin/xdg-open: 882: w3m: not found
2025-04-14T16:07:20.0447274Z xdg-open: no method available for opening '/tmp/tmp1c_k9dva.PNG'
2025-04-14T16:07:20.0448044Z /usr/bin/xdg-open: 882: w3m: not found
2025-04-14T16:07:20.0448702Z xdg-open: no method available for opening '/tmp/tmpw5amifay.PNG'
2025-04-14T16:07:20.0453379Z /usr/bin/xdg-open: 882: lynx: not found
2025-04-14T16:07:20.0468125Z /usr/bin/xdg-open: 882: w3m: not found
2025-04-14T16:07:20.0469262Z xdg-open: no method available for opening '/tmp/tmpsshi12og.PNG'
2025-04-14T16:07:20.1149248Z INFO:docling.document_converter:Finished converting document 2203.01017v2.pdf in 228.29 sec.
2025-04-14T16:07:21.4124594Z Skipping docs/examples/develop_picture_enrichment.py
2025-04-14T16:07:21.4138778Z Running example docs/examples/export_figures.py
2025-04-14T16:07:26.4430870Z INFO:docling.document_converter:Going to convert document batch...
2025-04-14T16:07:26.4432376Z INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash da04cf770b09a45fa6de4d8ea900b7bf
2025-04-14T16:07:26.4529590Z INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'
2025-04-14T16:07:26.4530974Z INFO:docling.models.factories:Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']
2025-04-14T16:07:26.4858152Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-14T16:07:28.1687783Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-14T16:07:28.9057136Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-14T16:07:29.2469485Z INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'
2025-04-14T16:07:29.2470597Z INFO:docling.models.factories:Registered picture descriptions: ['vlm', 'api']
2025-04-14T16:07:29.2471980Z INFO:docling.pipeline.base_pipeline:Processing document 2206.01062.pdf
2025-04-14T16:08:58.8975427Z INFO:docling.document_converter:Finished converting document 2206.01062.pdf in 92.47 sec.
2025-04-14T16:09:01.2029942Z INFO:__main__:Document converted and figures exported in 94.78 seconds.
2025-04-14T16:09:02.0468629Z Skipping docs/examples/export_multimodal.py
2025-04-14T16:09:02.0481826Z Running example docs/examples/export_tables.py
2025-04-14T16:09:06.9537094Z INFO:docling.document_converter:Going to convert document batch...
2025-04-14T16:09:06.9539266Z INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash 70041f74270850b7bedf7c8f5c2dcede
2025-04-14T16:09:06.9637778Z INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'
2025-04-14T16:09:06.9639423Z INFO:docling.models.factories:Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']
2025-04-14T16:09:06.9979182Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-14T16:09:08.6890504Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-14T16:09:09.7146256Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-14T16:09:10.0415829Z INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'
2025-04-14T16:09:10.0416872Z INFO:docling.models.factories:Registered picture descriptions: ['vlm', 'api']
2025-04-14T16:09:10.0417880Z INFO:docling.pipeline.base_pipeline:Processing document 2206.01062.pdf
2025-04-14T16:10:36.7934811Z INFO:docling.document_converter:Finished converting document 2206.01062.pdf in 89.86 sec.
2025-04-14T16:10:36.7999361Z INFO:__main__:Saving CSV table to scratch/2206.01062-table-1.csv
2025-04-14T16:10:36.8034264Z INFO:__main__:Saving HTML table to scratch/2206.01062-table-1.html
2025-04-14T16:10:36.9255452Z INFO:__main__:Saving CSV table to scratch/2206.01062-table-2.csv
2025-04-14T16:10:36.9261420Z INFO:__main__:Saving HTML table to scratch/2206.01062-table-2.html
2025-04-14T16:10:36.9578237Z INFO:__main__:Saving CSV table to scratch/2206.01062-table-3.csv
2025-04-14T16:10:36.9578924Z ## Table 0
2025-04-14T16:10:36.9582328Z |    | class label    |   Count |   % of Total.Train |   % of Total.Test |   % of Total.Val | triple inter-annotator mAP @0.5-0.95 (%).All   | triple inter-annotator mAP @0.5-0.95 (%).Fin   | triple inter-annotator mAP @0.5-0.95 (%).Man   | triple inter-annotator mAP @0.5-0.95 (%).Sci   | triple inter-annotator mAP @0.5-0.95 (%).Law   | triple inter-annotator mAP @0.5-0.95 (%).Pat   | triple inter-annotator mAP @0.5-0.95 (%).Ten   |
2025-04-14T16:10:36.9586016Z |---:|:---------------|--------:|-------------------:|------------------:|-----------------:|:-----------------------------------------------|:-----------------------------------------------|:-----------------------------------------------|:-----------------------------------------------|:-----------------------------------------------|:-----------------------------------------------|:-----------------------------------------------|
2025-04-14T16:10:36.9589162Z |  0 | Caption        |   22524 |               2.04 |              1.77 |             2.32 | 84-89                                          | 40-61                                          | 86-92                                          | 94-99                                          | 95-99                                          | 69-78                                          | n/a                                            |
2025-04-14T16:10:36.9590584Z |  1 | Footnote       |    6318 |               0.6  |              0.31 |             0.58 | 83-91                                          | n/a                                            | 100                                            | 62-88                                          | 85-94                                          | n/a                                            | 82-97                                          |
2025-04-14T16:10:36.9592621Z |  2 | Formula        |   25027 |               2.25 |              1.9  |             2.96 | 83-85                                          | n/a                                            | n/a                                            | 84-87                                          | 86-96                                          | n/a                                            | n/a                                            |
2025-04-14T16:10:36.9594253Z |  3 | List-item      |  185660 |              17.19 |             13.34 |            15.82 | 87-88                                          | 74-83                                          | 90-92                                          | 97-97                                          | 81-85                                          | 75-88                                          | 93-95                                          |
2025-04-14T16:10:36.9595723Z |  4 | Page-footer    |   70878 |               6.51 |              5.58 |             6    | 93-94                                          | 88-90                                          | 95-96                                          | 100                                            | 92-97                                          | 100                                            | 96-98                                          |
2025-04-14T16:10:36.9597321Z |  5 | Page-header    |   58022 |               5.1  |              6.7  |             5.06 | 85-89                                          | 66-76                                          | 90-94                                          | 98-100                                         | 91-92                                          | 97-99                                          | 81-86                                          |
2025-04-14T16:10:36.9598567Z |  6 | Picture        |   45976 |               4.21 |              2.78 |             5.31 | 69-71                                          | 56-59                                          | 82-86                                          | 69-82                                          | 80-95                                          | 66-71                                          | 59-76                                          |
2025-04-14T16:10:36.9599588Z |  7 | Section-header |  142884 |              12.6  |             15.77 |            12.85 | 83-84                                          | 76-81                                          | 90-92                                          | 94-95                                          | 87-94                                          | 69-73                                          | 78-86                                          |
2025-04-14T16:10:36.9600367Z |  8 | Table          |   34733 |               3.2  |              2.27 |             3.6  | 77-81                                          | 75-80                                          | 83-86                                          | 98-99                                          | 58-80                                          | 79-84                                          | 70-85                                          |
2025-04-14T16:10:36.9601147Z |  9 | Text           |  510377 |              45.82 |             49.28 |            45    | 84-86                                          | 81-86                                          | 88-93                                          | 89-93                                          | 87-92                                          | 71-79                                          | 87-95                                          |
2025-04-14T16:10:36.9601916Z | 10 | Title          |    5071 |               0.47 |              0.3  |             0.5  | 60-72                                          | 24-63                                          | 50-63                                          | 94-100                                         | 82-96                                          | 68-79                                          | 24-56                                          |
2025-04-14T16:10:36.9602840Z | 11 | Total          | 1107470 |          941123    |          99816    |         66531    | 82-83                                          | 71-74                                          | 79-81                                          | 89-94                                          | 86-91                                          | 71-76                                          | 68-85                                          |
2025-04-14T16:10:36.9603500Z ## Table 1
2025-04-14T16:10:36.9603832Z |    |                | human.   |   MRCNN.R50 |   MRCNN.R101 |   FRCNN.R101 |   YOLO.v5x6 |
2025-04-14T16:10:36.9604550Z |---:|:---------------|:---------|------------:|-------------:|-------------:|------------:|
2025-04-14T16:10:36.9605101Z |  0 | Caption        | 84-89    |        68.4 |         71.5 |         70.1 |        77.7 |
2025-04-14T16:10:36.9605541Z |  1 | Footnote       | 83-91    |        70.9 |         71.8 |         73.7 |        77.2 |
2025-04-14T16:10:36.9606036Z |  2 | Formula        | 83-85    |        60.1 |         63.4 |         63.5 |        66.2 |
2025-04-14T16:10:36.9606482Z |  3 | List-item      | 87-88    |        81.2 |         80.8 |         81   |        86.2 |
2025-04-14T16:10:36.9606929Z |  4 | Page-footer    | 93-94    |        61.6 |         59.3 |         58.9 |        61.1 |
2025-04-14T16:10:36.9607306Z |  5 | Page-header    | 85-89    |        71.9 |         70   |         72   |        67.9 |
2025-04-14T16:10:36.9607652Z |  6 | Picture        | 69-71    |        71.7 |         72.7 |         72   |        77.1 |
2025-04-14T16:10:36.9608157Z |  7 | Section-header | 83-84    |        67.6 |         69.3 |         68.4 |        74.6 |
2025-04-14T16:10:36.9608516Z |  8 | Table          | 77-81    |        82.2 |         82.9 |         82.2 |        86.3 |
2025-04-14T16:10:36.9608833Z |  9 | Text           | 84-86    |        84.6 |         85.8 |         85.4 |        88.1 |
2025-04-14T16:10:36.9609149Z | 10 | Title          | 60-72    |        76.7 |         80.4 |         79.9 |        82.7 |
2025-04-14T16:10:36.9609466Z | 11 | All            | 82-83    |        72.4 |         73.5 |         73.4 |        76.8 |
2025-04-14T16:10:36.9609732Z ## Table 2
2025-04-14T16:10:36.9609945Z |    | Class-count    |   11 | 6       | 5       | 4       |
2025-04-14T16:10:36.9610244Z |---:|:---------------|-----:|:--------|:--------|:--------|
2025-04-14T16:10:36.9610537Z |  0 | Caption        |   68 | Text    | Text    | Text    |
2025-04-14T16:10:36.9610832Z |  1 | Footnote       |   71 | Text    | Text    | Text    |
2025-04-14T16:10:36.9611125Z |  2 | Formula        |   60 | Text    | Text    | Text    |
2025-04-14T16:10:36.9611417Z |  3 | List-item      |   81 | Text    | 82      | Text    |
2025-04-14T16:10:36.9611711Z |  4 | Page-footer    |   62 | 62      | -       | -       |
2025-04-14T16:10:36.9612007Z |  5 | Page-header    |   72 | 68      | -       | -       |
2025-04-14T16:10:36.9612299Z |  6 | Picture        |   72 | 72      | 72      | 72      |
2025-04-14T16:10:36.9612600Z |  7 | Section-header |   68 | 67      | 69      | 68      |
2025-04-14T16:10:36.9612883Z |  8 | Table          |   82 | 83      | 82      | 82      |
2025-04-14T16:10:36.9613144Z |  9 | Text           |   85 | 84      | 84      | 84      |
2025-04-14T16:10:36.9613431Z | 10 | Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |
2025-04-14T16:10:36.9613792Z INFO:__main__:Saving HTML table to scratch/2206.01062-table-3.html
2025-04-14T16:10:36.9813088Z INFO:__main__:Saving CSV table to scratch/2206.01062-table-4.csv
2025-04-14T16:10:36.9819067Z INFO:__main__:Saving HTML table to scratch/2206.01062-table-4.html
2025-04-14T16:10:37.0058796Z INFO:__main__:Saving CSV table to scratch/2206.01062-table-5.csv
2025-04-14T16:10:37.0064451Z INFO:__main__:Saving HTML table to scratch/2206.01062-table-5.html
2025-04-14T16:10:37.0307232Z INFO:__main__:Document converted and tables exported in 90.09 seconds.
2025-04-14T16:10:37.0397779Z | 11 | Overall        |   72 | 73      | 78      | 77      |
2025-04-14T16:10:37.0398419Z ## Table 3
2025-04-14T16:10:37.0398923Z |    | Class-count.Split   |   11.Doc |   11.Page | 5.Doc   | 5.Page   |
2025-04-14T16:10:37.0399441Z |---:|:--------------------|---------:|----------:|:--------|:---------|
2025-04-14T16:10:37.0399891Z |  0 | Caption             |       68 |        83 |         |          |
2025-04-14T16:10:37.0400333Z |  1 | Footnote            |       71 |        84 |         |          |
2025-04-14T16:10:37.0400761Z |  2 | Formula             |       60 |        66 |         |          |
2025-04-14T16:10:37.0401214Z |  3 | List-item           |       81 |        88 | 82      | 88       |
2025-04-14T16:10:37.0401708Z |  4 | Page-footer         |       62 |        89 |         |          |
2025-04-14T16:10:37.0402183Z |  5 | Page-header         |       72 |        90 |         |          |
2025-04-14T16:10:37.0402629Z |  6 | Picture             |       72 |        82 | 72      | 82       |
2025-04-14T16:10:37.0403093Z |  7 | Section-header      |       68 |        83 | 69      | 83       |
2025-04-14T16:10:37.0403539Z |  8 | Table               |       82 |        89 | 82      | 90       |
2025-04-14T16:10:37.0404196Z |  9 | Text                |       85 |        91 | 84      | 90       |
2025-04-14T16:10:37.0404527Z | 10 | Title               |       77 |        81 |         |          |
2025-04-14T16:10:37.0404844Z | 11 | All                 |       72 |        84 | 78      | 87       |
2025-04-14T16:10:37.0405172Z ## Table 4
2025-04-14T16:10:37.0405485Z |    | Training on     | labels     |   Testing on.PLN | Testing on.DB   |   Testing on.DLN |
2025-04-14T16:10:37.0406225Z |---:|:----------------|:-----------|-----------------:|:----------------|-----------------:|
2025-04-14T16:10:37.0406649Z |  0 | PubLayNet (PLN) | Figure     |               96 | 43              |               23 |
2025-04-14T16:10:37.0407085Z |  1 | PubLayNet (PLN) | Sec-header |               87 | -               |               32 |
2025-04-14T16:10:37.0407489Z |  2 |                 | Table      |               95 | 24              |               49 |
2025-04-14T16:10:37.0407811Z |  3 |                 | Text       |               96 | -               |               42 |
2025-04-14T16:10:37.0408130Z |  4 |                 | total      |               93 | 34              |               30 |
2025-04-14T16:10:37.0408482Z |  5 | DocBank (DB)    | Figure     |               77 | 71              |               31 |
2025-04-14T16:10:37.0408869Z |  6 | DocBank (DB)    | Table      |               19 | 65              |               22 |
2025-04-14T16:10:37.0409244Z |  7 | DocBank (DB)    | total      |               48 | 68              |               27 |
2025-04-14T16:10:37.0409639Z |  8 | DocLayNet (DLN) | Figure     |               67 | 51              |               72 |
2025-04-14T16:10:37.0410063Z |  9 | DocLayNet (DLN) | Sec-header |               53 | -               |               68 |
2025-04-14T16:10:37.0410451Z | 10 |                 | Table      |               87 | 43              |               82 |
2025-04-14T16:10:37.0410777Z | 11 |                 | Text       |               77 | -               |               84 |
2025-04-14T16:10:37.0411101Z | 12 |                 | total      |               59 | 47              |               78 |
2025-04-14T16:10:38.0688631Z Running example docs/examples/full_page_ocr.py
2025-04-14T16:12:19.2190307Z ## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis
2025-04-14T16:12:19.2191886Z 
2025-04-14T16:12:19.2192696Z Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com
2025-04-14T16:12:19.2193479Z 
2025-04-14T16:12:19.2193829Z Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com
2025-04-14T16:12:19.2194762Z 
2025-04-14T16:12:19.2195155Z Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com
2025-04-14T16:12:19.2195665Z 
2025-04-14T16:12:19.2195994Z Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com
2025-04-14T16:12:19.2198327Z 
2025-04-14T16:12:19.2198609Z Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com
2025-04-14T16:12:19.2199020Z 
2025-04-14T16:12:19.2199131Z ## ABSTRACT
2025-04-14T16:12:19.2199291Z 
2025-04-14T16:12:19.2206869Z Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet, a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of doubleand triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.
2025-04-14T16:12:19.2212022Z 
2025-04-14T16:12:19.2212100Z ## CCS CONCEPTS
2025-04-14T16:12:19.2212219Z 
2025-04-14T16:12:19.2213401Z -Information systems — Document structure; + Applied computing — Document analysis; -Computing methodologies — Machine learning; Computer vision; Object detection;
2025-04-14T16:12:19.2214651Z 
2025-04-14T16:12:19.2216548Z Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).
2025-04-14T16:12:19.2217950Z 
2025-04-14T16:12:19.2218068Z KDD
2025-04-14T16:12:19.2218215Z 
2025-04-14T16:12:19.2218418Z ‘22,
2025-04-14T16:12:19.2218565Z 
2025-04-14T16:12:19.2218670Z August
2025-04-14T16:12:19.2218808Z 
2025-04-14T16:12:19.2218869Z 14-18,
2025-04-14T16:12:19.2218964Z 
2025-04-14T16:12:19.2219023Z 2022,
2025-04-14T16:12:19.2219106Z 
2025-04-14T16:12:19.2219179Z Washington,
2025-04-14T16:12:19.2219281Z 
2025-04-14T16:12:19.2219344Z DC,
2025-04-14T16:12:19.2219426Z 
2025-04-14T16:12:19.2219487Z USA
2025-04-14T16:12:19.2219565Z 
2025-04-14T16:12:19.2219646Z ©
2025-04-14T16:12:19.2219731Z 
2025-04-14T16:12:19.2219791Z 2022
2025-04-14T16:12:19.2219876Z 
2025-04-14T16:12:19.2219943Z Copyright held
2025-04-14T16:12:19.2220056Z 
2025-04-14T16:12:19.2220115Z by the
2025-04-14T16:12:19.2220199Z 
2025-04-14T16:12:19.2220269Z owner/author(s).
2025-04-14T16:12:19.2220376Z 
2025-04-14T16:12:19.2220439Z ACM
2025-04-14T16:12:19.2220519Z 
2025-04-14T16:12:19.2220582Z ISBN
2025-04-14T16:12:19.2220663Z 
2025-04-14T16:12:19.2220737Z 978-1-4503-9385-0/22/08.
2025-04-14T16:12:19.2220872Z 
2025-04-14T16:12:19.2220978Z https://doi.org/10.1145/3534678.3539043
2025-04-14T16:12:19.2221156Z 
2025-04-14T16:12:19.2221367Z Figure 1: Four examples of complex page layouts across different document categories
2025-04-14T16:12:19.2221668Z 
2025-04-14T16:12:19.2221740Z <!-- image -->
2025-04-14T16:12:19.2221845Z 
2025-04-14T16:12:19.2222104Z ## KEYWORDS
2025-04-14T16:12:19.2222195Z 
2025-04-14T16:12:19.2222444Z PDF document conversion, layout segmentation, object-detection, data set, Machine Learning
2025-04-14T16:12:19.2222767Z 
2025-04-14T16:12:19.2222849Z ## ACM Reference Format:
2025-04-14T16:12:19.2222976Z 
2025-04-14T16:12:19.2224281Z Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043
2025-04-14T16:12:19.2225491Z 
2025-04-14T16:12:19.2225570Z ## 1 INTRODUCTION
2025-04-14T16:12:19.2225682Z 
2025-04-14T16:12:19.2227747Z Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.
2025-04-14T16:12:19.2229871Z 
2025-04-14T16:12:19.2234859Z A key problem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet [6] and DocBank [7]. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories (PubMed and arXiv), which provide XML or KIEX sources. Those scientific documents present a limited variability in their layouts, because they are typeset in uniform templates provided by the publishers. Obviously, documents such as technical manuals, annual company reports, legal text, government tenders, etc. have very different and partially unique layouts. As a consequence, the layout predictions obtained from models trained on PubLayNet or DocBank is very reasonable when applied on scientific documents. However, for more artistic or free-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.
2025-04-14T16:12:19.2239723Z 
2025-04-14T16:12:19.2241072Z In this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry doubleor triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public! in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:
2025-04-14T16:12:19.2242484Z 
2025-04-14T16:12:19.2242880Z - (1) Human Annotation: In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.
2025-04-14T16:12:19.2243647Z - (2) Large Layout Variability: We include diverse and complex layouts from a large variety of public sources.
2025-04-14T16:12:19.2244761Z - (3) Detailed Label Set: We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.
2025-04-14T16:12:19.2245810Z - (4) Redundant Annotations: A fraction of the pages in the DocLayNet data set carry more than one human annotation.
2025-04-14T16:12:19.2246192Z 
2025-04-14T16:12:19.2246338Z Thttps://developeribm.com/exchanges/data/all/doclaynet
2025-04-14T16:12:19.2246558Z 
2025-04-14T16:12:19.2246787Z This enables experimentation with annotation uncertainty and quality control analysis.
2025-04-14T16:12:19.2247105Z 
2025-04-14T16:12:19.2247922Z - (5) Pre-defined Train-, Test-&amp; Validation-set: Like DocBank, we provide fixed train-, test-&amp; validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.
2025-04-14T16:12:19.2248799Z 
2025-04-14T16:12:19.2249626Z All aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.
2025-04-14T16:12:19.2250529Z 
2025-04-14T16:12:19.2252075Z In Section 5, we will present baseline accuracy numbers for a variety of object detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.
2025-04-14T16:12:19.2253583Z 
2025-04-14T16:12:19.2253652Z ## 2 RELATED WORK
2025-04-14T16:12:19.2253772Z 
2025-04-14T16:12:19.2255884Z While early approaches in document-layout analysis used rulebased algorithms and heuristics [8], the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection models [9-15]. In the last decade, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardisation effort of the ground-truth data format [16] and common deep-learning frameworks [17]. Reference data sets such as PubLayNet [6] and DocBank provide their data in the commonly accepted COCO format [16].
2025-04-14T16:12:19.2257843Z 
2025-04-14T16:12:19.2259265Z Lately, new types of ML models for document-layout analysis have emerged in the community [18-21]. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.
2025-04-14T16:12:19.2260750Z 
2025-04-14T16:12:19.2260833Z ## 3 THE DOCLAYNET DATASET
2025-04-14T16:12:19.2260967Z 
2025-04-14T16:12:19.2262474Z DocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular boundingboxes. We define 11 distinct labels for layout features, namely Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, and Title. Our reasoning for picking this particular label set is detailed in Section 4.
2025-04-14T16:12:19.2264222Z 
2025-04-14T16:12:19.2264743Z In addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents
2025-04-14T16:12:19.2265469Z 
2025-04-14T16:12:19.2265644Z Figure 2: Distribution of DocLayNet pages across document categories.
2025-04-14T16:12:19.2265896Z 
2025-04-14T16:12:19.2265969Z <!-- image -->
2025-04-14T16:12:19.2266071Z 
2025-04-14T16:12:19.2267536Z to a minimum, since they introduce difficulties in annotation (see Section 4). As a second condition, we focussed on medium to large documents (&gt; 10 pages) with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing "text in the wild".
2025-04-14T16:12:19.2269059Z 
2025-04-14T16:12:19.2271456Z The pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports, Manuals, Scientific Articles, Laws &amp; Regulations, Patents and Government Tenders. Each document category was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports? which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories (Financial Reports and Manuals) contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.
2025-04-14T16:12:19.2274148Z 
2025-04-14T16:12:19.2275817Z We did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet (close to 95%) are published in English language. However, DocLayNet also contains a number of documents in other languages such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.
2025-04-14T16:12:19.2277391Z 
2025-04-14T16:12:19.2278550Z To ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, testand validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, testand validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.
2025-04-14T16:12:19.2279788Z 
2025-04-14T16:12:19.2279914Z 2e.g. AAPL from https://www.annualreports.com/
2025-04-14T16:12:19.2280115Z 
2025-04-14T16:12:19.2281353Z Table 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.
2025-04-14T16:12:19.2282667Z 
2025-04-14T16:12:19.2284599Z In order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format [16]. This entails the standard COCO ground-truth file (in JSON format) with the associated page images (in PNG format, 1025x1025 pixels). Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates (in JSON). All additional files are linked to the primary page images by their matching filenames.
2025-04-14T16:12:19.2286473Z 
2025-04-14T16:12:19.2292072Z Despite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, "invisible" tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as "invisible" list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a "natural" upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.
2025-04-14T16:12:19.2297950Z 
2025-04-14T16:12:19.2298036Z ## 4 ANNOTATION CAMPAIGN
2025-04-14T16:12:19.2298166Z 
2025-04-14T16:12:19.2299455Z The annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,
2025-04-14T16:12:19.2300820Z 
2025-04-14T16:12:19.2301777Z Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row "Total") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.
2025-04-14T16:12:19.2302806Z 
2025-04-14T16:12:19.2303587Z |                |         | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |
2025-04-14T16:12:19.2305067Z |----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|
2025-04-14T16:12:19.2305985Z | class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |
2025-04-14T16:12:19.2306791Z | Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |
2025-04-14T16:12:19.2307432Z | Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |
2025-04-14T16:12:19.2308076Z | Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |
2025-04-14T16:12:19.2308719Z | List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |
2025-04-14T16:12:19.2309477Z | Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |
2025-04-14T16:12:19.2310148Z | Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |
2025-04-14T16:12:19.2310800Z | Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |
2025-04-14T16:12:19.2311467Z | Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |
2025-04-14T16:12:19.2312120Z | Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |
2025-04-14T16:12:19.2312736Z | Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |
2025-04-14T16:12:19.2313455Z | Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |
2025-04-14T16:12:19.2314247Z | Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |
2025-04-14T16:12:19.2314640Z 
2025-04-14T16:12:19.2315424Z Figure 3: Corpus Conversion Service annotation user interface. The PDF page is shown in the background, with overlaid text-cells (in darker shades). The annotation boxes can be drawn by dragging a rectangle over each segment with the respective label from the palette on the right.
2025-04-14T16:12:19.2316292Z 
2025-04-14T16:12:19.2316359Z <!-- image -->
2025-04-14T16:12:19.2316463Z 
2025-04-14T16:12:19.2317105Z we distributed the annotation workload and performed continuous quality controls. Phase one and two required a small team of experts only. For phases three and four, a group of 40 dedicated annotators were assembled and supervised.
2025-04-14T16:12:19.2317908Z 
2025-04-14T16:12:19.2319679Z Phase 1: Data selection and preparation. Our inclusion criteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources include publication repositories such as arXiv", government offices, company websites as well as data directory services for financial reports and patents. Scanned documents were excluded wherever possible because they can be rotated or skewed. This would not allow us to perform annotation with rectangular bounding-boxes and therefore complicate the annotation process.
2025-04-14T16:12:19.2321392Z 
2025-04-14T16:12:19.2323573Z Preparation work included uploading and parsing the sourced PDF documents in the Corpus Conversion Service (CCS) [22], a cloud-native platform which provides a visual annotation interface and allows for dataset inspection and analysis. The annotation interface of CCS is shown in Figure 3. The desired balance of pages between the different document categories was achieved by selective subsampling of pages with certain desired properties. For example, we made sure to include the title page of each document and bias the remaining page selection to those with figures or tables. The latter was achieved by leveraging pre-trained object detection models from PubLayNet, which helped us estimate how many figures and tables a given page contains.
2025-04-14T16:12:19.2325923Z 
2025-04-14T16:12:19.2329189Z Phase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recurrent layout elements and lead us to the definition of 11 distinct class labels. These 11 class labels are Caption, Footnote, Formula, List-item, Pagefooter, Page-header, Picture, Section-header, Table, Text, and Title. Critical factors that were considered for the choice of these class labels were (1) the overall occurrence of the label, (2) the specificity of the label, (3) recognisability on a single page (i.e. no need for context from previous or next page) and (4) overall coverage of the page. Specificity ensures that the choice of label is not ambiguous, while coverage ensures that all meaningful items on a page can be annotated. We refrained from class labels that are very specific to a document category, such as Abstract in the Scientific Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation, as seen in DocBank, are often only distinguishable by discriminating on
2025-04-14T16:12:19.2332760Z 
2025-04-14T16:12:19.2332840Z 3https://arxiv.org/
2025-04-14T16:12:19.2333002Z 
2025-04-14T16:12:19.2333368Z the textual content of an element, which goes beyond visual layout recognition, in particular outside the Scientific Articles category.
2025-04-14T16:12:19.2333815Z 
2025-04-14T16:12:19.2336280Z At first sight, the task of visual document-layout interpretation appears intuitive enough to obtain plausible annotations in most cases. However, during early trial-runs in the core team, we observed many cases in which annotators use different annotation styles, especially for documents with challenging layouts. For example, if a figure is presented with subfigures, one annotator might draw a single figure bounding-box, while another might annotate each subfigure separately. The same applies for lists, where one might annotate all list items in one block or each list item separately. In essence, we observed that challenging layouts would be annotated in different but plausible ways. To illustrate this, we show in Figure 4 multiple examples of plausible but inconsistent annotations on the same pages.
2025-04-14T16:12:19.2338708Z 
2025-04-14T16:12:19.2340256Z Obviously, this inconsistency in annotations is not desirable for datasets which are intended to be used for model training. To minimise these inconsistencies, we created a detailed annotation guideline. While perfect consistency across 40 annotation staff members is clearly not possible to achieve, we saw a huge improvement in annotation consistency after the introduction of our annotation guideline. A few selected, non-trivial highlights of the guideline are:
2025-04-14T16:12:19.2341683Z 
2025-04-14T16:12:19.2342239Z - (1) Every list-item is an individual object instance with class label List-item. This definition is different from PubLayNet and DocBank, where all list-items are grouped together into one List object.
2025-04-14T16:12:19.2343455Z - (2) A List-item is a paragraph with hanging indentation. Singleline elements can qualify as List-item if the neighbour elements expose hanging indentation. Bullet or enumeration symbols are not a requirement.
2025-04-14T16:12:19.2344437Z - (3) For every Caption, there must be exactly one corresponding Picture or Table.
2025-04-14T16:12:19.2344879Z - (4) Connected sub-pictures are grouped together in one Picture object.
2025-04-14T16:12:19.2345256Z - (5) Formula numbers are included in a Formula object.
2025-04-14T16:12:19.2345873Z - (6) Emphasised text (e.g. in italic or bold) at the beginning of a paragraph is not considered a Section-header, unless it appears exclusively on its own line.
2025-04-14T16:12:19.2346367Z 
2025-04-14T16:12:19.2346974Z The complete annotation guideline is over 100 pages long and a detailed description is obviously out of scope for this paper. Nevertheless, it will be made publicly available alongside with DocLayNet for future reference.
2025-04-14T16:12:19.2347670Z 
2025-04-14T16:12:19.2349960Z Phase 3: Training. After a first trial with a small group of people, we realised that providing the annotation guideline and a set of random practice pages did not yield the desired quality level for layout annotation. Therefore we prepared a subset of pages with two different complexity levels, each with a practice and an exam part. 974 pages were reference-annotated by one proficient core team member. Annotation staff were then given the task to annotate the same subsets (blinded from the reference). By comparing the annotations of each staff member with the reference annotations, we could quantify how closely their annotations matched the reference. Only after passing two exam levels with high annotation quality, staff were admitted into the production phase. Practice iterations
2025-04-14T16:12:19.2352315Z 
2025-04-14T16:12:19.2352798Z Figure 4: Examples of plausible annotation alternatives for the same page. Criteria in our annotation guideline can resolve cases A to C, while the case D remains ambiguous.
2025-04-14T16:12:19.2353479Z 
2025-04-14T16:12:19.2353546Z <!-- image -->
2025-04-14T16:12:19.2353653Z 
2025-04-14T16:12:19.2354057Z were carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.
2025-04-14T16:12:19.2354447Z 
2025-04-14T16:12:19.2357338Z Phase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement (see Table 1). We wanted
2025-04-14T16:12:19.2360266Z 
2025-04-14T16:12:19.2361727Z Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.
2025-04-14T16:12:19.2363151Z 
2025-04-14T16:12:19.2363263Z |                | human   | MRCNN   | MRCNN   | FRCNN   | YOLO   |
2025-04-14T16:12:19.2363576Z |----------------|---------|---------|---------|---------|--------|
2025-04-14T16:12:19.2363954Z |                |         | R50     | R101    | R101    | v5x6   |
2025-04-14T16:12:19.2364235Z | Caption        | 84-89   | 68.4    | 71.5    | 70.1    | 77.7   |
2025-04-14T16:12:19.2364537Z | Footnote       | 83-91   | 70.9    | 71.8    | 73.7    | 77.2   |
2025-04-14T16:12:19.2364833Z | Formula        | 83-85   | 60.1    | 63.4    | 63.5    | 66.2   |
2025-04-14T16:12:19.2365133Z | List-item      | 87-88   | 81.2    | 80.8    | 81.0    | 86.2   |
2025-04-14T16:12:19.2365455Z | Page-footer    | 93-94   | 61.6    | 59.3    | 58.9    | 61.1   |
2025-04-14T16:12:19.2365783Z | Page-header    | 85-89   | 71.9    | 70.0    | 72.0    | 67.9   |
2025-04-14T16:12:19.2366093Z | Picture        | 69-71   | 71.7    | 72.7    | 72.0    | 77.1   |
2025-04-14T16:12:19.2366420Z | Section-header | 83-84   | 67.6    | 69.3    | 68.4    | 74.6   |
2025-04-14T16:12:19.2366735Z | Table          | 77-81   | 82.2    | 82.9    | 82.2    | 86.3   |
2025-04-14T16:12:19.2367017Z | Text           | 84-86   | 84.6    | 85.8    | 85.4    | 88.1   |
2025-04-14T16:12:19.2367317Z | Title          | 60-72   | 76.7    | 80.4    | 79.9    | 82.7   |
2025-04-14T16:12:19.2367598Z | All            | 82-83   | 72.4    | 73.5    | 73.4    | 76.8   |
2025-04-14T16:12:19.2367782Z 
2025-04-14T16:12:19.2371445Z to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture. For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.
2025-04-14T16:12:19.2375530Z 
2025-04-14T16:12:19.2375606Z ## 5 EXPERIMENTS
2025-04-14T16:12:19.2375713Z 
2025-04-14T16:12:19.2377656Z The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this
2025-04-14T16:12:19.2379566Z 
2025-04-14T16:12:19.2380647Z Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNN network with ResNet50 backbone trained on increasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.
2025-04-14T16:12:19.2381641Z 
2025-04-14T16:12:19.2381713Z <!-- image -->
2025-04-14T16:12:19.2381814Z 
2025-04-14T16:12:19.2382082Z paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.
2025-04-14T16:12:19.2382429Z 
2025-04-14T16:12:19.2383497Z In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].
2025-04-14T16:12:19.2384760Z 
2025-04-14T16:12:19.2384852Z ## Baselines for Object Detection
2025-04-14T16:12:19.2385008Z 
2025-04-14T16:12:19.2388243Z In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 x 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text, Table and Picture. This is not entirely surprising, as Text, Table and Picture are abundant and the most visually distinctive in a document.
2025-04-14T16:12:19.2391671Z 
2025-04-14T16:12:19.2392210Z Table 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obtained by either down-mapping or dropping labels.
2025-04-14T16:12:19.2392873Z 
2025-04-14T16:12:19.2392988Z | Class-count    |   11 | 6       | 5       | 4       |
2025-04-14T16:12:19.2393404Z |----------------|------|---------|---------|---------|
2025-04-14T16:12:19.2393676Z | Caption        |   68 | Text    | Text    | Text    |
2025-04-14T16:12:19.2394090Z | Footnote       |   71 | Text    | Text    | Text    |
2025-04-14T16:12:19.2394370Z | Formula        |   60 | Text    | Text    | Text    |
2025-04-14T16:12:19.2394666Z | List-item      |   81 | Text    | 82      | Text    |
2025-04-14T16:12:19.2394964Z | Page-footer    |   62 | 62      | -       | -       |
2025-04-14T16:12:19.2395254Z | Page-header    |   72 | 68      | -       | -       |
2025-04-14T16:12:19.2395534Z | Picture        |   72 | 72      | 72      | 72      |
2025-04-14T16:12:19.2395809Z | Section-header |   68 | 67      | 69      | 68      |
2025-04-14T16:12:19.2396084Z | Table          |   82 | 83      | 82      | 82      |
2025-04-14T16:12:19.2396653Z | Text           |   85 | 84      | 84      | 84      |
2025-04-14T16:12:19.2397068Z | Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |
2025-04-14T16:12:19.2397531Z | Overall        |   72 | 73      | 78      | 77      |
2025-04-14T16:12:19.2397815Z 
2025-04-14T16:12:19.2397927Z ## Learning Curve
2025-04-14T16:12:19.2398099Z 
2025-04-14T16:12:19.2402313Z One of the fundamental questions related to any dataset is if it is "large enough". To answer this question for DocLayNet, we performed a data ablation study in which we evaluated a Mask R-CNN model trained on increasing fractions of the DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in the beginning and eventually levels out. To estimate the error-bar on the metrics, we ran the training five times on the entire data-set. This resulted in a 1% error-bar, depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the exact same data-points, but with a logarithmic scale on the x-axis. As is expected, the mAP score increases linearly as a function of the data-size in the inset. The curve ultimately flattens out between the 80% and 100% mark, with the 80% mark falling within the error-bars of the 100% mark. This provides a good indication that the model would not improve significantly by yet increasing the data size. Rather, it would probably benefit more from improved data consistency (as discussed in Section 3), data augmentation methods [23], or the addition of more document categories and styles.
2025-04-14T16:12:19.2405916Z 
2025-04-14T16:12:19.2406006Z ## Impact of Class Labels
2025-04-14T16:12:19.2406148Z 
2025-04-14T16:12:19.2409874Z The choice and number of labels can have a significant effect on the overall model performance. Since PubLayNet, DocBank and DocLayNet all have different label sets, it is of particular interest to understand and quantify this influence of the label set on the model performance. We investigate this by either down-mapping labels into more common ones (e.g. Caption— Text) or excluding them from the annotations entirely. Furthermore, it must be stressed that all mappings and exclusions were performed on the data before model training. In Table 3, we present the mAP scores for a Mask R-CNN R50 network on different label sets. Where a label is down-mapped, we show its corresponding label, otherwise it was excluded. We present three different label sets, with 6, 5 and 4 different labels respectively. The set of 5 labels contains the same labels as PubLayNet. However, due to the different definition of
2025-04-14T16:12:19.2412544Z 
2025-04-14T16:12:19.2413033Z Table 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise split for different label sets. Naive page-wise split will result in -10% point improvement.
2025-04-14T16:12:19.2413582Z 
2025-04-14T16:12:19.2413688Z | Class-count    | 11   | 11   | 5   | 5    |
2025-04-14T16:12:19.2414065Z |----------------|------|------|-----|------|
2025-04-14T16:12:19.2414343Z | Split          | Doc  | Page | Doc | Page |
2025-04-14T16:12:19.2414602Z | Caption        | 68   | 83   |     |      |
2025-04-14T16:12:19.2415001Z | Footnote       | 71   | 84   |     |      |
2025-04-14T16:12:19.2415241Z | Formula        | 60   | 66   |     |      |
2025-04-14T16:12:19.2415482Z | List-item      | 81   | 88   | 82  | 88   |
2025-04-14T16:12:19.2415726Z | Page-footer    | 62   | 89   |     |      |
2025-04-14T16:12:19.2415972Z | Page-header    | 72   | 90   |     |      |
2025-04-14T16:12:19.2416214Z | Picture        | 72   | 82   | 72  | 82   |
2025-04-14T16:12:19.2416457Z | Section-header | 68   | 83   | 69  | 83   |
2025-04-14T16:12:19.2416699Z | Table          | 82   | 89   | 82  | 90   |
2025-04-14T16:12:19.2416932Z | Text           | 85   | 91   | 84  | 90   |
2025-04-14T16:12:19.2417165Z | Title          | 77   | 81   |     |      |
2025-04-14T16:12:19.2417401Z | All            | 72   | 84   | 78  | 87   |
2025-04-14T16:12:19.2417559Z 
2025-04-14T16:12:19.2418891Z lists in PubLayNet (grouped list-items) versus DocLayNet (separate list-items), the label set of size 4 is the closest to PubLayNet, in the assumption that the List is down-mapped to Text in PubLayNet. The results in Table 3 show that the prediction accuracy on the remaining class labels does not change significantly when other classes are merged into them. The overall macro-average improves by around 5%, in particular when Page-footer and Page-header are excluded.
2025-04-14T16:12:19.2420295Z 
2025-04-14T16:12:19.2420404Z ## Impact of Document Split in Train and Test Set
2025-04-14T16:12:19.2420601Z 
2025-04-14T16:12:19.2423137Z Many documents in DocLayNet have a unique styling. In order to avoid overfitting on a particular style, we have split the train-, testand validation-sets of DocLayNet on document boundaries, i.e. every document contributes pages to only one set. To the best of our knowledge, this was not considered in PubLayNet or DocBank. To quantify how this affects model performance, we trained and evaluated a Mask R-CNN R50 model on a modified dataset version. Here, the train-, testand validation-sets were obtained by a randomised draw over the individual pages. As can be seen in Table 4, the difference in model performance is surprisingly large: pagewise splitting gains 10% in mAP over the document-wise splitting. Thus, random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance and should be avoided.
2025-04-14T16:12:19.2425728Z 
2025-04-14T16:12:19.2425811Z ## Dataset Comparison
2025-04-14T16:12:19.2425930Z 
2025-04-14T16:12:19.2427394Z Throughout this paper, we claim that DocLayNet's wider variety of document layouts leads to more robust layout detection models. In Table 5, we provide evidence for that. We trained models on each of the available datasets (PubLayNet, DocBank and DocLayNet) and evaluated them on the test sets of the other datasets. Due to the different label sets and annotation styles, a direct comparison is not possible. Hence, we focussed on the common labels among the datasets. Between PubLayNet and DocLayNet, these are Picture,
2025-04-14T16:12:19.2428911Z 
2025-04-14T16:12:19.2429746Z Table 5: Prediction Performance (mAP@0.5-0.95) of a Mask R-CNN R50 network across the PubLayNet, DocBank &amp; DocLayNet data-sets. By evaluating on common label classes of each dataset, we observe that the DocLayNet-trained model has much less pronounced variations in performance across all datasets.
2025-04-14T16:12:19.2430648Z 
2025-04-14T16:12:19.2430765Z |                 |            | Testing on   | Testing on   | Testing on   |
2025-04-14T16:12:19.2431098Z |-----------------|------------|--------------|--------------|--------------|
2025-04-14T16:12:19.2431451Z | Training on     | labels     | PLN          | DB           | DLN          |
2025-04-14T16:12:19.2431825Z | PubLayNet (PLN) | Figure     | 96           | 43           | 23           |
2025-04-14T16:12:19.2432202Z | PubLayNet (PLN) | Sec-header | 87           | -            | 32           |
2025-04-14T16:12:19.2432543Z |                 | Table      | 95           | 24           | 49           |
2025-04-14T16:12:19.2432834Z |                 | Text       | 96           | -            | 42           |
2025-04-14T16:12:19.2433279Z |                 | total      | 93           | 34           | 30           |
2025-04-14T16:12:19.2433599Z | DocBank (DB)    | Figure     | 77           | 71           | 31           |
2025-04-14T16:12:19.2434040Z | DocBank (DB)    | Table      | 19           | 65           | 22           |
2025-04-14T16:12:19.2434381Z | DocBank (DB)    | total      | 48           | 68           | 27           |
2025-04-14T16:12:19.2434728Z | DocLayNet (DLN) | Figure     | 67           | 51           | 72           |
2025-04-14T16:12:19.2435105Z | DocLayNet (DLN) | Sec-header | 53           | -            | 68           |
2025-04-14T16:12:19.2435444Z |                 | Table      | 87           | 43           | 82           |
2025-04-14T16:12:19.2435734Z |                 | Text       | 77           | -            | 84           |
2025-04-14T16:12:19.2436022Z |                 | total      | 59           | 47           | 78           |
2025-04-14T16:12:19.2436214Z 
2025-04-14T16:12:19.2437037Z Section-header, Table and Text. Before training, we either mapped or excluded DocLayNet's other labels as specified in table 3, and also PubLayNet's List to Text. Note that the different clustering of lists (by list-element vs. whole list objects) naturally decreases the mAP score for Text.
2025-04-14T16:12:19.2437913Z 
2025-04-14T16:12:19.2440327Z For comparison of DocBank with DocLayNet, we trained only on Picture and Table clusters of each dataset. We had to exclude Text because successive paragraphs are often grouped together into a single object in DocBank. This paragraph grouping is incompatible with the individual paragraphs of DocLayNet. As can be seen in Table 5, DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.
2025-04-14T16:12:19.2442682Z 
2025-04-14T16:12:19.2442760Z ## Example Predictions
2025-04-14T16:12:19.2442892Z 
2025-04-14T16:12:19.2444403Z To conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.
2025-04-14T16:12:19.2445891Z 
2025-04-14T16:12:19.2445958Z ## 6 CONCLUSION
2025-04-14T16:12:19.2446072Z 
2025-04-14T16:12:19.2447546Z In this paper, we presented the DocLayNet dataset. It provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. In contrast to many other datasets, DocLayNet was created by human annotation in order to obtain reliable layout ground-truth on a wide variety of publicationand typesettingstyles. Including a large proportion of documents outside the scientific publishing domain adds significant value in this respect.
2025-04-14T16:12:19.2449107Z 
2025-04-14T16:12:19.2450790Z From the dataset, we have derived on the one hand reference metrics for human performance on document-layout annotation (through double and triple annotations) and on the other hand evaluated the baseline performance of commonly used object detection methods. We also illustrated the impact of various dataset-related aspects on model performance through data-ablation experiments, both from a size and class-label perspective. Last but not least, we compared the accuracy of models trained on other public datasets and showed that DocLayNet trained models are more robust.
2025-04-14T16:12:19.2452644Z 
2025-04-14T16:12:19.2453143Z To date, there is still a significant gap between human and ML accuracy on the layout interpretation task, and we hope that this work will inspire the research community to close that gap.
2025-04-14T16:12:19.2453717Z 
2025-04-14T16:12:19.2453789Z ## REFERENCES
2025-04-14T16:12:19.2453984Z 
2025-04-14T16:12:19.2454676Z - 1] Max Göbel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition, pages 1449-1453, 2013.
2025-04-14T16:12:19.2456088Z - 2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Icdar2017 competition on recognition of documents with complex layouts -rdcl2017. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 01, pages 1404-1410, 2017.
2025-04-14T16:12:19.2457720Z - 3] Hervé Déjean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition (cTDaR), April 2019. http://sac.founderit.com/.
2025-04-14T16:12:19.2459105Z - 4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on scientific literature parsing. In Proceedings of the International Conference on Document Analysis and Recognition, ICDAR, pages 605-617. LNCS 12824, SpringerVerlag, sep 2021.
2025-04-14T16:12:19.2460614Z - 5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis: not dead yet. International Journal on Document Analysis and Recognition (IJDAR), pages 1-11, 01 2022.
2025-04-14T16:12:19.2462195Z - 6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset ever for document layout analysis. In Proceedings of the International Conference on Document Analysis and Recognition, ICDAR, pages 1015-1022, sep 2019.
2025-04-14T16:12:19.2463817Z - 7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics, COLING, pages 949-960. International Committee on Computational Linguistics, dec 2020.
2025-04-14T16:12:19.2465345Z - 8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC, 2016.
2025-04-14T16:12:19.2466653Z - 9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 580-587. IEEE Computer Society, jun 2014.
2025-04-14T16:12:19.2467874Z - [10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision, ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.
2025-04-14T16:12:19.2468941Z - [11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(6):1137-1149, 2017.
2025-04-14T16:12:19.2470268Z - [12] Kaiming He, Georgia Gkioxari, Piotr Dollär, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision, ICCV, pages 2980-2988. IEEE Computer Society, Oct 2017.
2025-04-14T16:12:19.2471690Z - [13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012, TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu
2025-04-14T16:12:19.2472626Z 
2025-04-14T16:12:19.2472898Z OText OCaption Olistttem @Formula OTable OPicture @Section-Header (O Page-Header OPage-Footer OTitle
2025-04-14T16:12:19.2473260Z 
2025-04-14T16:12:19.2473326Z <!-- image -->
2025-04-14T16:12:19.2473446Z 
2025-04-14T16:12:19.2474870Z Figure 6: Example layout predictions on selected pages from the DocLayNet test-set. (A, D) exhibit favourable results on coloured backgrounds. (B, C) show accurate list-item and paragraph differentiation despite densely-spaced lines. (E) demonstrates good table and figure distinction. (F) shows predictions on a Chinese patent with multiple overlaps, label confusion and missing boxes.
2025-04-14T16:12:19.2476061Z 
2025-04-14T16:12:19.2476658Z Diaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ultralytics/yolov5: v6.0 -yolov5n nano models, roboflow integration, tensorflow export, opencv dnn support, October 2021.
2025-04-14T16:12:19.2477276Z 
2025-04-14T16:12:19.2477668Z - [20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout: Fusion of visual and text features for document layout analysis, 2021.
2025-04-14T16:12:19.2478648Z - 14 Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR, abs/2005.12872, 2020.
2025-04-14T16:12:19.2479583Z - 15 Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. CoRR, abs/1911.09070, 2019.
2025-04-14T16:12:19.2480709Z - 16 Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollär, and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014.
2025-04-14T16:12:19.2481605Z - 17 Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.
2025-04-14T16:12:19.2483016Z - 18 Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks. In Proceedings of the 35th Conference on Artificial Intelligence, AAAI, pages 1513715145, feb 2021.
2025-04-14T16:12:19.2485129Z - 19 Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD, pages 1192-1200, New York, USA, 2020. Association for Computing Machinery.
2025-04-14T16:12:19.2486607Z - [21] Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei Wu. Vsr: A unified framework for document layout analysis combining vision, semantics and relations, 2021.
2025-04-14T16:12:19.2487968Z - [22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD, pages 774-782. ACM, 2018.
2025-04-14T16:12:19.2489234Z - [23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data, 6(1):60, 2019.
2025-04-14T16:12:20.0329484Z Running example docs/examples/inspect_picture_content.py
2025-04-14T16:12:37.5808912Z Figure 7-26. Self-locking nuts.  contains these elements:
2025-04-14T16:12:37.5809470Z Figure 7-26. Self-locking nuts.
2025-04-14T16:12:37.5809779Z Boots aircraft nut
2025-04-14T16:12:37.5810003Z Flexloc nut
2025-04-14T16:12:37.5810205Z Fiber locknut
2025-04-14T16:12:37.5810416Z Elastic stop nut
2025-04-14T16:12:37.5810641Z Elastic anchor nut
2025-04-14T16:12:37.5810782Z 
2025-04-14T16:12:37.5810788Z 
2025-04-14T16:12:37.5811026Z Figure 7-27. Stainless steel self-locking nut.  contains these elements:
2025-04-14T16:12:37.5811509Z Figure 7-27. Stainless steel self-locking nut.
2025-04-14T16:12:37.5811859Z Tightened nut
2025-04-14T16:12:37.5812074Z Untightened nut
2025-04-14T16:12:37.5812284Z Nut case
2025-04-14T16:12:37.5812476Z Threaded nut core
2025-04-14T16:12:37.5812694Z Locking shoulder
2025-04-14T16:12:37.5812899Z Keyway
2025-04-14T16:12:37.5825483Z 
2025-04-14T16:12:37.5825492Z 
2025-04-14T16:12:38.5447578Z Skipping docs/examples/minimal.py
2025-04-14T16:12:38.5460851Z Skipping docs/examples/minimal_vlm_pipeline.py
2025-04-14T16:12:38.5478317Z Skipping docs/examples/pictures_description_api.py
2025-04-14T16:12:38.5488476Z Skipping docs/examples/rapidocr_with_custom_models.py
2025-04-14T16:12:38.5501561Z Running example docs/examples/run_md.py
2025-04-14T16:12:39.9814284Z Document README.md converted.
2025-04-14T16:12:39.9814797Z Saved markdown output to: scratch
2025-04-14T16:12:40.1134769Z Running example docs/examples/run_with_accelerator.py
2025-04-14T16:14:43.0444809Z ## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis
2025-04-14T16:14:43.0445642Z 
2025-04-14T16:14:43.0446121Z Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com
2025-04-14T16:14:43.0446680Z 
2025-04-14T16:14:43.0446952Z Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com
2025-04-14T16:14:43.0447328Z 
2025-04-14T16:14:43.0447581Z Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com
2025-04-14T16:14:43.0447953Z 
2025-04-14T16:14:43.0448205Z Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com
2025-04-14T16:14:43.0448562Z 
2025-04-14T16:14:43.0448793Z Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com
2025-04-14T16:14:43.0449147Z 
2025-04-14T16:14:43.0449241Z ## ABSTRACT
2025-04-14T16:14:43.0449376Z 
2025-04-14T16:14:43.0455789Z Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.
2025-04-14T16:14:43.0462340Z 
2025-04-14T16:14:43.0462426Z ## CCS CONCEPTS
2025-04-14T16:14:43.0462544Z 
2025-04-14T16:14:43.0463373Z · Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;
2025-04-14T16:14:43.0464097Z 
2025-04-14T16:14:43.0465236Z Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).
2025-04-14T16:14:43.0466425Z 
2025-04-14T16:14:43.0466540Z KDD '22, August 14-18, 2022, Washington, DC, USA
2025-04-14T16:14:43.0466730Z 
2025-04-14T16:14:43.0466889Z © 2022 Copyright held by the owner/author(s).
2025-04-14T16:14:43.0467073Z 
2025-04-14T16:14:43.0467154Z ACM ISBN 978-1-4503-9385-0/22/08.
2025-04-14T16:14:43.0467518Z 
2025-04-14T16:14:43.0467619Z https://doi.org/10.1145/3534678.3539043
2025-04-14T16:14:43.0467794Z 
2025-04-14T16:14:43.0468007Z Figure 1: Four examples of complex page layouts across different document categories
2025-04-14T16:14:43.0468310Z 
2025-04-14T16:14:43.0468381Z <!-- image -->
2025-04-14T16:14:43.0468493Z 
2025-04-14T16:14:43.0468558Z ## KEYWORDS
2025-04-14T16:14:43.0468649Z 
2025-04-14T16:14:43.0468894Z PDF document conversion, layout segmentation, object-detection, data set, Machine Learning
2025-04-14T16:14:43.0469222Z 
2025-04-14T16:14:43.0469309Z ## ACMReference Format:
2025-04-14T16:14:43.0469433Z 
2025-04-14T16:14:43.0470470Z Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043
2025-04-14T16:14:43.0471571Z 
2025-04-14T16:14:43.0471648Z ## 1 INTRODUCTION
2025-04-14T16:14:43.0471757Z 
2025-04-14T16:14:43.0474058Z Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.
2025-04-14T16:14:43.0476189Z 
2025-04-14T16:14:43.0480838Z Akeyproblem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet [6] and DocBank [7]. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories (PubMed and arXiv), which provide XML or L A T E X sources. Those scientific documents present a limited variability in their layouts, because they are typeset in uniform templates provided by the publishers. Obviously, documents such as technical manuals, annual company reports, legal text, government tenders, etc. have very different and partially unique layouts. As a consequence, the layout predictions obtained from models trained on PubLayNet or DocBank is very reasonable when applied on scientific documents. However, for more artistic or free-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.
2025-04-14T16:14:43.0485816Z 
2025-04-14T16:14:43.0487164Z In this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry double- or triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public 1 in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:
2025-04-14T16:14:43.0488698Z 
2025-04-14T16:14:43.0489095Z - (1) Human Annotation : In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.
2025-04-14T16:14:43.0489854Z - (2) Large Layout Variability : We include diverse and complex layouts from a large variety of public sources.
2025-04-14T16:14:43.0490699Z - (3) Detailed Label Set : We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.
2025-04-14T16:14:43.0491554Z - (4) Redundant Annotations : A fraction of the pages in the DocLayNet data set carry more than one human annotation.
2025-04-14T16:14:43.0491938Z 
2025-04-14T16:14:43.0492083Z 1 https://developer.ibm.com/exchanges/data/all/doclaynet
2025-04-14T16:14:43.0492309Z 
2025-04-14T16:14:43.0492541Z This enables experimentation with annotation uncertainty and quality control analysis.
2025-04-14T16:14:43.0492855Z 
2025-04-14T16:14:43.0493665Z - (5) Pre-defined Train-, Test- &amp; Validation-set : Like DocBank, we provide fixed train-, test- &amp; validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.
2025-04-14T16:14:43.0494824Z 
2025-04-14T16:14:43.0495657Z All aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.
2025-04-14T16:14:43.0496563Z 
2025-04-14T16:14:43.0498126Z In Section 5, we will present baseline accuracy numbers for a variety of object detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.
2025-04-14T16:14:43.0499643Z 
2025-04-14T16:14:43.0499715Z ## 2 RELATED WORK
2025-04-14T16:14:43.0499831Z 
2025-04-14T16:14:43.0501722Z While early approaches in document-layout analysis used rulebased algorithms and heuristics [8], the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection models [9-15]. In the last decade, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardisation effort of the ground-truth data format [16] and common deep-learning frameworks [17]. Reference data sets such as PubLayNet [6] and DocBank provide their data in the commonly accepted COCO format [16].
2025-04-14T16:14:43.0503668Z 
2025-04-14T16:14:43.0505200Z Lately, new types of ML models for document-layout analysis have emerged in the community [18-21]. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.
2025-04-14T16:14:43.0506674Z 
2025-04-14T16:14:43.0506760Z ## 3 THE DOCLAYNET DATASET
2025-04-14T16:14:43.0506899Z 
2025-04-14T16:14:43.0508409Z DocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular boundingboxes. We define 11 distinct labels for layout features, namely Caption , Footnote , Formula List-item , , Page-footer , Page-header , Picture , Section-header , Table , Text , and Title . Our reasoning for picking this particular label set is detailed in Section 4.
2025-04-14T16:14:43.0510099Z 
2025-04-14T16:14:43.0510619Z In addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents
2025-04-14T16:14:43.0511210Z 
2025-04-14T16:14:43.0511388Z Figure 2: Distribution of DocLayNet pages across document categories.
2025-04-14T16:14:43.0511644Z 
2025-04-14T16:14:43.0511718Z <!-- image -->
2025-04-14T16:14:43.0511821Z 
2025-04-14T16:14:43.0513285Z to a minimum, since they introduce difficulties in annotation (see Section 4). As a second condition, we focussed on medium to large documents ( &gt; 10 pages) with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing 'text in the wild".
2025-04-14T16:14:43.0514906Z 
2025-04-14T16:14:43.0517424Z The pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports , Manuals Scientific Articles , , Laws &amp; Regulations , Patents and Government Tenders . Each document category was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports 2 which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories ( Financial Reports and Manuals ) contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.
2025-04-14T16:14:43.0519895Z 
2025-04-14T16:14:43.0521390Z We did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet (close to 95%) are published in English language. However, DocLayNet also contains a number of documents in other languages such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.
2025-04-14T16:14:43.0522969Z 
2025-04-14T16:14:43.0524379Z To ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, test- and validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, test- and validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.
2025-04-14T16:14:43.0525960Z 
2025-04-14T16:14:43.0526091Z 2 e.g. AAPL from https://www.annualreports.com/
2025-04-14T16:14:43.0526298Z 
2025-04-14T16:14:43.0527542Z Table 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.
2025-04-14T16:14:43.0528852Z 
2025-04-14T16:14:43.0531165Z In order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format [16]. This entails the standard COCO ground-truth file (in JSON format) with the associated page images (in PNG format, 1025 × 1025 pixels). Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates (in JSON). All additional files are linked to the primary page images by their matching filenames.
2025-04-14T16:14:43.0533221Z 
2025-04-14T16:14:43.0539178Z Despite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, 'invisible' tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as 'invisible' list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a 'natural' upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.
2025-04-14T16:14:43.0545002Z 
2025-04-14T16:14:43.0545086Z ## 4 ANNOTATION CAMPAIGN
2025-04-14T16:14:43.0545225Z 
2025-04-14T16:14:43.0546516Z The annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,
2025-04-14T16:14:43.0547881Z 
2025-04-14T16:14:43.0548859Z Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.
2025-04-14T16:14:43.0549892Z 
2025-04-14T16:14:43.0550680Z |                |         | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |
2025-04-14T16:14:43.0552063Z |----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|
2025-04-14T16:14:43.0553103Z | class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |
2025-04-14T16:14:43.0553795Z | Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |
2025-04-14T16:14:43.0554525Z | Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |
2025-04-14T16:14:43.0555155Z | Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |
2025-04-14T16:14:43.0555900Z | List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |
2025-04-14T16:14:43.0556594Z | Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |
2025-04-14T16:14:43.0557265Z | Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |
2025-04-14T16:14:43.0557911Z | Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |
2025-04-14T16:14:43.0558572Z | Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |
2025-04-14T16:14:43.0559224Z | Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |
2025-04-14T16:14:43.0559949Z | Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |
2025-04-14T16:14:43.0560576Z | Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |
2025-04-14T16:14:43.0561230Z | Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |
2025-04-14T16:14:43.0561623Z 
2025-04-14T16:14:43.0562416Z Figure 3: Corpus Conversion Service annotation user interface. The PDF page is shown in the background, with overlaid text-cells (in darker shades). The annotation boxes can be drawn by dragging a rectangle over each segment with the respective label from the palette on the right.
2025-04-14T16:14:43.0563286Z 
2025-04-14T16:14:43.0563437Z <!-- image -->
2025-04-14T16:14:43.0563546Z 
2025-04-14T16:14:43.0564294Z we distributed the annotation workload and performed continuous quality controls. Phase one and two required a small team of experts only. For phases three and four, a group of 40 dedicated annotators were assembled and supervised.
2025-04-14T16:14:43.0565023Z 
2025-04-14T16:14:43.0566696Z Phase 1: Data selection and preparation. Our inclusion criteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources include publication repositories such as arXiv 3 , government offices, company websites as well as data directory services for financial reports and patents. Scanned documents were excluded wherever possible because they can be rotated or skewed. This would not allow us to perform annotation with rectangular bounding-boxes and therefore complicate the annotation process.
2025-04-14T16:14:43.0568418Z 
2025-04-14T16:14:43.0570604Z Preparation work included uploading and parsing the sourced PDF documents in the Corpus Conversion Service (CCS) [22], a cloud-native platform which provides a visual annotation interface and allows for dataset inspection and analysis. The annotation interface of CCS is shown in Figure 3. The desired balance of pages between the different document categories was achieved by selective subsampling of pages with certain desired properties. For example, we made sure to include the title page of each document and bias the remaining page selection to those with figures or tables. The latter was achieved by leveraging pre-trained object detection models from PubLayNet, which helped us estimate how many figures and tables a given page contains.
2025-04-14T16:14:43.0572844Z 
2025-04-14T16:14:43.0576383Z Phase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recurrent layout elements and lead us to the definition of 11 distinct class labels. These 11 class labels are Caption , Footnote , Formula List-item , , Pagefooter , Page-header , Picture , Section-header , Table , Text , and Title . Critical factors that were considered for the choice of these class labels were (1) the overall occurrence of the label, (2) the specificity of the label, (3) recognisability on a single page (i.e. no need for context from previous or next page) and (4) overall coverage of the page. Specificity ensures that the choice of label is not ambiguous, while coverage ensures that all meaningful items on a page can be annotated. We refrained from class labels that are very specific to a document category, such as Abstract in the Scientific Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation , as seen in DocBank, are often only distinguishable by discriminating on
2025-04-14T16:14:43.0580022Z 
2025-04-14T16:14:43.0580115Z 3 https://arxiv.org/
2025-04-14T16:14:43.0580244Z 
2025-04-14T16:14:43.0580614Z the textual content of an element, which goes beyond visual layout recognition, in particular outside the Scientific Articles category.
2025-04-14T16:14:43.0581057Z 
2025-04-14T16:14:43.0583559Z At first sight, the task of visual document-layout interpretation appears intuitive enough to obtain plausible annotations in most cases. However, during early trial-runs in the core team, we observed many cases in which annotators use different annotation styles, especially for documents with challenging layouts. For example, if a figure is presented with subfigures, one annotator might draw a single figure bounding-box, while another might annotate each subfigure separately. The same applies for lists, where one might annotate all list items in one block or each list item separately. In essence, we observed that challenging layouts would be annotated in different but plausible ways. To illustrate this, we show in Figure 4 multiple examples of plausible but inconsistent annotations on the same pages.
2025-04-14T16:14:43.0586114Z 
2025-04-14T16:14:43.0587478Z Obviously, this inconsistency in annotations is not desirable for datasets which are intended to be used for model training. To minimise these inconsistencies, we created a detailed annotation guideline. While perfect consistency across 40 annotation staff members is clearly not possible to achieve, we saw a huge improvement in annotation consistency after the introduction of our annotation guideline. A few selected, non-trivial highlights of the guideline are:
2025-04-14T16:14:43.0588915Z 
2025-04-14T16:14:43.0589467Z - (1) Every list-item is an individual object instance with class label List-item . This definition is different from PubLayNet and DocBank, where all list-items are grouped together into one List object.
2025-04-14T16:14:43.0590682Z - (2) A List-item is a paragraph with hanging indentation. Singleline elements can qualify as List-item if the neighbour elements expose hanging indentation. Bullet or enumeration symbols are not a requirement.
2025-04-14T16:14:43.0591548Z - (3) For every Caption , there must be exactly one corresponding Picture or Table .
2025-04-14T16:14:43.0592008Z - (4) Connected sub-pictures are grouped together in one Picture object.
2025-04-14T16:14:43.0592383Z - (5) Formula numbers are included in a Formula object.
2025-04-14T16:14:43.0593001Z - (6) Emphasised text (e.g. in italic or bold) at the beginning of a paragraph is not considered a Section-header , unless it appears exclusively on its own line.
2025-04-14T16:14:43.0593490Z 
2025-04-14T16:14:43.0594204Z The complete annotation guideline is over 100 pages long and a detailed description is obviously out of scope for this paper. Nevertheless, it will be made publicly available alongside with DocLayNet for future reference.
2025-04-14T16:14:43.0594893Z 
2025-04-14T16:14:43.0597193Z Phase 3: Training. After a first trial with a small group of people, we realised that providing the annotation guideline and a set of random practice pages did not yield the desired quality level for layout annotation. Therefore we prepared a subset of pages with two different complexity levels, each with a practice and an exam part. 974 pages were reference-annotated by one proficient core team member. Annotation staff were then given the task to annotate the same subsets (blinded from the reference). By comparing the annotations of each staff member with the reference annotations, we could quantify how closely their annotations matched the reference. Only after passing two exam levels with high annotation quality, staff were admitted into the production phase. Practice iterations
2025-04-14T16:14:43.0599757Z 
2025-04-14T16:14:43.0599828Z <!-- image -->
2025-04-14T16:14:43.0599941Z 
2025-04-14T16:14:43.0600107Z 05237a14f2524e3f53c8454b074409d05078038a6a36b770fcc8ec7e540deae0
2025-04-14T16:14:43.0600364Z 
2025-04-14T16:14:43.0600847Z Figure 4: Examples of plausible annotation alternatives for the same page. Criteria in our annotation guideline can resolve cases A to C, while the case D remains ambiguous.
2025-04-14T16:14:43.0601411Z 
2025-04-14T16:14:43.0601717Z were carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.
2025-04-14T16:14:43.0602106Z 
2025-04-14T16:14:43.0605206Z Phase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement (see Table 1). We wanted
2025-04-14T16:14:43.0608164Z 
2025-04-14T16:14:43.0609524Z Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.
2025-04-14T16:14:43.0610937Z 
2025-04-14T16:14:43.0611057Z |                | human   | MRCNN   | MRCNN   | FRCNN   | YOLO   |
2025-04-14T16:14:43.0611374Z |----------------|---------|---------|---------|---------|--------|
2025-04-14T16:14:43.0611649Z |                |         | R50     | R101    | R101    | v5x6   |
2025-04-14T16:14:43.0611932Z | Caption        | 84-89   | 68.4    | 71.5    | 70.1    | 77.7   |
2025-04-14T16:14:43.0612240Z | Footnote       | 83-91   | 70.9    | 71.8    | 73.7    | 77.2   |
2025-04-14T16:14:43.0612551Z | Formula        | 83-85   | 60.1    | 63.4    | 63.5    | 66.2   |
2025-04-14T16:14:43.0612857Z | List-item      | 87-88   | 81.2    | 80.8    | 81.0    | 86.2   |
2025-04-14T16:14:43.0613174Z | Page-footer    | 93-94   | 61.6    | 59.3    | 58.9    | 61.1   |
2025-04-14T16:14:43.0613499Z | Page-header    | 85-89   | 71.9    | 70.0    | 72.0    | 67.9   |
2025-04-14T16:14:43.0613808Z | Picture        | 69-71   | 71.7    | 72.7    | 72.0    | 77.1   |
2025-04-14T16:14:43.0614373Z | Section-header | 83-84   | 67.6    | 69.3    | 68.4    | 74.6   |
2025-04-14T16:14:43.0614694Z | Table          | 77-81   | 82.2    | 82.9    | 82.2    | 86.3   |
2025-04-14T16:14:43.0614986Z | Text           | 84-86   | 84.6    | 85.8    | 85.4    | 88.1   |
2025-04-14T16:14:43.0615274Z | Title          | 60-72   | 76.7    | 80.4    | 79.9    | 82.7   |
2025-04-14T16:14:43.0615555Z | All            | 82-83   | 72.4    | 73.5    | 73.4    | 76.8   |
2025-04-14T16:14:43.0615734Z 
2025-04-14T16:14:43.0619420Z to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.
2025-04-14T16:14:43.0623450Z 
2025-04-14T16:14:43.0623531Z ## 5 EXPERIMENTS
2025-04-14T16:14:43.0623643Z 
2025-04-14T16:14:43.0625513Z The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this
2025-04-14T16:14:43.0627237Z 
2025-04-14T16:14:43.0628197Z Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.
2025-04-14T16:14:43.0629233Z 
2025-04-14T16:14:43.0629304Z <!-- image -->
2025-04-14T16:14:43.0629406Z 
2025-04-14T16:14:43.0629676Z paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.
2025-04-14T16:14:43.0630027Z 
2025-04-14T16:14:43.0631102Z In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].
2025-04-14T16:14:43.0632239Z 
2025-04-14T16:14:43.0632334Z ## Baselines for Object Detection
2025-04-14T16:14:43.0632490Z 
2025-04-14T16:14:43.0636618Z In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.
2025-04-14T16:14:43.0640223Z 
2025-04-14T16:14:43.0640771Z Table 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obtained by either down-mapping or dropping labels.
2025-04-14T16:14:43.0641392Z 
2025-04-14T16:14:43.0641504Z | Class-count    |   11 | 6       | 5       | 4       |
2025-04-14T16:14:43.0641793Z |----------------|------|---------|---------|---------|
2025-04-14T16:14:43.0642078Z | Caption        |   68 | Text    | Text    | Text    |
2025-04-14T16:14:43.0642360Z | Footnote       |   71 | Text    | Text    | Text    |
2025-04-14T16:14:43.0642648Z | Formula        |   60 | Text    | Text    | Text    |
2025-04-14T16:14:43.0642918Z | List-item      |   81 | Text    | 82      | Text    |
2025-04-14T16:14:43.0643206Z | Page-footer    |   62 | 62      | -       | -       |
2025-04-14T16:14:43.0643495Z | Page-header    |   72 | 68      | -       | -       |
2025-04-14T16:14:43.0643769Z | Picture        |   72 | 72      | 72      | 72      |
2025-04-14T16:14:43.0644149Z | Section-header |   68 | 67      | 69      | 68      |
2025-04-14T16:14:43.0644423Z | Table          |   82 | 83      | 82      | 82      |
2025-04-14T16:14:43.0644676Z | Text           |   85 | 84      | 84      | 84      |
2025-04-14T16:14:43.0644941Z | Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |
2025-04-14T16:14:43.0645212Z | Overall        |   72 | 73      | 78      | 77      |
2025-04-14T16:14:43.0645513Z 
2025-04-14T16:14:43.0645587Z ## Learning Curve
2025-04-14T16:14:43.0645703Z 
2025-04-14T16:14:43.0648869Z One of the fundamental questions related to any dataset is if it is 'large enough'. To answer this question for DocLayNet, we performed a data ablation study in which we evaluated a Mask R-CNN model trained on increasing fractions of the DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in the beginning and eventually levels out. To estimate the error-bar on the metrics, we ran the training five times on the entire data-set. This resulted in a 1% error-bar, depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the exact same data-points, but with a logarithmic scale on the x-axis. As is expected, the mAP score increases linearly as a function of the data-size in the inset. The curve ultimately flattens out between the 80% and 100% mark, with the 80% mark falling within the error-bars of the 100% mark. This provides a good indication that the model would not improve significantly by yet increasing the data size. Rather, it would probably benefit more from improved data consistency (as discussed in Section 3), data augmentation methods [23], or the addition of more document categories and styles.
2025-04-14T16:14:43.0652228Z 
2025-04-14T16:14:43.0652323Z ## Impact of Class Labels
2025-04-14T16:14:43.0652457Z 
2025-04-14T16:14:43.0655703Z The choice and number of labels can have a significant effect on the overall model performance. Since PubLayNet, DocBank and DocLayNet all have different label sets, it is of particular interest to understand and quantify this influence of the label set on the model performance. We investigate this by either down-mapping labels into more common ones (e.g. Caption → Text ) or excluding them from the annotations entirely. Furthermore, it must be stressed that all mappings and exclusions were performed on the data before model training. In Table 3, we present the mAP scores for a Mask R-CNN R50 network on different label sets. Where a label is down-mapped, we show its corresponding label, otherwise it was excluded. We present three different label sets, with 6, 5 and 4 different labels respectively. The set of 5 labels contains the same labels as PubLayNet. However, due to the different definition of
2025-04-14T16:14:43.0658487Z 
2025-04-14T16:14:43.0658999Z Table 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise split for different label sets. Naive page-wise split will result in /tildelow 10% point improvement.
2025-04-14T16:14:43.0659571Z 
2025-04-14T16:14:43.0659670Z | Class-count    | 11   | 11   | 5   | 5    |
2025-04-14T16:14:43.0659928Z |----------------|------|------|-----|------|
2025-04-14T16:14:43.0660183Z | Split          | Doc  | Page | Doc | Page |
2025-04-14T16:14:43.0660430Z | Caption        | 68   | 83   |     |      |
2025-04-14T16:14:43.0660670Z | Footnote       | 71   | 84   |     |      |
2025-04-14T16:14:43.0660911Z | Formula        | 60   | 66   |     |      |
2025-04-14T16:14:43.0661156Z | List-item      | 81   | 88   | 82  | 88   |
2025-04-14T16:14:43.0661404Z | Page-footer    | 62   | 89   |     |      |
2025-04-14T16:14:43.0661650Z | Page-header    | 72   | 90   |     |      |
2025-04-14T16:14:43.0661889Z | Picture        | 72   | 82   | 72  | 82   |
2025-04-14T16:14:43.0662143Z | Section-header | 68   | 83   | 69  | 83   |
2025-04-14T16:14:43.0662385Z | Table          | 82   | 89   | 82  | 90   |
2025-04-14T16:14:43.0662618Z | Text           | 85   | 91   | 84  | 90   |
2025-04-14T16:14:43.0662852Z | Title          | 77   | 81   |     |      |
2025-04-14T16:14:43.0663083Z | All            | 72   | 84   | 78  | 87   |
2025-04-14T16:14:43.0663239Z 
2025-04-14T16:14:43.0664802Z lists in PubLayNet (grouped list-items) versus DocLayNet (separate list-items), the label set of size 4 is the closest to PubLayNet, in the assumption that the List is down-mapped to Text in PubLayNet. The results in Table 3 show that the prediction accuracy on the remaining class labels does not change significantly when other classes are merged into them. The overall macro-average improves by around 5%, in particular when Page-footer and Page-header are excluded.
2025-04-14T16:14:43.0666201Z 
2025-04-14T16:14:43.0666319Z ## Impact of Document Split in Train and Test Set
2025-04-14T16:14:43.0666508Z 
2025-04-14T16:14:43.0669436Z Many documents in DocLayNet have a unique styling. In order to avoid overfitting on a particular style, we have split the train-, test- and validation-sets of DocLayNet on document boundaries, i.e. every document contributes pages to only one set. To the best of our knowledge, this was not considered in PubLayNet or DocBank. To quantify how this affects model performance, we trained and evaluated a Mask R-CNN R50 model on a modified dataset version. Here, the train-, test- and validation-sets were obtained by a randomised draw over the individual pages. As can be seen in Table 4, the difference in model performance is surprisingly large: pagewise splitting gains ˜ 0% in mAP over the document-wise splitting. 1 Thus, random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance and should be avoided.
2025-04-14T16:14:43.0671899Z 
2025-04-14T16:14:43.0671975Z ## Dataset Comparison
2025-04-14T16:14:43.0672101Z 
2025-04-14T16:14:43.0673558Z Throughout this paper, we claim that DocLayNet's wider variety of document layouts leads to more robust layout detection models. In Table 5, we provide evidence for that. We trained models on each of the available datasets (PubLayNet, DocBank and DocLayNet) and evaluated them on the test sets of the other datasets. Due to the different label sets and annotation styles, a direct comparison is not possible. Hence, we focussed on the common labels among the datasets. Between PubLayNet and DocLayNet, these are Picture ,
2025-04-14T16:14:43.0675184Z 
2025-04-14T16:14:43.0676017Z Table 5: Prediction Performance (mAP@0.5-0.95) of a Mask R-CNN R50 network across the PubLayNet, DocBank &amp; DocLayNet data-sets. By evaluating on common label classes of each dataset, we observe that the DocLayNet-trained model has much less pronounced variations in performance across all datasets.
2025-04-14T16:14:43.0676924Z 
2025-04-14T16:14:43.0677040Z |                 |            | Testing on   | Testing on   | Testing on   |
2025-04-14T16:14:43.0677488Z |-----------------|------------|--------------|--------------|--------------|
2025-04-14T16:14:43.0677842Z | Training on     | labels     | PLN          | DB           | DLN          |
2025-04-14T16:14:43.0678216Z | PubLayNet (PLN) | Figure     | 96           | 43           | 23           |
2025-04-14T16:14:43.0678593Z | PubLayNet (PLN) | Sec-header | 87           | -            | 32           |
2025-04-14T16:14:43.0678932Z |                 | Table      | 95           | 24           | 49           |
2025-04-14T16:14:43.0679216Z |                 | Text       | 96           | -            | 42           |
2025-04-14T16:14:43.0679508Z |                 | total      | 93           | 34           | 30           |
2025-04-14T16:14:43.0679825Z | DocBank (DB)    | Figure     | 77           | 71           | 31           |
2025-04-14T16:14:43.0680171Z | DocBank (DB)    | Table      | 19           | 65           | 22           |
2025-04-14T16:14:43.0680511Z | DocBank (DB)    | total      | 48           | 68           | 27           |
2025-04-14T16:14:43.0680870Z | DocLayNet (DLN) | Figure     | 67           | 51           | 72           |
2025-04-14T16:14:43.0681243Z | DocLayNet (DLN) | Sec-header | 53           | -            | 68           |
2025-04-14T16:14:43.0681582Z |                 | Table      | 87           | 43           | 82           |
2025-04-14T16:14:43.0681865Z |                 | Text       | 77           | -            | 84           |
2025-04-14T16:14:43.0682154Z |                 | total      | 59           | 47           | 78           |
2025-04-14T16:14:43.0682347Z 
2025-04-14T16:14:43.0683265Z Section-header , Table and Text . Before training, we either mapped or excluded DocLayNet's other labels as specified in table 3, and also PubLayNet's List to Text . Note that the different clustering of lists (by list-element vs. whole list objects) naturally decreases the mAP score for Text .
2025-04-14T16:14:43.0684259Z 
2025-04-14T16:14:43.0686543Z For comparison of DocBank with DocLayNet, we trained only on Picture and Table clusters of each dataset. We had to exclude Text because successive paragraphs are often grouped together into a single object in DocBank. This paragraph grouping is incompatible with the individual paragraphs of DocLayNet. As can be seen in Table 5, DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.
2025-04-14T16:14:43.0688889Z 
2025-04-14T16:14:43.0688973Z ## Example Predictions
2025-04-14T16:14:43.0689096Z 
2025-04-14T16:14:43.0690508Z To conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.
2025-04-14T16:14:43.0691997Z 
2025-04-14T16:14:43.0692069Z ## 6 CONCLUSION
2025-04-14T16:14:43.0692174Z 
2025-04-14T16:14:43.0693662Z In this paper, we presented the DocLayNet dataset. It provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. In contrast to many other datasets, DocLayNet was created by human annotation in order to obtain reliable layout ground-truth on a wide variety of publication- and typesettingstyles. Including a large proportion of documents outside the scientific publishing domain adds significant value in this respect.
2025-04-14T16:14:43.0695426Z 
2025-04-14T16:14:43.0697110Z From the dataset, we have derived on the one hand reference metrics for human performance on document-layout annotation (through double and triple annotations) and on the other hand evaluated the baseline performance of commonly used object detection methods. We also illustrated the impact of various dataset-related aspects on model performance through data-ablation experiments, both from a size and class-label perspective. Last but not least, we compared the accuracy of models trained on other public datasets and showed that DocLayNet trained models are more robust.
2025-04-14T16:14:43.0698854Z 
2025-04-14T16:14:43.0699351Z To date, there is still a significant gap between human and ML accuracy on the layout interpretation task, and we hope that this work will inspire the research community to close that gap.
2025-04-14T16:14:43.0699929Z 
2025-04-14T16:14:43.0699997Z ## REFERENCES
2025-04-14T16:14:43.0700102Z 
2025-04-14T16:14:43.0700802Z - [1] Max Göbel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition , pages 1449-1453, 2013.
2025-04-14T16:14:43.0702225Z - [2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Icdar2017 competition on recognition of documents with complex layouts rdcl2017. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR) , volume 01, pages 1404-1410, 2017.
2025-04-14T16:14:43.0703941Z - [3] Hervé Déjean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition (cTDaR), April 2019. http://sac.founderit.com/.
2025-04-14T16:14:43.0707916Z - [4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on scientific literature parsing. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 605-617. LNCS 12824, SpringerVerlag, sep 2021.
2025-04-14T16:14:43.0710368Z - [5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis: not dead yet. International Journal on Document Analysis and Recognition (IJDAR) , pages 1-11, 01 2022.
2025-04-14T16:14:43.0713037Z - [6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset ever for document layout analysis. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 1015-1022, sep 2019.
2025-04-14T16:14:43.0715103Z - [7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics , COLING, pages 949-960. International Committee on Computational Linguistics, dec 2020.
2025-04-14T16:14:43.0716558Z - [8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC , 2016.
2025-04-14T16:14:43.0717887Z - [9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition , CVPR, pages 580-587. IEEE Computer Society, jun 2014.
2025-04-14T16:14:43.0719100Z - [10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision , ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.
2025-04-14T16:14:43.0720212Z - [11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(6):1137-1149, 2017.
2025-04-14T16:14:43.0721629Z - [12] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision , ICCV, pages 2980-2988. IEEE Computer Society, Oct 2017.
2025-04-14T16:14:43.0723269Z - [13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012, TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu
2025-04-14T16:14:43.0724393Z 
2025-04-14T16:14:43.0724648Z Text Caption List-Item Formula Table Section-Header Picture Page-Header Page-Footer Title
2025-04-14T16:14:43.0724973Z 
2025-04-14T16:14:43.0725049Z <!-- image -->
2025-04-14T16:14:43.0725156Z 
2025-04-14T16:14:43.0726286Z Figure 6: Example layout predictions on selected pages from the DocLayNet test-set. (A, D) exhibit favourable results on coloured backgrounds. (B, C) show accurate list-item and paragraph differentiation despite densely-spaced lines. (E) demonstrates good table and figure distinction. (F) shows predictions on a Chinese patent with multiple overlaps, label confusion and missing boxes.
2025-04-14T16:14:43.0727501Z 
2025-04-14T16:14:43.0728045Z Diaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ultralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow export, opencv dnn support, October 2021.
2025-04-14T16:14:43.0728659Z 
2025-04-14T16:14:43.0729038Z - [20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout: Fusion of visual and text features for document layout analysis, 2021.
2025-04-14T16:14:43.0730016Z - [14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR , abs/2005.12872, 2020.
2025-04-14T16:14:43.0731070Z - [15] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. CoRR , abs/1911.09070, 2019.
2025-04-14T16:14:43.0732231Z - [16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014.
2025-04-14T16:14:43.0733140Z - [17] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.
2025-04-14T16:14:43.0734618Z - [18] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks. In Proceedings of the 35th Conference on Artificial Intelligence , AAAI, pages 1513715145, feb 2021.
2025-04-14T16:14:43.0736588Z - [19] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 1192-1200, New York, USA, 2020. Association for Computing Machinery.
2025-04-14T16:14:43.0738049Z - [21] Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei Wu. Vsr: A unified framework for document layout analysis combining vision, semantics and relations, 2021.
2025-04-14T16:14:43.0739422Z - [22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 774-782. ACM, 2018.
2025-04-14T16:14:43.0740713Z - [23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data , 6(1):60, 2019.
2025-04-14T16:14:43.0741241Z Conversion secs: [115.02827145100014]
2025-04-14T16:14:44.0046330Z Running example docs/examples/run_with_formats.py
2025-04-14T16:17:17.4528322Z Document README.md converted.
2025-04-14T16:17:17.4528904Z Saved markdown output to: scratch
2025-04-14T16:17:17.4529368Z Document wiki_duck.html converted.
2025-04-14T16:17:17.4529788Z Saved markdown output to: scratch
2025-04-14T16:17:17.4530543Z Document word_sample.docx converted.
2025-04-14T16:17:17.4530927Z Saved markdown output to: scratch
2025-04-14T16:17:17.4531339Z Document lorem_ipsum.docx converted.
2025-04-14T16:17:17.4531760Z Saved markdown output to: scratch
2025-04-14T16:17:17.4532145Z Document powerpoint_sample.pptx converted.
2025-04-14T16:17:17.4532560Z Saved markdown output to: scratch
2025-04-14T16:17:17.4532956Z Document 2305.03393v1-pg9-img.png converted.
2025-04-14T16:17:17.4533383Z Saved markdown output to: scratch
2025-04-14T16:17:17.4533779Z Document 2206.01062.pdf converted.
2025-04-14T16:17:17.4534385Z Saved markdown output to: scratch
2025-04-14T16:17:17.4534801Z Document test_01.asciidoc converted.
2025-04-14T16:17:17.4535327Z Saved markdown output to: scratch
2025-04-14T16:17:18.4283404Z Running example docs/examples/tesseract_lang_detection.py
2025-04-14T16:17:34.7544925Z Tesseract detected the script 'Fraktur' and language 'Fraktur'. However this language is not installed in your system and will be ignored.
2025-04-14T16:18:20.8544497Z Tesseract cannot detect the script of the page
2025-04-14T16:18:21.1235306Z Tesseract cannot detect the script of the page
2025-04-14T16:18:21.4320441Z Tesseract cannot detect the script of the page
2025-04-14T16:18:22.8386701Z ## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis
2025-04-14T16:18:22.8387255Z 
2025-04-14T16:18:22.8387585Z Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com
2025-04-14T16:18:22.8388039Z 
2025-04-14T16:18:22.8388331Z Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com
2025-04-14T16:18:22.8388766Z 
2025-04-14T16:18:22.8389528Z Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com
2025-04-14T16:18:22.8389974Z 
2025-04-14T16:18:22.8390270Z Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com
2025-04-14T16:18:22.8390700Z 
2025-04-14T16:18:22.8390978Z Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com
2025-04-14T16:18:22.8391417Z 
2025-04-14T16:18:22.8391527Z ## ABSTRACT
2025-04-14T16:18:22.8391687Z 
2025-04-14T16:18:22.8400049Z Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.
2025-04-14T16:18:22.8408235Z 
2025-04-14T16:18:22.8408359Z ## CCS CONCEPTS
2025-04-14T16:18:22.8408522Z 
2025-04-14T16:18:22.8409772Z · Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;
2025-04-14T16:18:22.8410738Z 
2025-04-14T16:18:22.8412902Z Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).
2025-04-14T16:18:22.8415055Z 
2025-04-14T16:18:22.8415223Z KDD '22, August 14-18, 2022, Washington, DC, USA
2025-04-14T16:18:22.8415507Z 
2025-04-14T16:18:22.8415798Z © 2022 Copyright held by the owner/author(s).
2025-04-14T16:18:22.8416078Z 
2025-04-14T16:18:22.8416207Z ACM ISBN 978-1-4503-9385-0/22/08.
2025-04-14T16:18:22.8416437Z 
2025-04-14T16:18:22.8416579Z https://doi.org/10.1145/3534678.3539043
2025-04-14T16:18:22.8416857Z 
2025-04-14T16:18:22.8417209Z Figure 1: Four examples of complex page layouts across different document categories
2025-04-14T16:18:22.8417732Z 
2025-04-14T16:18:22.8417847Z <!-- image -->
2025-04-14T16:18:22.8418028Z 
2025-04-14T16:18:22.8418133Z ## KEYWORDS
2025-04-14T16:18:22.8418282Z 
2025-04-14T16:18:22.8418669Z PDF document conversion, layout segmentation, object-detection, data set, Machine Learning
2025-04-14T16:18:22.8419191Z 
2025-04-14T16:18:22.8419313Z ## ACMReference Format:
2025-04-14T16:18:22.8419505Z 
2025-04-14T16:18:22.8421474Z Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043
2025-04-14T16:18:22.8423270Z 
2025-04-14T16:18:22.8423398Z ## 1 INTRODUCTION
2025-04-14T16:18:22.8423581Z 
2025-04-14T16:18:22.8427264Z Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.
2025-04-14T16:18:22.8430742Z 
2025-04-14T16:18:22.8438777Z Akeyproblem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet [6] and DocBank [7]. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories (PubMed and arXiv), which provide XML or L A T E X sources. Those scientific documents present a limited variability in their layouts, because they are typeset in uniform templates provided by the publishers. Obviously, documents such as technical manuals, annual company reports, legal text, government tenders, etc. have very different and partially unique layouts. As a consequence, the layout predictions obtained from models trained on PubLayNet or DocBank is very reasonable when applied on scientific documents. However, for more artistic or free-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.
2025-04-14T16:18:22.8447150Z 
2025-04-14T16:18:22.8449381Z In this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry double- or triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public 1 in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:
2025-04-14T16:18:22.8451682Z 
2025-04-14T16:18:22.8452318Z - (1) Human Annotation : In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.
2025-04-14T16:18:22.8453634Z - (2) Large Layout Variability : We include diverse and complex layouts from a large variety of public sources.
2025-04-14T16:18:22.8455382Z - (3) Detailed Label Set : We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.
2025-04-14T16:18:22.8456808Z - (4) Redundant Annotations : A fraction of the pages in the DocLayNet data set carry more than one human annotation.
2025-04-14T16:18:22.8457447Z 
2025-04-14T16:18:22.8457681Z 1 https://developer.ibm.com/exchanges/data/all/doclaynet
2025-04-14T16:18:22.8458055Z 
2025-04-14T16:18:22.8458411Z This enables experimentation with annotation uncertainty and quality control analysis.
2025-04-14T16:18:22.8458929Z 
2025-04-14T16:18:22.8460467Z - (5) Pre-defined Train-, Test- &amp; Validation-set : Like DocBank, we provide fixed train-, test- &amp; validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.
2025-04-14T16:18:22.8461966Z 
2025-04-14T16:18:22.8463341Z All aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.
2025-04-14T16:18:22.8464990Z 
2025-04-14T16:18:22.8467358Z In Section 5, we will present baseline accuracy numbers for a variety of object detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.
2025-04-14T16:18:22.8469856Z 
2025-04-14T16:18:22.8469966Z ## 2 RELATED WORK
2025-04-14T16:18:22.8470142Z 
2025-04-14T16:18:22.8473281Z While early approaches in document-layout analysis used rulebased algorithms and heuristics [8], the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection models [9-15]. In the last decade, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardisation effort of the ground-truth data format [16] and common deep-learning frameworks [17]. Reference data sets such as PubLayNet [6] and DocBank provide their data in the commonly accepted COCO format [16].
2025-04-14T16:18:22.8476733Z 
2025-04-14T16:18:22.8479073Z Lately, new types of ML models for document-layout analysis have emerged in the community [18-21]. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.
2025-04-14T16:18:22.8481727Z 
2025-04-14T16:18:22.8481869Z ## 3 THE DOCLAYNET DATASET
2025-04-14T16:18:22.8482077Z 
2025-04-14T16:18:22.8484734Z DocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular boundingboxes. We define 11 distinct labels for layout features, namely Caption , Footnote , Formula List-item , , Page-footer , Page-header , Picture , Section-header , Table , Text , and Title . Our reasoning for picking this particular label set is detailed in Section 4.
2025-04-14T16:18:22.8487294Z 
2025-04-14T16:18:22.8488131Z In addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents
2025-04-14T16:18:22.8489139Z 
2025-04-14T16:18:22.8489418Z Figure 2: Distribution of DocLayNet pages across document categories.
2025-04-14T16:18:22.8490013Z 
2025-04-14T16:18:22.8490121Z <!-- image -->
2025-04-14T16:18:22.8490278Z 
2025-04-14T16:18:22.8492813Z to a minimum, since they introduce difficulties in annotation (see Section 4). As a second condition, we focussed on medium to large documents ( &gt; 10 pages) with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing 'text in the wild".
2025-04-14T16:18:22.8495401Z 
2025-04-14T16:18:22.8499266Z The pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports , Manuals Scientific Articles , , Laws &amp; Regulations , Patents and Government Tenders . Each document category was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports 2 which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories ( Financial Reports and Manuals ) contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.
2025-04-14T16:18:22.8503254Z 
2025-04-14T16:18:22.8505843Z We did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet (close to 95%) are published in English language. However, DocLayNet also contains a number of documents in other languages such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.
2025-04-14T16:18:22.8508586Z 
2025-04-14T16:18:22.8510563Z To ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, test- and validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, test- and validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.
2025-04-14T16:18:22.8512571Z 
2025-04-14T16:18:22.8512766Z 2 e.g. AAPL from https://www.annualreports.com/
2025-04-14T16:18:22.8513078Z 
2025-04-14T16:18:22.8515351Z Table 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.
2025-04-14T16:18:22.8517287Z 
2025-04-14T16:18:22.8520197Z In order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format [16]. This entails the standard COCO ground-truth file (in JSON format) with the associated page images (in PNG format, 1025 × 1025 pixels). Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates (in JSON). All additional files are linked to the primary page images by their matching filenames.
2025-04-14T16:18:22.8522535Z 
2025-04-14T16:18:22.8529284Z Despite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, 'invisible' tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as 'invisible' list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a 'natural' upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.
2025-04-14T16:18:22.8535560Z 
2025-04-14T16:18:22.8535645Z ## 4 ANNOTATION CAMPAIGN
2025-04-14T16:18:22.8535786Z 
2025-04-14T16:18:22.8537082Z The annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,
2025-04-14T16:18:22.8538441Z 
2025-04-14T16:18:22.8539403Z Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.
2025-04-14T16:18:22.8540428Z 
2025-04-14T16:18:22.8541219Z |                |         | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |
2025-04-14T16:18:22.8542707Z |----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|
2025-04-14T16:18:22.8543632Z | class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |
2025-04-14T16:18:22.8544416Z | Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |
2025-04-14T16:18:22.8545156Z | Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |
2025-04-14T16:18:22.8545786Z | Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |
2025-04-14T16:18:22.8546432Z | List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |
2025-04-14T16:18:22.8547083Z | Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |
2025-04-14T16:18:22.8547744Z | Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |
2025-04-14T16:18:22.8548391Z | Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |
2025-04-14T16:18:22.8549053Z | Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |
2025-04-14T16:18:22.8549901Z | Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |
2025-04-14T16:18:22.8550515Z | Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |
2025-04-14T16:18:22.8551140Z | Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |
2025-04-14T16:18:22.8551871Z | Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |
2025-04-14T16:18:22.8552263Z 
2025-04-14T16:18:22.8553055Z Figure 3: Corpus Conversion Service annotation user interface. The PDF page is shown in the background, with overlaid text-cells (in darker shades). The annotation boxes can be drawn by dragging a rectangle over each segment with the respective label from the palette on the right.
2025-04-14T16:18:22.8554045Z 
2025-04-14T16:18:22.8554125Z <!-- image -->
2025-04-14T16:18:22.8554232Z 
2025-04-14T16:18:22.8555127Z we distributed the annotation workload and performed continuous quality controls. Phase one and two required a small team of experts only. For phases three and four, a group of 40 dedicated annotators were assembled and supervised.
2025-04-14T16:18:22.8555863Z 
2025-04-14T16:18:22.8557525Z Phase 1: Data selection and preparation. Our inclusion criteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources include publication repositories such as arXiv 3 , government offices, company websites as well as data directory services for financial reports and patents. Scanned documents were excluded wherever possible because they can be rotated or skewed. This would not allow us to perform annotation with rectangular bounding-boxes and therefore complicate the annotation process.
2025-04-14T16:18:22.8559239Z 
2025-04-14T16:18:22.8561423Z Preparation work included uploading and parsing the sourced PDF documents in the Corpus Conversion Service (CCS) [22], a cloud-native platform which provides a visual annotation interface and allows for dataset inspection and analysis. The annotation interface of CCS is shown in Figure 3. The desired balance of pages between the different document categories was achieved by selective subsampling of pages with certain desired properties. For example, we made sure to include the title page of each document and bias the remaining page selection to those with figures or tables. The latter was achieved by leveraging pre-trained object detection models from PubLayNet, which helped us estimate how many figures and tables a given page contains.
2025-04-14T16:18:22.8563829Z 
2025-04-14T16:18:22.8567210Z Phase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recurrent layout elements and lead us to the definition of 11 distinct class labels. These 11 class labels are Caption , Footnote , Formula List-item , , Pagefooter , Page-header , Picture , Section-header , Table , Text , and Title . Critical factors that were considered for the choice of these class labels were (1) the overall occurrence of the label, (2) the specificity of the label, (3) recognisability on a single page (i.e. no need for context from previous or next page) and (4) overall coverage of the page. Specificity ensures that the choice of label is not ambiguous, while coverage ensures that all meaningful items on a page can be annotated. We refrained from class labels that are very specific to a document category, such as Abstract in the Scientific Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation , as seen in DocBank, are often only distinguishable by discriminating on
2025-04-14T16:18:22.8570684Z 
2025-04-14T16:18:22.8570779Z 3 https://arxiv.org/
2025-04-14T16:18:22.8570906Z 
2025-04-14T16:18:22.8571285Z the textual content of an element, which goes beyond visual layout recognition, in particular outside the Scientific Articles category.
2025-04-14T16:18:22.8571730Z 
2025-04-14T16:18:22.8574331Z At first sight, the task of visual document-layout interpretation appears intuitive enough to obtain plausible annotations in most cases. However, during early trial-runs in the core team, we observed many cases in which annotators use different annotation styles, especially for documents with challenging layouts. For example, if a figure is presented with subfigures, one annotator might draw a single figure bounding-box, while another might annotate each subfigure separately. The same applies for lists, where one might annotate all list items in one block or each list item separately. In essence, we observed that challenging layouts would be annotated in different but plausible ways. To illustrate this, we show in Figure 4 multiple examples of plausible but inconsistent annotations on the same pages.
2025-04-14T16:18:22.8576767Z 
2025-04-14T16:18:22.8578129Z Obviously, this inconsistency in annotations is not desirable for datasets which are intended to be used for model training. To minimise these inconsistencies, we created a detailed annotation guideline. While perfect consistency across 40 annotation staff members is clearly not possible to achieve, we saw a huge improvement in annotation consistency after the introduction of our annotation guideline. A few selected, non-trivial highlights of the guideline are:
2025-04-14T16:18:22.8579567Z 
2025-04-14T16:18:22.8580115Z - (1) Every list-item is an individual object instance with class label List-item . This definition is different from PubLayNet and DocBank, where all list-items are grouped together into one List object.
2025-04-14T16:18:22.8581329Z - (2) A List-item is a paragraph with hanging indentation. Singleline elements can qualify as List-item if the neighbour elements expose hanging indentation. Bullet or enumeration symbols are not a requirement.
2025-04-14T16:18:22.8582192Z - (3) For every Caption , there must be exactly one corresponding Picture or Table .
2025-04-14T16:18:22.8582646Z - (4) Connected sub-pictures are grouped together in one Picture object.
2025-04-14T16:18:22.8583026Z - (5) Formula numbers are included in a Formula object.
2025-04-14T16:18:22.8583639Z - (6) Emphasised text (e.g. in italic or bold) at the beginning of a paragraph is not considered a Section-header , unless it appears exclusively on its own line.
2025-04-14T16:18:22.8584229Z 
2025-04-14T16:18:22.8584844Z The complete annotation guideline is over 100 pages long and a detailed description is obviously out of scope for this paper. Nevertheless, it will be made publicly available alongside with DocLayNet for future reference.
2025-04-14T16:18:22.8585646Z 
2025-04-14T16:18:22.8587955Z Phase 3: Training. After a first trial with a small group of people, we realised that providing the annotation guideline and a set of random practice pages did not yield the desired quality level for layout annotation. Therefore we prepared a subset of pages with two different complexity levels, each with a practice and an exam part. 974 pages were reference-annotated by one proficient core team member. Annotation staff were then given the task to annotate the same subsets (blinded from the reference). By comparing the annotations of each staff member with the reference annotations, we could quantify how closely their annotations matched the reference. Only after passing two exam levels with high annotation quality, staff were admitted into the production phase. Practice iterations
2025-04-14T16:18:22.8590294Z 
2025-04-14T16:18:22.8590364Z <!-- image -->
2025-04-14T16:18:22.8590473Z 
2025-04-14T16:18:22.8590637Z 05237a14f2524e3f53c8454b074409d05078038a6a36b770fcc8ec7e540deae0
2025-04-14T16:18:22.8590901Z 
2025-04-14T16:18:22.8591380Z Figure 4: Examples of plausible annotation alternatives for the same page. Criteria in our annotation guideline can resolve cases A to C, while the case D remains ambiguous.
2025-04-14T16:18:22.8591946Z 
2025-04-14T16:18:22.8592249Z were carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.
2025-04-14T16:18:22.8592636Z 
2025-04-14T16:18:22.8595758Z Phase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement (see Table 1). We wanted
2025-04-14T16:18:22.8598711Z 
2025-04-14T16:18:22.8600072Z Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.
2025-04-14T16:18:22.8601494Z 
2025-04-14T16:18:22.8601614Z |                | human   | MRCNN   | MRCNN   | FRCNN   | YOLO   |
2025-04-14T16:18:22.8601926Z |----------------|---------|---------|---------|---------|--------|
2025-04-14T16:18:22.8602202Z |                |         | R50     | R101    | R101    | v5x6   |
2025-04-14T16:18:22.8602478Z | Caption        | 84-89   | 68.4    | 71.5    | 70.1    | 77.7   |
2025-04-14T16:18:22.8602784Z | Footnote       | 83-91   | 70.9    | 71.8    | 73.7    | 77.2   |
2025-04-14T16:18:22.8603093Z | Formula        | 83-85   | 60.1    | 63.4    | 63.5    | 66.2   |
2025-04-14T16:18:22.8603400Z | List-item      | 87-88   | 81.2    | 80.8    | 81.0    | 86.2   |
2025-04-14T16:18:22.8603720Z | Page-footer    | 93-94   | 61.6    | 59.3    | 58.9    | 61.1   |
2025-04-14T16:18:22.8604150Z | Page-header    | 85-89   | 71.9    | 70.0    | 72.0    | 67.9   |
2025-04-14T16:18:22.8604579Z | Picture        | 69-71   | 71.7    | 72.7    | 72.0    | 77.1   |
2025-04-14T16:18:22.8604905Z | Section-header | 83-84   | 67.6    | 69.3    | 68.4    | 74.6   |
2025-04-14T16:18:22.8605218Z | Table          | 77-81   | 82.2    | 82.9    | 82.2    | 86.3   |
2025-04-14T16:18:22.8605502Z | Text           | 84-86   | 84.6    | 85.8    | 85.4    | 88.1   |
2025-04-14T16:18:22.8605785Z | Title          | 60-72   | 76.7    | 80.4    | 79.9    | 82.7   |
2025-04-14T16:18:22.8606065Z | All            | 82-83   | 72.4    | 73.5    | 73.4    | 76.8   |
2025-04-14T16:18:22.8606242Z 
2025-04-14T16:18:22.8610018Z to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.
2025-04-14T16:18:22.8614025Z 
2025-04-14T16:18:22.8614099Z ## 5 EXPERIMENTS
2025-04-14T16:18:22.8614215Z 
2025-04-14T16:18:22.8615873Z The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this
2025-04-14T16:18:22.8617593Z 
2025-04-14T16:18:22.8618543Z Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.
2025-04-14T16:18:22.8619568Z 
2025-04-14T16:18:22.8619640Z <!-- image -->
2025-04-14T16:18:22.8619749Z 
2025-04-14T16:18:22.8620012Z paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.
2025-04-14T16:18:22.8620367Z 
2025-04-14T16:18:22.8621428Z In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].
2025-04-14T16:18:22.8622564Z 
2025-04-14T16:18:22.8622652Z ## Baselines for Object Detection
2025-04-14T16:18:22.8622814Z 
2025-04-14T16:18:22.8626873Z In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.
2025-04-14T16:18:22.8630430Z 
2025-04-14T16:18:22.8630972Z Table 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obtained by either down-mapping or dropping labels.
2025-04-14T16:18:22.8631588Z 
2025-04-14T16:18:22.8631703Z | Class-count    |   11 | 6       | 5       | 4       |
2025-04-14T16:18:22.8631992Z |----------------|------|---------|---------|---------|
2025-04-14T16:18:22.8632292Z | Caption        |   68 | Text    | Text    | Text    |
2025-04-14T16:18:22.8632572Z | Footnote       |   71 | Text    | Text    | Text    |
2025-04-14T16:18:22.8632848Z | Formula        |   60 | Text    | Text    | Text    |
2025-04-14T16:18:22.8633125Z | List-item      |   81 | Text    | 82      | Text    |
2025-04-14T16:18:22.8633542Z | Page-footer    |   62 | 62      | -       | -       |
2025-04-14T16:18:22.8633832Z | Page-header    |   72 | 68      | -       | -       |
2025-04-14T16:18:22.8634235Z | Picture        |   72 | 72      | 72      | 72      |
2025-04-14T16:18:22.8634514Z | Section-header |   68 | 67      | 69      | 68      |
2025-04-14T16:18:22.8634789Z | Table          |   82 | 83      | 82      | 82      |
2025-04-14T16:18:22.8635045Z | Text           |   85 | 84      | 84      | 84      |
2025-04-14T16:18:22.8635311Z | Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |
2025-04-14T16:18:22.8635582Z | Overall        |   72 | 73      | 78      | 77      |
2025-04-14T16:18:22.8635755Z 
2025-04-14T16:18:22.8635831Z ## Learning Curve
2025-04-14T16:18:22.8635939Z 
2025-04-14T16:18:22.8639107Z One of the fundamental questions related to any dataset is if it is 'large enough'. To answer this question for DocLayNet, we performed a data ablation study in which we evaluated a Mask R-CNN model trained on increasing fractions of the DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in the beginning and eventually levels out. To estimate the error-bar on the metrics, we ran the training five times on the entire data-set. This resulted in a 1% error-bar, depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the exact same data-points, but with a logarithmic scale on the x-axis. As is expected, the mAP score increases linearly as a function of the data-size in the inset. The curve ultimately flattens out between the 80% and 100% mark, with the 80% mark falling within the error-bars of the 100% mark. This provides a good indication that the model would not improve significantly by yet increasing the data size. Rather, it would probably benefit more from improved data consistency (as discussed in Section 3), data augmentation methods [23], or the addition of more document categories and styles.
2025-04-14T16:18:22.8642482Z 
2025-04-14T16:18:22.8642564Z ## Impact of Class Labels
2025-04-14T16:18:22.8642704Z 
2025-04-14T16:18:22.8645897Z The choice and number of labels can have a significant effect on the overall model performance. Since PubLayNet, DocBank and DocLayNet all have different label sets, it is of particular interest to understand and quantify this influence of the label set on the model performance. We investigate this by either down-mapping labels into more common ones (e.g. Caption → Text ) or excluding them from the annotations entirely. Furthermore, it must be stressed that all mappings and exclusions were performed on the data before model training. In Table 3, we present the mAP scores for a Mask R-CNN R50 network on different label sets. Where a label is down-mapped, we show its corresponding label, otherwise it was excluded. We present three different label sets, with 6, 5 and 4 different labels respectively. The set of 5 labels contains the same labels as PubLayNet. However, due to the different definition of
2025-04-14T16:18:22.8648647Z 
2025-04-14T16:18:22.8649153Z Table 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise split for different label sets. Naive page-wise split will result in /tildelow 10% point improvement.
2025-04-14T16:18:22.8649725Z 
2025-04-14T16:18:22.8649820Z | Class-count    | 11   | 11   | 5   | 5    |
2025-04-14T16:18:22.8650074Z |----------------|------|------|-----|------|
2025-04-14T16:18:22.8650337Z | Split          | Doc  | Page | Doc | Page |
2025-04-14T16:18:22.8650592Z | Caption        | 68   | 83   |     |      |
2025-04-14T16:18:22.8650833Z | Footnote       | 71   | 84   |     |      |
2025-04-14T16:18:22.8651072Z | Formula        | 60   | 66   |     |      |
2025-04-14T16:18:22.8651316Z | List-item      | 81   | 88   | 82  | 88   |
2025-04-14T16:18:22.8651562Z | Page-footer    | 62   | 89   |     |      |
2025-04-14T16:18:22.8651808Z | Page-header    | 72   | 90   |     |      |
2025-04-14T16:18:22.8652055Z | Picture        | 72   | 82   | 72  | 82   |
2025-04-14T16:18:22.8652418Z | Section-header | 68   | 83   | 69  | 83   |
2025-04-14T16:18:22.8652667Z | Table          | 82   | 89   | 82  | 90   |
2025-04-14T16:18:22.8652897Z | Text           | 85   | 91   | 84  | 90   |
2025-04-14T16:18:22.8653130Z | Title          | 77   | 81   |     |      |
2025-04-14T16:18:22.8653358Z | All            | 72   | 84   | 78  | 87   |
2025-04-14T16:18:22.8653527Z 
2025-04-14T16:18:22.8654966Z lists in PubLayNet (grouped list-items) versus DocLayNet (separate list-items), the label set of size 4 is the closest to PubLayNet, in the assumption that the List is down-mapped to Text in PubLayNet. The results in Table 3 show that the prediction accuracy on the remaining class labels does not change significantly when other classes are merged into them. The overall macro-average improves by around 5%, in particular when Page-footer and Page-header are excluded.
2025-04-14T16:18:22.8656368Z 
2025-04-14T16:18:22.8656478Z ## Impact of Document Split in Train and Test Set
2025-04-14T16:18:22.8656678Z 
2025-04-14T16:18:22.8659598Z Many documents in DocLayNet have a unique styling. In order to avoid overfitting on a particular style, we have split the train-, test- and validation-sets of DocLayNet on document boundaries, i.e. every document contributes pages to only one set. To the best of our knowledge, this was not considered in PubLayNet or DocBank. To quantify how this affects model performance, we trained and evaluated a Mask R-CNN R50 model on a modified dataset version. Here, the train-, test- and validation-sets were obtained by a randomised draw over the individual pages. As can be seen in Table 4, the difference in model performance is surprisingly large: pagewise splitting gains ˜ 0% in mAP over the document-wise splitting. 1 Thus, random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance and should be avoided.
2025-04-14T16:18:22.8662060Z 
2025-04-14T16:18:22.8662142Z ## Dataset Comparison
2025-04-14T16:18:22.8662261Z 
2025-04-14T16:18:22.8663725Z Throughout this paper, we claim that DocLayNet's wider variety of document layouts leads to more robust layout detection models. In Table 5, we provide evidence for that. We trained models on each of the available datasets (PubLayNet, DocBank and DocLayNet) and evaluated them on the test sets of the other datasets. Due to the different label sets and annotation styles, a direct comparison is not possible. Hence, we focussed on the common labels among the datasets. Between PubLayNet and DocLayNet, these are Picture ,
2025-04-14T16:18:22.8665478Z 
2025-04-14T16:18:22.8666306Z Table 5: Prediction Performance (mAP@0.5-0.95) of a Mask R-CNN R50 network across the PubLayNet, DocBank &amp; DocLayNet data-sets. By evaluating on common label classes of each dataset, we observe that the DocLayNet-trained model has much less pronounced variations in performance across all datasets.
2025-04-14T16:18:22.8667215Z 
2025-04-14T16:18:22.8667328Z |                 |            | Testing on   | Testing on   | Testing on   |
2025-04-14T16:18:22.8667661Z |-----------------|------------|--------------|--------------|--------------|
2025-04-14T16:18:22.8668017Z | Training on     | labels     | PLN          | DB           | DLN          |
2025-04-14T16:18:22.8668386Z | PubLayNet (PLN) | Figure     | 96           | 43           | 23           |
2025-04-14T16:18:22.8668878Z | PubLayNet (PLN) | Sec-header | 87           | -            | 32           |
2025-04-14T16:18:22.8669222Z |                 | Table      | 95           | 24           | 49           |
2025-04-14T16:18:22.8669506Z |                 | Text       | 96           | -            | 42           |
2025-04-14T16:18:22.8669791Z |                 | total      | 93           | 34           | 30           |
2025-04-14T16:18:22.8670112Z | DocBank (DB)    | Figure     | 77           | 71           | 31           |
2025-04-14T16:18:22.8670450Z | DocBank (DB)    | Table      | 19           | 65           | 22           |
2025-04-14T16:18:22.8670906Z | DocBank (DB)    | total      | 48           | 68           | 27           |
2025-04-14T16:18:22.8671265Z | DocLayNet (DLN) | Figure     | 67           | 51           | 72           |
2025-04-14T16:18:22.8671639Z | DocLayNet (DLN) | Sec-header | 53           | -            | 68           |
2025-04-14T16:18:22.8671975Z |                 | Table      | 87           | 43           | 82           |
2025-04-14T16:18:22.8672266Z |                 | Text       | 77           | -            | 84           |
2025-04-14T16:18:22.8672550Z |                 | total      | 59           | 47           | 78           |
2025-04-14T16:18:22.8672735Z 
2025-04-14T16:18:22.8673551Z Section-header , Table and Text . Before training, we either mapped or excluded DocLayNet's other labels as specified in table 3, and also PubLayNet's List to Text . Note that the different clustering of lists (by list-element vs. whole list objects) naturally decreases the mAP score for Text .
2025-04-14T16:18:22.8674558Z 
2025-04-14T16:18:22.8676852Z For comparison of DocBank with DocLayNet, we trained only on Picture and Table clusters of each dataset. We had to exclude Text because successive paragraphs are often grouped together into a single object in DocBank. This paragraph grouping is incompatible with the individual paragraphs of DocLayNet. As can be seen in Table 5, DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.
2025-04-14T16:18:22.8679197Z 
2025-04-14T16:18:22.8679276Z ## Example Predictions
2025-04-14T16:18:22.8679407Z 
2025-04-14T16:18:22.8680821Z To conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.
2025-04-14T16:18:22.8682438Z 
2025-04-14T16:18:22.8682507Z ## 6 CONCLUSION
2025-04-14T16:18:22.8682621Z 
2025-04-14T16:18:22.8684192Z In this paper, we presented the DocLayNet dataset. It provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. In contrast to many other datasets, DocLayNet was created by human annotation in order to obtain reliable layout ground-truth on a wide variety of publication- and typesettingstyles. Including a large proportion of documents outside the scientific publishing domain adds significant value in this respect.
2025-04-14T16:18:22.8685745Z 
2025-04-14T16:18:22.8687481Z From the dataset, we have derived on the one hand reference metrics for human performance on document-layout annotation (through double and triple annotations) and on the other hand evaluated the baseline performance of commonly used object detection methods. We also illustrated the impact of various dataset-related aspects on model performance through data-ablation experiments, both from a size and class-label perspective. Last but not least, we compared the accuracy of models trained on other public datasets and showed that DocLayNet trained models are more robust.
2025-04-14T16:18:22.8689223Z 
2025-04-14T16:18:22.8689713Z To date, there is still a significant gap between human and ML accuracy on the layout interpretation task, and we hope that this work will inspire the research community to close that gap.
2025-04-14T16:18:22.8690289Z 
2025-04-14T16:18:22.8690355Z ## REFERENCES
2025-04-14T16:18:22.8690454Z 
2025-04-14T16:18:22.8691275Z - [1] Max Göbel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition , pages 1449-1453, 2013.
2025-04-14T16:18:22.8695986Z - [2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Icdar2017 competition on recognition of documents with complex layouts rdcl2017. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR) , volume 01, pages 1404-1410, 2017.
2025-04-14T16:18:22.8699026Z - [3] Hervé Déjean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition (cTDaR), April 2019. http://sac.founderit.com/.
2025-04-14T16:18:22.8701452Z - [4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on scientific literature parsing. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 605-617. LNCS 12824, SpringerVerlag, sep 2021.
2025-04-14T16:18:22.8704298Z - [5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis: not dead yet. International Journal on Document Analysis and Recognition (IJDAR) , pages 1-11, 01 2022.
2025-04-14T16:18:22.8706893Z - [6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset ever for document layout analysis. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 1015-1022, sep 2019.
2025-04-14T16:18:22.8709843Z - [7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics , COLING, pages 949-960. International Committee on Computational Linguistics, dec 2020.
2025-04-14T16:18:22.8711439Z - [8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC , 2016.
2025-04-14T16:18:22.8712770Z - [9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition , CVPR, pages 580-587. IEEE Computer Society, jun 2014.
2025-04-14T16:18:22.8714461Z - [10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision , ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.
2025-04-14T16:18:22.8715553Z - [11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(6):1137-1149, 2017.
2025-04-14T16:18:22.8716958Z - [12] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision , ICCV, pages 2980-2988. IEEE Computer Society, Oct 2017.
2025-04-14T16:18:22.8718419Z - [13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012, TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu
2025-04-14T16:18:22.8719372Z 
2025-04-14T16:18:22.8719718Z Text Caption List-Item Formula Table Section-Header Picture Page-Header Page-Footer Title © © @
2025-04-14T16:18:22.8720060Z 
2025-04-14T16:18:22.8720136Z <!-- image -->
2025-04-14T16:18:22.8720245Z 
2025-04-14T16:18:22.8721372Z Figure 6: Example layout predictions on selected pages from the DocLayNet test-set. (A, D) exhibit favourable results on coloured backgrounds. (B, C) show accurate list-item and paragraph differentiation despite densely-spaced lines. (E) demonstrates good table and figure distinction. (F) shows predictions on a Chinese patent with multiple overlaps, label confusion and missing boxes.
2025-04-14T16:18:22.8722551Z 
2025-04-14T16:18:22.8723244Z Diaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ultralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow export, opencv dnn support, October 2021.
2025-04-14T16:18:22.8724039Z 
2025-04-14T16:18:22.8724429Z - [20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout: Fusion of visual and text features for document layout analysis, 2021.
2025-04-14T16:18:22.8725423Z - [14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR , abs/2005.12872, 2020.
2025-04-14T16:18:22.8726354Z - [15] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. CoRR , abs/1911.09070, 2019.
2025-04-14T16:18:22.8727507Z - [16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014.
2025-04-14T16:18:22.8728420Z - [17] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.
2025-04-14T16:18:22.8729720Z - [18] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks. In Proceedings of the 35th Conference on Artificial Intelligence , AAAI, pages 1513715145, feb 2021.
2025-04-14T16:18:22.8731695Z - [19] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 1192-1200, New York, USA, 2020. Association for Computing Machinery.
2025-04-14T16:18:22.8733177Z - [21] Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei Wu. Vsr: A unified framework for document layout analysis combining vision, semantics and relations, 2021.
2025-04-14T16:18:22.8734741Z - [22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 774-782. ACM, 2018.
2025-04-14T16:18:22.8736127Z - [23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data , 6(1):60, 2019.
2025-04-14T16:18:23.7629698Z Running example docs/examples/translate.py
2025-04-14T16:18:29.5751805Z Skipping docs/examples/vlm_pipeline_api_model.py
2025-04-14T16:18:29.5779943Z ##[group]Run poetry build
2025-04-14T16:18:29.5780215Z [36;1mpoetry build[0m
2025-04-14T16:18:29.5830393Z shell: /usr/bin/bash -e {0}
2025-04-14T16:18:29.5830629Z env:
2025-04-14T16:18:29.5830803Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-14T16:18:29.5831028Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-14T16:18:29.5831287Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-14T16:18:29.5831645Z   pythonLocation: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T16:18:29.5832044Z   PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.11.11/x64/lib/pkgconfig
2025-04-14T16:18:29.5832446Z   Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T16:18:29.5832785Z   Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T16:18:29.5833137Z   Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.11/x64
2025-04-14T16:18:29.5833486Z   LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.11.11/x64/lib
2025-04-14T16:18:29.5833776Z ##[endgroup]
2025-04-14T16:18:30.0187798Z Building docling (2.30.0)
2025-04-14T16:18:30.0238608Z   - Building sdist
2025-04-14T16:18:30.2075063Z   - Built docling-2.30.0.tar.gz
2025-04-14T16:18:30.2088396Z   - Building wheel
2025-04-14T16:18:30.2852087Z   - Built docling-2.30.0-py3-none-any.whl
2025-04-14T16:18:30.3312596Z Post job cleanup.
2025-04-14T16:18:30.3344401Z Post job cleanup.
2025-04-14T16:18:30.4873719Z Cache hit occurred on the primary key setup-python-Linux-x64-python-3.11.11-poetry-v2-de9cc18c777c5ffb2a903b64fdaa7ce75e28e674116098f03bcf95bf424206d0, not saving cache.
2025-04-14T16:18:30.4958365Z Post job cleanup.
2025-04-14T16:18:30.6342454Z Cache hit occurred on the primary key huggingface-cache-py3.11, not saving cache.
2025-04-14T16:18:30.6433804Z Post job cleanup.
2025-04-14T16:18:30.7417897Z [command]/usr/bin/git version
2025-04-14T16:18:30.7454016Z git version 2.49.0
2025-04-14T16:18:30.7497501Z Temporarily overriding HOME='/home/runner/work/_temp/6a603a2c-fe1b-4f6b-8a92-a95322beb817' before making global git config changes
2025-04-14T16:18:30.7498826Z Adding repository directory to the temporary git global config as a safe directory
2025-04-14T16:18:30.7510516Z [command]/usr/bin/git config --global --add safe.directory /home/runner/work/docling/docling
2025-04-14T16:18:30.7545447Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2025-04-14T16:18:30.7578533Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2025-04-14T16:18:30.7820418Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2025-04-14T16:18:30.7841103Z http.https://github.com/.extraheader
2025-04-14T16:18:30.7854235Z [command]/usr/bin/git config --local --unset-all http.https://github.com/.extraheader
2025-04-14T16:18:30.7885176Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2025-04-14T16:18:30.8219495Z Cleaning up orphan processes
