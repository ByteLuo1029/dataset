2025-04-11T03:22:02.8585794Z Requested labels: gpu-h20-10
2025-04-11T03:22:02.8585978Z Job defined at: hpcaitech/ColossalAI/.github/workflows/build_on_pr.yml@refs/pull/6254/merge
2025-04-11T03:22:02.8586064Z Waiting for a runner to pick up this job...
2025-04-11T03:22:03.0405817Z Job is about to start running on the runner: gpu-h20-10 (repository)
2025-04-11T03:22:09.8501470Z Current runner version: '2.323.0'
2025-04-11T03:22:09.8507283Z Runner name: 'gpu-h20-10'
2025-04-11T03:22:09.8508235Z Runner group name: 'Default'
2025-04-11T03:22:09.8509368Z Machine name: 'gpu-h20-10'
2025-04-11T03:22:09.8514006Z ##[group]GITHUB_TOKEN Permissions
2025-04-11T03:22:09.8516348Z Actions: read
2025-04-11T03:22:09.8517164Z Attestations: read
2025-04-11T03:22:09.8517866Z Checks: read
2025-04-11T03:22:09.8518541Z Contents: read
2025-04-11T03:22:09.8519200Z Deployments: read
2025-04-11T03:22:09.8519903Z Discussions: read
2025-04-11T03:22:09.8520580Z Issues: read
2025-04-11T03:22:09.8521247Z Metadata: read
2025-04-11T03:22:09.8521924Z Models: read
2025-04-11T03:22:09.8522537Z Packages: read
2025-04-11T03:22:09.8523208Z Pages: read
2025-04-11T03:22:09.8523849Z PullRequests: read
2025-04-11T03:22:09.8524607Z RepositoryProjects: read
2025-04-11T03:22:09.8525328Z SecurityEvents: read
2025-04-11T03:22:09.8525991Z Statuses: read
2025-04-11T03:22:09.8526643Z ##[endgroup]
2025-04-11T03:22:09.8529801Z Secret source: None
2025-04-11T03:22:09.8531105Z Runner is running behind proxy server 'http://vpn.luchentech.com:32171' for all HTTP requests.
2025-04-11T03:22:09.8532661Z Runner is running behind proxy server 'http://vpn.luchentech.com:32171' for all HTTPS requests.
2025-04-11T03:22:09.8533861Z Prepare workflow directory
2025-04-11T03:22:09.8960761Z Prepare all required actions
2025-04-11T03:22:09.8996534Z Getting action download info
2025-04-11T03:22:10.3525353Z Download action repository 'actions/checkout@v2' (SHA:ee0669bd1cc54295c223e0bb666b733df41de1c5)
2025-04-11T03:22:11.8073107Z Download action repository 'actions/upload-artifact@v4' (SHA:ea165f8d65b6e75b540449e92b4886f43607fa02)
2025-04-11T03:22:13.9184664Z Complete job name: Build and Test Colossal-AI
2025-04-11T03:22:14.0750007Z ##[group]Checking docker version
2025-04-11T03:22:14.0761188Z ##[command]/usr/bin/docker version --format '{{.Server.APIVersion}}'
2025-04-11T03:22:14.1011936Z '1.41'
2025-04-11T03:22:14.1023072Z Docker daemon API version: '1.41'
2025-04-11T03:22:14.1023539Z ##[command]/usr/bin/docker version --format '{{.Client.APIVersion}}'
2025-04-11T03:22:14.1240742Z '1.41'
2025-04-11T03:22:14.1251368Z Docker client API version: '1.41'
2025-04-11T03:22:14.1255583Z ##[endgroup]
2025-04-11T03:22:14.1257401Z ##[group]Clean up resources from previous jobs
2025-04-11T03:22:14.1261569Z ##[command]/usr/bin/docker ps --all --quiet --no-trunc --filter "label=81704a"
2025-04-11T03:22:14.1436838Z ##[command]/usr/bin/docker network prune --force --filter "label=81704a"
2025-04-11T03:22:14.1602254Z ##[endgroup]
2025-04-11T03:22:14.1602520Z ##[group]Create local container network
2025-04-11T03:22:14.1609952Z ##[command]/usr/bin/docker network create --label 81704a github_network_b05f10a0d18f44629e5127de4c67c8e5
2025-04-11T03:22:14.1989033Z 216a75110e1fcb32b349cd9792b2c1ff6ff232679fc31936e6d7b2e3cf24ee22
2025-04-11T03:22:14.2003422Z ##[endgroup]
2025-04-11T03:22:14.2025196Z ##[group]Starting job container
2025-04-11T03:22:14.2042199Z ##[command]/usr/bin/docker pull image-cloud.luchentech.com/hpcaitech/pytorch-cuda:2.2.2-12.1.0
2025-04-11T03:22:14.7818018Z 2.2.2-12.1.0: Pulling from hpcaitech/pytorch-cuda
2025-04-11T03:22:14.7831169Z Digest: sha256:c2a777c1361b156ea23925ab311db35b1b450443d25dc292205058aeffea7e64
2025-04-11T03:22:14.7831697Z Status: Image is up to date for image-cloud.luchentech.com/hpcaitech/pytorch-cuda:2.2.2-12.1.0
2025-04-11T03:22:14.7837727Z image-cloud.luchentech.com/hpcaitech/pytorch-cuda:2.2.2-12.1.0
2025-04-11T03:22:14.7906867Z ##[command]/usr/bin/docker create --name a6f9b4cb19ae4283bedcab830f5b7400_imagecloudluchentechcomhpcaitechpytorchcuda2221210_b76ab0 --label 81704a --workdir /__w/ColossalAI/ColossalAI --network github_network_b05f10a0d18f44629e5127de4c67c8e5 --gpus all --shm-size=2g --rm -v /dev/shm -v /data/scratch:/data/scratch -e "HTTP_PROXY=http://vpn.luchentech.com:32171" -e "http_proxy=http://vpn.luchentech.com:32171" -e "HTTPS_PROXY=http://vpn.luchentech.com:32171" -e "https_proxy=http://vpn.luchentech.com:32171" -e "HOME=/github/home" -e GITHUB_ACTIONS=true -e CI=true -v "/var/run/docker.sock":"/var/run/docker.sock" -v "/root/actions-runner/github-gpu":"/__w" -v "/root/actions-runner/externals":"/__e":ro -v "/root/actions-runner/github-gpu/_temp":"/__w/_temp" -v "/root/actions-runner/github-gpu/_actions":"/__w/_actions" -v "/root/actions-runner/github-gpu/_tool":"/__w/_tool" -v "/root/actions-runner/github-gpu/_temp/_github_home":"/github/home" -v "/root/actions-runner/github-gpu/_temp/_github_workflow":"/github/workflow" --entrypoint "tail" image-cloud.luchentech.com/hpcaitech/pytorch-cuda:2.2.2-12.1.0 "-f" "/dev/null"
2025-04-11T03:22:14.8155676Z a22674922e87e0d0962e2f956706f403d9456d353f40b8411642994284af24b7
2025-04-11T03:22:14.8175663Z ##[command]/usr/bin/docker start a22674922e87e0d0962e2f956706f403d9456d353f40b8411642994284af24b7
2025-04-11T03:22:15.6068009Z a22674922e87e0d0962e2f956706f403d9456d353f40b8411642994284af24b7
2025-04-11T03:22:15.6085490Z ##[command]/usr/bin/docker ps --all --filter id=a22674922e87e0d0962e2f956706f403d9456d353f40b8411642994284af24b7 --filter status=running --no-trunc --format "{{.ID}} {{.Status}}"
2025-04-11T03:22:15.6262784Z a22674922e87e0d0962e2f956706f403d9456d353f40b8411642994284af24b7 Up Less than a second
2025-04-11T03:22:15.6279743Z ##[command]/usr/bin/docker inspect --format "{{range .Config.Env}}{{println .}}{{end}}" a22674922e87e0d0962e2f956706f403d9456d353f40b8411642994284af24b7
2025-04-11T03:22:15.6443858Z GITHUB_ACTIONS=true
2025-04-11T03:22:15.6444106Z CI=true
2025-04-11T03:22:15.6444376Z HTTP_PROXY=http://vpn.luchentech.com:32171
2025-04-11T03:22:15.6444710Z http_proxy=http://vpn.luchentech.com:32171
2025-04-11T03:22:15.6445081Z HTTPS_PROXY=http://vpn.luchentech.com:32171
2025-04-11T03:22:15.6445654Z https_proxy=http://vpn.luchentech.com:32171
2025-04-11T03:22:15.6445970Z HOME=/github/home
2025-04-11T03:22:15.6446520Z PATH=/opt/conda/envs/pytorch/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
2025-04-11T03:22:15.6447086Z NVARCH=x86_64
2025-04-11T03:22:15.6448979Z NVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526
2025-04-11T03:22:15.6450895Z NV_CUDA_CUDART_VERSION=12.1.55-1
2025-04-11T03:22:15.6451185Z NV_CUDA_COMPAT_PACKAGE=cuda-compat-12-1
2025-04-11T03:22:15.6451464Z CUDA_VERSION=12.1.0
2025-04-11T03:22:15.6451754Z LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2025-04-11T03:22:15.6452075Z NVIDIA_VISIBLE_DEVICES=all
2025-04-11T03:22:15.6452333Z NVIDIA_DRIVER_CAPABILITIES=compute,utility
2025-04-11T03:22:15.6452754Z NV_CUDA_LIB_VERSION=12.1.0-1
2025-04-11T03:22:15.6452999Z NV_NVTX_VERSION=12.1.66-1
2025-04-11T03:22:15.6453240Z NV_LIBNPP_VERSION=12.0.2.50-1
2025-04-11T03:22:15.6453500Z NV_LIBNPP_PACKAGE=libnpp-12-1=12.0.2.50-1
2025-04-11T03:22:15.6453782Z NV_LIBCUSPARSE_VERSION=12.0.2.55-1
2025-04-11T03:22:15.6454050Z NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-1
2025-04-11T03:22:15.6454330Z NV_LIBCUBLAS_VERSION=12.1.0.26-1
2025-04-11T03:22:15.6454667Z NV_LIBCUBLAS_PACKAGE=libcublas-12-1=12.1.0.26-1
2025-04-11T03:22:15.6454956Z NV_LIBNCCL_PACKAGE_NAME=libnccl2
2025-04-11T03:22:15.6455212Z NV_LIBNCCL_PACKAGE_VERSION=2.17.1-1
2025-04-11T03:22:15.6455476Z NCCL_VERSION=2.17.1-1
2025-04-11T03:22:15.6455719Z NV_LIBNCCL_PACKAGE=libnccl2=2.17.1-1+cuda12.1
2025-04-11T03:22:15.6456001Z NVIDIA_PRODUCT_NAME=CUDA
2025-04-11T03:22:15.6456376Z NVIDIA_CUDA_END_OF_LIFE=1
2025-04-11T03:22:15.6456622Z NV_CUDA_CUDART_DEV_VERSION=12.1.55-1
2025-04-11T03:22:15.6456886Z NV_NVML_DEV_VERSION=12.1.55-1
2025-04-11T03:22:15.6457140Z NV_LIBCUSPARSE_DEV_VERSION=12.0.2.55-1
2025-04-11T03:22:15.6457452Z NV_LIBNPP_DEV_VERSION=12.0.2.50-1
2025-04-11T03:22:15.6457734Z NV_LIBNPP_DEV_PACKAGE=libnpp-dev-12-1=12.0.2.50-1
2025-04-11T03:22:15.6458024Z NV_LIBCUBLAS_DEV_VERSION=12.1.0.26-1
2025-04-11T03:22:15.6458309Z NV_LIBCUBLAS_DEV_PACKAGE_NAME=libcublas-dev-12-1
2025-04-11T03:22:15.6458628Z NV_LIBCUBLAS_DEV_PACKAGE=libcublas-dev-12-1=12.1.0.26-1
2025-04-11T03:22:15.6458925Z NV_CUDA_NSIGHT_COMPUTE_VERSION=12.1.0-1
2025-04-11T03:22:15.6459265Z NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-1=12.1.0-1
2025-04-11T03:22:15.6459599Z NV_NVPROF_VERSION=12.1.55-1
2025-04-11T03:22:15.6459854Z NV_NVPROF_DEV_PACKAGE=cuda-nvprof-12-1=12.1.55-1
2025-04-11T03:22:15.6460148Z NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
2025-04-11T03:22:15.6460407Z NV_LIBNCCL_DEV_PACKAGE_VERSION=2.17.1-1
2025-04-11T03:22:15.6460699Z NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.17.1-1+cuda12.1
2025-04-11T03:22:15.6460991Z LIBRARY_PATH=/usr/local/cuda/lib64/stubs
2025-04-11T03:22:15.6461248Z NV_CUDNN_VERSION=8.9.0.131
2025-04-11T03:22:15.6461473Z NV_CUDNN_PACKAGE_NAME=libcudnn8
2025-04-11T03:22:15.6461735Z NV_CUDNN_PACKAGE=libcudnn8=8.9.0.131-1+cuda12.1
2025-04-11T03:22:15.6462037Z NV_CUDNN_PACKAGE_DEV=libcudnn8-dev=8.9.0.131-1+cuda12.1
2025-04-11T03:22:15.6462317Z CUDA_HOME=/usr/local/cuda
2025-04-11T03:22:15.6468644Z ##[endgroup]
2025-04-11T03:22:15.6476680Z ##[group]Waiting for all services to be ready
2025-04-11T03:22:15.6477982Z ##[endgroup]
2025-04-11T03:22:15.6672312Z ##[group]Run actions/checkout@v2
2025-04-11T03:22:15.6672760Z with:
2025-04-11T03:22:15.6672993Z   repository: hpcaitech/TensorNVMe
2025-04-11T03:22:15.6673324Z   path: TensorNVMe
2025-04-11T03:22:15.6673720Z   token: ***
2025-04-11T03:22:15.6674042Z   ssh-strict: true
2025-04-11T03:22:15.6674299Z   persist-credentials: true
2025-04-11T03:22:15.6674650Z   clean: true
2025-04-11T03:22:15.6674949Z   fetch-depth: 1
2025-04-11T03:22:15.6675178Z   lfs: false
2025-04-11T03:22:15.6675396Z   submodules: false
2025-04-11T03:22:15.6675616Z   set-safe-directory: true
2025-04-11T03:22:15.6675981Z ##[endgroup]
2025-04-11T03:22:15.6724855Z ##[command]/usr/bin/docker exec  a22674922e87e0d0962e2f956706f403d9456d353f40b8411642994284af24b7 sh -c "cat /etc/*release | grep ^ID"
2025-04-11T03:22:15.8926879Z Syncing repository: hpcaitech/TensorNVMe
2025-04-11T03:22:15.8928016Z ##[group]Getting Git version info
2025-04-11T03:22:15.8928379Z Working directory is '/__w/ColossalAI/ColossalAI/TensorNVMe'
2025-04-11T03:22:15.8928943Z [command]/usr/bin/git version
2025-04-11T03:22:15.8929189Z git version 2.25.1
2025-04-11T03:22:15.8942665Z ##[endgroup]
2025-04-11T03:22:15.8952466Z Temporarily overriding HOME='/__w/_temp/755ba090-b19a-4c7d-8f9c-695d5bc8c58e' before making global git config changes
2025-04-11T03:22:15.8953072Z Adding repository directory to the temporary git global config as a safe directory
2025-04-11T03:22:15.8955657Z [command]/usr/bin/git config --global --add safe.directory /__w/ColossalAI/ColossalAI/TensorNVMe
2025-04-11T03:22:15.8977729Z ##[group]Initializing the repository
2025-04-11T03:22:15.8980356Z [command]/usr/bin/git init /__w/ColossalAI/ColossalAI/TensorNVMe
2025-04-11T03:22:15.9002755Z Initialized empty Git repository in /__w/ColossalAI/ColossalAI/TensorNVMe/.git/
2025-04-11T03:22:15.9009811Z [command]/usr/bin/git remote add origin https://github.com/hpcaitech/TensorNVMe
2025-04-11T03:22:15.9027912Z ##[endgroup]
2025-04-11T03:22:15.9028301Z ##[group]Disabling automatic garbage collection
2025-04-11T03:22:15.9030968Z [command]/usr/bin/git config --local gc.auto 0
2025-04-11T03:22:15.9049860Z ##[endgroup]
2025-04-11T03:22:15.9050196Z ##[group]Setting up auth
2025-04-11T03:22:15.9055633Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2025-04-11T03:22:15.9076210Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2025-04-11T03:22:15.9242581Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2025-04-11T03:22:15.9260715Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2025-04-11T03:22:15.9415572Z [command]/usr/bin/git config --local http.https://github.com/.extraheader AUTHORIZATION: basic ***
2025-04-11T03:22:15.9438607Z ##[endgroup]
2025-04-11T03:22:15.9438967Z ##[group]Determining the default branch
2025-04-11T03:22:15.9441150Z Retrieving the default branch name
2025-04-11T03:22:16.6165874Z Default branch 'main'
2025-04-11T03:22:16.6166394Z ##[endgroup]
2025-04-11T03:22:16.6166791Z ##[group]Fetching the repository
2025-04-11T03:22:16.6170020Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --progress --no-recurse-submodules --depth=1 origin +refs/heads/main:refs/remotes/origin/main
2025-04-11T03:22:17.9630306Z remote: Enumerating objects: 60, done.        
2025-04-11T03:22:17.9700965Z remote: Counting objects:   1% (1/60)        
2025-04-11T03:22:17.9701253Z remote: Counting objects:   3% (2/60)        
2025-04-11T03:22:17.9701609Z remote: Counting objects:   5% (3/60)        
2025-04-11T03:22:17.9701893Z remote: Counting objects:   6% (4/60)        
2025-04-11T03:22:17.9702170Z remote: Counting objects:   8% (5/60)        
2025-04-11T03:22:17.9702673Z remote: Counting objects:  10% (6/60)        
2025-04-11T03:22:17.9702963Z remote: Counting objects:  11% (7/60)        
2025-04-11T03:22:17.9703238Z remote: Counting objects:  13% (8/60)        
2025-04-11T03:22:17.9703539Z remote: Counting objects:  15% (9/60)        
2025-04-11T03:22:17.9703831Z remote: Counting objects:  16% (10/60)        
2025-04-11T03:22:17.9704136Z remote: Counting objects:  18% (11/60)        
2025-04-11T03:22:17.9704416Z remote: Counting objects:  20% (12/60)        
2025-04-11T03:22:17.9704756Z remote: Counting objects:  21% (13/60)        
2025-04-11T03:22:17.9705033Z remote: Counting objects:  23% (14/60)        
2025-04-11T03:22:17.9705302Z remote: Counting objects:  25% (15/60)        
2025-04-11T03:22:17.9705576Z remote: Counting objects:  26% (16/60)        
2025-04-11T03:22:17.9705849Z remote: Counting objects:  28% (17/60)        
2025-04-11T03:22:17.9706122Z remote: Counting objects:  30% (18/60)        
2025-04-11T03:22:17.9706396Z remote: Counting objects:  31% (19/60)        
2025-04-11T03:22:17.9706665Z remote: Counting objects:  33% (20/60)        
2025-04-11T03:22:17.9706937Z remote: Counting objects:  35% (21/60)        
2025-04-11T03:22:17.9707208Z remote: Counting objects:  36% (22/60)        
2025-04-11T03:22:17.9707480Z remote: Counting objects:  38% (23/60)        
2025-04-11T03:22:17.9707750Z remote: Counting objects:  40% (24/60)        
2025-04-11T03:22:17.9708064Z remote: Counting objects:  41% (25/60)        
2025-04-11T03:22:17.9708371Z remote: Counting objects:  43% (26/60)        
2025-04-11T03:22:17.9708749Z remote: Counting objects:  45% (27/60)        
2025-04-11T03:22:17.9709040Z remote: Counting objects:  46% (28/60)        
2025-04-11T03:22:17.9709316Z remote: Counting objects:  48% (29/60)        
2025-04-11T03:22:17.9709595Z remote: Counting objects:  50% (30/60)        
2025-04-11T03:22:17.9709866Z remote: Counting objects:  51% (31/60)        
2025-04-11T03:22:17.9710146Z remote: Counting objects:  53% (32/60)        
2025-04-11T03:22:17.9710429Z remote: Counting objects:  55% (33/60)        
2025-04-11T03:22:17.9710709Z remote: Counting objects:  56% (34/60)        
2025-04-11T03:22:17.9710984Z remote: Counting objects:  58% (35/60)        
2025-04-11T03:22:17.9711255Z remote: Counting objects:  60% (36/60)        
2025-04-11T03:22:17.9711533Z remote: Counting objects:  61% (37/60)        
2025-04-11T03:22:17.9711948Z remote: Counting objects:  63% (38/60)        
2025-04-11T03:22:17.9712231Z remote: Counting objects:  65% (39/60)        
2025-04-11T03:22:17.9712512Z remote: Counting objects:  66% (40/60)        
2025-04-11T03:22:17.9712788Z remote: Counting objects:  68% (41/60)        
2025-04-11T03:22:17.9713073Z remote: Counting objects:  70% (42/60)        
2025-04-11T03:22:17.9713357Z remote: Counting objects:  71% (43/60)        
2025-04-11T03:22:17.9713643Z remote: Counting objects:  73% (44/60)        
2025-04-11T03:22:17.9713928Z remote: Counting objects:  75% (45/60)        
2025-04-11T03:22:17.9714215Z remote: Counting objects:  76% (46/60)        
2025-04-11T03:22:17.9714493Z remote: Counting objects:  78% (47/60)        
2025-04-11T03:22:17.9714776Z remote: Counting objects:  80% (48/60)        
2025-04-11T03:22:17.9715062Z remote: Counting objects:  81% (49/60)        
2025-04-11T03:22:17.9715336Z remote: Counting objects:  83% (50/60)        
2025-04-11T03:22:17.9715629Z remote: Counting objects:  85% (51/60)        
2025-04-11T03:22:17.9715912Z remote: Counting objects:  86% (52/60)        
2025-04-11T03:22:17.9716191Z remote: Counting objects:  88% (53/60)        
2025-04-11T03:22:17.9716469Z remote: Counting objects:  90% (54/60)        
2025-04-11T03:22:17.9716737Z remote: Counting objects:  91% (55/60)        
2025-04-11T03:22:17.9717016Z remote: Counting objects:  93% (56/60)        
2025-04-11T03:22:17.9717289Z remote: Counting objects:  95% (57/60)        
2025-04-11T03:22:17.9717567Z remote: Counting objects:  96% (58/60)        
2025-04-11T03:22:17.9717843Z remote: Counting objects:  98% (59/60)        
2025-04-11T03:22:17.9718216Z remote: Counting objects: 100% (60/60)        
2025-04-11T03:22:17.9718518Z remote: Counting objects: 100% (60/60), done.        
2025-04-11T03:22:17.9718837Z remote: Compressing objects:   1% (1/51)        
2025-04-11T03:22:17.9719138Z remote: Compressing objects:   3% (2/51)        
2025-04-11T03:22:17.9719427Z remote: Compressing objects:   5% (3/51)        
2025-04-11T03:22:17.9719754Z remote: Compressing objects:   7% (4/51)        
2025-04-11T03:22:17.9720084Z remote: Compressing objects:   9% (5/51)        
2025-04-11T03:22:17.9720364Z remote: Compressing objects:  11% (6/51)        
2025-04-11T03:22:17.9720670Z remote: Compressing objects:  13% (7/51)        
2025-04-11T03:22:17.9720962Z remote: Compressing objects:  15% (8/51)        
2025-04-11T03:22:17.9721246Z remote: Compressing objects:  17% (9/51)        
2025-04-11T03:22:17.9721534Z remote: Compressing objects:  19% (10/51)        
2025-04-11T03:22:17.9721822Z remote: Compressing objects:  21% (11/51)        
2025-04-11T03:22:17.9722114Z remote: Compressing objects:  23% (12/51)        
2025-04-11T03:22:17.9722405Z remote: Compressing objects:  25% (13/51)        
2025-04-11T03:22:17.9722694Z remote: Compressing objects:  27% (14/51)        
2025-04-11T03:22:17.9722978Z remote: Compressing objects:  29% (15/51)        
2025-04-11T03:22:17.9723255Z remote: Compressing objects:  31% (16/51)        
2025-04-11T03:22:17.9723543Z remote: Compressing objects:  33% (17/51)        
2025-04-11T03:22:17.9723834Z remote: Compressing objects:  35% (18/51)        
2025-04-11T03:22:17.9724129Z remote: Compressing objects:  37% (19/51)        
2025-04-11T03:22:17.9724416Z remote: Compressing objects:  39% (20/51)        
2025-04-11T03:22:17.9724706Z remote: Compressing objects:  41% (21/51)        
2025-04-11T03:22:17.9724989Z remote: Compressing objects:  43% (22/51)        
2025-04-11T03:22:17.9725277Z remote: Compressing objects:  45% (23/51)        
2025-04-11T03:22:17.9725562Z remote: Compressing objects:  47% (24/51)        
2025-04-11T03:22:17.9725850Z remote: Compressing objects:  49% (25/51)        
2025-04-11T03:22:17.9726138Z remote: Compressing objects:  50% (26/51)        
2025-04-11T03:22:17.9726417Z remote: Compressing objects:  52% (27/51)        
2025-04-11T03:22:17.9726704Z remote: Compressing objects:  54% (28/51)        
2025-04-11T03:22:17.9726990Z remote: Compressing objects:  56% (29/51)        
2025-04-11T03:22:17.9727377Z remote: Compressing objects:  58% (30/51)        
2025-04-11T03:22:17.9727660Z remote: Compressing objects:  60% (31/51)        
2025-04-11T03:22:17.9727948Z remote: Compressing objects:  62% (32/51)        
2025-04-11T03:22:17.9728245Z remote: Compressing objects:  64% (33/51)        
2025-04-11T03:22:17.9728544Z remote: Compressing objects:  66% (34/51)        
2025-04-11T03:22:17.9728848Z remote: Compressing objects:  68% (35/51)        
2025-04-11T03:22:17.9729156Z remote: Compressing objects:  70% (36/51)        
2025-04-11T03:22:17.9729462Z remote: Compressing objects:  72% (37/51)        
2025-04-11T03:22:17.9729759Z remote: Compressing objects:  74% (38/51)        
2025-04-11T03:22:17.9730059Z remote: Compressing objects:  76% (39/51)        
2025-04-11T03:22:17.9730360Z remote: Compressing objects:  78% (40/51)        
2025-04-11T03:22:17.9730656Z remote: Compressing objects:  80% (41/51)        
2025-04-11T03:22:17.9730951Z remote: Compressing objects:  82% (42/51)        
2025-04-11T03:22:17.9731243Z remote: Compressing objects:  84% (43/51)        
2025-04-11T03:22:17.9731535Z remote: Compressing objects:  86% (44/51)        
2025-04-11T03:22:17.9731826Z remote: Compressing objects:  88% (45/51)        
2025-04-11T03:22:17.9732120Z remote: Compressing objects:  90% (46/51)        
2025-04-11T03:22:17.9732412Z remote: Compressing objects:  92% (47/51)        
2025-04-11T03:22:17.9732695Z remote: Compressing objects:  94% (48/51)        
2025-04-11T03:22:17.9732987Z remote: Compressing objects:  96% (49/51)        
2025-04-11T03:22:17.9733280Z remote: Compressing objects:  98% (50/51)        
2025-04-11T03:22:17.9733666Z remote: Compressing objects: 100% (51/51)        
2025-04-11T03:22:17.9733979Z remote: Compressing objects: 100% (51/51), done.        
2025-04-11T03:22:18.1842812Z remote: Total 60 (delta 6), reused 24 (delta 1), pack-reused 0 (from 0)        
2025-04-11T03:22:18.1997779Z From https://github.com/hpcaitech/TensorNVMe
2025-04-11T03:22:18.1998091Z  * [new branch]      main       -> origin/main
2025-04-11T03:22:18.2011136Z ##[endgroup]
2025-04-11T03:22:18.2011510Z ##[group]Determining the checkout info
2025-04-11T03:22:18.2012820Z ##[endgroup]
2025-04-11T03:22:18.2013205Z ##[group]Checking out the ref
2025-04-11T03:22:18.2015650Z [command]/usr/bin/git checkout --progress --force -B main refs/remotes/origin/main
2025-04-11T03:22:18.2094101Z Switched to a new branch 'main'
2025-04-11T03:22:18.2094422Z Branch 'main' set up to track remote branch 'main' from 'origin'.
2025-04-11T03:22:18.2097522Z ##[endgroup]
2025-04-11T03:22:18.2121470Z [command]/usr/bin/git log -1 --format='%H'
2025-04-11T03:22:18.2137800Z '6403388f7c8929f43e407c8f7f39c6453d78c5d4'
2025-04-11T03:22:18.2291940Z ##[group]Run if [ -d /github/home/tensornvme_cache ] && [ ! -z "$(ls -A /github/home/tensornvme_cache/)" ]; then
2025-04-11T03:22:18.2292569Z [36;1mif [ -d /github/home/tensornvme_cache ] && [ ! -z "$(ls -A /github/home/tensornvme_cache/)" ]; then[0m
2025-04-11T03:22:18.2293078Z [36;1m  cp -p -r /github/home/tensornvme_cache/* /__w/ColossalAI/ColossalAI/TensorNVMe[0m
2025-04-11T03:22:18.2293457Z [36;1mfi[0m
2025-04-11T03:22:18.2297138Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:22:18.2297442Z ##[endgroup]
2025-04-11T03:22:18.3245585Z ##[group]Run cd TensorNVMe
2025-04-11T03:22:18.3245863Z [36;1mcd TensorNVMe[0m
2025-04-11T03:22:18.3246092Z [36;1mconda install cmake[0m
2025-04-11T03:22:18.3246351Z [36;1mpip install -r requirements.txt[0m
2025-04-11T03:22:18.3246667Z [36;1mDISABLE_URING=1 pip install -v --no-cache-dir .[0m
2025-04-11T03:22:18.3247072Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:22:18.3247351Z ##[endgroup]
2025-04-11T03:22:20.0030393Z Channels:
2025-04-11T03:22:20.0030631Z  - defaults
2025-04-11T03:22:20.0030839Z Platform: linux-64
2025-04-11T03:22:25.7878087Z Collecting package metadata (repodata.json): ...working... done
2025-04-11T03:22:25.9656942Z Solving environment: ...working... done
2025-04-11T03:22:26.0212327Z 
2025-04-11T03:22:26.0212687Z ## Package Plan ##
2025-04-11T03:22:26.0212818Z 
2025-04-11T03:22:26.0212946Z   environment location: /opt/conda
2025-04-11T03:22:26.0213114Z 
2025-04-11T03:22:26.0213221Z   added / updated specs:
2025-04-11T03:22:26.0213505Z     - cmake
2025-04-11T03:22:26.0213616Z 
2025-04-11T03:22:26.0213620Z 
2025-04-11T03:22:26.0213784Z The following packages will be downloaded:
2025-04-11T03:22:26.0213968Z 
2025-04-11T03:22:26.0214084Z     package                    |            build
2025-04-11T03:22:26.0214395Z     ---------------------------|-----------------
2025-04-11T03:22:26.0214732Z     archspec-0.2.3             |     pyhd3eb1b0_0          47 KB
2025-04-11T03:22:26.0215093Z     ca-certificates-2025.2.25  |       h06a4308_0         129 KB
2025-04-11T03:22:26.0215435Z     certifi-2025.1.31          |  py311h06a4308_0         163 KB
2025-04-11T03:22:26.0215785Z     cmake-3.31.2               |       h27e300b_0        20.9 MB
2025-04-11T03:22:26.0216084Z     conda-24.11.3              |  py311h06a4308_0         1.2 MB
2025-04-11T03:22:26.0216404Z     expat-2.6.4                |       h6a678d5_0         180 KB
2025-04-11T03:22:26.0216716Z     frozendict-2.4.2           |  py311h06a4308_0          37 KB
2025-04-11T03:22:26.0217032Z     libuv-1.48.0               |       h5eee18b_0         950 KB
2025-04-11T03:22:26.0217340Z     openssl-3.0.16             |       h5eee18b_0         5.2 MB
2025-04-11T03:22:26.0217640Z     rhash-1.4.3                |       hdbd6064_0         220 KB
2025-04-11T03:22:26.0217920Z     xz-5.6.4                   |       h5eee18b_1         567 KB
2025-04-11T03:22:26.0218213Z     ------------------------------------------------------------
2025-04-11T03:22:26.0218497Z                                            Total:        29.5 MB
2025-04-11T03:22:26.0218659Z 
2025-04-11T03:22:26.0218783Z The following NEW packages will be INSTALLED:
2025-04-11T03:22:26.0218961Z 
2025-04-11T03:22:26.0222674Z   cmake              pkgs/main/linux-64::cmake-3.31.2-h27e300b_0 
2025-04-11T03:22:26.0223075Z   expat              pkgs/main/linux-64::expat-2.6.4-h6a678d5_0 
2025-04-11T03:22:26.0223453Z   frozendict         pkgs/main/linux-64::frozendict-2.4.2-py311h06a4308_0 
2025-04-11T03:22:26.0223847Z   libuv              pkgs/main/linux-64::libuv-1.48.0-h5eee18b_0 
2025-04-11T03:22:26.0224175Z   rhash              pkgs/main/linux-64::rhash-1.4.3-hdbd6064_0 
2025-04-11T03:22:26.0224374Z 
2025-04-11T03:22:26.0224490Z The following packages will be UPDATED:
2025-04-11T03:22:26.0224660Z 
2025-04-11T03:22:26.0224823Z   archspec                               0.2.1-pyhd3eb1b0_0 --> 0.2.3-pyhd3eb1b0_0 
2025-04-11T03:22:26.0225196Z   ca-certificates                     2023.12.12-h06a4308_0 --> 2025.2.25-h06a4308_0 
2025-04-11T03:22:26.0225573Z   certifi                        2023.11.17-py311h06a4308_0 --> 2025.1.31-py311h06a4308_0 
2025-04-11T03:22:26.0225931Z   conda                             23.11.0-py311h06a4308_0 --> 24.11.3-py311h06a4308_0 
2025-04-11T03:22:26.0226266Z   openssl                                 3.0.12-h7f8727e_0 --> 3.0.16-h5eee18b_0 
2025-04-11T03:22:26.0226584Z   xz                                       5.4.5-h5eee18b_0 --> 5.6.4-h5eee18b_1 
2025-04-11T03:22:26.0226763Z 
2025-04-11T03:22:26.0226768Z 
2025-04-11T03:22:26.0227116Z Proceed ([y]/n)? 
2025-04-11T03:22:28.7115385Z 
2025-04-11T03:22:28.7115794Z Downloading and Extracting Packages: ...working... done
2025-04-11T03:22:28.8003608Z Preparing transaction: ...working... done
2025-04-11T03:22:29.1699776Z Verifying transaction: ...working... done
2025-04-11T03:22:30.2283916Z Executing transaction: ...working... done
2025-04-11T03:22:30.6718134Z Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (24.1)
2025-04-11T03:22:31.0512353Z Collecting click (from -r requirements.txt (line 2))
2025-04-11T03:22:31.3546301Z   Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)
2025-04-11T03:22:31.3565210Z Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.2.2)
2025-04-11T03:22:31.3621932Z Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.13.1)
2025-04-11T03:22:31.3626575Z Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (4.11.0)
2025-04-11T03:22:31.3629170Z Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (1.12)
2025-04-11T03:22:31.3631570Z Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.2.1)
2025-04-11T03:22:31.3633676Z Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.1.4)
2025-04-11T03:22:31.3635829Z Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (2024.6.1)
2025-04-11T03:22:31.4360990Z Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.3)
2025-04-11T03:22:31.4505231Z Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)
2025-04-11T03:22:31.5474428Z Downloading click-8.1.8-py3-none-any.whl (98 kB)
2025-04-11T03:22:31.6962289Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.2/98.2 kB 599.3 kB/s eta 0:00:00
2025-04-11T03:22:31.7439032Z Installing collected packages: click
2025-04-11T03:22:31.7935174Z Successfully installed click-8.1.8
2025-04-11T03:22:31.7938001Z WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-11T03:22:32.4619490Z Using pip 24.0 from /opt/conda/envs/pytorch/lib/python3.10/site-packages/pip (python 3.10)
2025-04-11T03:22:32.5198985Z Processing /__w/ColossalAI/ColossalAI/TensorNVMe
2025-04-11T03:22:32.5205399Z   Preparing metadata (setup.py): started
2025-04-11T03:22:32.5206799Z   Running command python setup.py egg_info
2025-04-11T03:22:32.6230538Z   running egg_info
2025-04-11T03:22:32.6232485Z   creating /tmp/pip-pip-egg-info-8f8fnnj8/tensornvme.egg-info
2025-04-11T03:22:32.6246235Z   writing /tmp/pip-pip-egg-info-8f8fnnj8/tensornvme.egg-info/PKG-INFO
2025-04-11T03:22:32.6249395Z   writing dependency_links to /tmp/pip-pip-egg-info-8f8fnnj8/tensornvme.egg-info/dependency_links.txt
2025-04-11T03:22:32.6250576Z   writing entry points to /tmp/pip-pip-egg-info-8f8fnnj8/tensornvme.egg-info/entry_points.txt
2025-04-11T03:22:32.6251592Z   writing requirements to /tmp/pip-pip-egg-info-8f8fnnj8/tensornvme.egg-info/requires.txt
2025-04-11T03:22:32.6252610Z   writing top-level names to /tmp/pip-pip-egg-info-8f8fnnj8/tensornvme.egg-info/top_level.txt
2025-04-11T03:22:32.6253632Z   writing manifest file '/tmp/pip-pip-egg-info-8f8fnnj8/tensornvme.egg-info/SOURCES.txt'
2025-04-11T03:22:32.6301966Z   reading manifest file '/tmp/pip-pip-egg-info-8f8fnnj8/tensornvme.egg-info/SOURCES.txt'
2025-04-11T03:22:32.6303062Z   reading manifest template 'MANIFEST.in'
2025-04-11T03:22:32.6316289Z   writing manifest file '/tmp/pip-pip-egg-info-8f8fnnj8/tensornvme.egg-info/SOURCES.txt'
2025-04-11T03:22:32.6474925Z   Preparing metadata (setup.py): finished with status 'done'
2025-04-11T03:22:32.6503660Z Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensornvme==0.1.0) (24.1)
2025-04-11T03:22:32.6506351Z Requirement already satisfied: click in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensornvme==0.1.0) (8.1.8)
2025-04-11T03:22:32.6508801Z Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensornvme==0.1.0) (2.2.2)
2025-04-11T03:22:32.6558590Z Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (3.13.1)
2025-04-11T03:22:32.6563495Z Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (4.11.0)
2025-04-11T03:22:32.6565971Z Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (1.12)
2025-04-11T03:22:32.6568331Z Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (3.2.1)
2025-04-11T03:22:32.6570542Z Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (3.1.4)
2025-04-11T03:22:32.6572731Z Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (2024.6.1)
2025-04-11T03:22:32.7286297Z Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch->tensornvme==0.1.0) (2.1.3)
2025-04-11T03:22:32.7426662Z Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch->tensornvme==0.1.0) (1.3.0)
2025-04-11T03:22:32.7489744Z Building wheels for collected packages: tensornvme
2025-04-11T03:22:32.7493388Z   Building wheel for tensornvme (setup.py): started
2025-04-11T03:22:32.7494737Z   Running command python setup.py bdist_wheel
2025-04-11T03:22:32.8891609Z   -- The CXX compiler identification is GNU 9.4.0
2025-04-11T03:22:32.8960872Z   -- Detecting CXX compiler ABI info
2025-04-11T03:22:32.9570066Z   -- Detecting CXX compiler ABI info - done
2025-04-11T03:22:32.9701075Z   -- Check for working CXX compiler: /usr/bin/c++ - skipped
2025-04-11T03:22:32.9704111Z   -- Detecting CXX compile features
2025-04-11T03:22:32.9708729Z   -- Detecting CXX compile features - done
2025-04-11T03:22:32.9837286Z   liburing is not found, install in /github/home/.tensornvme
2025-04-11T03:22:32.9977254Z   libaio is not found, install in /github/home/.tensornvme
2025-04-11T03:22:33.0090469Z   -- Configuring done (0.2s)
2025-04-11T03:22:33.0161881Z   -- Generating done (0.0s)
2025-04-11T03:22:33.0163834Z   -- Build files have been written to: /__w/ColossalAI/ColossalAI/TensorNVMe/cmake-build
2025-04-11T03:22:33.0427087Z   [ 12%] Creating directories for 'extern_aio'
2025-04-11T03:22:33.0550852Z   [ 25%] Performing download step (git clone) for 'extern_aio'
2025-04-11T03:22:33.0644246Z   Cloning into 'libaio'...
2025-04-11T03:22:37.1454935Z   HEAD is now at 1b18bfa bump libaio version
2025-04-11T03:22:37.1775790Z   [ 37%] No update step for 'extern_aio'
2025-04-11T03:22:37.1899152Z   [ 50%] No patch step for 'extern_aio'
2025-04-11T03:22:37.2019650Z   [ 62%] No configure step for 'extern_aio'
2025-04-11T03:22:37.2137748Z   [ 75%] Performing build step for 'extern_aio'
2025-04-11T03:22:37.5045208Z   ar: creating libaio.a
2025-04-11T03:22:37.8311516Z   [ 87%] No install step for 'extern_aio'
2025-04-11T03:22:37.8427729Z   [100%] Completed 'extern_aio'
2025-04-11T03:22:37.8585526Z   [100%] Built target extern_aio
2025-04-11T03:22:37.8631645Z   /github/home/.bashrc is changed, please source it.
2025-04-11T03:22:39.8979785Z   running bdist_wheel
2025-04-11T03:22:39.9302039Z   running build
2025-04-11T03:22:39.9303105Z   running build_py
2025-04-11T03:22:39.9318035Z   creating build
2025-04-11T03:22:39.9319107Z   creating build/lib.linux-x86_64-cpython-310
2025-04-11T03:22:39.9320270Z   creating build/lib.linux-x86_64-cpython-310/tensornvme
2025-04-11T03:22:39.9321313Z   copying tensornvme/offload.py -> build/lib.linux-x86_64-cpython-310/tensornvme
2025-04-11T03:22:39.9322507Z   copying tensornvme/__init__.py -> build/lib.linux-x86_64-cpython-310/tensornvme
2025-04-11T03:22:39.9323330Z   copying tensornvme/async_file_io.py -> build/lib.linux-x86_64-cpython-310/tensornvme
2025-04-11T03:22:39.9324301Z   creating build/lib.linux-x86_64-cpython-310/tensornvme/cli
2025-04-11T03:22:39.9325452Z   copying tensornvme/cli/check.py -> build/lib.linux-x86_64-cpython-310/tensornvme/cli
2025-04-11T03:22:39.9326447Z   copying tensornvme/cli/__init__.py -> build/lib.linux-x86_64-cpython-310/tensornvme/cli
2025-04-11T03:22:39.9327495Z   copying tensornvme/cli/cli.py -> build/lib.linux-x86_64-cpython-310/tensornvme/cli
2025-04-11T03:22:39.9328578Z   running build_ext
2025-04-11T03:22:39.9361360Z   building 'tensornvme._C' extension
2025-04-11T03:22:39.9362708Z   creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310
2025-04-11T03:22:39.9363777Z   creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w
2025-04-11T03:22:39.9364837Z   creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI
2025-04-11T03:22:39.9365761Z   creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI
2025-04-11T03:22:39.9366801Z   creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe
2025-04-11T03:22:39.9367884Z   creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc
2025-04-11T03:22:39.9607549Z   Emitting ninja build file /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:22:39.9608285Z   Compiling objects...
2025-04-11T03:22:39.9609626Z   Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:22:40.2176232Z   [1/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/space_mgr.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/space_mgr.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/space_mgr.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:22:55.2676868Z   [2/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/async_file_io.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/async_file_io.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/async_file_io.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:22:55.3333997Z   [3/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/offload.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/offload.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/offload.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:22:55.5345648Z   [4/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/aio.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/aio.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/aio.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:22:55.9571550Z   [5/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:22:55.9575934Z   In file included from /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.cpp:15:
2025-04-11T03:22:55.9577376Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h: In constructor ‘PthreadAsyncIO::PthreadAsyncIO(unsigned int, unsigned int)’:
2025-04-11T03:22:55.9578534Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:38:18: warning: ‘PthreadAsyncIO::total_tasks’ will be initialized after [-Wreorder]
2025-04-11T03:22:55.9579125Z      38 |     unsigned int total_tasks;
2025-04-11T03:22:55.9579393Z         |                  ^~~~~~~~~~~
2025-04-11T03:22:55.9580082Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:29:18: warning:   ‘unsigned int PthreadAsyncIO::total_h2d’ [-Wreorder]
2025-04-11T03:22:55.9580604Z      29 |     unsigned int total_h2d;
2025-04-11T03:22:55.9580863Z         |                  ^~~~~~~~~
2025-04-11T03:22:55.9581336Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:41:5: warning:   when initialized here [-Wreorder]
2025-04-11T03:22:55.9581892Z      41 |     PthreadAsyncIO(unsigned int n_entries, unsigned int n_tasks)
2025-04-11T03:22:55.9582230Z         |     ^~~~~~~~~~~~~~
2025-04-11T03:22:56.5556680Z   [6/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/py_api.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/py_api.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/py_api.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:22:56.9833342Z   [7/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:22:56.9837504Z   In file included from /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.cpp:1:
2025-04-11T03:22:56.9838638Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h: In constructor ‘PthreadAsyncIO::PthreadAsyncIO(unsigned int, unsigned int)’:
2025-04-11T03:22:56.9839943Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:38:18: warning: ‘PthreadAsyncIO::total_tasks’ will be initialized after [-Wreorder]
2025-04-11T03:22:56.9840542Z      38 |     unsigned int total_tasks;
2025-04-11T03:22:56.9840808Z         |                  ^~~~~~~~~~~
2025-04-11T03:22:56.9841482Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:29:18: warning:   ‘unsigned int PthreadAsyncIO::total_h2d’ [-Wreorder]
2025-04-11T03:22:56.9842009Z      29 |     unsigned int total_h2d;
2025-04-11T03:22:56.9842260Z         |                  ^~~~~~~~~
2025-04-11T03:22:56.9842718Z   /__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:41:5: warning:   when initialized here [-Wreorder]
2025-04-11T03:22:56.9843264Z      41 |     PthreadAsyncIO(unsigned int n_entries, unsigned int n_tasks)
2025-04-11T03:22:56.9843599Z         |     ^~~~~~~~~~~~~~
2025-04-11T03:22:56.9882411Z   g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/aio.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/async_file_io.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/offload.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/py_api.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/space_mgr.o -L/github/home/.tensornvme/lib -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -laio -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/tensornvme/_C.cpython-310-x86_64-linux-gnu.so
2025-04-11T03:22:57.1621916Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
2025-04-11T03:22:57.1622526Z   !!
2025-04-11T03:22:57.1623186Z 
2025-04-11T03:22:57.1624397Z           ********************************************************************************
2025-04-11T03:22:57.1625366Z           Please avoid running ``setup.py`` directly.
2025-04-11T03:22:57.1626394Z           Instead, use pypa/build, pypa/installer or other
2025-04-11T03:22:57.1627329Z           standards-based tools.
2025-04-11T03:22:57.1628202Z 
2025-04-11T03:22:57.1629631Z           See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
2025-04-11T03:22:57.1630493Z           ********************************************************************************
2025-04-11T03:22:57.1631245Z 
2025-04-11T03:22:57.1632286Z   !!
2025-04-11T03:22:57.1633310Z     self.initialize_options()
2025-04-11T03:22:57.1637205Z   installing to build/bdist.linux-x86_64/wheel
2025-04-11T03:22:57.1638141Z   running install
2025-04-11T03:22:57.1681007Z   running install_lib
2025-04-11T03:22:57.1698553Z   creating build/bdist.linux-x86_64
2025-04-11T03:22:57.1699539Z   creating build/bdist.linux-x86_64/wheel
2025-04-11T03:22:57.1700580Z   creating build/bdist.linux-x86_64/wheel/tensornvme
2025-04-11T03:22:57.1701811Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/offload.py -> build/bdist.linux-x86_64/wheel/tensornvme
2025-04-11T03:22:57.1702745Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/__init__.py -> build/bdist.linux-x86_64/wheel/tensornvme
2025-04-11T03:22:57.1703909Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/_C.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/tensornvme
2025-04-11T03:22:57.1705985Z   creating build/bdist.linux-x86_64/wheel/tensornvme/cli
2025-04-11T03:22:57.1707131Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/cli/check.py -> build/bdist.linux-x86_64/wheel/tensornvme/cli
2025-04-11T03:22:57.1708098Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/cli/__init__.py -> build/bdist.linux-x86_64/wheel/tensornvme/cli
2025-04-11T03:22:57.1709146Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/cli/cli.py -> build/bdist.linux-x86_64/wheel/tensornvme/cli
2025-04-11T03:22:57.1710293Z   copying build/lib.linux-x86_64-cpython-310/tensornvme/async_file_io.py -> build/bdist.linux-x86_64/wheel/tensornvme
2025-04-11T03:22:57.1710955Z   running install_egg_info
2025-04-11T03:22:57.1729132Z   running egg_info
2025-04-11T03:22:57.1730124Z   creating tensornvme.egg-info
2025-04-11T03:22:57.1742857Z   writing tensornvme.egg-info/PKG-INFO
2025-04-11T03:22:57.1746838Z   writing dependency_links to tensornvme.egg-info/dependency_links.txt
2025-04-11T03:22:57.1748157Z   writing entry points to tensornvme.egg-info/entry_points.txt
2025-04-11T03:22:57.1749373Z   writing requirements to tensornvme.egg-info/requires.txt
2025-04-11T03:22:57.1750411Z   writing top-level names to tensornvme.egg-info/top_level.txt
2025-04-11T03:22:57.1751395Z   writing manifest file 'tensornvme.egg-info/SOURCES.txt'
2025-04-11T03:22:57.1770145Z   reading manifest file 'tensornvme.egg-info/SOURCES.txt'
2025-04-11T03:22:57.1771548Z   reading manifest template 'MANIFEST.in'
2025-04-11T03:22:57.1785401Z   writing manifest file 'tensornvme.egg-info/SOURCES.txt'
2025-04-11T03:22:57.1786982Z   Copying tensornvme.egg-info to build/bdist.linux-x86_64/wheel/tensornvme-0.1.0-py3.10.egg-info
2025-04-11T03:22:57.1791542Z   running install_scripts
2025-04-11T03:22:57.1873766Z   creating build/bdist.linux-x86_64/wheel/tensornvme-0.1.0.dist-info/WHEEL
2025-04-11T03:22:57.1877543Z   creating '/tmp/pip-wheel-jd4m66do/tensornvme-0.1.0-cp310-cp310-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it
2025-04-11T03:22:57.2070044Z   adding 'tensornvme/_C.cpython-310-x86_64-linux-gnu.so'
2025-04-11T03:22:57.2074849Z   adding 'tensornvme/__init__.py'
2025-04-11T03:22:57.2076639Z   adding 'tensornvme/async_file_io.py'
2025-04-11T03:22:57.2078286Z   adding 'tensornvme/offload.py'
2025-04-11T03:22:57.2079663Z   adding 'tensornvme/cli/__init__.py'
2025-04-11T03:22:57.2080867Z   adding 'tensornvme/cli/check.py'
2025-04-11T03:22:57.2081952Z   adding 'tensornvme/cli/cli.py'
2025-04-11T03:22:57.2084879Z   adding 'tensornvme-0.1.0.dist-info/METADATA'
2025-04-11T03:22:57.2085997Z   adding 'tensornvme-0.1.0.dist-info/WHEEL'
2025-04-11T03:22:57.2086912Z   adding 'tensornvme-0.1.0.dist-info/entry_points.txt'
2025-04-11T03:22:57.2087932Z   adding 'tensornvme-0.1.0.dist-info/top_level.txt'
2025-04-11T03:22:57.2088880Z   adding 'tensornvme-0.1.0.dist-info/RECORD'
2025-04-11T03:22:57.2089855Z   removing build/bdist.linux-x86_64/wheel
2025-04-11T03:22:57.5370664Z   Building wheel for tensornvme (setup.py): finished with status 'done'
2025-04-11T03:22:57.5375068Z   Created wheel for tensornvme: filename=tensornvme-0.1.0-cp310-cp310-linux_x86_64.whl size=147126 sha256=4076cec5b08d295f279c47472f373d0e8fbe30d494991431f80100abd27a7c6b
2025-04-11T03:22:57.5376081Z   Stored in directory: /tmp/pip-ephem-wheel-cache-l12p3nv7/wheels/cb/3c/a4/391c1ae2f1a321082a466078f0646d215673629d6cbb874a65
2025-04-11T03:22:57.5399052Z Successfully built tensornvme
2025-04-11T03:22:57.5839973Z Installing collected packages: tensornvme
2025-04-11T03:22:57.5913556Z   changing mode of /opt/conda/envs/pytorch/bin/tensornvme to 755
2025-04-11T03:22:57.5973761Z Successfully installed tensornvme-0.1.0
2025-04-11T03:22:57.5976548Z WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-11T03:22:58.0564571Z ##[group]Run cd TensorNVMe
2025-04-11T03:22:58.0564878Z [36;1mcd TensorNVMe[0m
2025-04-11T03:22:58.0565196Z [36;1mcp -p -r ./build /github/home/tensornvme_cache/[0m
2025-04-11T03:22:58.0565542Z [36;1mcp -p -r ./cmake-build /github/home/tensornvme_cache/[0m
2025-04-11T03:22:58.0565999Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:22:58.0566282Z ##[endgroup]
2025-04-11T03:22:58.1637200Z ##[group]Run actions/checkout@v2
2025-04-11T03:22:58.1637598Z with:
2025-04-11T03:22:58.1637895Z   repository: hpcaitech/ColossalAI
2025-04-11T03:22:58.1638381Z   token: ***
2025-04-11T03:22:58.1638683Z   ssh-strict: true
2025-04-11T03:22:58.1638959Z   persist-credentials: true
2025-04-11T03:22:58.1639288Z   clean: true
2025-04-11T03:22:58.1639575Z   fetch-depth: 1
2025-04-11T03:22:58.1639818Z   lfs: false
2025-04-11T03:22:58.1640137Z   submodules: false
2025-04-11T03:22:58.1640407Z   set-safe-directory: true
2025-04-11T03:22:58.1640679Z ##[endgroup]
2025-04-11T03:22:58.1644974Z ##[command]/usr/bin/docker exec  a22674922e87e0d0962e2f956706f403d9456d353f40b8411642994284af24b7 sh -c "cat /etc/*release | grep ^ID"
2025-04-11T03:22:58.3569415Z Syncing repository: hpcaitech/ColossalAI
2025-04-11T03:22:58.3572957Z ##[group]Getting Git version info
2025-04-11T03:22:58.3573279Z Working directory is '/__w/ColossalAI/ColossalAI'
2025-04-11T03:22:58.3597830Z [command]/usr/bin/git version
2025-04-11T03:22:58.3625841Z git version 2.25.1
2025-04-11T03:22:58.3645919Z ##[endgroup]
2025-04-11T03:22:58.3656358Z Temporarily overriding HOME='/__w/_temp/7f524312-8c4f-4721-8d1a-0bbb6ef8990d' before making global git config changes
2025-04-11T03:22:58.3656936Z Adding repository directory to the temporary git global config as a safe directory
2025-04-11T03:22:58.3659410Z [command]/usr/bin/git config --global --add safe.directory /__w/ColossalAI/ColossalAI
2025-04-11T03:22:58.3685707Z [command]/usr/bin/git config --local --get remote.origin.url
2025-04-11T03:22:58.3700679Z https://github.com/hpcaitech/ColossalAI
2025-04-11T03:22:58.3709157Z ##[group]Removing previously created refs, to avoid conflicts
2025-04-11T03:22:58.3712003Z [command]/usr/bin/git rev-parse --symbolic-full-name --verify --quiet HEAD
2025-04-11T03:22:58.3725694Z HEAD
2025-04-11T03:22:58.3732475Z [command]/usr/bin/git rev-parse --symbolic-full-name --branches
2025-04-11T03:22:58.3750797Z ##[endgroup]
2025-04-11T03:22:58.3751153Z ##[group]Cleaning the repository
2025-04-11T03:22:58.3753796Z [command]/usr/bin/git clean -ffdx
2025-04-11T03:22:58.3895527Z Removing TensorNVMe/
2025-04-11T03:22:58.3900867Z [command]/usr/bin/git reset --hard HEAD
2025-04-11T03:22:58.4002330Z HEAD is now at fd69a821 fix
2025-04-11T03:22:58.4006730Z ##[endgroup]
2025-04-11T03:22:58.4008289Z ##[group]Disabling automatic garbage collection
2025-04-11T03:22:58.4011026Z [command]/usr/bin/git config --local gc.auto 0
2025-04-11T03:22:58.4028638Z ##[endgroup]
2025-04-11T03:22:58.4028994Z ##[group]Setting up auth
2025-04-11T03:22:58.4034361Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2025-04-11T03:22:58.4053871Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2025-04-11T03:22:58.4222068Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2025-04-11T03:22:58.4239540Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2025-04-11T03:22:58.4399041Z [command]/usr/bin/git config --local http.https://github.com/.extraheader AUTHORIZATION: basic ***
2025-04-11T03:22:58.4424666Z ##[endgroup]
2025-04-11T03:22:58.4425025Z ##[group]Fetching the repository
2025-04-11T03:22:58.4430155Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --progress --no-recurse-submodules --depth=1 origin +05b5218ac29bf0c179452244096c42c9b4c170e1:refs/remotes/pull/6254/merge
2025-04-11T03:22:59.4291654Z remote: Enumerating objects: 25, done.        
2025-04-11T03:22:59.4999654Z remote: Counting objects:   4% (1/22)        
2025-04-11T03:22:59.4999951Z remote: Counting objects:   9% (2/22)        
2025-04-11T03:22:59.5000246Z remote: Counting objects:  13% (3/22)        
2025-04-11T03:22:59.5000750Z remote: Counting objects:  18% (4/22)        
2025-04-11T03:22:59.5001029Z remote: Counting objects:  22% (5/22)        
2025-04-11T03:22:59.5001309Z remote: Counting objects:  27% (6/22)        
2025-04-11T03:22:59.5001635Z remote: Counting objects:  31% (7/22)        
2025-04-11T03:22:59.5001903Z remote: Counting objects:  36% (8/22)        
2025-04-11T03:22:59.5002166Z remote: Counting objects:  40% (9/22)        
2025-04-11T03:22:59.5002433Z remote: Counting objects:  45% (10/22)        
2025-04-11T03:22:59.5002745Z remote: Counting objects:  50% (11/22)        
2025-04-11T03:22:59.5003014Z remote: Counting objects:  54% (12/22)        
2025-04-11T03:22:59.5003287Z remote: Counting objects:  59% (13/22)        
2025-04-11T03:22:59.5003556Z remote: Counting objects:  63% (14/22)        
2025-04-11T03:22:59.5003825Z remote: Counting objects:  68% (15/22)        
2025-04-11T03:22:59.5004095Z remote: Counting objects:  72% (16/22)        
2025-04-11T03:22:59.5004359Z remote: Counting objects:  77% (17/22)        
2025-04-11T03:22:59.5004633Z remote: Counting objects:  81% (18/22)        
2025-04-11T03:22:59.5004903Z remote: Counting objects:  86% (19/22)        
2025-04-11T03:22:59.5005172Z remote: Counting objects:  90% (20/22)        
2025-04-11T03:22:59.5005442Z remote: Counting objects:  95% (21/22)        
2025-04-11T03:22:59.5005709Z remote: Counting objects: 100% (22/22)        
2025-04-11T03:22:59.5005992Z remote: Counting objects: 100% (22/22), done.        
2025-04-11T03:22:59.5006300Z remote: Compressing objects:  33% (1/3)        
2025-04-11T03:22:59.5006593Z remote: Compressing objects:  66% (2/3)        
2025-04-11T03:22:59.5006874Z remote: Compressing objects: 100% (3/3)        
2025-04-11T03:22:59.5007171Z remote: Compressing objects: 100% (3/3), done.        
2025-04-11T03:22:59.5007529Z remote: Total 9 (delta 7), reused 6 (delta 5), pack-reused 0 (from 0)        
2025-04-11T03:22:59.5392398Z From https://github.com/hpcaitech/ColossalAI
2025-04-11T03:22:59.5392854Z  + 6f94bf4c...05b5218a 05b5218ac29bf0c179452244096c42c9b4c170e1 -> pull/6254/merge  (forced update)
2025-04-11T03:22:59.5411413Z ##[endgroup]
2025-04-11T03:22:59.5411805Z ##[group]Determining the checkout info
2025-04-11T03:22:59.5412969Z ##[endgroup]
2025-04-11T03:22:59.5413312Z ##[group]Checking out the ref
2025-04-11T03:22:59.5416363Z [command]/usr/bin/git checkout --progress --force refs/remotes/pull/6254/merge
2025-04-11T03:22:59.5501950Z Warning: you are leaving 43 commits behind, not connected to
2025-04-11T03:22:59.5502268Z any of your branches:
2025-04-11T03:22:59.5502414Z 
2025-04-11T03:22:59.5502512Z   fd69a821 fix
2025-04-11T03:22:59.5502705Z   db4c73f6 fix
2025-04-11T03:22:59.5502987Z   0950b07a [pre-commit.ci] auto fixes from pre-commit.com hooks
2025-04-11T03:22:59.5503296Z   910433f0 fix
2025-04-11T03:22:59.5503491Z  ... and 39 more.
2025-04-11T03:22:59.5503607Z 
2025-04-11T03:22:59.5503781Z If you want to keep them by creating a new branch, this may be a good time
2025-04-11T03:22:59.5504087Z to do so with:
2025-04-11T03:22:59.5504235Z 
2025-04-11T03:22:59.5504354Z  git branch <new-branch-name> fd69a821
2025-04-11T03:22:59.5504524Z 
2025-04-11T03:22:59.5505144Z HEAD is now at 05b5218a Merge fd69a821bb2059f45053d204f007d7bcbc66bdc0 into 44d4053fec005fe0b06b6bc755fdc962463145df
2025-04-11T03:22:59.5509108Z ##[endgroup]
2025-04-11T03:22:59.5533805Z [command]/usr/bin/git log -1 --format='%H'
2025-04-11T03:22:59.5550321Z '05b5218ac29bf0c179452244096c42c9b4c170e1'
2025-04-11T03:22:59.5679362Z ##[group]Run # -p flag is required to preserve the file timestamp to avoid ninja rebuild
2025-04-11T03:22:59.5680000Z [36;1m# -p flag is required to preserve the file timestamp to avoid ninja rebuild[0m
2025-04-11T03:22:59.5680482Z [36;1mif [ -d /github/home/cuda_ext_cache ] && [ ! -z "$(ls -A /github/home/cuda_ext_cache/)" ]; then[0m
2025-04-11T03:22:59.5680934Z [36;1m  cp -p -r /github/home/cuda_ext_cache/* /__w/ColossalAI/ColossalAI/[0m
2025-04-11T03:22:59.5681247Z [36;1mfi[0m
2025-04-11T03:22:59.5681593Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:22:59.5681879Z ##[endgroup]
2025-04-11T03:22:59.6511833Z ##[group]Run BUILD_EXT=1 pip install -v -e .
2025-04-11T03:22:59.6512165Z [36;1mBUILD_EXT=1 pip install -v -e .[0m
2025-04-11T03:22:59.6512524Z [36;1mpip install --no-cache-dir -r requirements/requirements-test.txt[0m
2025-04-11T03:22:59.6512976Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:22:59.6513258Z ##[endgroup]
2025-04-11T03:22:59.9132849Z Using pip 24.0 from /opt/conda/envs/pytorch/lib/python3.10/site-packages/pip (python 3.10)
2025-04-11T03:22:59.9691941Z Obtaining file:///__w/ColossalAI/ColossalAI
2025-04-11T03:22:59.9697426Z   Preparing metadata (setup.py): started
2025-04-11T03:22:59.9699004Z   Running command python setup.py egg_info
2025-04-11T03:23:02.2972240Z   [extension] Building extensionscpu_adam_x86, layernorm_cuda, moe_cuda, fused_optim_cuda, inference_ops_cuda, scaled_masked_softmax_cuda, scaled_upper_triangle_masked_softmax_cuda
2025-04-11T03:23:02.2972882Z   running egg_info
2025-04-11T03:23:02.2973302Z   creating /tmp/pip-pip-egg-info-u98al099/colossalai.egg-info
2025-04-11T03:23:02.2983692Z   writing /tmp/pip-pip-egg-info-u98al099/colossalai.egg-info/PKG-INFO
2025-04-11T03:23:02.2988680Z   writing dependency_links to /tmp/pip-pip-egg-info-u98al099/colossalai.egg-info/dependency_links.txt
2025-04-11T03:23:02.2990193Z   writing entry points to /tmp/pip-pip-egg-info-u98al099/colossalai.egg-info/entry_points.txt
2025-04-11T03:23:02.2991840Z   writing requirements to /tmp/pip-pip-egg-info-u98al099/colossalai.egg-info/requires.txt
2025-04-11T03:23:02.2992906Z   writing top-level names to /tmp/pip-pip-egg-info-u98al099/colossalai.egg-info/top_level.txt
2025-04-11T03:23:02.2993981Z   writing manifest file '/tmp/pip-pip-egg-info-u98al099/colossalai.egg-info/SOURCES.txt'
2025-04-11T03:23:02.3526320Z   reading manifest file '/tmp/pip-pip-egg-info-u98al099/colossalai.egg-info/SOURCES.txt'
2025-04-11T03:23:02.3527350Z   reading manifest template 'MANIFEST.in'
2025-04-11T03:23:02.3948395Z   warning: no files found matching '*.tr' under directory 'colossalai'
2025-04-11T03:23:02.4160279Z   warning: no files found matching '*.cc' under directory 'colossalai'
2025-04-11T03:23:02.4263710Z   warning: no files found matching '*.pyi' under directory 'colossalai'
2025-04-11T03:23:02.4323744Z   warning: no files found matching '*.tr' under directory 'extensions'
2025-04-11T03:23:02.4344627Z   warning: no files found matching '*.cc' under directory 'extensions'
2025-04-11T03:23:02.4355207Z   warning: no files found matching '*.pyi' under directory 'extensions'
2025-04-11T03:23:02.4356101Z   adding license file 'LICENSE'
2025-04-11T03:23:02.4429971Z   writing manifest file '/tmp/pip-pip-egg-info-u98al099/colossalai.egg-info/SOURCES.txt'
2025-04-11T03:23:02.7659956Z   Preparing metadata (setup.py): finished with status 'done'
2025-04-11T03:23:02.7831511Z Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (1.26.4)
2025-04-11T03:23:03.2044025Z Collecting tqdm (from colossalai==0.4.9)
2025-04-11T03:23:03.2046426Z   Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata
2025-04-11T03:23:03.5139928Z   Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
2025-04-11T03:23:03.6575774Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.7/57.7 kB 335.8 kB/s eta 0:00:00
2025-04-11T03:23:03.9565602Z Collecting psutil (from colossalai==0.4.9)
2025-04-11T03:23:03.9567988Z   Obtaining dependency information for psutil from https://files.pythonhosted.org/packages/bf/b9/b0eb3f3cbcb734d930fdf839431606844a825b23eaf9a6ab371edac8162c/psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:04.0448628Z   Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
2025-04-11T03:23:04.1172629Z Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (24.1)
2025-04-11T03:23:04.3138532Z Collecting pre-commit (from colossalai==0.4.9)
2025-04-11T03:23:04.3140740Z   Obtaining dependency information for pre-commit from https://files.pythonhosted.org/packages/88/74/a88bf1b1efeae488a0c0b7bdf71429c313722d1fc0f377537fbe554e6180/pre_commit-4.2.0-py2.py3-none-any.whl.metadata
2025-04-11T03:23:04.4022812Z   Downloading pre_commit-4.2.0-py2.py3-none-any.whl.metadata (1.3 kB)
2025-04-11T03:23:04.6176760Z Collecting rich (from colossalai==0.4.9)
2025-04-11T03:23:04.6178991Z   Obtaining dependency information for rich from https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl.metadata
2025-04-11T03:23:04.7058539Z   Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)
2025-04-11T03:23:04.7779359Z Requirement already satisfied: click in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (8.1.8)
2025-04-11T03:23:05.2053853Z Collecting fabric (from colossalai==0.4.9)
2025-04-11T03:23:05.2056081Z   Obtaining dependency information for fabric from https://files.pythonhosted.org/packages/d6/1f/e99e23ee01847147fa194e8d41cfcf2535a2dbfcb51414c541cadb15c5d7/fabric-3.2.2-py3-none-any.whl.metadata
2025-04-11T03:23:05.2945632Z   Downloading fabric-3.2.2-py3-none-any.whl.metadata (3.5 kB)
2025-04-11T03:23:05.4613598Z Collecting contexttimer (from colossalai==0.4.9)
2025-04-11T03:23:05.5508550Z   Downloading contexttimer-0.3.3.tar.gz (4.9 kB)
2025-04-11T03:23:05.5543223Z   Preparing metadata (setup.py): started
2025-04-11T03:23:05.5544686Z   Running command python setup.py egg_info
2025-04-11T03:23:05.6564661Z   running egg_info
2025-04-11T03:23:05.6573285Z   creating /tmp/pip-pip-egg-info-7ocpu6e5/contexttimer.egg-info
2025-04-11T03:23:05.6586733Z   writing /tmp/pip-pip-egg-info-7ocpu6e5/contexttimer.egg-info/PKG-INFO
2025-04-11T03:23:05.6590152Z   writing dependency_links to /tmp/pip-pip-egg-info-7ocpu6e5/contexttimer.egg-info/dependency_links.txt
2025-04-11T03:23:05.6592046Z   writing top-level names to /tmp/pip-pip-egg-info-7ocpu6e5/contexttimer.egg-info/top_level.txt
2025-04-11T03:23:05.6593090Z   writing manifest file '/tmp/pip-pip-egg-info-7ocpu6e5/contexttimer.egg-info/SOURCES.txt'
2025-04-11T03:23:05.6642507Z   reading manifest file '/tmp/pip-pip-egg-info-7ocpu6e5/contexttimer.egg-info/SOURCES.txt'
2025-04-11T03:23:05.6647114Z   writing manifest file '/tmp/pip-pip-egg-info-7ocpu6e5/contexttimer.egg-info/SOURCES.txt'
2025-04-11T03:23:05.6809635Z   Preparing metadata (setup.py): finished with status 'done'
2025-04-11T03:23:05.6816771Z Requirement already satisfied: ninja in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (1.11.1.1)
2025-04-11T03:23:05.6822640Z Requirement already satisfied: torch<=2.5.1,>=2.2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (2.2.2)
2025-04-11T03:23:05.9544061Z Collecting safetensors (from colossalai==0.4.9)
2025-04-11T03:23:05.9545915Z   Obtaining dependency information for safetensors from https://files.pythonhosted.org/packages/a6/f8/dae3421624fcc87a89d42e1898a798bc7ff72c61f38973a65d60df8f124c/safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:06.0425183Z   Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
2025-04-11T03:23:06.2115877Z Collecting einops (from colossalai==0.4.9)
2025-04-11T03:23:06.2118151Z   Obtaining dependency information for einops from https://files.pythonhosted.org/packages/87/62/9773de14fe6c45c23649e98b83231fffd7b9892b6cf863251dc2afa73643/einops-0.8.1-py3-none-any.whl.metadata
2025-04-11T03:23:06.3007585Z   Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)
2025-04-11T03:23:06.6604484Z Collecting pydantic (from colossalai==0.4.9)
2025-04-11T03:23:06.6606619Z   Obtaining dependency information for pydantic from https://files.pythonhosted.org/packages/b0/1d/407b29780a289868ed696d1616f4aad49d6388e5a77f567dcd2629dcd7b8/pydantic-2.11.3-py3-none-any.whl.metadata
2025-04-11T03:23:06.7485809Z   Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)
2025-04-11T03:23:06.8217338Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.2/65.2 kB 937.2 kB/s eta 0:00:00
2025-04-11T03:23:07.0671820Z Collecting ray (from colossalai==0.4.9)
2025-04-11T03:23:07.0673735Z   Obtaining dependency information for ray from https://files.pythonhosted.org/packages/93/f1/9108c4f878e3cacb767b7dfbbc3a26537c79ab516d2530b9f63b558ba4bb/ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:07.1561230Z   Downloading ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (19 kB)
2025-04-11T03:23:07.4256186Z Collecting sentencepiece (from colossalai==0.4.9)
2025-04-11T03:23:07.4257906Z   Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/a6/27/33019685023221ca8ed98e8ceb7ae5e166032686fa3662c68f1f1edf334e/sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:07.5149221Z   Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
2025-04-11T03:23:07.6823753Z Collecting google (from colossalai==0.4.9)
2025-04-11T03:23:07.6825859Z   Obtaining dependency information for google from https://files.pythonhosted.org/packages/ac/35/17c9141c4ae21e9a29a43acdfd848e3e468a810517f862cad07977bf8fe9/google-3.0.0-py2.py3-none-any.whl.metadata
2025-04-11T03:23:07.7707050Z   Downloading google-3.0.0-py2.py3-none-any.whl.metadata (627 bytes)
2025-04-11T03:23:08.0927381Z Collecting protobuf (from colossalai==0.4.9)
2025-04-11T03:23:08.0929424Z   Obtaining dependency information for protobuf from https://files.pythonhosted.org/packages/28/50/1925de813499546bc8ab3ae857e3ec84efe7d2f19b34529d0c7c3d02d11d/protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:08.1813224Z   Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)
2025-04-11T03:23:08.3952577Z Collecting transformers==4.39.3 (from colossalai==0.4.9)
2025-04-11T03:23:08.3954374Z   Obtaining dependency information for transformers==4.39.3 from https://files.pythonhosted.org/packages/15/fc/7b6dd7e1adc0a6407b845ed4be1999e98b6917d0694e57316d140cc85484/transformers-4.39.3-py3-none-any.whl.metadata
2025-04-11T03:23:08.4840915Z   Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)
2025-04-11T03:23:08.5590916Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 2.1 MB/s eta 0:00:00
2025-04-11T03:23:08.7355896Z Collecting peft<=0.13.2,>=0.7.1 (from colossalai==0.4.9)
2025-04-11T03:23:08.7358003Z   Obtaining dependency information for peft<=0.13.2,>=0.7.1 from https://files.pythonhosted.org/packages/78/9d/5f95bfb298c8d3b4e3a107701f9a4e7774a0d4d1f8eb0c9d5420b80f7c9d/peft-0.13.2-py3-none-any.whl.metadata
2025-04-11T03:23:08.8249004Z   Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)
2025-04-11T03:23:09.0708921Z Collecting bitsandbytes>=0.39.0 (from colossalai==0.4.9)
2025-04-11T03:23:09.0710929Z   Obtaining dependency information for bitsandbytes>=0.39.0 from https://files.pythonhosted.org/packages/07/b7/cb5ce4d1a382cf53c19ef06c5fc29e85f5e129b4da6527dd207d90a5b8ad/bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata
2025-04-11T03:23:09.1598365Z   Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)
2025-04-11T03:23:09.6399704Z Collecting rpyc==6.0.0 (from colossalai==0.4.9)
2025-04-11T03:23:09.6401831Z   Obtaining dependency information for rpyc==6.0.0 from https://files.pythonhosted.org/packages/f6/e7/38cd5f2d8ab0d259648f6672fd19de30688a22ae769f87616c6a2fe92581/rpyc-6.0.0-py3-none-any.whl.metadata
2025-04-11T03:23:09.7289260Z   Downloading rpyc-6.0.0-py3-none-any.whl.metadata (3.5 kB)
2025-04-11T03:23:10.0399858Z Collecting fastapi (from colossalai==0.4.9)
2025-04-11T03:23:10.0401197Z   Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/50/b3/b51f09c2ba432a576fe63758bddc81f78f0c6309d9e5c10d194313bf021e/fastapi-0.115.12-py3-none-any.whl.metadata
2025-04-11T03:23:10.1283638Z   Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)
2025-04-11T03:23:10.3832461Z Collecting uvicorn==0.29.0 (from colossalai==0.4.9)
2025-04-11T03:23:10.3834674Z   Obtaining dependency information for uvicorn==0.29.0 from https://files.pythonhosted.org/packages/73/f5/cbb16fcbe277c1e0b8b3ddd188f2df0e0947f545c49119b589643632d156/uvicorn-0.29.0-py3-none-any.whl.metadata
2025-04-11T03:23:10.4723540Z   Downloading uvicorn-0.29.0-py3-none-any.whl.metadata (6.3 kB)
2025-04-11T03:23:10.9518638Z Collecting galore_torch (from colossalai==0.4.9)
2025-04-11T03:23:10.9520273Z   Obtaining dependency information for galore_torch from https://files.pythonhosted.org/packages/2b/b9/e9e88f989c62edefaa45df07198ce280ac0d373f5e2842686c6ece2ddb1e/galore_torch-1.0-py3-none-any.whl.metadata
2025-04-11T03:23:11.0409583Z   Downloading galore_torch-1.0-py3-none-any.whl.metadata (355 bytes)
2025-04-11T03:23:11.2949499Z Collecting diffusers==0.29.0 (from colossalai==0.4.9)
2025-04-11T03:23:11.2951681Z   Obtaining dependency information for diffusers==0.29.0 from https://files.pythonhosted.org/packages/d5/68/a84d929518c9fbd65febcedd7810203bcac393f9cddd0603ec58df7f93f7/diffusers-0.29.0-py3-none-any.whl.metadata
2025-04-11T03:23:11.3841169Z   Downloading diffusers-0.29.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:23:11.6924344Z Collecting importlib-metadata (from diffusers==0.29.0->colossalai==0.4.9)
2025-04-11T03:23:11.6926368Z   Obtaining dependency information for importlib-metadata from https://files.pythonhosted.org/packages/79/9d/0fb148dc4d6fa4a7dd1d8378168d9b4cd8d4560a6fbf6f0121c5fc34eb68/importlib_metadata-8.6.1-py3-none-any.whl.metadata
2025-04-11T03:23:11.7807187Z   Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)
2025-04-11T03:23:11.7828061Z Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai==0.4.9) (3.13.1)
2025-04-11T03:23:11.9827250Z Collecting huggingface-hub>=0.23.2 (from diffusers==0.29.0->colossalai==0.4.9)
2025-04-11T03:23:11.9829492Z   Obtaining dependency information for huggingface-hub>=0.23.2 from https://files.pythonhosted.org/packages/93/27/1fb384a841e9661faad1c31cbfa62864f59632e876df5d795234da51c395/huggingface_hub-0.30.2-py3-none-any.whl.metadata
2025-04-11T03:23:12.0710218Z   Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)
2025-04-11T03:23:12.7487629Z Collecting regex!=2019.12.17 (from diffusers==0.29.0->colossalai==0.4.9)
2025-04-11T03:23:12.7489553Z   Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/f2/98/26d3830875b53071f1f0ae6d547f1d98e964dd29ad35cbf94439120bb67a/regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:12.8480894Z   Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
2025-04-11T03:23:12.9208214Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 518.7 kB/s eta 0:00:00
2025-04-11T03:23:12.9219269Z Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai==0.4.9) (2.32.2)
2025-04-11T03:23:12.9245276Z Requirement already satisfied: Pillow in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai==0.4.9) (10.3.0)
2025-04-11T03:23:13.2615107Z Collecting plumbum (from rpyc==6.0.0->colossalai==0.4.9)
2025-04-11T03:23:13.2617276Z   Obtaining dependency information for plumbum from https://files.pythonhosted.org/packages/4f/9d/d03542c93bb3d448406731b80f39c3d5601282f778328c22c77d270f4ed4/plumbum-1.9.0-py3-none-any.whl.metadata
2025-04-11T03:23:13.3506554Z   Downloading plumbum-1.9.0-py3-none-any.whl.metadata (10 kB)
2025-04-11T03:23:13.7109893Z Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.39.3->colossalai==0.4.9) (6.0.1)
2025-04-11T03:23:14.1055514Z Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3->colossalai==0.4.9)
2025-04-11T03:23:14.1057441Z   Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/1c/5d/cf5e122ce4f1a29f165b2a69dc33d1ff30bce303343d58a54775ddba5d51/tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:14.1947795Z   Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
2025-04-11T03:23:14.3824722Z Collecting h11>=0.8 (from uvicorn==0.29.0->colossalai==0.4.9)
2025-04-11T03:23:14.3826706Z   Obtaining dependency information for h11>=0.8 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata
2025-04-11T03:23:14.4708533Z   Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)
2025-04-11T03:23:14.4734058Z Requirement already satisfied: typing-extensions>=4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from uvicorn==0.29.0->colossalai==0.4.9) (4.11.0)
2025-04-11T03:23:14.6836748Z Collecting accelerate>=0.21.0 (from peft<=0.13.2,>=0.7.1->colossalai==0.4.9)
2025-04-11T03:23:14.6838546Z   Obtaining dependency information for accelerate>=0.21.0 from https://files.pythonhosted.org/packages/63/b1/8198e3cdd11a426b1df2912e3381018c4a4a55368f6d0857ba3ca418ef93/accelerate-1.6.0-py3-none-any.whl.metadata
2025-04-11T03:23:14.7724356Z   Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:23:14.8804189Z Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (1.12)
2025-04-11T03:23:14.8806530Z Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (3.2.1)
2025-04-11T03:23:14.8808843Z Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (3.1.4)
2025-04-11T03:23:14.8811110Z Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (2024.6.1)
2025-04-11T03:23:15.3136852Z Collecting invoke>=2.0 (from fabric->colossalai==0.4.9)
2025-04-11T03:23:15.3138838Z   Obtaining dependency information for invoke>=2.0 from https://files.pythonhosted.org/packages/0a/66/7f8c48009c72d73bc6bbe6eb87ac838d6a526146f7dab14af671121eb379/invoke-2.2.0-py3-none-any.whl.metadata
2025-04-11T03:23:15.4028804Z   Downloading invoke-2.2.0-py3-none-any.whl.metadata (3.3 kB)
2025-04-11T03:23:15.5869749Z Collecting paramiko>=2.4 (from fabric->colossalai==0.4.9)
2025-04-11T03:23:15.5871975Z   Obtaining dependency information for paramiko>=2.4 from https://files.pythonhosted.org/packages/15/f8/c7bd0ef12954a81a1d3cea60a13946bd9a49a0036a5927770c461eade7ae/paramiko-3.5.1-py3-none-any.whl.metadata
2025-04-11T03:23:15.6752712Z   Downloading paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)
2025-04-11T03:23:15.9162343Z Collecting decorator>=5 (from fabric->colossalai==0.4.9)
2025-04-11T03:23:15.9164486Z   Obtaining dependency information for decorator>=5 from https://files.pythonhosted.org/packages/4e/8c/f3147f5c4b73e7550fe5f9352eaa956ae838d5c51eb58e7a25b9f3e2643b/decorator-5.2.1-py3-none-any.whl.metadata
2025-04-11T03:23:16.0051601Z   Downloading decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)
2025-04-11T03:23:16.1098950Z Collecting deprecated>=1.2 (from fabric->colossalai==0.4.9)
2025-04-11T03:23:16.1101214Z   Obtaining dependency information for deprecated>=1.2 from https://files.pythonhosted.org/packages/6e/c6/ac0b6c1e2d138f1002bcf799d330bd6d85084fece321e662a14223794041/Deprecated-1.2.18-py2.py3-none-any.whl.metadata
2025-04-11T03:23:16.1980318Z   Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)
2025-04-11T03:23:16.4717498Z Collecting starlette<0.47.0,>=0.40.0 (from fastapi->colossalai==0.4.9)
2025-04-11T03:23:16.4719909Z   Obtaining dependency information for starlette<0.47.0,>=0.40.0 from https://files.pythonhosted.org/packages/a0/4b/528ccf7a982216885a1ff4908e886b8fb5f19862d1962f56a3fce2435a70/starlette-0.46.1-py3-none-any.whl.metadata
2025-04-11T03:23:16.5599061Z   Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)
2025-04-11T03:23:16.7636873Z Collecting annotated-types>=0.6.0 (from pydantic->colossalai==0.4.9)
2025-04-11T03:23:16.7639032Z   Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata
2025-04-11T03:23:16.8515242Z   Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
2025-04-11T03:23:17.9044411Z Collecting pydantic-core==2.33.1 (from pydantic->colossalai==0.4.9)
2025-04-11T03:23:17.9046613Z   Obtaining dependency information for pydantic-core==2.33.1 from https://files.pythonhosted.org/packages/f2/68/866ce83a51dd37e7c604ce0050ff6ad26de65a7799df89f4db87dd93d1d6/pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:17.9927883Z   Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
2025-04-11T03:23:18.2388286Z Collecting typing-extensions>=4.0 (from uvicorn==0.29.0->colossalai==0.4.9)
2025-04-11T03:23:18.2390643Z   Obtaining dependency information for typing-extensions>=4.0 from https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl.metadata
2025-04-11T03:23:18.3269244Z   Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)
2025-04-11T03:23:18.4949756Z Collecting typing-inspection>=0.4.0 (from pydantic->colossalai==0.4.9)
2025-04-11T03:23:18.4951998Z   Obtaining dependency information for typing-inspection>=0.4.0 from https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl.metadata
2025-04-11T03:23:18.5831577Z   Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)
2025-04-11T03:23:18.7670852Z Collecting beautifulsoup4 (from google->colossalai==0.4.9)
2025-04-11T03:23:18.7673071Z   Obtaining dependency information for beautifulsoup4 from https://files.pythonhosted.org/packages/f9/49/6abb616eb3cbab6a7cca303dc02fdf3836de2e0b834bf966a7f5271a34d8/beautifulsoup4-4.13.3-py3-none-any.whl.metadata
2025-04-11T03:23:18.8551916Z   Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)
2025-04-11T03:23:19.0356896Z Collecting cfgv>=2.0.0 (from pre-commit->colossalai==0.4.9)
2025-04-11T03:23:19.0359015Z   Obtaining dependency information for cfgv>=2.0.0 from https://files.pythonhosted.org/packages/c5/55/51844dd50c4fc7a33b653bfaba4c2456f06955289ca770a5dbd5fd267374/cfgv-3.4.0-py2.py3-none-any.whl.metadata
2025-04-11T03:23:19.1238162Z   Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)
2025-04-11T03:23:19.4069960Z Collecting identify>=1.0.0 (from pre-commit->colossalai==0.4.9)
2025-04-11T03:23:19.4072243Z   Obtaining dependency information for identify>=1.0.0 from https://files.pythonhosted.org/packages/07/ce/0845144ed1f0e25db5e7a79c2354c1da4b5ce392b8966449d5db8dca18f1/identify-2.6.9-py2.py3-none-any.whl.metadata
2025-04-11T03:23:19.4951259Z   Downloading identify-2.6.9-py2.py3-none-any.whl.metadata (4.4 kB)
2025-04-11T03:23:19.6699902Z Collecting nodeenv>=0.11.1 (from pre-commit->colossalai==0.4.9)
2025-04-11T03:23:19.6702094Z   Obtaining dependency information for nodeenv>=0.11.1 from https://files.pythonhosted.org/packages/d2/1d/1b658dbd2b9fa9c4c9f32accbfc0205d532c8c6194dc0f2a4c0428e7128a/nodeenv-1.9.1-py2.py3-none-any.whl.metadata
2025-04-11T03:23:19.7587323Z   Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)
2025-04-11T03:23:20.0530005Z Collecting virtualenv>=20.10.0 (from pre-commit->colossalai==0.4.9)
2025-04-11T03:23:20.0532437Z   Obtaining dependency information for virtualenv>=20.10.0 from https://files.pythonhosted.org/packages/4c/ed/3cfeb48175f0671ec430ede81f628f9fb2b1084c9064ca67ebe8c0ed6a05/virtualenv-20.30.0-py3-none-any.whl.metadata
2025-04-11T03:23:20.1410543Z   Downloading virtualenv-20.30.0-py3-none-any.whl.metadata (4.5 kB)
2025-04-11T03:23:20.5692165Z Collecting jsonschema (from ray->colossalai==0.4.9)
2025-04-11T03:23:20.5694171Z   Obtaining dependency information for jsonschema from https://files.pythonhosted.org/packages/69/4a/4f9dbeb84e8850557c02365a0eee0649abe5eb1d84af92a25731c6c0f922/jsonschema-4.23.0-py3-none-any.whl.metadata
2025-04-11T03:23:20.6572363Z   Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)
2025-04-11T03:23:20.9374758Z Collecting msgpack<2.0.0,>=1.0.0 (from ray->colossalai==0.4.9)
2025-04-11T03:23:20.9377070Z   Obtaining dependency information for msgpack<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/ff/75/09081792db60470bef19d9c2be89f024d366b1e1973c197bb59e6aabc647/msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:21.0257329Z   Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)
2025-04-11T03:23:21.1427700Z Collecting aiosignal (from ray->colossalai==0.4.9)
2025-04-11T03:23:21.1430152Z   Obtaining dependency information for aiosignal from https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl.metadata
2025-04-11T03:23:21.2309611Z   Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)
2025-04-11T03:23:21.5016225Z Collecting frozenlist (from ray->colossalai==0.4.9)
2025-04-11T03:23:21.5018599Z   Obtaining dependency information for frozenlist from https://files.pythonhosted.org/packages/ee/59/928322800306f6529d1852323014ee9008551e9bb027cc38d276cbc0b0e7/frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:21.5896947Z   Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)
2025-04-11T03:23:21.8911088Z Collecting markdown-it-py>=2.2.0 (from rich->colossalai==0.4.9)
2025-04-11T03:23:21.8913041Z   Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata
2025-04-11T03:23:21.9796189Z   Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)
2025-04-11T03:23:22.1604771Z Collecting pygments<3.0.0,>=2.13.0 (from rich->colossalai==0.4.9)
2025-04-11T03:23:22.1606730Z   Obtaining dependency information for pygments<3.0.0,>=2.13.0 from https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl.metadata
2025-04-11T03:23:22.2485619Z   Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)
2025-04-11T03:23:22.6344789Z Collecting wrapt<2,>=1.10 (from deprecated>=1.2->fabric->colossalai==0.4.9)
2025-04-11T03:23:22.6346466Z   Obtaining dependency information for wrapt<2,>=1.10 from https://files.pythonhosted.org/packages/90/ec/00759565518f268ed707dcc40f7eeec38637d46b098a1f5143bff488fe97/wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:22.7228538Z   Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)
2025-04-11T03:23:23.0583681Z Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->colossalai==0.4.9)
2025-04-11T03:23:23.0585823Z   Obtaining dependency information for mdurl~=0.1 from https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl.metadata
2025-04-11T03:23:23.1466545Z   Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)
2025-04-11T03:23:23.3632870Z Collecting bcrypt>=3.2 (from paramiko>=2.4->fabric->colossalai==0.4.9)
2025-04-11T03:23:23.3634826Z   Obtaining dependency information for bcrypt>=3.2 from https://files.pythonhosted.org/packages/cb/c6/8fedca4c2ada1b6e889c52d2943b2f968d3427e5d65f595620ec4c06fa2f/bcrypt-4.3.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata
2025-04-11T03:23:23.4660311Z   Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (10 kB)
2025-04-11T03:23:23.9214183Z Collecting cryptography>=3.3 (from paramiko>=2.4->fabric->colossalai==0.4.9)
2025-04-11T03:23:23.9215597Z   Obtaining dependency information for cryptography>=3.3 from https://files.pythonhosted.org/packages/78/2b/999b2a1e1ba2206f2d3bca267d68f350beb2b048a41ea827e08ce7260098/cryptography-44.0.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata
2025-04-11T03:23:24.0103653Z   Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.7 kB)
2025-04-11T03:23:24.2561442Z Collecting pynacl>=1.5 (from paramiko>=2.4->fabric->colossalai==0.4.9)
2025-04-11T03:23:24.2563897Z   Obtaining dependency information for pynacl>=1.5 from https://files.pythonhosted.org/packages/ee/87/f1bb6a595f14a327e8285b9eb54d41fef76c585a0edef0a45f6fc95de125/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata
2025-04-11T03:23:24.3445231Z   Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)
2025-04-11T03:23:24.5479998Z Collecting anyio<5,>=3.6.2 (from starlette<0.47.0,>=0.40.0->fastapi->colossalai==0.4.9)
2025-04-11T03:23:24.5482185Z   Obtaining dependency information for anyio<5,>=3.6.2 from https://files.pythonhosted.org/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl.metadata
2025-04-11T03:23:24.6363128Z   Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)
2025-04-11T03:23:24.7929310Z Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->colossalai==0.4.9)
2025-04-11T03:23:24.7931546Z   Obtaining dependency information for distlib<1,>=0.3.7 from https://files.pythonhosted.org/packages/91/a1/cf2472db20f7ce4a6be1253a81cfdf85ad9c7885ffbed7047fb72c24cf87/distlib-0.3.9-py2.py3-none-any.whl.metadata
2025-04-11T03:23:24.8857745Z   Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)
2025-04-11T03:23:25.0642926Z Collecting platformdirs<5,>=3.9.1 (from virtualenv>=20.10.0->pre-commit->colossalai==0.4.9)
2025-04-11T03:23:25.0645084Z   Obtaining dependency information for platformdirs<5,>=3.9.1 from https://files.pythonhosted.org/packages/6d/45/59578566b3275b8fd9157885918fcd0c4d74162928a5310926887b856a51/platformdirs-4.3.7-py3-none-any.whl.metadata
2025-04-11T03:23:25.1525938Z   Downloading platformdirs-4.3.7-py3-none-any.whl.metadata (11 kB)
2025-04-11T03:23:25.2921113Z Collecting soupsieve>1.2 (from beautifulsoup4->google->colossalai==0.4.9)
2025-04-11T03:23:25.2923004Z   Obtaining dependency information for soupsieve>1.2 from https://files.pythonhosted.org/packages/d1/c2/fe97d779f3ef3b15f05c94a2f1e3d21732574ed441687474db9d342a7315/soupsieve-2.6-py3-none-any.whl.metadata
2025-04-11T03:23:25.3801842Z   Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)
2025-04-11T03:23:25.6471367Z Collecting zipp>=3.20 (from importlib-metadata->diffusers==0.29.0->colossalai==0.4.9)
2025-04-11T03:23:25.6473408Z   Obtaining dependency information for zipp>=3.20 from https://files.pythonhosted.org/packages/b7/1a/7e4798e9339adc931158c9d69ecc34f5e6791489d469f5e50ec15e35f458/zipp-3.21.0-py3-none-any.whl.metadata
2025-04-11T03:23:25.7353678Z   Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)
2025-04-11T03:23:25.8150510Z Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (2.1.3)
2025-04-11T03:23:26.0028252Z Collecting attrs>=22.2.0 (from jsonschema->ray->colossalai==0.4.9)
2025-04-11T03:23:26.0029910Z   Obtaining dependency information for attrs>=22.2.0 from https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl.metadata
2025-04-11T03:23:26.0909453Z   Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)
2025-04-11T03:23:26.3338888Z Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->ray->colossalai==0.4.9)
2025-04-11T03:23:26.3341070Z   Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/d1/0f/8910b19ac0670a0f80ce1008e5e751c4a57e14d2c4c13a482aa6079fa9d6/jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata
2025-04-11T03:23:26.4223452Z   Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)
2025-04-11T03:23:26.6799839Z Collecting referencing>=0.28.4 (from jsonschema->ray->colossalai==0.4.9)
2025-04-11T03:23:26.6801716Z   Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/c1/b1/3baf80dc6d2b7bc27a95a67752d0208e410351e3feb4eb78de5f77454d8d/referencing-0.36.2-py3-none-any.whl.metadata
2025-04-11T03:23:26.7683168Z   Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)
2025-04-11T03:23:27.2805748Z Collecting rpds-py>=0.7.1 (from jsonschema->ray->colossalai==0.4.9)
2025-04-11T03:23:27.2809112Z   Obtaining dependency information for rpds-py>=0.7.1 from https://files.pythonhosted.org/packages/a7/a7/6d04d438f53d8bb2356bb000bea9cf5c96a9315e405b577117e344cc7404/rpds_py-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:27.3698623Z   Downloading rpds_py-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
2025-04-11T03:23:27.5015856Z Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->diffusers==0.29.0->colossalai==0.4.9) (2.0.4)
2025-04-11T03:23:27.5020159Z Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->diffusers==0.29.0->colossalai==0.4.9) (3.7)
2025-04-11T03:23:27.5024552Z Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->diffusers==0.29.0->colossalai==0.4.9) (2.2.2)
2025-04-11T03:23:27.5028807Z Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->diffusers==0.29.0->colossalai==0.4.9) (2024.6.2)
2025-04-11T03:23:27.5089723Z Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (1.3.0)
2025-04-11T03:23:27.6963393Z Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->colossalai==0.4.9)
2025-04-11T03:23:27.6965128Z   Obtaining dependency information for exceptiongroup>=1.0.2 from https://files.pythonhosted.org/packages/02/cc/b7e31358aac6ed1ef2bb790a9746ac2c69bcb3c8588b41616914eb106eaf/exceptiongroup-1.2.2-py3-none-any.whl.metadata
2025-04-11T03:23:27.7849759Z   Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)
2025-04-11T03:23:27.9523403Z Collecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->colossalai==0.4.9)
2025-04-11T03:23:27.9525146Z   Obtaining dependency information for sniffio>=1.1 from https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl.metadata
2025-04-11T03:23:28.0424400Z   Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)
2025-04-11T03:23:28.3708887Z Collecting cffi>=1.12 (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.4.9)
2025-04-11T03:23:28.3710956Z   Obtaining dependency information for cffi>=1.12 from https://files.pythonhosted.org/packages/8d/fb/4da72871d177d63649ac449aec2e8a29efe0274035880c7af59101ca2232/cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
2025-04-11T03:23:28.4596802Z   Downloading cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)
2025-04-11T03:23:28.7131389Z Collecting pycparser (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.4.9)
2025-04-11T03:23:28.7132515Z   Obtaining dependency information for pycparser from https://files.pythonhosted.org/packages/13/a3/a812df4e2dd5696d1f351d58b8fe16a405b234ad2886a0dab9183fb78109/pycparser-2.22-py3-none-any.whl.metadata
2025-04-11T03:23:28.8016988Z   Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)
2025-04-11T03:23:28.9263160Z Downloading diffusers-0.29.0-py3-none-any.whl (2.2 MB)
2025-04-11T03:23:29.2496271Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 7.2 MB/s eta 0:00:00
2025-04-11T03:23:29.3392366Z Downloading rpyc-6.0.0-py3-none-any.whl (74 kB)
2025-04-11T03:23:29.4120280Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.5/74.5 kB 906.3 kB/s eta 0:00:00
2025-04-11T03:23:29.5013053Z Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)
2025-04-11T03:23:29.8181788Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 27.9 MB/s eta 0:00:00
2025-04-11T03:23:29.9089602Z Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)
2025-04-11T03:23:29.9816887Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.8/60.8 kB 711.4 kB/s eta 0:00:00
2025-04-11T03:23:30.0702070Z Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)
2025-04-11T03:23:32.9942910Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.1/76.1 MB 14.8 MB/s eta 0:00:00
2025-04-11T03:23:33.0838179Z Downloading peft-0.13.2-py3-none-any.whl (320 kB)
2025-04-11T03:23:33.1571154Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 320.7/320.7 kB 4.3 MB/s eta 0:00:00
2025-04-11T03:23:33.2463248Z Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)
2025-04-11T03:23:33.3202426Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.6/471.6 kB 6.4 MB/s eta 0:00:00
2025-04-11T03:23:33.4085427Z Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)
2025-04-11T03:23:33.4823315Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 959.9 kB/s eta 0:00:00
2025-04-11T03:23:33.5717503Z Downloading einops-0.8.1-py3-none-any.whl (64 kB)
2025-04-11T03:23:33.6443092Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.4/64.4 kB 763.3 kB/s eta 0:00:00
2025-04-11T03:23:33.7335366Z Downloading fabric-3.2.2-py3-none-any.whl (59 kB)
2025-04-11T03:23:33.8062409Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.4/59.4 kB 691.6 kB/s eta 0:00:00
2025-04-11T03:23:33.8947834Z Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)
2025-04-11T03:23:33.9675408Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.2/95.2 kB 1.2 MB/s eta 0:00:00
2025-04-11T03:23:34.0562415Z Downloading pydantic-2.11.3-py3-none-any.whl (443 kB)
2025-04-11T03:23:34.1300430Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 443.6/443.6 kB 6.0 MB/s eta 0:00:00
2025-04-11T03:23:34.2189390Z Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
2025-04-11T03:23:34.3172707Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 20.6 MB/s eta 0:00:00
2025-04-11T03:23:34.4062564Z Downloading galore_torch-1.0-py3-none-any.whl (13 kB)
2025-04-11T03:23:34.5658685Z Downloading google-3.0.0-py2.py3-none-any.whl (45 kB)
2025-04-11T03:23:34.6385479Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.3/45.3 kB 492.6 kB/s eta 0:00:00
2025-04-11T03:23:34.7274260Z Downloading pre_commit-4.2.0-py2.py3-none-any.whl (220 kB)
2025-04-11T03:23:34.8005218Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 220.7/220.7 kB 2.9 MB/s eta 0:00:00
2025-04-11T03:23:34.8888215Z Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)
2025-04-11T03:23:34.9623360Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.2/316.2 kB 4.3 MB/s eta 0:00:00
2025-04-11T03:23:35.0504845Z Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)
2025-04-11T03:23:35.1237191Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 278.0/278.0 kB 3.7 MB/s eta 0:00:00
2025-04-11T03:23:35.2127781Z Downloading ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl (67.9 MB)
2025-04-11T03:23:38.0274048Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.9/67.9 MB 20.6 MB/s eta 0:00:00
2025-04-11T03:23:38.1160694Z Downloading rich-14.0.0-py3-none-any.whl (243 kB)
2025-04-11T03:23:38.1892536Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 243.2/243.2 kB 3.3 MB/s eta 0:00:00
2025-04-11T03:23:38.2784627Z Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
2025-04-11T03:23:38.3565495Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 17.0 MB/s eta 0:00:00
2025-04-11T03:23:38.4456838Z Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)
2025-04-11T03:23:38.5191933Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 354.7/354.7 kB 4.8 MB/s eta 0:00:00
2025-04-11T03:23:38.6071803Z Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
2025-04-11T03:23:38.7667786Z Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)
2025-04-11T03:23:38.8561193Z Downloading decorator-5.2.1-py3-none-any.whl (9.2 kB)
2025-04-11T03:23:38.9454022Z Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)
2025-04-11T03:23:39.1048029Z Downloading h11-0.14.0-py3-none-any.whl (58 kB)
2025-04-11T03:23:39.1776861Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 673.3 kB/s eta 0:00:00
2025-04-11T03:23:39.2660754Z Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)
2025-04-11T03:23:39.3403442Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 481.4/481.4 kB 6.5 MB/s eta 0:00:00
2025-04-11T03:23:39.4286094Z Downloading identify-2.6.9-py2.py3-none-any.whl (99 kB)
2025-04-11T03:23:39.5016490Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 1.2 MB/s eta 0:00:00
2025-04-11T03:23:39.5905063Z Downloading invoke-2.2.0-py3-none-any.whl (160 kB)
2025-04-11T03:23:39.6636959Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.3/160.3 kB 2.1 MB/s eta 0:00:00
2025-04-11T03:23:39.7519967Z Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)
2025-04-11T03:23:39.8249626Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.5/87.5 kB 1.1 MB/s eta 0:00:00
2025-04-11T03:23:39.9143919Z Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)
2025-04-11T03:23:39.9880739Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 378.0/378.0 kB 5.1 MB/s eta 0:00:00
2025-04-11T03:23:40.0778631Z Downloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)
2025-04-11T03:23:40.2374414Z Downloading paramiko-3.5.1-py3-none-any.whl (227 kB)
2025-04-11T03:23:40.3104553Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.3/227.3 kB 3.0 MB/s eta 0:00:00
2025-04-11T03:23:40.3988689Z Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)
2025-04-11T03:23:40.4752239Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 16.3 MB/s eta 0:00:00
2025-04-11T03:23:40.5642407Z Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)
2025-04-11T03:23:40.6389006Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.7/781.7 kB 10.6 MB/s eta 0:00:00
2025-04-11T03:23:40.7270438Z Downloading starlette-0.46.1-py3-none-any.whl (71 kB)
2025-04-11T03:23:40.8000494Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.0/72.0 kB 864.9 kB/s eta 0:00:00
2025-04-11T03:23:40.8908832Z Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
2025-04-11T03:23:41.0179935Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 28.7 MB/s eta 0:00:00
2025-04-11T03:23:41.1061056Z Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)
2025-04-11T03:23:41.1787559Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.8/45.8 kB 500.8 kB/s eta 0:00:00
2025-04-11T03:23:41.2673087Z Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)
2025-04-11T03:23:41.3569097Z Downloading virtualenv-20.30.0-py3-none-any.whl (4.3 MB)
2025-04-11T03:23:41.5042356Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 29.7 MB/s eta 0:00:00
2025-04-11T03:23:41.5925189Z Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)
2025-04-11T03:23:41.6827118Z Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)
2025-04-11T03:23:41.7561678Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.9/241.9 kB 3.2 MB/s eta 0:00:00
2025-04-11T03:23:41.8442124Z Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)
2025-04-11T03:23:41.9174187Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 186.0/186.0 kB 2.5 MB/s eta 0:00:00
2025-04-11T03:23:42.0057391Z Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)
2025-04-11T03:23:42.1653472Z Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)
2025-04-11T03:23:42.2380618Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.5/88.5 kB 1.1 MB/s eta 0:00:00
2025-04-11T03:23:42.3265860Z Downloading plumbum-1.9.0-py3-none-any.whl (127 kB)
2025-04-11T03:23:42.4000624Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.0/128.0 kB 1.6 MB/s eta 0:00:00
2025-04-11T03:23:42.4883672Z Downloading anyio-4.9.0-py3-none-any.whl (100 kB)
2025-04-11T03:23:42.5629840Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.9/100.9 kB 1.2 MB/s eta 0:00:00
2025-04-11T03:23:42.6509839Z Downloading attrs-25.3.0-py3-none-any.whl (63 kB)
2025-04-11T03:23:42.7239853Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.8/63.8 kB 750.0 kB/s eta 0:00:00
2025-04-11T03:23:42.8128275Z Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_28_x86_64.whl (284 kB)
2025-04-11T03:23:42.8863783Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 284.5/284.5 kB 3.8 MB/s eta 0:00:00
2025-04-11T03:23:42.9753560Z Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_28_x86_64.whl (4.2 MB)
2025-04-11T03:23:43.1162725Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 30.0 MB/s eta 0:00:00
2025-04-11T03:23:43.2041396Z Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)
2025-04-11T03:23:43.2779445Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 6.4 MB/s eta 0:00:00
2025-04-11T03:23:43.3659446Z Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)
2025-04-11T03:23:43.5252483Z Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)
2025-04-11T03:23:43.6146055Z Downloading platformdirs-4.3.7-py3-none-any.whl (18 kB)
2025-04-11T03:23:43.8703909Z Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)
2025-04-11T03:23:43.9452796Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 856.7/856.7 kB 17.4 MB/s eta 0:00:00
2025-04-11T03:23:44.0334287Z Downloading referencing-0.36.2-py3-none-any.whl (26 kB)
2025-04-11T03:23:44.1943046Z Downloading rpds_py-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (389 kB)
2025-04-11T03:23:44.2679135Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 389.5/389.5 kB 5.3 MB/s eta 0:00:00
2025-04-11T03:23:44.3560609Z Downloading soupsieve-2.6-py3-none-any.whl (36 kB)
2025-04-11T03:23:44.5158204Z Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)
2025-04-11T03:23:44.5889332Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.8/82.8 kB 1.0 MB/s eta 0:00:00
2025-04-11T03:23:44.6778137Z Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)
2025-04-11T03:23:44.8385591Z Downloading cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)
2025-04-11T03:23:44.9126828Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 446.2/446.2 kB 6.0 MB/s eta 0:00:00
2025-04-11T03:23:45.0009432Z Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)
2025-04-11T03:23:45.1609630Z Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)
2025-04-11T03:23:45.6321648Z Downloading pycparser-2.22-py3-none-any.whl (117 kB)
2025-04-11T03:23:45.7050052Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.6/117.6 kB 1.5 MB/s eta 0:00:00
2025-04-11T03:23:45.9366560Z Building wheels for collected packages: contexttimer
2025-04-11T03:23:45.9371009Z   Building wheel for contexttimer (setup.py): started
2025-04-11T03:23:45.9372584Z   Running command python setup.py bdist_wheel
2025-04-11T03:23:46.0445080Z   running bdist_wheel
2025-04-11T03:23:46.0571847Z   running build
2025-04-11T03:23:46.0572649Z   running build_py
2025-04-11T03:23:46.0592300Z   creating build
2025-04-11T03:23:46.0593190Z   creating build/lib
2025-04-11T03:23:46.0594423Z   creating build/lib/contexttimer
2025-04-11T03:23:46.0595273Z   copying contexttimer/__init__.py -> build/lib/contexttimer
2025-04-11T03:23:46.0596464Z   copying contexttimer/timeout.py -> build/lib/contexttimer
2025-04-11T03:23:46.0620705Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
2025-04-11T03:23:46.0621373Z   !!
2025-04-11T03:23:46.0621914Z 
2025-04-11T03:23:46.0623282Z           ********************************************************************************
2025-04-11T03:23:46.0624048Z           Please avoid running ``setup.py`` directly.
2025-04-11T03:23:46.0625047Z           Instead, use pypa/build, pypa/installer or other
2025-04-11T03:23:46.0625929Z           standards-based tools.
2025-04-11T03:23:46.0626718Z 
2025-04-11T03:23:46.0628012Z           See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
2025-04-11T03:23:46.0628848Z           ********************************************************************************
2025-04-11T03:23:46.0629707Z 
2025-04-11T03:23:46.0630680Z   !!
2025-04-11T03:23:46.0631841Z     self.initialize_options()
2025-04-11T03:23:46.0638217Z   installing to build/bdist.linux-x86_64/wheel
2025-04-11T03:23:46.0639110Z   running install
2025-04-11T03:23:46.0679862Z   running install_lib
2025-04-11T03:23:46.0696943Z   creating build/bdist.linux-x86_64
2025-04-11T03:23:46.0697693Z   creating build/bdist.linux-x86_64/wheel
2025-04-11T03:23:46.0698635Z   creating build/bdist.linux-x86_64/wheel/contexttimer
2025-04-11T03:23:46.0699758Z   copying build/lib/contexttimer/__init__.py -> build/bdist.linux-x86_64/wheel/contexttimer
2025-04-11T03:23:46.0700670Z   copying build/lib/contexttimer/timeout.py -> build/bdist.linux-x86_64/wheel/contexttimer
2025-04-11T03:23:46.0701514Z   running install_egg_info
2025-04-11T03:23:46.0726731Z   running egg_info
2025-04-11T03:23:46.0741028Z   writing contexttimer.egg-info/PKG-INFO
2025-04-11T03:23:46.0745045Z   writing dependency_links to contexttimer.egg-info/dependency_links.txt
2025-04-11T03:23:46.0747129Z   writing top-level names to contexttimer.egg-info/top_level.txt
2025-04-11T03:23:46.0764493Z   reading manifest file 'contexttimer.egg-info/SOURCES.txt'
2025-04-11T03:23:46.0769680Z   writing manifest file 'contexttimer.egg-info/SOURCES.txt'
2025-04-11T03:23:46.0771166Z   Copying contexttimer.egg-info to build/bdist.linux-x86_64/wheel/contexttimer-0.3.3-py3.10.egg-info
2025-04-11T03:23:46.0774796Z   running install_scripts
2025-04-11T03:23:46.0782725Z   creating build/bdist.linux-x86_64/wheel/contexttimer-0.3.3.dist-info/WHEEL
2025-04-11T03:23:46.0785078Z   creating '/tmp/pip-wheel-g4gk4esw/contexttimer-0.3.3-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it
2025-04-11T03:23:46.0788533Z   adding 'contexttimer/__init__.py'
2025-04-11T03:23:46.0790703Z   adding 'contexttimer/timeout.py'
2025-04-11T03:23:46.0793614Z   adding 'contexttimer-0.3.3.dist-info/METADATA'
2025-04-11T03:23:46.0794725Z   adding 'contexttimer-0.3.3.dist-info/WHEEL'
2025-04-11T03:23:46.0795705Z   adding 'contexttimer-0.3.3.dist-info/top_level.txt'
2025-04-11T03:23:46.0796769Z   adding 'contexttimer-0.3.3.dist-info/RECORD'
2025-04-11T03:23:46.0797734Z   removing build/bdist.linux-x86_64/wheel
2025-04-11T03:23:46.0981828Z   Building wheel for contexttimer (setup.py): finished with status 'done'
2025-04-11T03:23:46.0985227Z   Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5804 sha256=f90fe22713bebaef107d419dd37b253f3869cb170e5949828ec7705d0df41672
2025-04-11T03:23:46.0986275Z   Stored in directory: /github/home/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4
2025-04-11T03:23:46.0998493Z Successfully built contexttimer
2025-04-11T03:23:46.9843475Z Installing collected packages: sentencepiece, distlib, contexttimer, zipp, wrapt, typing-extensions, tqdm, soupsieve, sniffio, safetensors, rpds-py, regex, pygments, pycparser, psutil, protobuf, plumbum, platformdirs, nodeenv, msgpack, mdurl, invoke, identify, h11, frozenlist, exceptiongroup, einops, decorator, cfgv, bcrypt, attrs, annotated-types, virtualenv, uvicorn, typing-inspection, rpyc, referencing, pydantic-core, markdown-it-py, importlib-metadata, huggingface-hub, deprecated, cffi, beautifulsoup4, anyio, aiosignal, tokenizers, starlette, rich, pynacl, pydantic, pre-commit, jsonschema-specifications, google, diffusers, cryptography, bitsandbytes, accelerate, transformers, paramiko, jsonschema, fastapi, ray, peft, galore_torch, fabric, colossalai
2025-04-11T03:23:47.0852151Z   Attempting uninstall: typing-extensions
2025-04-11T03:23:47.0857688Z     Found existing installation: typing_extensions 4.11.0
2025-04-11T03:23:47.0870656Z     Uninstalling typing_extensions-4.11.0:
2025-04-11T03:23:47.1570142Z       Removing file or directory /opt/conda/envs/pytorch/lib/python3.10/site-packages/__pycache__/typing_extensions.cpython-310.pyc
2025-04-11T03:23:47.1580008Z       Removing file or directory /opt/conda/envs/pytorch/lib/python3.10/site-packages/typing_extensions-4.11.0.dist-info/
2025-04-11T03:23:47.1586138Z       Removing file or directory /opt/conda/envs/pytorch/lib/python3.10/site-packages/typing_extensions.py
2025-04-11T03:23:47.1587234Z       Successfully uninstalled typing_extensions-4.11.0
2025-04-11T03:23:47.2052759Z   changing mode of /opt/conda/envs/pytorch/bin/tqdm to 755
2025-04-11T03:23:47.7654603Z   changing mode of /opt/conda/envs/pytorch/bin/pygmentize to 755
2025-04-11T03:23:48.1066281Z   changing mode of /opt/conda/envs/pytorch/bin/nodeenv to 755
2025-04-11T03:23:48.1902632Z   changing mode of /opt/conda/envs/pytorch/bin/inv to 755
2025-04-11T03:23:48.1904654Z   changing mode of /opt/conda/envs/pytorch/bin/invoke to 755
2025-04-11T03:23:48.2028559Z   changing mode of /opt/conda/envs/pytorch/bin/identify-cli to 755
2025-04-11T03:23:48.4107168Z   changing mode of /opt/conda/envs/pytorch/bin/virtualenv to 755
2025-04-11T03:23:48.4462122Z   changing mode of /opt/conda/envs/pytorch/bin/uvicorn to 755
2025-04-11T03:23:48.4874754Z   changing mode of /opt/conda/envs/pytorch/bin/rpyc_classic to 755
2025-04-11T03:23:48.4877137Z   changing mode of /opt/conda/envs/pytorch/bin/rpyc_classic.py to 755
2025-04-11T03:23:48.4879357Z   changing mode of /opt/conda/envs/pytorch/bin/rpyc_registry to 755
2025-04-11T03:23:48.4881511Z   changing mode of /opt/conda/envs/pytorch/bin/rpyc_registry.py to 755
2025-04-11T03:23:48.5906265Z   changing mode of /opt/conda/envs/pytorch/bin/markdown-it to 755
2025-04-11T03:23:48.7581504Z   changing mode of /opt/conda/envs/pytorch/bin/huggingface-cli to 755
2025-04-11T03:23:49.5032450Z   changing mode of /opt/conda/envs/pytorch/bin/pre-commit to 755
2025-04-11T03:23:50.3270868Z   changing mode of /opt/conda/envs/pytorch/bin/diffusers-cli to 755
2025-04-11T03:23:51.7603627Z   changing mode of /opt/conda/envs/pytorch/bin/accelerate to 755
2025-04-11T03:23:51.7605588Z   changing mode of /opt/conda/envs/pytorch/bin/accelerate-config to 755
2025-04-11T03:23:51.7607735Z   changing mode of /opt/conda/envs/pytorch/bin/accelerate-estimate-memory to 755
2025-04-11T03:23:51.7609575Z   changing mode of /opt/conda/envs/pytorch/bin/accelerate-launch to 755
2025-04-11T03:23:51.7611760Z   changing mode of /opt/conda/envs/pytorch/bin/accelerate-merge-weights to 755
2025-04-11T03:23:55.2988950Z   changing mode of /opt/conda/envs/pytorch/bin/transformers-cli to 755
2025-04-11T03:23:55.4347925Z   changing mode of /opt/conda/envs/pytorch/bin/jsonschema to 755
2025-04-11T03:23:55.4849407Z   changing mode of /opt/conda/envs/pytorch/bin/fastapi to 755
2025-04-11T03:23:58.4682546Z   changing mode of /opt/conda/envs/pytorch/bin/ray to 755
2025-04-11T03:23:58.4683775Z   changing mode of /opt/conda/envs/pytorch/bin/serve to 755
2025-04-11T03:23:58.4685571Z   changing mode of /opt/conda/envs/pytorch/bin/tune to 755
2025-04-11T03:23:58.6339043Z   changing mode of /opt/conda/envs/pytorch/bin/fab to 755
2025-04-11T03:23:58.6350475Z   Running setup.py develop for colossalai
2025-04-11T03:23:58.6352579Z     Running command python setup.py develop
2025-04-11T03:24:00.3205279Z     [extension] Building extensionscpu_adam_x86, layernorm_cuda, moe_cuda, fused_optim_cuda, inference_ops_cuda, scaled_masked_softmax_cuda, scaled_upper_triangle_masked_softmax_cuda
2025-04-11T03:24:00.3206003Z     running develop
2025-04-11T03:24:00.3206737Z     /opt/conda/envs/pytorch/lib/python3.10/site-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.
2025-04-11T03:24:00.3207445Z     !!
2025-04-11T03:24:00.3207584Z 
2025-04-11T03:24:00.3208409Z             ********************************************************************************
2025-04-11T03:24:00.3209414Z             Please avoid running ``setup.py`` and ``easy_install``.
2025-04-11T03:24:00.3210334Z             Instead, use pypa/build, pypa/installer or other
2025-04-11T03:24:00.3211268Z             standards-based tools.
2025-04-11T03:24:00.3212084Z 
2025-04-11T03:24:00.3213254Z             See https://github.com/pypa/setuptools/issues/917 for details.
2025-04-11T03:24:00.3214039Z             ********************************************************************************
2025-04-11T03:24:00.3214903Z 
2025-04-11T03:24:00.3215958Z     !!
2025-04-11T03:24:00.3216945Z       easy_install.initialize_options(self)
2025-04-11T03:24:00.3263744Z     /opt/conda/envs/pytorch/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
2025-04-11T03:24:00.3264388Z     !!
2025-04-11T03:24:00.3264736Z 
2025-04-11T03:24:00.3265836Z             ********************************************************************************
2025-04-11T03:24:00.3266786Z             Please avoid running ``setup.py`` directly.
2025-04-11T03:24:00.3267802Z             Instead, use pypa/build, pypa/installer or other
2025-04-11T03:24:00.3268962Z             standards-based tools.
2025-04-11T03:24:00.3309656Z 
2025-04-11T03:24:00.3315471Z             See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
2025-04-11T03:24:00.3316202Z             ********************************************************************************
2025-04-11T03:24:00.3317166Z 
2025-04-11T03:24:00.3318239Z     !!
2025-04-11T03:24:00.3319281Z       self.initialize_options()
2025-04-11T03:24:00.3703649Z     running egg_info
2025-04-11T03:24:00.3704678Z     creating colossalai.egg-info
2025-04-11T03:24:00.3740847Z     writing colossalai.egg-info/PKG-INFO
2025-04-11T03:24:00.3749593Z     writing dependency_links to colossalai.egg-info/dependency_links.txt
2025-04-11T03:24:00.3750324Z     writing entry points to colossalai.egg-info/entry_points.txt
2025-04-11T03:24:00.3751300Z     writing requirements to colossalai.egg-info/requires.txt
2025-04-11T03:24:00.3752576Z     writing top-level names to colossalai.egg-info/top_level.txt
2025-04-11T03:24:00.3753263Z     writing manifest file 'colossalai.egg-info/SOURCES.txt'
2025-04-11T03:24:00.4295604Z     reading manifest file 'colossalai.egg-info/SOURCES.txt'
2025-04-11T03:24:00.4296758Z     reading manifest template 'MANIFEST.in'
2025-04-11T03:24:00.4716178Z     warning: no files found matching '*.tr' under directory 'colossalai'
2025-04-11T03:24:00.4923105Z     warning: no files found matching '*.cc' under directory 'colossalai'
2025-04-11T03:24:00.5025061Z     warning: no files found matching '*.pyi' under directory 'colossalai'
2025-04-11T03:24:00.5082608Z     warning: no files found matching '*.tr' under directory 'extensions'
2025-04-11T03:24:00.5103439Z     warning: no files found matching '*.cc' under directory 'extensions'
2025-04-11T03:24:00.5114100Z     warning: no files found matching '*.pyi' under directory 'extensions'
2025-04-11T03:24:00.5114974Z     adding license file 'LICENSE'
2025-04-11T03:24:00.5185718Z     writing manifest file 'colossalai.egg-info/SOURCES.txt'
2025-04-11T03:24:00.5187857Z     running build_ext
2025-04-11T03:24:00.5233429Z     /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/cpp_extension.py:425: UserWarning: There are no g++ version bounds defined for CUDA version 12.1
2025-04-11T03:24:00.5234297Z       warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')
2025-04-11T03:24:00.5241778Z     building 'colossalai._C.cpu_adam_x86' extension
2025-04-11T03:24:00.5242500Z     creating /__w/ColossalAI/ColossalAI/build
2025-04-11T03:24:00.5243914Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310
2025-04-11T03:24:00.5244651Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w
2025-04-11T03:24:00.5245828Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI
2025-04-11T03:24:00.5246755Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI
2025-04-11T03:24:00.5247773Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions
2025-04-11T03:24:00.5248736Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc
2025-04-11T03:24:00.5249741Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel
2025-04-11T03:24:00.5250990Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86
2025-04-11T03:24:00.5493800Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:24:00.5494346Z     Compiling objects...
2025-04-11T03:24:00.5495605Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:24:26.9292147Z     [1/1] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -std=c++14 -std=c++17 -lcudart -lcublas -g -Wno-reorder -fopenmp -march=native -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=cpu_adam_x86 -D_GLIBCXX_USE_CXX11_ABI=0
2025-04-11T03:24:26.9296784Z     /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp:237: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
2025-04-11T03:24:26.9297420Z       237 | #pragma unroll 4
2025-04-11T03:24:26.9297713Z           |
2025-04-11T03:24:26.9298284Z     /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp:352: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
2025-04-11T03:24:26.9299248Z       352 | #pragma unroll 8
2025-04-11T03:24:26.9299583Z           |
2025-04-11T03:24:26.9300226Z     In file included from /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/Exceptions.h:14,
2025-04-11T03:24:26.9301005Z                      from /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/python.h:11,
2025-04-11T03:24:26.9301904Z                      from /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/extension.h:9,
2025-04-11T03:24:26.9302511Z                      from /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.h:29,
2025-04-11T03:24:26.9303075Z                      from /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp:22:
2025-04-11T03:24:26.9304353Z     /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘class pybind11::class_<Adam_Optimizer>’:
2025-04-11T03:24:26.9305223Z     /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp:443:51:   required from here
2025-04-11T03:24:26.9306532Z     /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/pybind11/pybind11.h:1496:7: warning: ‘pybind11::class_<Adam_Optimizer>’ declared with greater visibility than its base ‘pybind11::detail::generic_type’ [-Wattributes]
2025-04-11T03:24:26.9307856Z      1496 | class class_ : public detail::generic_type {
2025-04-11T03:24:26.9308369Z           |       ^~~~~~
2025-04-11T03:24:26.9362457Z     creating build/lib.linux-x86_64-cpython-310
2025-04-11T03:24:26.9362918Z     creating build/lib.linux-x86_64-cpython-310/colossalai
2025-04-11T03:24:26.9363362Z     creating build/lib.linux-x86_64-cpython-310/colossalai/_C
2025-04-11T03:24:26.9367272Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/cpu_adam_x86.cpython-310-x86_64-linux-gnu.so
2025-04-11T03:24:27.2946730Z     building 'colossalai._C.layernorm_cuda' extension
2025-04-11T03:24:27.2947410Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda
2025-04-11T03:24:27.2948198Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind
2025-04-11T03:24:27.2950125Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/layernorm
2025-04-11T03:24:27.3217113Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:24:27.3219549Z     Compiling objects...
2025-04-11T03:24:27.3223731Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:24:44.0229253Z     [1/2] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/layernorm/layer_norm.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/usr/local/cuda/include -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/layernorm/layer_norm.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/layernorm/layer_norm.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=layernorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:25:21.5282640Z     [2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/layer_norm_kernel.o.d -I/usr/local/cuda/include -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/layer_norm_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/layer_norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -maxrregcount=50 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=layernorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:25:21.5330254Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/layer_norm_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/layernorm/layer_norm.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/layernorm_cuda.cpython-310-x86_64-linux-gnu.so
2025-04-11T03:25:21.8561892Z     building 'colossalai._C.moe_cuda' extension
2025-04-11T03:25:21.8564826Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/moe
2025-04-11T03:25:21.8829415Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:25:21.8877105Z     Compiling objects...
2025-04-11T03:25:21.8892578Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:25:39.1505473Z     [1/2] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/moe/moe.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/moe/moe.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/moe/moe.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=moe_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:28:06.4662582Z     [2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/moe_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/moe_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/moe_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=moe_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:28:06.4703697Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/moe_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/moe/moe.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/moe_cuda.cpython-310-x86_64-linux-gnu.so
2025-04-11T03:28:06.6961225Z     building 'colossalai._C.fused_optim_cuda' extension
2025-04-11T03:28:06.6963730Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/optimizer
2025-04-11T03:28:06.7224017Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:28:06.7225134Z     Compiling objects...
2025-04-11T03:28:06.7226438Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:28:23.4378185Z     [1/6] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/optimizer/optimizer.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/optimizer/optimizer.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/optimizer/optimizer.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:28:56.4245845Z     [2/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:28:57.8729833Z     [3/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:28:57.9866629Z     [4/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:28:58.5008650Z     [5/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:28:58.6544586Z     [6/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:28:58.6588389Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/optimizer/optimizer.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so
2025-04-11T03:28:58.9053907Z     building 'colossalai._C.inference_ops_cuda' extension
2025-04-11T03:28:58.9055179Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/inference
2025-04-11T03:28:58.9315443Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:28:58.9317820Z     Compiling objects...
2025-04-11T03:28:58.9318460Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:29:15.1166605Z     [1/9] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/inference/inference.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/inference/inference.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/inference/inference.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:31:32.4955115Z     [2/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:31:33.4107840Z     [3/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/convert_fp8_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/convert_fp8_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/convert_fp8_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:31:33.5770273Z     [4/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/activation_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/activation_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/activation_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:31:38.1197789Z     [5/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:31:39.4245234Z     [6/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:31:41.3597845Z     [7/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/rms_layernorm_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/rms_layernorm_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/rms_layernorm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:31:45.8869719Z     [8/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:35:44.6402487Z     [9/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:35:44.6447308Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/activation_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/convert_fp8_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/rms_layernorm_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/inference/inference.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/inference_ops_cuda.cpython-310-x86_64-linux-gnu.so
2025-04-11T03:35:45.0783016Z     building 'colossalai._C.scaled_masked_softmax_cuda' extension
2025-04-11T03:35:45.0783956Z     creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax
2025-04-11T03:35:45.1045717Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:35:45.1047046Z     Compiling objects...
2025-04-11T03:35:45.1048325Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:36:00.5825666Z     [1/2] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_masked_softmax.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_masked_softmax.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:36:30.3298878Z     [2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -std=c++14 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90
2025-04-11T03:36:30.3304060Z     nvcc warning : incompatible redefinition for option 'std', the last value of this option was used
2025-04-11T03:36:30.3317119Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_masked_softmax.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so
2025-04-11T03:36:30.5516386Z     building 'colossalai._C.scaled_upper_triangle_masked_softmax_cuda' extension
2025-04-11T03:36:30.5775345Z     Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
2025-04-11T03:36:30.5776219Z     Compiling objects...
2025-04-11T03:36:30.5777487Z     Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
2025-04-11T03:36:45.7041647Z     [1/2] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_upper_triangle_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:39:11.8258830Z     [2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_upper_triangle_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
2025-04-11T03:39:11.8276164Z     g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/scaled_upper_triangle_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so
2025-04-11T03:39:12.0519284Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/cpu_adam_x86.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T03:39:12.0613925Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/layernorm_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T03:39:12.0629517Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/moe_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T03:39:12.0653068Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T03:39:12.0687582Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/inference_ops_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T03:39:12.1211593Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T03:39:12.1217544Z     copying build/lib.linux-x86_64-cpython-310/colossalai/_C/scaled_upper_triangle_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
2025-04-11T03:39:12.1235587Z     Creating /opt/conda/envs/pytorch/lib/python3.10/site-packages/colossalai.egg-link (link to .)
2025-04-11T03:39:12.1240842Z     Adding colossalai 0.4.9 to easy-install.pth file
2025-04-11T03:39:12.1252301Z     Installing colossalai script to /opt/conda/envs/pytorch/bin
2025-04-11T03:39:12.1255072Z 
2025-04-11T03:39:12.1256138Z     Installed /__w/ColossalAI/ColossalAI
2025-04-11T03:39:13.6391645Z Successfully installed accelerate-1.6.0 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.9.0 attrs-25.3.0 bcrypt-4.3.0 beautifulsoup4-4.13.3 bitsandbytes-0.45.5 cffi-1.17.1 cfgv-3.4.0 colossalai-0.4.9 contexttimer-0.3.3 cryptography-44.0.2 decorator-5.2.1 deprecated-1.2.18 diffusers-0.29.0 distlib-0.3.9 einops-0.8.1 exceptiongroup-1.2.2 fabric-3.2.2 fastapi-0.115.12 frozenlist-1.5.0 galore_torch-1.0 google-3.0.0 h11-0.14.0 huggingface-hub-0.30.2 identify-2.6.9 importlib-metadata-8.6.1 invoke-2.2.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 markdown-it-py-3.0.0 mdurl-0.1.2 msgpack-1.1.0 nodeenv-1.9.1 paramiko-3.5.1 peft-0.13.2 platformdirs-4.3.7 plumbum-1.9.0 pre-commit-4.2.0 protobuf-6.30.2 psutil-7.0.0 pycparser-2.22 pydantic-2.11.3 pydantic-core-2.33.1 pygments-2.19.1 pynacl-1.5.0 ray-2.44.1 referencing-0.36.2 regex-2024.11.6 rich-14.0.0 rpds-py-0.24.0 rpyc-6.0.0 safetensors-0.5.3 sentencepiece-0.2.0 sniffio-1.3.1 soupsieve-2.6 starlette-0.46.1 tokenizers-0.15.2 tqdm-4.67.1 transformers-4.39.3 typing-extensions-4.13.2 typing-inspection-0.4.0 uvicorn-0.29.0 virtualenv-20.30.0 wrapt-1.17.2 zipp-3.21.0
2025-04-11T03:39:13.6395103Z WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-11T03:39:14.4461281Z Collecting git+https://github.com/hpcaitech/pytest-testmon (from -r requirements/requirements-test.txt (line 3))
2025-04-11T03:39:14.4465814Z   Cloning https://github.com/hpcaitech/pytest-testmon to /tmp/pip-req-build-cz3l41q1
2025-04-11T03:39:14.4479776Z   Running command git clone --filter=blob:none --quiet https://github.com/hpcaitech/pytest-testmon /tmp/pip-req-build-cz3l41q1
2025-04-11T03:39:16.5662086Z   Resolved https://github.com/hpcaitech/pytest-testmon to commit be30f4ac384656daae1a9157e6de97825caaf0ba
2025-04-11T03:39:16.5692676Z   Installing build dependencies: started
2025-04-11T03:39:20.6377881Z   Installing build dependencies: finished with status 'done'
2025-04-11T03:39:20.6383143Z   Getting requirements to build wheel: started
2025-04-11T03:39:20.8007839Z   Getting requirements to build wheel: finished with status 'done'
2025-04-11T03:39:20.8014362Z   Preparing metadata (pyproject.toml): started
2025-04-11T03:39:20.9624485Z   Preparing metadata (pyproject.toml): finished with status 'done'
2025-04-11T03:39:21.4576538Z Collecting pytest (from -r requirements/requirements-test.txt (line 1))
2025-04-11T03:39:21.7532060Z   Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)
2025-04-11T03:39:22.4180692Z Collecting coverage==7.2.3 (from -r requirements/requirements-test.txt (line 2))
2025-04-11T03:39:22.5085494Z   Downloading coverage-7.2.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)
2025-04-11T03:39:22.5097896Z Requirement already satisfied: torchvision in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 4)) (0.17.2)
2025-04-11T03:39:22.6896834Z Collecting timm (from -r requirements/requirements-test.txt (line 5))
2025-04-11T03:39:22.7794278Z   Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)
2025-04-11T03:39:22.8517344Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.0/52.0 kB 749.3 kB/s eta 0:00:00
2025-04-11T03:39:23.1888463Z Collecting titans (from -r requirements/requirements-test.txt (line 6))
2025-04-11T03:39:23.2795063Z   Downloading titans-0.0.7.tar.gz (38 kB)
2025-04-11T03:39:23.3621926Z   Preparing metadata (setup.py): started
2025-04-11T03:39:23.5103397Z   Preparing metadata (setup.py): finished with status 'done'
2025-04-11T03:39:23.5112885Z Requirement already satisfied: torchaudio>=0.13.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 7)) (2.2.2)
2025-04-11T03:39:24.0955106Z Collecting torchx-nightly==2022.6.29 (from -r requirements/requirements-test.txt (line 8))
2025-04-11T03:39:24.1858672Z   Downloading torchx_nightly-2022.6.29-py3-none-any.whl.metadata (4.9 kB)
2025-04-11T03:39:24.5945758Z Collecting torchrec==0.2.0 (from -r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:24.6841919Z   Downloading torchrec-0.2.0-py39-none-any.whl.metadata (6.7 kB)
2025-04-11T03:39:24.7553557Z Requirement already satisfied: contexttimer in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 10)) (0.3.3)
2025-04-11T03:39:24.7556085Z Requirement already satisfied: einops in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 11)) (0.8.1)
2025-04-11T03:39:24.7558535Z Requirement already satisfied: triton in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 12)) (2.2.0)
2025-04-11T03:39:24.9411143Z Collecting requests==2.27.1 (from -r requirements/requirements-test.txt (line 13))
2025-04-11T03:39:25.0307829Z   Downloading requests-2.27.1-py2.py3-none-any.whl.metadata (5.0 kB)
2025-04-11T03:39:25.0318500Z Requirement already satisfied: SentencePiece in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 14)) (0.2.0)
2025-04-11T03:39:25.0321108Z Requirement already satisfied: ninja in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 15)) (1.11.1.1)
2025-04-11T03:39:25.2090108Z Collecting flash_attn (from -r requirements/requirements-test.txt (line 16))
2025-04-11T03:39:25.2987221Z   Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)
2025-04-11T03:39:25.7837174Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 12.8 MB/s eta 0:00:00
2025-04-11T03:39:26.8226321Z   Preparing metadata (setup.py): started
2025-04-11T03:39:29.8623736Z   Preparing metadata (setup.py): finished with status 'done'
2025-04-11T03:39:30.0484931Z Collecting datasets (from -r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:30.1387026Z   Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:39:30.2101965Z Requirement already satisfied: pydantic in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 18)) (2.11.3)
2025-04-11T03:39:30.2104204Z Requirement already satisfied: ray in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 19)) (2.44.1)
2025-04-11T03:39:30.2110617Z Requirement already satisfied: peft>=0.7.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 20)) (0.13.2)
2025-04-11T03:39:30.3404183Z Collecting pyre-extensions (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
2025-04-11T03:39:30.4334763Z   Downloading pyre_extensions-0.0.32-py3-none-any.whl.metadata (4.0 kB)
2025-04-11T03:39:30.6039826Z Collecting docstring-parser==0.8.1 (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
2025-04-11T03:39:30.6941508Z   Downloading docstring_parser-0.8.1.tar.gz (14 kB)
2025-04-11T03:39:30.7708498Z   Installing build dependencies: started
2025-04-11T03:39:33.1530527Z   Installing build dependencies: finished with status 'done'
2025-04-11T03:39:33.1539919Z   Getting requirements to build wheel: started
2025-04-11T03:39:33.2856151Z   Getting requirements to build wheel: finished with status 'done'
2025-04-11T03:39:33.2863174Z   Preparing metadata (pyproject.toml): started
2025-04-11T03:39:33.4189755Z   Preparing metadata (pyproject.toml): finished with status 'done'
2025-04-11T03:39:33.4201496Z Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8)) (6.0.1)
2025-04-11T03:39:33.6013000Z Collecting docker (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
2025-04-11T03:39:33.6944838Z   Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)
2025-04-11T03:39:33.6953721Z Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8)) (3.13.1)
2025-04-11T03:39:33.6956519Z Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8)) (2024.6.1)
2025-04-11T03:39:33.8166495Z Collecting arrow (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:33.9150845Z   Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)
2025-04-11T03:39:33.9159705Z Requirement already satisfied: attrs in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (25.3.0)
2025-04-11T03:39:33.9162336Z Requirement already satisfied: certifi in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (2024.6.2)
2025-04-11T03:39:33.9164751Z Requirement already satisfied: charset-normalizer in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (2.0.4)
2025-04-11T03:39:34.1931828Z Collecting cmake (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:34.2852568Z   Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)
2025-04-11T03:39:34.7471511Z Collecting Cython (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:34.8383926Z   Downloading Cython-3.0.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)
2025-04-11T03:39:35.0065719Z Collecting distro (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:35.0956786Z   Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)
2025-04-11T03:39:35.7243431Z Collecting hypothesis (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:35.8167347Z   Downloading hypothesis-6.131.0-py3-none-any.whl.metadata (5.6 kB)
2025-04-11T03:39:35.8885253Z Requirement already satisfied: idna in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (3.7)
2025-04-11T03:39:36.0008538Z Collecting iopath (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:36.0902918Z   Downloading iopath-0.1.10.tar.gz (42 kB)
2025-04-11T03:39:36.1622893Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.2/42.2 kB 579.1 kB/s eta 0:00:00
2025-04-11T03:39:36.1705071Z   Preparing metadata (setup.py): started
2025-04-11T03:39:36.3193971Z   Preparing metadata (setup.py): finished with status 'done'
2025-04-11T03:39:36.3204649Z Requirement already satisfied: Jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (3.1.4)
2025-04-11T03:39:36.3209260Z Requirement already satisfied: MarkupSafe in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (2.1.3)
2025-04-11T03:39:36.4169478Z Collecting mypy-extensions (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:36.5075462Z   Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)
2025-04-11T03:39:36.5083657Z Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (1.26.4)
2025-04-11T03:39:36.5086260Z Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (24.1)
2025-04-11T03:39:36.7803504Z Collecting pandas (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:36.8704239Z   Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)
2025-04-11T03:39:36.9421747Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.9/89.9 kB 1.4 MB/s eta 0:00:00
2025-04-11T03:39:37.0448549Z Collecting portalocker (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:37.1363493Z   Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)
2025-04-11T03:39:37.4328448Z Collecting pyarrow (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:37.5233980Z   Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)
2025-04-11T03:39:37.6227820Z Collecting pyDeprecate (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:37.7125819Z   Downloading pyDeprecate-0.3.2-py3-none-any.whl.metadata (10 kB)
2025-04-11T03:39:37.9684372Z Collecting pyparsing (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:38.0573642Z   Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)
2025-04-11T03:39:38.0632892Z Collecting pyre-extensions (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
2025-04-11T03:39:38.1535857Z   Downloading pyre_extensions-0.0.27-py3-none-any.whl.metadata (4.0 kB)
2025-04-11T03:39:38.2561343Z Collecting python-dateutil (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:38.3454400Z   Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
2025-04-11T03:39:38.5422000Z Collecting pytz (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:38.6312334Z   Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)
2025-04-11T03:39:38.8760158Z Collecting scikit-build (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:38.9665760Z   Downloading scikit_build-0.18.1-py3-none-any.whl.metadata (18 kB)
2025-04-11T03:39:39.1367354Z Collecting six (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:39.2258873Z   Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
2025-04-11T03:39:39.3252687Z Collecting sortedcontainers (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:39.4147803Z   Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)
2025-04-11T03:39:39.6537003Z Collecting tabulate (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:39.7427777Z   Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)
2025-04-11T03:39:39.9935145Z Collecting torchmetrics (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:40.0826665Z   Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)
2025-04-11T03:39:40.1543407Z Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (4.67.1)
2025-04-11T03:39:40.2520280Z Collecting typing-inspect (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:40.3464231Z   Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)
2025-04-11T03:39:40.3472286Z Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (4.13.2)
2025-04-11T03:39:40.3474928Z Requirement already satisfied: urllib3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (2.2.2)
2025-04-11T03:39:40.7542660Z Collecting usort (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:40.8439656Z   Downloading usort-1.0.8.post1-py3-none-any.whl.metadata (3.5 kB)
2025-04-11T03:39:41.0928641Z Collecting websocket-client (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:41.1827968Z   Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)
2025-04-11T03:39:41.5877351Z Collecting fbgemm-gpu (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:41.6782928Z   Downloading fbgemm_gpu-1.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (2.8 kB)
2025-04-11T03:39:41.9545318Z Collecting urllib3 (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:42.0436244Z   Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)
2025-04-11T03:39:42.1158586Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.1/50.1 kB 719.5 kB/s eta 0:00:00
2025-04-11T03:39:42.1331231Z Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pytest->-r requirements/requirements-test.txt (line 1)) (1.2.2)
2025-04-11T03:39:42.2284239Z Collecting iniconfig (from pytest->-r requirements/requirements-test.txt (line 1))
2025-04-11T03:39:42.3174891Z   Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)
2025-04-11T03:39:42.4199453Z Collecting pluggy<2,>=1.5 (from pytest->-r requirements/requirements-test.txt (line 1))
2025-04-11T03:39:42.5093072Z   Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)
2025-04-11T03:39:42.6136583Z Collecting tomli>=1 (from pytest->-r requirements/requirements-test.txt (line 1))
2025-04-11T03:39:42.7027968Z   Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)
2025-04-11T03:39:42.7912104Z Collecting pytest (from -r requirements/requirements-test.txt (line 1))
2025-04-11T03:39:42.8810739Z   Downloading pytest-7.4.4-py3-none-any.whl.metadata (7.9 kB)
2025-04-11T03:39:42.9882948Z Collecting requirements-parser (from pytest-testmon==2.0.7b1->-r requirements/requirements-test.txt (line 3))
2025-04-11T03:39:43.0784733Z   Downloading requirements_parser-0.11.0-py3-none-any.whl.metadata (4.7 kB)
2025-04-11T03:39:43.1347207Z Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchvision->-r requirements/requirements-test.txt (line 4)) (2.2.2)
2025-04-11T03:39:43.1352060Z Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchvision->-r requirements/requirements-test.txt (line 4)) (10.3.0)
2025-04-11T03:39:43.1395060Z Requirement already satisfied: huggingface_hub in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from timm->-r requirements/requirements-test.txt (line 5)) (0.30.2)
2025-04-11T03:39:43.1397620Z Requirement already satisfied: safetensors in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from timm->-r requirements/requirements-test.txt (line 5)) (0.5.3)
2025-04-11T03:39:43.1430689Z Requirement already satisfied: colossalai in /__w/ColossalAI/ColossalAI (from titans->-r requirements/requirements-test.txt (line 6)) (0.4.9)
2025-04-11T03:39:43.3502995Z Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:43.4531785Z   Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)
2025-04-11T03:39:43.4591077Z INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.
2025-04-11T03:39:43.4631155Z Collecting datasets (from -r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:43.5532285Z   Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:39:43.7972934Z   Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:39:44.0407064Z   Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:39:44.2831837Z   Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:39:44.5241689Z   Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:39:44.7677351Z   Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)
2025-04-11T03:39:45.0101100Z   Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)
2025-04-11T03:39:45.1578571Z INFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.
2025-04-11T03:39:45.2517857Z   Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)
2025-04-11T03:39:45.4902564Z   Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)
2025-04-11T03:39:45.7304172Z   Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:39:45.9672987Z   Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)
2025-04-11T03:39:46.2206520Z   Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:39:46.4510278Z Collecting pyarrow-hotfix (from datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:46.5406445Z   Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)
2025-04-11T03:39:46.6118419Z INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
2025-04-11T03:39:46.6155976Z Collecting datasets (from -r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:46.7059412Z   Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:39:46.9432695Z   Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)
2025-04-11T03:39:47.3076884Z Collecting xxhash (from datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:47.3972590Z   Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
2025-04-11T03:39:47.6438976Z Collecting multiprocess (from datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:47.7338222Z   Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)
2025-04-11T03:39:47.9190368Z Collecting fsspec (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
2025-04-11T03:39:48.0090337Z   Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)
2025-04-11T03:39:48.7773059Z Collecting aiohttp (from datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:48.8704577Z   Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
2025-04-11T03:39:48.9488026Z Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic->-r requirements/requirements-test.txt (line 18)) (0.7.0)
2025-04-11T03:39:48.9492638Z Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic->-r requirements/requirements-test.txt (line 18)) (2.33.1)
2025-04-11T03:39:48.9499835Z Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic->-r requirements/requirements-test.txt (line 18)) (0.4.0)
2025-04-11T03:39:49.1081648Z Requirement already satisfied: click>=7.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (8.1.8)
2025-04-11T03:39:49.1086416Z Requirement already satisfied: jsonschema in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (4.23.0)
2025-04-11T03:39:49.1090877Z Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (1.1.0)
2025-04-11T03:39:49.1096142Z Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (6.30.2)
2025-04-11T03:39:49.1099269Z Requirement already satisfied: aiosignal in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (1.3.2)
2025-04-11T03:39:49.1101491Z Requirement already satisfied: frozenlist in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (1.5.0)
2025-04-11T03:39:49.1247826Z Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (7.0.0)
2025-04-11T03:39:49.1251844Z Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (4.39.3)
2025-04-11T03:39:49.1256978Z Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (1.6.0)
2025-04-11T03:39:49.4155803Z Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:49.5053131Z   Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)
2025-04-11T03:39:49.6069222Z Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:49.6960679Z   Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)
2025-04-11T03:39:50.2106249Z Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:50.2997482Z   Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)
2025-04-11T03:39:50.5623151Z Collecting propcache>=0.2.0 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:50.6514363Z   Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
2025-04-11T03:39:51.1655807Z Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:51.2546696Z   Downloading yarl-1.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)
2025-04-11T03:39:51.3264716Z      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.8/71.8 kB 1.1 MB/s eta 0:00:00
2025-04-11T03:39:51.5051217Z Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->torchvision->-r requirements/requirements-test.txt (line 4)) (1.12)
2025-04-11T03:39:51.5053815Z Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->torchvision->-r requirements/requirements-test.txt (line 4)) (3.2.1)
2025-04-11T03:39:51.6627194Z Collecting types-python-dateutil>=2.8.10 (from arrow->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:51.7525252Z   Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)
2025-04-11T03:39:51.7860944Z Requirement already satisfied: pre-commit in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.2.0)
2025-04-11T03:39:51.7863295Z Requirement already satisfied: rich in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (14.0.0)
2025-04-11T03:39:51.7866268Z Requirement already satisfied: fabric in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.2.2)
2025-04-11T03:39:51.7872630Z Requirement already satisfied: google in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.0.0)
2025-04-11T03:39:51.7879562Z Requirement already satisfied: bitsandbytes>=0.39.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.45.5)
2025-04-11T03:39:51.7884781Z Requirement already satisfied: rpyc==6.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (6.0.0)
2025-04-11T03:39:51.7887250Z Requirement already satisfied: fastapi in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.115.12)
2025-04-11T03:39:51.7891356Z Requirement already satisfied: uvicorn==0.29.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.29.0)
2025-04-11T03:39:51.7895333Z Requirement already satisfied: galore_torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.0)
2025-04-11T03:39:51.7899709Z Requirement already satisfied: diffusers==0.29.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.29.0)
2025-04-11T03:39:52.0701995Z Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers->peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (2024.11.6)
2025-04-11T03:39:52.0709498Z Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers->peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (0.15.2)
2025-04-11T03:39:52.1119013Z Requirement already satisfied: importlib-metadata in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (8.6.1)
2025-04-11T03:39:52.1189204Z Requirement already satisfied: plumbum in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rpyc==6.0.0->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.9.0)
2025-04-11T03:39:52.1318936Z Requirement already satisfied: h11>=0.8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from uvicorn==0.29.0->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.14.0)
2025-04-11T03:39:52.3442351Z Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jsonschema->ray->-r requirements/requirements-test.txt (line 19)) (2024.10.1)
2025-04-11T03:39:52.3446867Z Requirement already satisfied: referencing>=0.28.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jsonschema->ray->-r requirements/requirements-test.txt (line 19)) (0.36.2)
2025-04-11T03:39:52.3451292Z Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jsonschema->ray->-r requirements/requirements-test.txt (line 19)) (0.24.0)
2025-04-11T03:39:52.3536869Z INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.
2025-04-11T03:39:52.3576281Z Collecting multiprocess (from datasets->-r requirements/requirements-test.txt (line 17))
2025-04-11T03:39:52.4490698Z   Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)
2025-04-11T03:39:52.7074449Z Collecting tzdata>=2022.7 (from pandas->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:52.7964417Z   Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)
2025-04-11T03:39:53.0652451Z Collecting types-setuptools>=69.1.0 (from requirements-parser->pytest-testmon==2.0.7b1->-r requirements/requirements-test.txt (line 3))
2025-04-11T03:39:53.1557580Z   Downloading types_setuptools-78.1.0.20250329-py3-none-any.whl.metadata (2.2 kB)
2025-04-11T03:39:53.1777012Z Requirement already satisfied: setuptools>=42.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scikit-build->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (69.5.1)
2025-04-11T03:39:53.1782595Z Requirement already satisfied: wheel>=0.32.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scikit-build->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (0.43.0)
2025-04-11T03:39:53.4503714Z Collecting lightning-utilities>=0.8.0 (from torchmetrics->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:53.5396232Z   Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)
2025-04-11T03:39:53.7713882Z Collecting LibCST>=0.3.7 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:53.8615523Z   Downloading libcst-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)
2025-04-11T03:39:54.1032728Z Collecting moreorless>=0.3.0 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:54.2022466Z   Downloading moreorless-0.4.0-py2.py3-none-any.whl.metadata (1.5 kB)
2025-04-11T03:39:54.3744034Z Collecting stdlibs>=2021.4.1 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:54.4642492Z   Downloading stdlibs-2025.4.4-py3-none-any.whl.metadata (5.0 kB)
2025-04-11T03:39:54.6319043Z Collecting toml>=0.10.0 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:54.7209568Z   Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)
2025-04-11T03:39:54.8201153Z Collecting trailrunner>=1.0 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:54.9127695Z   Downloading trailrunner-1.4.0-py3-none-any.whl.metadata (4.2 kB)
2025-04-11T03:39:55.2453620Z Collecting pathspec>=0.8.1 (from trailrunner>=1.0->usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
2025-04-11T03:39:55.3344040Z   Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)
2025-04-11T03:39:55.4807107Z Requirement already satisfied: invoke>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.2.0)
2025-04-11T03:39:55.4811650Z Requirement already satisfied: paramiko>=2.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.5.1)
2025-04-11T03:39:55.4815867Z Requirement already satisfied: decorator>=5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (5.2.1)
2025-04-11T03:39:55.4820089Z Requirement already satisfied: deprecated>=1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.2.18)
2025-04-11T03:39:55.5037433Z Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fastapi->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.46.1)
2025-04-11T03:39:55.5228397Z Requirement already satisfied: beautifulsoup4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.13.3)
2025-04-11T03:39:55.5549390Z Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.4.0)
2025-04-11T03:39:55.5554201Z Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.6.9)
2025-04-11T03:39:55.5558621Z Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.9.1)
2025-04-11T03:39:55.5564014Z Requirement already satisfied: virtualenv>=20.10.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (20.30.0)
2025-04-11T03:39:55.5691421Z Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.0.0)
2025-04-11T03:39:55.5696268Z Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.19.1)
2025-04-11T03:39:55.5800357Z Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch->torchvision->-r requirements/requirements-test.txt (line 4)) (1.3.0)
2025-04-11T03:39:55.6104922Z Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from deprecated>=1.2->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.17.2)
2025-04-11T03:39:55.6614624Z Requirement already satisfied: mdurl~=0.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.1.2)
2025-04-11T03:39:55.7012675Z Requirement already satisfied: bcrypt>=3.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.3.0)
2025-04-11T03:39:55.7017249Z Requirement already satisfied: cryptography>=3.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (44.0.2)
2025-04-11T03:39:55.7021757Z Requirement already satisfied: pynacl>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.5.0)
2025-04-11T03:39:55.7379313Z Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from starlette<0.47.0,>=0.40.0->fastapi->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.9.0)
2025-04-11T03:39:55.7631394Z Requirement already satisfied: distlib<1,>=0.3.7 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.3.9)
2025-04-11T03:39:55.7637188Z Requirement already satisfied: platformdirs<5,>=3.9.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.3.7)
2025-04-11T03:39:55.7804887Z Requirement already satisfied: soupsieve>1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from beautifulsoup4->google->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.6)
2025-04-11T03:39:55.8034846Z Requirement already satisfied: zipp>=3.20 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.29.0->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.21.0)
2025-04-11T03:39:55.8451377Z Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.3.1)
2025-04-11T03:39:55.8813069Z Requirement already satisfied: cffi>=1.12 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.17.1)
2025-04-11T03:39:55.9792606Z Requirement already satisfied: pycparser in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.22)
2025-04-11T03:39:56.1014695Z Downloading coverage-7.2.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)
2025-04-11T03:39:56.1908722Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.0/228.0 kB 3.0 MB/s eta 0:00:00
2025-04-11T03:39:56.2806043Z Downloading torchx_nightly-2022.6.29-py3-none-any.whl (177 kB)
2025-04-11T03:39:56.3533091Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.8/177.8 kB 2.4 MB/s eta 0:00:00
2025-04-11T03:39:56.4431628Z Downloading torchrec-0.2.0-py39-none-any.whl (293 kB)
2025-04-11T03:39:56.5157065Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 293.4/293.4 kB 4.1 MB/s eta 0:00:00
2025-04-11T03:39:56.6048752Z Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)
2025-04-11T03:39:56.6765771Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.1/63.1 kB 756.7 kB/s eta 0:00:00
2025-04-11T03:39:56.7660770Z Downloading pyre_extensions-0.0.27-py3-none-any.whl (12 kB)
2025-04-11T03:39:56.9254964Z Downloading pytest-7.4.4-py3-none-any.whl (325 kB)
2025-04-11T03:39:56.9981131Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 325.3/325.3 kB 4.5 MB/s eta 0:00:00
2025-04-11T03:39:57.0914769Z Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)
2025-04-11T03:39:57.2502842Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 15.0 MB/s eta 0:00:00
2025-04-11T03:39:57.3402918Z Downloading datasets-2.19.1-py3-none-any.whl (542 kB)
2025-04-11T03:39:57.4122867Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 7.6 MB/s eta 0:00:00
2025-04-11T03:39:57.5037073Z Downloading dill-0.3.8-py3-none-any.whl (116 kB)
2025-04-11T03:39:57.5748224Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 1.5 MB/s eta 0:00:00
2025-04-11T03:39:57.6644301Z Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)
2025-04-11T03:39:57.7366191Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.0/172.0 kB 2.3 MB/s eta 0:00:00
2025-04-11T03:39:57.8257285Z Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)
2025-04-11T03:39:57.8979829Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 22.5 MB/s eta 0:00:00
2025-04-11T03:39:57.9868753Z Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)
2025-04-11T03:39:58.1461828Z Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)
2025-04-11T03:39:59.7697774Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 MB 27.6 MB/s eta 0:00:00
2025-04-11T03:39:59.8587283Z Downloading tomli-2.2.1-py3-none-any.whl (14 kB)
2025-04-11T03:40:00.0179358Z Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)
2025-04-11T03:40:00.0896989Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 144.2/144.2 kB 1.9 MB/s eta 0:00:00
2025-04-11T03:40:00.1785966Z Downloading arrow-1.3.0-py3-none-any.whl (66 kB)
2025-04-11T03:40:00.2504183Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.4/66.4 kB 800.4 kB/s eta 0:00:00
2025-04-11T03:40:00.3398104Z Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
2025-04-11T03:40:00.4113141Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 3.1 MB/s eta 0:00:00
2025-04-11T03:40:00.5002730Z Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)
2025-04-11T03:40:00.6607654Z Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)
2025-04-11T03:40:01.6157194Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 27.9/27.9 MB 30.2 MB/s eta 0:00:00
2025-04-11T03:40:01.7071587Z Downloading Cython-3.0.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
2025-04-11T03:40:01.8238596Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 31.5 MB/s eta 0:00:00
2025-04-11T03:40:01.9147540Z Downloading distro-1.9.0-py3-none-any.whl (20 kB)
2025-04-11T03:40:02.0744038Z Downloading docker-7.1.0-py3-none-any.whl (147 kB)
2025-04-11T03:40:02.1471636Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.8/147.8 kB 2.0 MB/s eta 0:00:00
2025-04-11T03:40:02.2384838Z Downloading fbgemm_gpu-1.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (417.2 MB)
2025-04-11T03:40:18.7628150Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 417.2/417.2 MB 30.6 MB/s eta 0:00:00
2025-04-11T03:40:18.8545449Z Downloading hypothesis-6.131.0-py3-none-any.whl (495 kB)
2025-04-11T03:40:18.9259275Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 495.7/495.7 kB 7.0 MB/s eta 0:00:00
2025-04-11T03:40:19.0155569Z Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)
2025-04-11T03:40:19.1750520Z Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)
2025-04-11T03:40:19.3788331Z Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)
2025-04-11T03:40:19.4504910Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 1.8 MB/s eta 0:00:00
2025-04-11T03:40:19.5395814Z Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)
2025-04-11T03:40:19.6284286Z Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)
2025-04-11T03:40:20.0432811Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 31.2 MB/s eta 0:00:00
2025-04-11T03:40:20.1326194Z Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)
2025-04-11T03:40:20.2041672Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 509.2/509.2 kB 7.1 MB/s eta 0:00:00
2025-04-11T03:40:20.2936340Z Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)
2025-04-11T03:40:20.4528690Z Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)
2025-04-11T03:40:20.6550855Z Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)
2025-04-11T03:40:20.8146157Z Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)
2025-04-11T03:40:20.8863714Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 111.1/111.1 kB 1.4 MB/s eta 0:00:00
2025-04-11T03:40:20.9761969Z Downloading requirements_parser-0.11.0-py3-none-any.whl (14 kB)
2025-04-11T03:40:21.1354955Z Downloading scikit_build-0.18.1-py3-none-any.whl (85 kB)
2025-04-11T03:40:21.2072929Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 1.1 MB/s eta 0:00:00
2025-04-11T03:40:21.2963003Z Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)
2025-04-11T03:40:21.4558081Z Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)
2025-04-11T03:40:21.5289362Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 961.5/961.5 kB 13.6 MB/s eta 0:00:00
2025-04-11T03:40:21.6185560Z Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)
2025-04-11T03:40:21.8227775Z Downloading usort-1.0.8.post1-py3-none-any.whl (37 kB)
2025-04-11T03:40:21.9830416Z Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)
2025-04-11T03:40:22.0546666Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 694.4 kB/s eta 0:00:00
2025-04-11T03:40:22.1459555Z Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)
2025-04-11T03:40:22.2174717Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 2.6 MB/s eta 0:00:00
2025-04-11T03:40:22.3078743Z Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)
2025-04-11T03:40:22.4668003Z Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)
2025-04-11T03:40:22.5557326Z Downloading libcst-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)
2025-04-11T03:40:22.6573658Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 23.4 MB/s eta 0:00:00
2025-04-11T03:40:22.7469460Z Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)
2025-04-11T03:40:23.1152973Z Downloading moreorless-0.4.0-py2.py3-none-any.whl (9.3 kB)
2025-04-11T03:40:23.2761272Z Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)
2025-04-11T03:40:23.3478361Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 219.8/219.8 kB 3.0 MB/s eta 0:00:00
2025-04-11T03:40:23.4370904Z Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)
2025-04-11T03:40:23.5087438Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206.6/206.6 kB 2.8 MB/s eta 0:00:00
2025-04-11T03:40:23.5987524Z Downloading stdlibs-2025.4.4-py3-none-any.whl (57 kB)
2025-04-11T03:40:23.6713055Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.2/57.2 kB 664.8 kB/s eta 0:00:00
2025-04-11T03:40:23.7607827Z Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)
2025-04-11T03:40:23.9213441Z Downloading trailrunner-1.4.0-py3-none-any.whl (11 kB)
2025-04-11T03:40:24.0105129Z Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)
2025-04-11T03:40:24.1007121Z Downloading types_setuptools-78.1.0.20250329-py3-none-any.whl (66 kB)
2025-04-11T03:40:24.1732102Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 809.1 kB/s eta 0:00:00
2025-04-11T03:40:24.2628154Z Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)
2025-04-11T03:40:24.3347605Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 347.8/347.8 kB 4.8 MB/s eta 0:00:00
2025-04-11T03:40:24.4240479Z Downloading yarl-1.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (334 kB)
2025-04-11T03:40:24.4963262Z    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 334.0/334.0 kB 4.6 MB/s eta 0:00:00
2025-04-11T03:40:24.5875350Z Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)
2025-04-11T03:40:25.2556974Z Building wheels for collected packages: docstring-parser, pytest-testmon, titans, flash_attn, iopath
2025-04-11T03:40:25.2569672Z   Building wheel for docstring-parser (pyproject.toml): started
2025-04-11T03:40:25.4154683Z   Building wheel for docstring-parser (pyproject.toml): finished with status 'done'
2025-04-11T03:40:25.4159117Z   Created wheel for docstring-parser: filename=docstring_parser-0.8.1-py3-none-any.whl size=19697 sha256=d644c838a83d53f17a411ca5985c4fba5e2c8dab000f3c2c5145e17751cef9d0
2025-04-11T03:40:25.4160110Z   Stored in directory: /tmp/pip-ephem-wheel-cache-8f65ekvf/wheels/26/e4/54/64439f1d0c5d3721041ddc0f001e4b57756a394880a2af8981
2025-04-11T03:40:25.4180446Z   Building wheel for pytest-testmon (pyproject.toml): started
2025-04-11T03:40:25.5986159Z   Building wheel for pytest-testmon (pyproject.toml): finished with status 'done'
2025-04-11T03:40:25.5990170Z   Created wheel for pytest-testmon: filename=pytest_testmon-2.0.7b1-py3-none-any.whl size=35044 sha256=27decf2f896e47ddd6362305a1fb6f9a1a55b67b4b7dc0a0162970ccc19bbc74
2025-04-11T03:40:25.5991237Z   Stored in directory: /tmp/pip-ephem-wheel-cache-8f65ekvf/wheels/cb/f6/db/609602674f7b7c7ecbfd91b3d67b1ea34fe598f7e344495c44
2025-04-11T03:40:25.6008409Z   Building wheel for titans (setup.py): started
2025-04-11T03:40:25.8295075Z   Building wheel for titans (setup.py): finished with status 'done'
2025-04-11T03:40:25.8298185Z   Created wheel for titans: filename=titans-0.0.7-py3-none-any.whl size=63319 sha256=777912ac0c4c2c9f50c54bc8ec86f13b3163970bb9a8f07014e1e32ad6c09e3c
2025-04-11T03:40:25.8299262Z   Stored in directory: /tmp/pip-ephem-wheel-cache-8f65ekvf/wheels/65/21/1b/3dfb7cdd10cdc650f08fbb72b6f28dd75e1e9b7b42f6695a16
2025-04-11T03:40:25.8316180Z   Building wheel for flash_attn (setup.py): started
2025-04-11T03:40:37.8188934Z   Building wheel for flash_attn (setup.py): finished with status 'done'
2025-04-11T03:40:37.9848682Z   Created wheel for flash_attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187696268 sha256=1776769f7ae3a8be3b31ec3a4c875ad1764da74be2d9b1751e5c01162ad0096f
2025-04-11T03:40:37.9850882Z   Stored in directory: /tmp/pip-ephem-wheel-cache-8f65ekvf/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca
2025-04-11T03:40:37.9875663Z   Building wheel for iopath (setup.py): started
2025-04-11T03:40:38.1910671Z   Building wheel for iopath (setup.py): finished with status 'done'
2025-04-11T03:40:38.1913397Z   Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=9dd7cebdc0578e3e11ea51542ce127777145a632a1775df8681cef8eddb93337
2025-04-11T03:40:38.1914467Z   Stored in directory: /tmp/pip-ephem-wheel-cache-8f65ekvf/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d
2025-04-11T03:40:38.1926705Z Successfully built docstring-parser pytest-testmon titans flash_attn iopath
2025-04-11T03:40:38.7407156Z Installing collected packages: sortedcontainers, pytz, xxhash, websocket-client, urllib3, tzdata, types-setuptools, types-python-dateutil, tomli, toml, tabulate, stdlibs, six, pyparsing, pyDeprecate, pyarrow-hotfix, pyarrow, propcache, portalocker, pluggy, pathspec, mypy-extensions, multidict, moreorless, lightning-utilities, LibCST, iniconfig, hypothesis, fsspec, fbgemm-gpu, docstring-parser, distro, dill, Cython, coverage, cmake, async-timeout, aiohappyeyeballs, yarl, typing-inspect, trailrunner, scikit-build, requirements-parser, requests, python-dateutil, pytest, multiprocess, iopath, usort, torchmetrics, pytest-testmon, pyre-extensions, pandas, flash_attn, docker, arrow, aiohttp, torchx-nightly, timm, torchrec, datasets, titans
2025-04-11T03:40:38.8615105Z   Attempting uninstall: urllib3
2025-04-11T03:40:38.8622770Z     Found existing installation: urllib3 2.2.2
2025-04-11T03:40:38.8657770Z     Uninstalling urllib3-2.2.2:
2025-04-11T03:40:38.8734308Z       Successfully uninstalled urllib3-2.2.2
2025-04-11T03:40:40.8066553Z   Attempting uninstall: fsspec
2025-04-11T03:40:40.8074501Z     Found existing installation: fsspec 2024.6.1
2025-04-11T03:40:40.8115825Z     Uninstalling fsspec-2024.6.1:
2025-04-11T03:40:40.8212247Z       Successfully uninstalled fsspec-2024.6.1
2025-04-11T03:40:48.5262305Z   Attempting uninstall: requests
2025-04-11T03:40:48.5267530Z     Found existing installation: requests 2.32.2
2025-04-11T03:40:48.5289653Z     Uninstalling requests-2.32.2:
2025-04-11T03:40:48.5332053Z       Successfully uninstalled requests-2.32.2
2025-04-11T03:40:56.5099828Z Successfully installed Cython-3.0.12 LibCST-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 arrow-1.3.0 async-timeout-5.0.1 cmake-4.0.0 coverage-7.2.3 datasets-2.19.1 dill-0.3.8 distro-1.9.0 docker-7.1.0 docstring-parser-0.8.1 fbgemm-gpu-1.1.0 flash_attn-2.7.4.post1 fsspec-2024.3.1 hypothesis-6.131.0 iniconfig-2.1.0 iopath-0.1.10 lightning-utilities-0.14.3 moreorless-0.4.0 multidict-6.4.3 multiprocess-0.70.16 mypy-extensions-1.0.0 pandas-2.2.3 pathspec-0.12.1 pluggy-1.5.0 portalocker-3.1.1 propcache-0.3.1 pyDeprecate-0.3.2 pyarrow-19.0.1 pyarrow-hotfix-0.6 pyparsing-3.2.3 pyre-extensions-0.0.27 pytest-7.4.4 pytest-testmon-2.0.7b1 python-dateutil-2.9.0.post0 pytz-2025.2 requests-2.27.1 requirements-parser-0.11.0 scikit-build-0.18.1 six-1.17.0 sortedcontainers-2.4.0 stdlibs-2025.4.4 tabulate-0.9.0 timm-1.0.15 titans-0.0.7 toml-0.10.2 tomli-2.2.1 torchmetrics-1.7.1 torchrec-0.2.0 torchx-nightly-2022.6.29 trailrunner-1.4.0 types-python-dateutil-2.9.0.20241206 types-setuptools-78.1.0.20250329 typing-inspect-0.9.0 tzdata-2025.2 urllib3-1.26.20 usort-1.0.8.post1 websocket-client-1.8.0 xxhash-3.5.0 yarl-1.19.0
2025-04-11T03:40:56.5103408Z WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-11T03:40:57.6878550Z ##[group]Run # -p flag is required to preserve the file timestamp to avoid ninja rebuild
2025-04-11T03:40:57.6879078Z [36;1m# -p flag is required to preserve the file timestamp to avoid ninja rebuild[0m
2025-04-11T03:40:57.6879545Z [36;1mcp -p -r /__w/ColossalAI/ColossalAI/build /github/home/cuda_ext_cache/[0m
2025-04-11T03:40:57.6880033Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:40:57.6880313Z ##[endgroup]
2025-04-11T03:40:57.9071331Z ##[group]Run CURL_CA_BUNDLE="" PYTHONPATH=$PWD FAST_TEST=1 pytest \
2025-04-11T03:40:57.9071786Z [36;1mCURL_CA_BUNDLE="" PYTHONPATH=$PWD FAST_TEST=1 pytest \[0m
2025-04-11T03:40:57.9072101Z [36;1m-m "not largedist" \[0m
2025-04-11T03:40:57.9072345Z [36;1m--durations=0 \[0m
2025-04-11T03:40:57.9072585Z [36;1m--ignore tests/test_analyzer \[0m
2025-04-11T03:40:57.9072874Z [36;1m--ignore tests/test_auto_parallel \[0m
2025-04-11T03:40:57.9073146Z [36;1m--ignore tests/test_fx \[0m
2025-04-11T03:40:57.9073405Z [36;1m--ignore tests/test_autochunk \[0m
2025-04-11T03:40:57.9073674Z [36;1m--ignore tests/test_gptq \[0m
2025-04-11T03:40:57.9073932Z [36;1m--ignore tests/test_infer_ops \[0m
2025-04-11T03:40:57.9074394Z [36;1m--ignore tests/test_legacy \[0m
2025-04-11T03:40:57.9074665Z [36;1m--ignore tests/test_smoothquant \[0m
2025-04-11T03:40:57.9074920Z [36;1mtests/[0m
2025-04-11T03:40:57.9075301Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:40:57.9075613Z env:
2025-04-11T03:40:57.9075952Z   LD_LIBRARY_PATH: /github/home/.tensornvme/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2025-04-11T03:40:57.9076350Z   LLAMA_PATH: /data/scratch/llama-tiny
2025-04-11T03:40:57.9076616Z   MOE_TENSOR_PATH: /data/scratch/moe_tensors
2025-04-11T03:40:57.9076899Z   HF_ENDPOINT: https://hf-mirror.com
2025-04-11T03:40:57.9077164Z ##[endgroup]
2025-04-11T03:41:07.9561680Z ============================= test session starts ==============================
2025-04-11T03:41:07.9562112Z platform linux -- Python 3.10.14, pytest-7.4.4, pluggy-1.5.0
2025-04-11T03:41:07.9562448Z rootdir: /__w/ColossalAI/ColossalAI
2025-04-11T03:41:07.9562708Z configfile: pytest.ini
2025-04-11T03:41:07.9563027Z plugins: hypothesis-6.131.0, anyio-4.9.0, testmon-2.0.7b1
2025-04-11T03:41:07.9563365Z collected 865 items / 23 deselected / 842 selected
2025-04-11T03:41:07.9563557Z 
2025-04-11T03:41:08.1651226Z tests/test_booster/test_accelerator.py F                                 [  0%]
2025-04-11T03:41:08.1655844Z tests/test_booster/test_mixed_precision/test_fp16_torch.py s             [  0%]
2025-04-11T03:41:16.9176361Z tests/test_booster/test_plugin/test_3d_plugin.py F                       [  0%]
2025-04-11T03:41:22.1080224Z tests/test_booster/test_plugin/test_dp_plugin_base.py F                  [  0%]
2025-04-11T03:41:31.6081011Z tests/test_booster/test_plugin/test_gemini_plugin.py F                   [  0%]
2025-04-11T03:41:38.4531984Z tests/test_booster/test_plugin/test_low_level_zero_plugin.py F           [  0%]
2025-04-11T03:41:46.2271686Z tests/test_booster/test_plugin/test_torch_ddp_plugin.py F                [  0%]
2025-04-11T03:41:53.5966862Z tests/test_booster/test_plugin/test_torch_fsdp_plugin.py F               [  0%]
2025-04-11T03:42:02.1925867Z tests/test_checkpoint_io/test_gemini_checkpoint_io.py F                  [  1%]
2025-04-11T03:42:09.8876804Z tests/test_checkpoint_io/test_gemini_torch_compability.py F              [  1%]
2025-04-11T03:42:14.9195356Z tests/test_checkpoint_io/test_general_checkpoint_io.py F..FFFFFF         [  2%]
2025-04-11T03:42:22.7912993Z tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py F  [  2%]
2025-04-11T03:42:23.0164271Z tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py F          [  2%]
2025-04-11T03:42:29.6316593Z tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py F     [  2%]
2025-04-11T03:42:31.0848592Z tests/test_checkpoint_io/test_safetensors_async_io.py FFFFF              [  3%]
2025-04-11T03:42:35.8861700Z tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py F               [  3%]
2025-04-11T03:42:40.5821379Z tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py F              [  3%]
2025-04-11T03:42:45.3426607Z tests/test_cluster/test_device_mesh_manager.py .                         [  3%]
2025-04-11T03:42:50.4166402Z tests/test_cluster/test_process_group_mesh.py .                          [  3%]
2025-04-11T03:42:50.5297203Z tests/test_config/test_load_config.py .                                  [  3%]
2025-04-11T03:42:50.5302644Z tests/test_device/test_alpha_beta.py s                                   [  3%]
2025-04-11T03:42:55.4350944Z tests/test_device/test_device_mesh.py ..                                 [  4%]
2025-04-11T03:42:55.4356509Z tests/test_device/test_extract_alpha_beta.py s                           [  4%]
2025-04-11T03:43:00.9540816Z tests/test_device/test_init_logical_pg.py F                              [  4%]
2025-04-11T03:43:00.9545305Z tests/test_device/test_search_logical_device_mesh.py s                   [  4%]
2025-04-11T03:43:07.3926890Z tests/test_fp8/test_all_to_all_single.py F                               [  4%]
2025-04-11T03:43:13.9992612Z tests/test_fp8/test_fp8_all_to_all.py F                                  [  4%]
2025-04-11T03:43:19.9205608Z tests/test_fp8/test_fp8_all_to_all_single.py F                           [  4%]
2025-04-11T03:43:25.9966341Z tests/test_fp8/test_fp8_allgather.py F                                   [  4%]
2025-04-11T03:43:32.4139111Z tests/test_fp8/test_fp8_allreduce.py F                                   [  5%]
2025-04-11T03:43:32.7249649Z tests/test_fp8/test_fp8_cast.py F                                        [  5%]
2025-04-11T03:43:39.9960710Z tests/test_fp8/test_fp8_fsdp_comm_hook.py F                              [  5%]
2025-04-11T03:43:40.3493850Z tests/test_fp8/test_fp8_hook.py F                                        [  5%]
2025-04-11T03:43:41.4408843Z tests/test_fp8/test_fp8_linear.py FFFF                                   [  5%]
2025-04-11T03:43:47.2856970Z tests/test_fp8/test_fp8_reduce_scatter.py F                              [  6%]
2025-04-11T03:43:47.5378944Z tests/test_infer/test_batch_bucket.py F                                  [  6%]
2025-04-11T03:43:51.9554800Z tests/test_infer/test_config_and_struct.py .                             [  6%]
2025-04-11T03:44:07.4093322Z tests/test_infer/test_continuous_batching.py F                           [  6%]
2025-04-11T03:44:18.3032205Z tests/test_infer/test_drafter.py FF                                      [  6%]
2025-04-11T03:44:22.6593198Z tests/test_infer/test_kvcache_manager.py .F                              [  6%]
2025-04-11T03:44:26.9987787Z tests/test_infer/test_request_handler.py F                               [  7%]
2025-04-11T03:44:33.5864890Z tests/test_infer/test_streamingllm.py F                                  [  7%]
2025-04-11T03:44:33.7564835Z tests/test_infer/test_async_engine/test_async_engine.py s                [  7%]
2025-04-11T03:44:33.9030031Z tests/test_infer/test_async_engine/test_request_tracer.py .              [  7%]
2025-04-11T03:44:33.9108742Z tests/test_infer/test_kernels/cuda/test_convert_fp8.py sssssssssssssssss [  9%]
2025-04-11T03:44:33.9429414Z ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 17%]
2025-04-11T03:44:33.9515867Z sssssssssssssssssss                                                      [ 20%]
2025-04-11T03:44:34.4716510Z tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py FF   [ 20%]
2025-04-11T03:44:34.9648979Z tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py FF            [ 20%]
2025-04-11T03:44:38.2009428Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py FFFFFFFFFFFFF [ 22%]
2025-04-11T03:44:43.9636564Z FFFFFFFFFFFFFFFFFFFFFFF                                                  [ 24%]
2025-04-11T03:44:47.0162062Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py FFFFFFFFFFFFFFF [ 26%]
2025-04-11T03:44:47.2120258Z F                                                                        [ 26%]
2025-04-11T03:44:48.1292629Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py FFFF    [ 27%]
2025-04-11T03:44:48.5177830Z tests/test_infer/test_kernels/cuda/test_silu_and_mul.py FF               [ 27%]
2025-04-11T03:44:49.2551239Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py ........ [ 28%]
2025-04-11T03:45:00.7607583Z ........................FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 37%]
2025-04-11T03:45:10.7589630Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF                         [ 42%]
2025-04-11T03:45:12.0324408Z tests/test_infer/test_kernels/triton/test_decoding_attn.py sssssssssssss [ 44%]
2025-04-11T03:45:20.8308639Z sssssssssssssssssssssssssssssssssssssssssssssssssssFFFFFFFFFFFFFFFFFFFFF [ 52%]
2025-04-11T03:45:36.6793412Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 61%]
2025-04-11T03:45:51.9281208Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 69%]
2025-04-11T03:45:57.3741437Z FFFFFFFFFFFFFFFFFFFFFFFFFFF                                              [ 73%]
2025-04-11T03:45:57.3747378Z tests/test_infer/test_kernels/triton/test_fused_rotary_embedding.py s    [ 73%]
2025-04-11T03:46:00.1254045Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py FFFFFFFFFFFFFF [ 74%]
2025-04-11T03:46:06.7307055Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF                                       [ 78%]
2025-04-11T03:46:06.9237388Z tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py F            [ 79%]
2025-04-11T03:46:07.3434547Z tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py FF    [ 79%]
2025-04-11T03:46:07.5395748Z tests/test_infer/test_kernels/triton/test_xine_copy.py F                 [ 79%]
2025-04-11T03:46:07.5413053Z tests/test_infer/test_models/test_attention.py ssss                      [ 79%]
2025-04-11T03:46:07.6344955Z tests/test_infer/test_models/test_custom_model.py s                      [ 80%]
2025-04-11T03:46:12.1703391Z tests/test_lazy/test_from_pretrained.py .                                [ 80%]
2025-04-11T03:46:29.1368313Z tests/test_lazy/test_models.py .F                                        [ 80%]
2025-04-11T03:46:29.3460787Z tests/test_lazy/test_ops.py F                                            [ 80%]
2025-04-11T03:46:35.9147055Z tests/test_lora/test_lora.py F                                           [ 80%]
2025-04-11T03:46:35.9152388Z tests/test_moe/test_deepseek_layer.py s                                  [ 80%]
2025-04-11T03:46:36.3332830Z tests/test_moe/test_kernel.py FF                                         [ 80%]
2025-04-11T03:46:36.3338213Z tests/test_moe/test_mixtral_layer.py s                                   [ 81%]
2025-04-11T03:46:42.4999000Z tests/test_moe/test_moe_checkpoint.py F                                  [ 81%]
2025-04-11T03:46:42.5004013Z tests/test_moe/test_moe_ep_tp.py s                                       [ 81%]
2025-04-11T03:46:42.5009287Z tests/test_moe/test_moe_ep_zero.py s                                     [ 81%]
2025-04-11T03:46:47.6350447Z tests/test_optimizer/test_adam_kernel.py FFFFFFFFFFFFFFFFFFFF........... [ 85%]
2025-04-11T03:46:47.7269934Z .                                                                        [ 85%]
2025-04-11T03:46:58.9615030Z tests/test_optimizer/test_adam_optim.py F.FFFF.FFFF.FFFF.FFFF.FFFF.FFF   [ 88%]
2025-04-11T03:47:06.3823025Z tests/test_optimizer/test_dist_adafactor.py F                            [ 88%]
2025-04-11T03:47:15.4516432Z tests/test_optimizer/test_dist_came.py F                                 [ 89%]
2025-04-11T03:47:23.2218243Z tests/test_optimizer/test_dist_galore.py F                               [ 89%]
2025-04-11T03:47:31.0025037Z tests/test_optimizer/test_dist_lamb.py F                                 [ 89%]
2025-04-11T03:47:31.1264251Z tests/test_optimizer/test_lr_scheduler.py .                              [ 89%]
2025-04-11T03:47:31.1270422Z tests/test_optimizer/test_nvme.py s                                      [ 89%]
2025-04-11T03:47:35.2045180Z tests/test_pipeline/test_p2p_communication.py F                          [ 89%]
2025-04-11T03:47:40.1825813Z tests/test_pipeline/test_stage_manager.py F                              [ 89%]
2025-04-11T03:47:40.4030966Z tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py ..     [ 90%]
2025-04-11T03:47:40.4973267Z tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py . [ 90%]
2025-04-11T03:47:40.5912035Z .                                                                        [ 90%]
2025-04-11T03:48:01.6507505Z tests/test_pipeline/test_schedule/test_interleaved.py FFFF               [ 90%]
2025-04-11T03:48:22.8671846Z tests/test_pipeline/test_schedule/test_oneF_oneB.py FFFF                 [ 91%]
2025-04-11T03:48:23.1730544Z tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py ...    [ 91%]
2025-04-11T03:48:28.1638011Z tests/test_pipeline/test_schedule/test_zerobubble_pp.py F                [ 91%]
2025-04-11T03:48:28.3960151Z tests/test_shardformer/test_flash_attention.py F                         [ 91%]
2025-04-11T03:48:28.6360768Z tests/test_shardformer/test_shard_utils.py F                             [ 91%]
2025-04-11T03:48:29.0364606Z tests/test_shardformer/test_with_torch_ddp.py F                          [ 92%]
2025-04-11T03:48:29.2567541Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py F [ 92%]
2025-04-11T03:48:29.2569584Z                                                                          [ 92%]
2025-04-11T03:48:29.4576516Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py F [ 92%]
2025-04-11T03:48:29.4576972Z                                                                          [ 92%]
2025-04-11T03:48:29.6580783Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py F [ 92%]
2025-04-11T03:48:29.6581210Z                                                                          [ 92%]
2025-04-11T03:48:34.7174794Z tests/test_shardformer/test_layer/test_dist_crossentropy.py F            [ 92%]
2025-04-11T03:48:38.7384520Z tests/test_shardformer/test_layer/test_dropout.py F                      [ 92%]
2025-04-11T03:48:43.7916156Z tests/test_shardformer/test_layer/test_embedding.py F                    [ 92%]
2025-04-11T03:48:47.9636073Z tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py F     [ 92%]
2025-04-11T03:48:53.1048155Z tests/test_shardformer/test_layer/test_layernorm.py F                    [ 92%]
2025-04-11T03:48:57.2109247Z tests/test_shardformer/test_layer/test_linear_1d.py F                    [ 93%]
2025-04-11T03:49:02.2460897Z tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py F          [ 93%]
2025-04-11T03:49:12.4026900Z tests/test_shardformer/test_layer/test_ring_attn.py FF                   [ 93%]
2025-04-11T03:49:17.4439849Z tests/test_shardformer/test_layer/test_sequence_parallel.py F            [ 93%]
2025-04-11T03:49:21.6465800Z tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py F  [ 93%]
2025-04-11T03:49:21.8781486Z tests/test_shardformer/test_model/test_shard_bert.py F                   [ 93%]
2025-04-11T03:49:22.0810722Z tests/test_shardformer/test_model/test_shard_blip2.py F                  [ 93%]
2025-04-11T03:49:22.2842919Z tests/test_shardformer/test_model/test_shard_bloom.py F                  [ 94%]
2025-04-11T03:49:22.4851900Z tests/test_shardformer/test_model/test_shard_chatglm2.py F               [ 94%]
2025-04-11T03:49:22.6883490Z tests/test_shardformer/test_model/test_shard_command.py F                [ 94%]
2025-04-11T03:49:31.1429842Z tests/test_shardformer/test_model/test_shard_deepseek.py F               [ 94%]
2025-04-11T03:49:42.0005558Z tests/test_shardformer/test_model/test_shard_deepseek_v3.py F            [ 94%]
2025-04-11T03:49:42.2295765Z tests/test_shardformer/test_model/test_shard_falcon.py F                 [ 94%]
2025-04-11T03:49:42.4397041Z tests/test_shardformer/test_model/test_shard_gpt2.py F                   [ 94%]
2025-04-11T03:49:42.4402833Z tests/test_shardformer/test_model/test_shard_gptj.py s                   [ 94%]
2025-04-11T03:49:42.6462017Z tests/test_shardformer/test_model/test_shard_llama.py F                  [ 95%]
2025-04-11T03:49:42.8513316Z tests/test_shardformer/test_model/test_shard_mistral.py F                [ 95%]
2025-04-11T03:49:49.1175035Z tests/test_shardformer/test_model/test_shard_mixtral.py F                [ 95%]
2025-04-11T03:49:49.3433873Z tests/test_shardformer/test_model/test_shard_opt.py F                    [ 95%]
2025-04-11T03:49:49.5494327Z tests/test_shardformer/test_model/test_shard_qwen2.py F                  [ 95%]
2025-04-11T03:49:49.7563981Z tests/test_shardformer/test_model/test_shard_sam.py F                    [ 95%]
2025-04-11T03:49:49.9609896Z tests/test_shardformer/test_model/test_shard_t5.py F                     [ 95%]
2025-04-11T03:49:50.1660372Z tests/test_shardformer/test_model/test_shard_vit.py F                    [ 95%]
2025-04-11T03:49:50.3710046Z tests/test_shardformer/test_model/test_shard_whisper.py F                [ 95%]
2025-04-11T03:49:55.5246008Z tests/test_tensor/test_comm_spec_apply.py F                              [ 96%]
2025-04-11T03:49:55.5251371Z tests/test_tensor/test_mix_gather.py s                                   [ 96%]
2025-04-11T03:49:59.9214542Z tests/test_tensor/test_padded_tensor.py F                                [ 96%]
2025-04-11T03:50:00.1398099Z tests/test_tensor/test_shape_consistency.py ..                           [ 96%]
2025-04-11T03:50:05.0565173Z tests/test_tensor/test_shape_consistency_apply.py F                      [ 96%]
2025-04-11T03:50:05.1786146Z tests/test_tensor/test_sharding_spec.py .                                [ 96%]
2025-04-11T03:50:10.1686106Z tests/test_tensor/test_dtensor/test_comm_spec.py F                       [ 96%]
2025-04-11T03:50:14.1989800Z tests/test_tensor/test_dtensor/test_dtensor.py F                         [ 97%]
2025-04-11T03:50:14.3210139Z tests/test_tensor/test_dtensor/test_dtensor_sharding_spec.py .           [ 97%]
2025-04-11T03:50:27.0356202Z tests/test_tensor/test_dtensor/test_layout_converter.py F                [ 97%]
2025-04-11T03:50:31.2366968Z tests/test_zero/test_gemini/test_chunk_mgrv2.py F                        [ 97%]
2025-04-11T03:50:44.2622988Z tests/test_zero/test_gemini/test_chunkv2.py FFF                          [ 97%]
2025-04-11T03:50:44.2633216Z tests/test_zero/test_gemini/test_gemini_use_rmt.py ss                    [ 97%]
2025-04-11T03:50:50.8158723Z tests/test_zero/test_gemini/test_grad_accum.py F                         [ 98%]
2025-04-11T03:51:04.4952143Z tests/test_zero/test_gemini/test_grad_clip.py FF                         [ 98%]
2025-04-11T03:51:19.4766486Z tests/test_zero/test_gemini/test_inference.py FF                         [ 98%]
2025-04-11T03:51:27.7148332Z tests/test_zero/test_gemini/test_optim.py F                              [ 98%]
2025-04-11T03:51:27.7153563Z tests/test_zero/test_gemini/test_runtime_mem_tracer.py s                 [ 98%]
2025-04-11T03:51:38.0526301Z tests/test_zero/test_gemini/test_search.py FF                            [ 99%]
2025-04-11T03:51:46.2248483Z tests/test_zero/test_gemini/test_zeroddp_state_dict.py F                 [ 99%]
2025-04-11T03:51:46.2258142Z tests/test_zero/test_gemini/test_zerooptim_state_dict.py ss              [ 99%]
2025-04-11T03:51:51.1545429Z tests/test_zero/test_low_level/test_coll_nd.py F                         [ 99%]
2025-04-11T03:51:55.3587285Z tests/test_zero/test_low_level/test_grad_acc.py F                        [ 99%]
2025-04-11T03:52:00.4581954Z tests/test_zero/test_low_level/test_mem_leak.py F                        [ 99%]
2025-04-11T03:52:06.3752731Z tests/test_zero/test_low_level/test_zero1_2.py F                         [ 99%]
2025-04-11T03:52:12.4071568Z tests/test_zero/test_low_level/test_zero_ckpt.py F                       [100%]
2025-04-11T03:52:12.4071925Z 
2025-04-11T03:52:12.4072052Z =================================== FAILURES ===================================
2025-04-11T03:52:12.4072399Z _______________________________ test_accelerator _______________________________
2025-04-11T03:52:12.4072612Z 
2025-04-11T03:52:12.4072712Z args = (), kwargs = {}
2025-04-11T03:52:12.4072872Z 
2025-04-11T03:52:12.4072984Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.4073319Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.4073601Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.4073909Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.4074232Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.4075139Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.4075317Z 
2025-04-11T03:52:12.4075437Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.4075726Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4076088Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.4076433Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.4076718Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4076900Z 
2025-04-11T03:52:12.4076999Z device = None
2025-04-11T03:52:12.4077118Z 
2025-04-11T03:52:12.4077388Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.4078034Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.4078360Z     
2025-04-11T03:52:12.4078554Z         Args:
2025-04-11T03:52:12.4078844Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.4079275Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.4079647Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.4079915Z         """
2025-04-11T03:52:12.4080120Z         _lazy_init()
2025-04-11T03:52:12.4080367Z         with torch.cuda.device(device):
2025-04-11T03:52:12.4080647Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4080952Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4081442Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4081960Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4082343Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4082580Z 
2025-04-11T03:52:12.4082848Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.4083298Z ________________________________ test_3d_plugin ________________________________
2025-04-11T03:52:12.4083522Z 
2025-04-11T03:52:12.4083622Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4084400Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4085102Z 
2025-04-11T03:52:12.4085217Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4085481Z         try_count = 0
2025-04-11T03:52:12.4085715Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4085977Z             max_try, int
2025-04-11T03:52:12.4086257Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4086552Z     
2025-04-11T03:52:12.4086773Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4087043Z             try:
2025-04-11T03:52:12.4087254Z                 try_count += 1
2025-04-11T03:52:12.4087484Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4087735Z                 return ret
2025-04-11T03:52:12.4087974Z             except exception_type as e:
2025-04-11T03:52:12.4088242Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4088605Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4089005Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4089352Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4089732Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4090051Z                     continue
2025-04-11T03:52:12.4090275Z                 else:
2025-04-11T03:52:12.4090746Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4091121Z >                   raise e
2025-04-11T03:52:12.4091266Z 
2025-04-11T03:52:12.4091368Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4091651Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4091971Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4092270Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4092592Z tests/test_booster/test_plugin/test_3d_plugin.py:277: in test_3d_plugin
2025-04-11T03:52:12.4092956Z     spawn(run_dist, 4, early_stop=early_stop)
2025-04-11T03:52:12.4093243Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4093525Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4094083Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4094610Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4095157Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4095616Z     while not context.join():
2025-04-11T03:52:12.4095886Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4096069Z 
2025-04-11T03:52:12.4096283Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0319540>
2025-04-11T03:52:12.4096642Z timeout = None
2025-04-11T03:52:12.4096761Z 
2025-04-11T03:52:12.4096873Z     def join(self, timeout=None):
2025-04-11T03:52:12.4097153Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4097425Z     
2025-04-11T03:52:12.4097676Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4098040Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4098420Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4098768Z         of the first process exiting.
2025-04-11T03:52:12.4099013Z     
2025-04-11T03:52:12.4099272Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4099630Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4099922Z     
2025-04-11T03:52:12.4100100Z         Args:
2025-04-11T03:52:12.4100383Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4100669Z         """
2025-04-11T03:52:12.4100917Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4101217Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4101455Z             return True
2025-04-11T03:52:12.4101660Z     
2025-04-11T03:52:12.4101896Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4102218Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4102499Z             self.sentinels.keys(),
2025-04-11T03:52:12.4102737Z             timeout=timeout,
2025-04-11T03:52:12.4102960Z         )
2025-04-11T03:52:12.4103138Z     
2025-04-11T03:52:12.4103328Z         error_index = None
2025-04-11T03:52:12.4103558Z         for sentinel in ready:
2025-04-11T03:52:12.4103819Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4104150Z             process = self.processes[index]
2025-04-11T03:52:12.4104419Z             process.join()
2025-04-11T03:52:12.4104676Z             if process.exitcode != 0:
2025-04-11T03:52:12.4104939Z                 error_index = index
2025-04-11T03:52:12.4105171Z                 break
2025-04-11T03:52:12.4105374Z     
2025-04-11T03:52:12.4105573Z         # Return if there was no error.
2025-04-11T03:52:12.4105830Z         if error_index is None:
2025-04-11T03:52:12.4106109Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4106439Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4106685Z     
2025-04-11T03:52:12.4107048Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4107379Z         for process in self.processes:
2025-04-11T03:52:12.4107657Z             if process.is_alive():
2025-04-11T03:52:12.4107918Z                 process.terminate()
2025-04-11T03:52:12.4108181Z             process.join()
2025-04-11T03:52:12.4108407Z     
2025-04-11T03:52:12.4108748Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4109074Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4109375Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4109690Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4109988Z             if exitcode < 0:
2025-04-11T03:52:12.4110398Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4110697Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4111024Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4111360Z                     error_index=error_index,
2025-04-11T03:52:12.4111644Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4111926Z                     exit_code=exitcode,
2025-04-11T03:52:12.4112185Z                     signal_name=name,
2025-04-11T03:52:12.4112421Z                 )
2025-04-11T03:52:12.4112629Z             else:
2025-04-11T03:52:12.4112897Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4113240Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4113585Z                     error_index=error_index,
2025-04-11T03:52:12.4113856Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4114127Z                     exit_code=exitcode,
2025-04-11T03:52:12.4114372Z                 )
2025-04-11T03:52:12.4114560Z     
2025-04-11T03:52:12.4114808Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4115199Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4115543Z         msg += original_trace
2025-04-11T03:52:12.4115878Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4116286Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4116616Z E       
2025-04-11T03:52:12.4116866Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4117176Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4117660Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4118142Z E           fn(i, *args)
2025-04-11T03:52:12.4118573Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 271, in run_dist
2025-04-11T03:52:12.4119034Z E           check_3d_plugin(early_stop=early_stop)
2025-04-11T03:52:12.4119489Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4119926Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4120378Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 104, in check_3d_plugin
2025-04-11T03:52:12.4120903Z E           err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:12.4121363Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4121769Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4122212Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4122677Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4123144Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4123599Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4124008Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4124486Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4124984Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4125356Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4125580Z 
2025-04-11T03:52:12.4125895Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4126423Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4126817Z [04/11/25 03:41:16] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4127331Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4127674Z                              :75 launch                                         
2025-04-11T03:52:12.4128013Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4128359Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.4128747Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4129166Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4130549Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4131921Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4133283Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4134670Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4136029Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4137400Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4138764Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4140125Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4141067Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4141906Z   warnings.warn(
2025-04-11T03:52:12.4142831Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4143650Z   warnings.warn(
2025-04-11T03:52:12.4144441Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4145257Z   warnings.warn(
2025-04-11T03:52:12.4146042Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4146985Z   warnings.warn(
2025-04-11T03:52:12.4147943Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4149038Z   warnings.warn(
2025-04-11T03:52:12.4149960Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4150906Z   warnings.warn(
2025-04-11T03:52:12.4151812Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4152756Z   warnings.warn(
2025-04-11T03:52:12.4153660Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4154588Z   warnings.warn(
2025-04-11T03:52:12.4155481Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4156415Z   warnings.warn(
2025-04-11T03:52:12.4157321Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4158260Z   warnings.warn(
2025-04-11T03:52:12.4159160Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4160076Z   warnings.warn(
2025-04-11T03:52:12.4160970Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4161899Z   warnings.warn(
2025-04-11T03:52:12.4162921Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4163871Z   warnings.warn(
2025-04-11T03:52:12.4164772Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4165853Z   warnings.warn(
2025-04-11T03:52:12.4166748Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4167681Z   warnings.warn(
2025-04-11T03:52:12.4168567Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4169498Z   warnings.warn(
2025-04-11T03:52:12.4169917Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:53862 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4170557Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:53862 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4171191Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:53862 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4172096Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4172800Z   warnings.warn(
2025-04-11T03:52:12.4173457Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4174119Z   warnings.warn(
2025-04-11T03:52:12.4174771Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4175461Z   warnings.warn(
2025-04-11T03:52:12.4176105Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4176792Z   warnings.warn(
2025-04-11T03:52:12.4177454Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4178130Z   warnings.warn(
2025-04-11T03:52:12.4178777Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4179450Z   warnings.warn(
2025-04-11T03:52:12.4180096Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4180786Z   warnings.warn(
2025-04-11T03:52:12.4181540Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4182227Z   warnings.warn(
2025-04-11T03:52:12.4182872Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4183541Z   warnings.warn(
2025-04-11T03:52:12.4184193Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4184992Z   warnings.warn(
2025-04-11T03:52:12.4185651Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4186345Z   warnings.warn(
2025-04-11T03:52:12.4186981Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4188280Z   warnings.warn(
2025-04-11T03:52:12.4188598Z __________________________ test_dp_plugin_dataloader ___________________________
2025-04-11T03:52:12.4188837Z 
2025-04-11T03:52:12.4188936Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4189701Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4190358Z 
2025-04-11T03:52:12.4190469Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4190733Z         try_count = 0
2025-04-11T03:52:12.4190975Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4191244Z             max_try, int
2025-04-11T03:52:12.4191526Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4191824Z     
2025-04-11T03:52:12.4192052Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4192319Z             try:
2025-04-11T03:52:12.4192534Z                 try_count += 1
2025-04-11T03:52:12.4192768Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4193025Z                 return ret
2025-04-11T03:52:12.4193265Z             except exception_type as e:
2025-04-11T03:52:12.4193543Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4193911Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4194292Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4194638Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4195022Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4195337Z                     continue
2025-04-11T03:52:12.4195558Z                 else:
2025-04-11T03:52:12.4195900Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4196286Z >                   raise e
2025-04-11T03:52:12.4196431Z 
2025-04-11T03:52:12.4196534Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4196817Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4197142Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4197440Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4197787Z tests/test_booster/test_plugin/test_dp_plugin_base.py:94: in test_dp_plugin_dataloader
2025-04-11T03:52:12.4198152Z     spawn(run_dist, 2)
2025-04-11T03:52:12.4198388Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4198835Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4199290Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4199797Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4200350Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4200806Z     while not context.join():
2025-04-11T03:52:12.4201084Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4201266Z 
2025-04-11T03:52:12.4201485Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6b47c1f940>
2025-04-11T03:52:12.4201959Z timeout = None
2025-04-11T03:52:12.4202075Z 
2025-04-11T03:52:12.4202172Z     def join(self, timeout=None):
2025-04-11T03:52:12.4202466Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4202741Z     
2025-04-11T03:52:12.4202997Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4203358Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4203741Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4204067Z         of the first process exiting.
2025-04-11T03:52:12.4204310Z     
2025-04-11T03:52:12.4204619Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4204977Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4205256Z     
2025-04-11T03:52:12.4205429Z         Args:
2025-04-11T03:52:12.4205687Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4205981Z         """
2025-04-11T03:52:12.4206232Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4206533Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4206780Z             return True
2025-04-11T03:52:12.4207000Z     
2025-04-11T03:52:12.4207238Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4207565Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4207851Z             self.sentinels.keys(),
2025-04-11T03:52:12.4208091Z             timeout=timeout,
2025-04-11T03:52:12.4208308Z         )
2025-04-11T03:52:12.4208488Z     
2025-04-11T03:52:12.4208680Z         error_index = None
2025-04-11T03:52:12.4208905Z         for sentinel in ready:
2025-04-11T03:52:12.4209168Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4209456Z             process = self.processes[index]
2025-04-11T03:52:12.4209726Z             process.join()
2025-04-11T03:52:12.4209972Z             if process.exitcode != 0:
2025-04-11T03:52:12.4210216Z                 error_index = index
2025-04-11T03:52:12.4210457Z                 break
2025-04-11T03:52:12.4210657Z     
2025-04-11T03:52:12.4210850Z         # Return if there was no error.
2025-04-11T03:52:12.4211105Z         if error_index is None:
2025-04-11T03:52:12.4211379Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4211683Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4211927Z     
2025-04-11T03:52:12.4212171Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4212480Z         for process in self.processes:
2025-04-11T03:52:12.4212727Z             if process.is_alive():
2025-04-11T03:52:12.4212982Z                 process.terminate()
2025-04-11T03:52:12.4213224Z             process.join()
2025-04-11T03:52:12.4213439Z     
2025-04-11T03:52:12.4213679Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4214004Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4214302Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4214605Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4215004Z             if exitcode < 0:
2025-04-11T03:52:12.4215297Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4215586Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4215925Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4216261Z                     error_index=error_index,
2025-04-11T03:52:12.4216541Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4216816Z                     exit_code=exitcode,
2025-04-11T03:52:12.4217076Z                     signal_name=name,
2025-04-11T03:52:12.4217309Z                 )
2025-04-11T03:52:12.4217510Z             else:
2025-04-11T03:52:12.4217735Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4218192Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4218531Z                     error_index=error_index,
2025-04-11T03:52:12.4218792Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4219064Z                     exit_code=exitcode,
2025-04-11T03:52:12.4219304Z                 )
2025-04-11T03:52:12.4219501Z     
2025-04-11T03:52:12.4219745Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4220120Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4220467Z         msg += original_trace
2025-04-11T03:52:12.4220804Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4221229Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4221558Z E       
2025-04-11T03:52:12.4221800Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4222107Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4222598Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4223077Z E           fn(i, *args)
2025-04-11T03:52:12.4223493Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 89, in run_dist
2025-04-11T03:52:12.4223977Z E           check_dataloader_sharding()
2025-04-11T03:52:12.4224453Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 69, in check_dataloader_sharding
2025-04-11T03:52:12.4224976Z E           batch = next(iter(train_dataloader))[0].cuda()
2025-04-11T03:52:12.4225278Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4225750Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4226249Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4226625Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4226857Z 
2025-04-11T03:52:12.4227177Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4227727Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4228116Z [04/11/25 03:41:21] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4228514Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4228831Z                              :75 launch                                         
2025-04-11T03:52:12.4229152Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4229485Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4229895Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4230316Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4230947Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:32320 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4231448Z ______________________________ test_gemini_plugin ______________________________
2025-04-11T03:52:12.4231655Z 
2025-04-11T03:52:12.4231762Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4232515Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4233180Z 
2025-04-11T03:52:12.4233295Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4233560Z         try_count = 0
2025-04-11T03:52:12.4233937Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4234199Z             max_try, int
2025-04-11T03:52:12.4234482Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4234766Z     
2025-04-11T03:52:12.4235004Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4235275Z             try:
2025-04-11T03:52:12.4235496Z                 try_count += 1
2025-04-11T03:52:12.4235747Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4235992Z                 return ret
2025-04-11T03:52:12.4236237Z             except exception_type as e:
2025-04-11T03:52:12.4236532Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4236903Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4237283Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4237624Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4237993Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4238315Z                     continue
2025-04-11T03:52:12.4238539Z                 else:
2025-04-11T03:52:12.4238893Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4239273Z >                   raise e
2025-04-11T03:52:12.4239415Z 
2025-04-11T03:52:12.4239514Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4239792Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4240127Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4240421Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4240758Z tests/test_booster/test_plugin/test_gemini_plugin.py:172: in test_gemini_plugin
2025-04-11T03:52:12.4241127Z     spawn(run_dist, 4, early_stop=early_stop)
2025-04-11T03:52:12.4241394Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4241675Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4242110Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4242626Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4243164Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4243608Z     while not context.join():
2025-04-11T03:52:12.4243871Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4244070Z 
2025-04-11T03:52:12.4244268Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0319c90>
2025-04-11T03:52:12.4244620Z timeout = None
2025-04-11T03:52:12.4244735Z 
2025-04-11T03:52:12.4244839Z     def join(self, timeout=None):
2025-04-11T03:52:12.4245129Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4245395Z     
2025-04-11T03:52:12.4245644Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4246009Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4246496Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4246835Z         of the first process exiting.
2025-04-11T03:52:12.4247064Z     
2025-04-11T03:52:12.4247316Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4247674Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4247957Z     
2025-04-11T03:52:12.4248142Z         Args:
2025-04-11T03:52:12.4248394Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4248688Z         """
2025-04-11T03:52:12.4248944Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4249251Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4249613Z             return True
2025-04-11T03:52:12.4249813Z     
2025-04-11T03:52:12.4250050Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4250374Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4250659Z             self.sentinels.keys(),
2025-04-11T03:52:12.4250924Z             timeout=timeout,
2025-04-11T03:52:12.4251145Z         )
2025-04-11T03:52:12.4251343Z     
2025-04-11T03:52:12.4251542Z         error_index = None
2025-04-11T03:52:12.4251771Z         for sentinel in ready:
2025-04-11T03:52:12.4252030Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4252303Z             process = self.processes[index]
2025-04-11T03:52:12.4252565Z             process.join()
2025-04-11T03:52:12.4252803Z             if process.exitcode != 0:
2025-04-11T03:52:12.4253058Z                 error_index = index
2025-04-11T03:52:12.4253295Z                 break
2025-04-11T03:52:12.4253486Z     
2025-04-11T03:52:12.4253681Z         # Return if there was no error.
2025-04-11T03:52:12.4253937Z         if error_index is None:
2025-04-11T03:52:12.4254260Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4254569Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4254802Z     
2025-04-11T03:52:12.4255049Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4255359Z         for process in self.processes:
2025-04-11T03:52:12.4255617Z             if process.is_alive():
2025-04-11T03:52:12.4255864Z                 process.terminate()
2025-04-11T03:52:12.4256098Z             process.join()
2025-04-11T03:52:12.4256312Z     
2025-04-11T03:52:12.4256552Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4256877Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4257177Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4257474Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4257759Z             if exitcode < 0:
2025-04-11T03:52:12.4258014Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4258300Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4258633Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4258946Z                     error_index=error_index,
2025-04-11T03:52:12.4259219Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4259487Z                     exit_code=exitcode,
2025-04-11T03:52:12.4259739Z                     signal_name=name,
2025-04-11T03:52:12.4259976Z                 )
2025-04-11T03:52:12.4260166Z             else:
2025-04-11T03:52:12.4260396Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4260738Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4261075Z                     error_index=error_index,
2025-04-11T03:52:12.4261344Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4261606Z                     exit_code=exitcode,
2025-04-11T03:52:12.4261841Z                 )
2025-04-11T03:52:12.4262033Z     
2025-04-11T03:52:12.4262277Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4262798Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4263133Z         msg += original_trace
2025-04-11T03:52:12.4263468Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4263887Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4264217Z E       
2025-04-11T03:52:12.4264471Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4264773Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4265238Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4265714Z E           fn(i, *args)
2025-04-11T03:52:12.4266269Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 167, in run_dist
2025-04-11T03:52:12.4266741Z E           check_gemini_plugin(early_stop=early_stop)
2025-04-11T03:52:12.4267192Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4267622Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4268026Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4268478Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4268884Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4269291Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4269556Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.4270025Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 149, in check_gemini_plugin
2025-04-11T03:52:12.4270606Z E           err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn, zero_size, tp_size)
2025-04-11T03:52:12.4271104Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4271504Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4271941Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4272382Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4272834Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4273298Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4273585Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4274049Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4274545Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4274914Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4275138Z 
2025-04-11T03:52:12.4275449Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4275983Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4276371Z [04/11/25 03:41:30] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4276733Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4277051Z                              :75 launch                                         
2025-04-11T03:52:12.4277376Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4277710Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.4278112Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4278527Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4279993Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4281347Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4282692Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4284164Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4285497Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4286811Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4288127Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4289444Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4290351Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4291173Z   warnings.warn(
2025-04-11T03:52:12.4291964Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4292803Z   warnings.warn(
2025-04-11T03:52:12.4293592Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4294408Z   warnings.warn(
2025-04-11T03:52:12.4295189Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4295997Z   warnings.warn(
2025-04-11T03:52:12.4296925Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4297880Z   warnings.warn(
2025-04-11T03:52:12.4298893Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4299835Z   warnings.warn(
2025-04-11T03:52:12.4300738Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4301672Z   warnings.warn(
2025-04-11T03:52:12.4302573Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4303631Z   warnings.warn(
2025-04-11T03:52:12.4304534Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4305535Z   warnings.warn(
2025-04-11T03:52:12.4306435Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4307378Z   warnings.warn(
2025-04-11T03:52:12.4308270Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4309252Z   warnings.warn(
2025-04-11T03:52:12.4310161Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4311094Z   warnings.warn(
2025-04-11T03:52:12.4311996Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4312940Z   warnings.warn(
2025-04-11T03:52:12.4313865Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4314807Z   warnings.warn(
2025-04-11T03:52:12.4315722Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4316678Z   warnings.warn(
2025-04-11T03:52:12.4317711Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4318645Z   warnings.warn(
2025-04-11T03:52:12.4319062Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:20466 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4319707Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:20466 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4320603Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4321304Z   warnings.warn(
2025-04-11T03:52:12.4321961Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4322778Z   warnings.warn(
2025-04-11T03:52:12.4323433Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4324122Z   warnings.warn(
2025-04-11T03:52:12.4324765Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4325459Z   warnings.warn(
2025-04-11T03:52:12.4326135Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4326825Z   warnings.warn(
2025-04-11T03:52:12.4327470Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4328145Z   warnings.warn(
2025-04-11T03:52:12.4328800Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4329483Z   warnings.warn(
2025-04-11T03:52:12.4330123Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4330795Z   warnings.warn(
2025-04-11T03:52:12.4331439Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4332115Z   warnings.warn(
2025-04-11T03:52:12.4332756Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4333432Z   warnings.warn(
2025-04-11T03:52:12.4334079Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4334759Z   warnings.warn(
2025-04-11T03:52:12.4335400Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4336080Z   warnings.warn(
2025-04-11T03:52:12.4336341Z __________________________ test_low_level_zero_plugin __________________________
2025-04-11T03:52:12.4336569Z 
2025-04-11T03:52:12.4336666Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4337533Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4338197Z 
2025-04-11T03:52:12.4338306Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4338566Z         try_count = 0
2025-04-11T03:52:12.4338802Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4339056Z             max_try, int
2025-04-11T03:52:12.4339347Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4339643Z     
2025-04-11T03:52:12.4339866Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4340251Z             try:
2025-04-11T03:52:12.4340458Z                 try_count += 1
2025-04-11T03:52:12.4340708Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4340972Z                 return ret
2025-04-11T03:52:12.4341222Z             except exception_type as e:
2025-04-11T03:52:12.4341520Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4341885Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4342280Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4342637Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4343020Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4343341Z                     continue
2025-04-11T03:52:12.4343570Z                 else:
2025-04-11T03:52:12.4343917Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4344304Z >                   raise e
2025-04-11T03:52:12.4344449Z 
2025-04-11T03:52:12.4344551Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4344839Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4345167Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4345467Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4345848Z tests/test_booster/test_plugin/test_low_level_zero_plugin.py:141: in test_low_level_zero_plugin
2025-04-11T03:52:12.4346267Z     spawn(run_dist, 2, early_stop=early_stop)
2025-04-11T03:52:12.4346552Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4346835Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4347272Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4347777Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4348320Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4348828Z     while not context.join():
2025-04-11T03:52:12.4349098Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4349298Z 
2025-04-11T03:52:12.4349514Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f031a590>
2025-04-11T03:52:12.4349870Z timeout = None
2025-04-11T03:52:12.4349984Z 
2025-04-11T03:52:12.4350094Z     def join(self, timeout=None):
2025-04-11T03:52:12.4350385Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4350663Z     
2025-04-11T03:52:12.4350921Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4351282Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4351658Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4351997Z         of the first process exiting.
2025-04-11T03:52:12.4352233Z     
2025-04-11T03:52:12.4352486Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4352967Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4353246Z     
2025-04-11T03:52:12.4353428Z         Args:
2025-04-11T03:52:12.4353693Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4353988Z         """
2025-04-11T03:52:12.4354241Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4354536Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4354787Z             return True
2025-04-11T03:52:12.4355003Z     
2025-04-11T03:52:12.4355244Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4355577Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4355860Z             self.sentinels.keys(),
2025-04-11T03:52:12.4356242Z             timeout=timeout,
2025-04-11T03:52:12.4356470Z         )
2025-04-11T03:52:12.4356653Z     
2025-04-11T03:52:12.4356843Z         error_index = None
2025-04-11T03:52:12.4357065Z         for sentinel in ready:
2025-04-11T03:52:12.4357328Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4357613Z             process = self.processes[index]
2025-04-11T03:52:12.4357881Z             process.join()
2025-04-11T03:52:12.4358122Z             if process.exitcode != 0:
2025-04-11T03:52:12.4358369Z                 error_index = index
2025-04-11T03:52:12.4358612Z                 break
2025-04-11T03:52:12.4358815Z     
2025-04-11T03:52:12.4359010Z         # Return if there was no error.
2025-04-11T03:52:12.4359262Z         if error_index is None:
2025-04-11T03:52:12.4359535Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4359840Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4360083Z     
2025-04-11T03:52:12.4360329Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4360641Z         for process in self.processes:
2025-04-11T03:52:12.4360895Z             if process.is_alive():
2025-04-11T03:52:12.4361145Z                 process.terminate()
2025-04-11T03:52:12.4361402Z             process.join()
2025-04-11T03:52:12.4361627Z     
2025-04-11T03:52:12.4361878Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4362204Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4362506Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4362816Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4363099Z             if exitcode < 0:
2025-04-11T03:52:12.4363352Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4363635Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4363965Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4364289Z                     error_index=error_index,
2025-04-11T03:52:12.4364566Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4364837Z                     exit_code=exitcode,
2025-04-11T03:52:12.4365083Z                     signal_name=name,
2025-04-11T03:52:12.4365341Z                 )
2025-04-11T03:52:12.4365546Z             else:
2025-04-11T03:52:12.4365803Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4366142Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4366470Z                     error_index=error_index,
2025-04-11T03:52:12.4366739Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4367004Z                     exit_code=exitcode,
2025-04-11T03:52:12.4367240Z                 )
2025-04-11T03:52:12.4367433Z     
2025-04-11T03:52:12.4367663Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4368047Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4368389Z         msg += original_trace
2025-04-11T03:52:12.4368711Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4369248Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4369561Z E       
2025-04-11T03:52:12.4369792Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4370089Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4370561Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4371015Z E           fn(i, *args)
2025-04-11T03:52:12.4371450Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 135, in run_dist
2025-04-11T03:52:12.4371937Z E           check_low_level_zero_plugin(early_stop=early_stop)
2025-04-11T03:52:12.4372525Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4372947Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4373443Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 84, in check_low_level_zero_plugin
2025-04-11T03:52:12.4373998Z E           err = run_fn(stage, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:12.4374436Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4374822Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4375257Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4375697Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4376170Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4376639Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4376924Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4377379Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4377879Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4378249Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4378486Z 
2025-04-11T03:52:12.4378795Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4379330Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4379717Z [04/11/25 03:41:37] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4380076Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4380389Z                              :75 launch                                         
2025-04-11T03:52:12.4380711Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4381061Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4381457Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4381876Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4383219Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4384569Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4386019Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4387344Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4388257Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4389131Z   warnings.warn(
2025-04-11T03:52:12.4389935Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4390912Z   warnings.warn(
2025-04-11T03:52:12.4391847Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4392803Z   warnings.warn(
2025-04-11T03:52:12.4393720Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4394684Z   warnings.warn(
2025-04-11T03:52:12.4395583Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4396521Z   warnings.warn(
2025-04-11T03:52:12.4397420Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4398362Z   warnings.warn(
2025-04-11T03:52:12.4399273Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4400215Z   warnings.warn(
2025-04-11T03:52:12.4401126Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4402061Z   warnings.warn(
2025-04-11T03:52:12.4402732Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4403429Z   warnings.warn(
2025-04-11T03:52:12.4404099Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4404793Z   warnings.warn(
2025-04-11T03:52:12.4405564Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4406312Z   warnings.warn(
2025-04-11T03:52:12.4406957Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4407629Z   warnings.warn(
2025-04-11T03:52:12.4407892Z ____________________________ test_torch_ddp_plugin _____________________________
2025-04-11T03:52:12.4408122Z 
2025-04-11T03:52:12.4408221Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4409004Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4409816Z 
2025-04-11T03:52:12.4409926Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4410210Z         try_count = 0
2025-04-11T03:52:12.4410462Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4410729Z             max_try, int
2025-04-11T03:52:12.4411029Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4411340Z     
2025-04-11T03:52:12.4411590Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4411885Z             try:
2025-04-11T03:52:12.4412091Z                 try_count += 1
2025-04-11T03:52:12.4412340Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4412614Z                 return ret
2025-04-11T03:52:12.4412887Z             except exception_type as e:
2025-04-11T03:52:12.4413167Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4413556Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4413957Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4414323Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4414811Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4415194Z                     continue
2025-04-11T03:52:12.4415433Z                 else:
2025-04-11T03:52:12.4415795Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4416179Z >                   raise e
2025-04-11T03:52:12.4416317Z 
2025-04-11T03:52:12.4416431Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4416712Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4417044Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4417359Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4417718Z tests/test_booster/test_plugin/test_torch_ddp_plugin.py:119: in test_torch_ddp_plugin
2025-04-11T03:52:12.4418094Z     spawn(run_dist, 2)
2025-04-11T03:52:12.4418339Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4418616Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4419070Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4419591Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4420141Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4420603Z     while not context.join():
2025-04-11T03:52:12.4420869Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4421064Z 
2025-04-11T03:52:12.4421269Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f031a680>
2025-04-11T03:52:12.4421629Z timeout = None
2025-04-11T03:52:12.4421756Z 
2025-04-11T03:52:12.4421857Z     def join(self, timeout=None):
2025-04-11T03:52:12.4422155Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4422552Z     
2025-04-11T03:52:12.4422798Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4423163Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4423546Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4423875Z         of the first process exiting.
2025-04-11T03:52:12.4424124Z     
2025-04-11T03:52:12.4424382Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4424760Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4425045Z     
2025-04-11T03:52:12.4425226Z         Args:
2025-04-11T03:52:12.4425599Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4425884Z         """
2025-04-11T03:52:12.4426135Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4426440Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4426695Z             return True
2025-04-11T03:52:12.4426904Z     
2025-04-11T03:52:12.4427131Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4427462Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4427756Z             self.sentinels.keys(),
2025-04-11T03:52:12.4428013Z             timeout=timeout,
2025-04-11T03:52:12.4428236Z         )
2025-04-11T03:52:12.4428407Z     
2025-04-11T03:52:12.4428642Z         error_index = None
2025-04-11T03:52:12.4428878Z         for sentinel in ready:
2025-04-11T03:52:12.4429136Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4429418Z             process = self.processes[index]
2025-04-11T03:52:12.4429677Z             process.join()
2025-04-11T03:52:12.4429917Z             if process.exitcode != 0:
2025-04-11T03:52:12.4430174Z                 error_index = index
2025-04-11T03:52:12.4430418Z                 break
2025-04-11T03:52:12.4430624Z     
2025-04-11T03:52:12.4430819Z         # Return if there was no error.
2025-04-11T03:52:12.4431081Z         if error_index is None:
2025-04-11T03:52:12.4431365Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4431672Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4431918Z     
2025-04-11T03:52:12.4432152Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4432471Z         for process in self.processes:
2025-04-11T03:52:12.4432734Z             if process.is_alive():
2025-04-11T03:52:12.4432985Z                 process.terminate()
2025-04-11T03:52:12.4433239Z             process.join()
2025-04-11T03:52:12.4433442Z     
2025-04-11T03:52:12.4433692Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4434036Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4434342Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4434651Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4434928Z             if exitcode < 0:
2025-04-11T03:52:12.4435184Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4435477Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4435803Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4436126Z                     error_index=error_index,
2025-04-11T03:52:12.4436388Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4436655Z                     exit_code=exitcode,
2025-04-11T03:52:12.4436905Z                     signal_name=name,
2025-04-11T03:52:12.4437137Z                 )
2025-04-11T03:52:12.4437332Z             else:
2025-04-11T03:52:12.4437549Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4437884Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4438212Z                     error_index=error_index,
2025-04-11T03:52:12.4438605Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4438875Z                     exit_code=exitcode,
2025-04-11T03:52:12.4439104Z                 )
2025-04-11T03:52:12.4439293Z     
2025-04-11T03:52:12.4439532Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4439912Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4440249Z         msg += original_trace
2025-04-11T03:52:12.4440567Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4440983Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4441308Z E       
2025-04-11T03:52:12.4441711Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4442012Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4442498Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4442980Z E           fn(i, *args)
2025-04-11T03:52:12.4443414Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 113, in run_dist
2025-04-11T03:52:12.4443891Z E           check_torch_ddp_plugin()
2025-04-11T03:52:12.4444385Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 52, in check_torch_ddp_plugin
2025-04-11T03:52:12.4444929Z E           run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:12.4445365Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4445765Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4446214Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4446672Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4447141Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4447603Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4447893Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4448355Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4448868Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4449241Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4449467Z 
2025-04-11T03:52:12.4449789Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4450333Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4450723Z [04/11/25 03:41:45] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4451079Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4451397Z                              :75 launch                                         
2025-04-11T03:52:12.4451730Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4452091Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4452490Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4452909Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4454431Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4455778Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4457146Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4458501Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4459460Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4460408Z   warnings.warn(
2025-04-11T03:52:12.4461255Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4462084Z   warnings.warn(
2025-04-11T03:52:12.4463017Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4463970Z   warnings.warn(
2025-04-11T03:52:12.4464882Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4465827Z   warnings.warn(
2025-04-11T03:52:12.4466722Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4467654Z   warnings.warn(
2025-04-11T03:52:12.4468598Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4469533Z   warnings.warn(
2025-04-11T03:52:12.4470439Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4471376Z   warnings.warn(
2025-04-11T03:52:12.4472278Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4473214Z   warnings.warn(
2025-04-11T03:52:12.4473630Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56766 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4474687Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4475390Z   warnings.warn(
2025-04-11T03:52:12.4476049Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4476738Z   warnings.warn(
2025-04-11T03:52:12.4477391Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4478074Z   warnings.warn(
2025-04-11T03:52:12.4478703Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4479523Z   warnings.warn(
2025-04-11T03:52:12.4479799Z ____________________________ test_torch_fsdp_plugin ____________________________
2025-04-11T03:52:12.4480034Z 
2025-04-11T03:52:12.4480150Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4480910Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4481560Z 
2025-04-11T03:52:12.4481680Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4481946Z         try_count = 0
2025-04-11T03:52:12.4482180Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4482450Z             max_try, int
2025-04-11T03:52:12.4482740Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4483056Z     
2025-04-11T03:52:12.4483288Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4483560Z             try:
2025-04-11T03:52:12.4483767Z                 try_count += 1
2025-04-11T03:52:12.4484021Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4484282Z                 return ret
2025-04-11T03:52:12.4484531Z             except exception_type as e:
2025-04-11T03:52:12.4484804Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4485166Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4485556Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4485905Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4486291Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4486623Z                     continue
2025-04-11T03:52:12.4486853Z                 else:
2025-04-11T03:52:12.4487195Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4487576Z >                   raise e
2025-04-11T03:52:12.4487722Z 
2025-04-11T03:52:12.4487827Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4488132Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4488458Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4488757Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4489119Z tests/test_booster/test_plugin/test_torch_fsdp_plugin.py:83: in test_torch_fsdp_plugin
2025-04-11T03:52:12.4526565Z     spawn(run_dist, 2)
2025-04-11T03:52:12.4526993Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4527307Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4527774Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4528354Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4528935Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4529653Z     while not context.join():
2025-04-11T03:52:12.4529965Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4530162Z 
2025-04-11T03:52:12.4530390Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0164850>
2025-04-11T03:52:12.4530783Z timeout = None
2025-04-11T03:52:12.4530918Z 
2025-04-11T03:52:12.4531019Z     def join(self, timeout=None):
2025-04-11T03:52:12.4531322Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4531603Z     
2025-04-11T03:52:12.4531870Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4532248Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4532816Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4533176Z         of the first process exiting.
2025-04-11T03:52:12.4533433Z     
2025-04-11T03:52:12.4533699Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4534069Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4534349Z     
2025-04-11T03:52:12.4534538Z         Args:
2025-04-11T03:52:12.4534807Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4535108Z         """
2025-04-11T03:52:12.4535367Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4535664Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4535915Z             return True
2025-04-11T03:52:12.4536124Z     
2025-04-11T03:52:12.4536359Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4536693Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4536974Z             self.sentinels.keys(),
2025-04-11T03:52:12.4537231Z             timeout=timeout,
2025-04-11T03:52:12.4537454Z         )
2025-04-11T03:52:12.4537636Z     
2025-04-11T03:52:12.4537824Z         error_index = None
2025-04-11T03:52:12.4538054Z         for sentinel in ready:
2025-04-11T03:52:12.4538317Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4538599Z             process = self.processes[index]
2025-04-11T03:52:12.4538864Z             process.join()
2025-04-11T03:52:12.4539105Z             if process.exitcode != 0:
2025-04-11T03:52:12.4539352Z                 error_index = index
2025-04-11T03:52:12.4539591Z                 break
2025-04-11T03:52:12.4539796Z     
2025-04-11T03:52:12.4539990Z         # Return if there was no error.
2025-04-11T03:52:12.4540236Z         if error_index is None:
2025-04-11T03:52:12.4540521Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4540832Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4541073Z     
2025-04-11T03:52:12.4541318Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4541626Z         for process in self.processes:
2025-04-11T03:52:12.4541887Z             if process.is_alive():
2025-04-11T03:52:12.4542139Z                 process.terminate()
2025-04-11T03:52:12.4542384Z             process.join()
2025-04-11T03:52:12.4542598Z     
2025-04-11T03:52:12.4542834Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4543162Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4543461Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4543771Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4544051Z             if exitcode < 0:
2025-04-11T03:52:12.4544302Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4544587Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4544920Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4545243Z                     error_index=error_index,
2025-04-11T03:52:12.4545517Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4545914Z                     exit_code=exitcode,
2025-04-11T03:52:12.4546168Z                     signal_name=name,
2025-04-11T03:52:12.4546415Z                 )
2025-04-11T03:52:12.4546619Z             else:
2025-04-11T03:52:12.4546853Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4547205Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4547546Z                     error_index=error_index,
2025-04-11T03:52:12.4547828Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4548148Z                     exit_code=exitcode,
2025-04-11T03:52:12.4548397Z                 )
2025-04-11T03:52:12.4548654Z     
2025-04-11T03:52:12.4549068Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4549460Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4549809Z         msg += original_trace
2025-04-11T03:52:12.4550153Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4550587Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4550902Z E       
2025-04-11T03:52:12.4551156Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4551461Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4551949Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4552413Z E           fn(i, *args)
2025-04-11T03:52:12.4552857Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 77, in run_dist
2025-04-11T03:52:12.4553331Z E           check_torch_fsdp_plugin()
2025-04-11T03:52:12.4553838Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 70, in check_torch_fsdp_plugin
2025-04-11T03:52:12.4554370Z E           run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:12.4554792Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4555199Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4555631Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4556064Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4556544Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4557010Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4557295Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4557767Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4558268Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4558634Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4558880Z 
2025-04-11T03:52:12.4559198Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4559735Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4560133Z [04/11/25 03:41:52] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4560498Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4560825Z                              :75 launch                                         
2025-04-11T03:52:12.4561141Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4561491Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4561788Z custom_hanging_param_model
2025-04-11T03:52:12.4562029Z custom_hanging_param_model
2025-04-11T03:52:12.4562491Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4562905Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4564258Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4565609Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4567073Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4568434Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4569368Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4570218Z   warnings.warn(
2025-04-11T03:52:12.4571036Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4571880Z   warnings.warn(
2025-04-11T03:52:12.4572829Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4573792Z   warnings.warn(
2025-04-11T03:52:12.4574716Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4575661Z   warnings.warn(
2025-04-11T03:52:12.4576583Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4577548Z   warnings.warn(
2025-04-11T03:52:12.4578466Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4579407Z   warnings.warn(
2025-04-11T03:52:12.4580310Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4581254Z   warnings.warn(
2025-04-11T03:52:12.4582263Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4583198Z   warnings.warn(
2025-04-11T03:52:12.4583855Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4584551Z   warnings.warn(
2025-04-11T03:52:12.4585197Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4585992Z   warnings.warn(
2025-04-11T03:52:12.4586666Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4587365Z   warnings.warn(
2025-04-11T03:52:12.4588008Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4588733Z   warnings.warn(
2025-04-11T03:52:12.4588994Z ______________________________ test_gemini_ckpIO _______________________________
2025-04-11T03:52:12.4589215Z 
2025-04-11T03:52:12.4589316Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4590068Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4590716Z 
2025-04-11T03:52:12.4590839Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4591098Z         try_count = 0
2025-04-11T03:52:12.4591331Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4591598Z             max_try, int
2025-04-11T03:52:12.4591894Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4592194Z     
2025-04-11T03:52:12.4592422Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4592680Z             try:
2025-04-11T03:52:12.4592895Z                 try_count += 1
2025-04-11T03:52:12.4593139Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4593390Z                 return ret
2025-04-11T03:52:12.4593630Z             except exception_type as e:
2025-04-11T03:52:12.4593893Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4594267Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4594651Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4594998Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4595379Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4595714Z                     continue
2025-04-11T03:52:12.4595934Z                 else:
2025-04-11T03:52:12.4596283Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4596667Z >                   raise e
2025-04-11T03:52:12.4596807Z 
2025-04-11T03:52:12.4596911Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4597188Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4597511Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4597800Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4598141Z tests/test_checkpoint_io/test_gemini_checkpoint_io.py:220: in test_gemini_ckpIO
2025-04-11T03:52:12.4598478Z     spawn(run_dist, 4)
2025-04-11T03:52:12.4598713Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4599105Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4599540Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4600064Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4600612Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4601079Z     while not context.join():
2025-04-11T03:52:12.4601356Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4601540Z 
2025-04-11T03:52:12.4601753Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f01a07c0>
2025-04-11T03:52:12.4602229Z timeout = None
2025-04-11T03:52:12.4602356Z 
2025-04-11T03:52:12.4602455Z     def join(self, timeout=None):
2025-04-11T03:52:12.4602750Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4603025Z     
2025-04-11T03:52:12.4603280Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4603632Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4604013Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4604341Z         of the first process exiting.
2025-04-11T03:52:12.4604583Z     
2025-04-11T03:52:12.4604857Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4605225Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4605501Z     
2025-04-11T03:52:12.4605679Z         Args:
2025-04-11T03:52:12.4605934Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4606228Z         """
2025-04-11T03:52:12.4606475Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4606764Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4607072Z             return True
2025-04-11T03:52:12.4607276Z     
2025-04-11T03:52:12.4607507Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4607831Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4608111Z             self.sentinels.keys(),
2025-04-11T03:52:12.4608360Z             timeout=timeout,
2025-04-11T03:52:12.4608577Z         )
2025-04-11T03:52:12.4608755Z     
2025-04-11T03:52:12.4608930Z         error_index = None
2025-04-11T03:52:12.4609158Z         for sentinel in ready:
2025-04-11T03:52:12.4609415Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4609698Z             process = self.processes[index]
2025-04-11T03:52:12.4609958Z             process.join()
2025-04-11T03:52:12.4610192Z             if process.exitcode != 0:
2025-04-11T03:52:12.4610444Z                 error_index = index
2025-04-11T03:52:12.4610680Z                 break
2025-04-11T03:52:12.4610876Z     
2025-04-11T03:52:12.4611068Z         # Return if there was no error.
2025-04-11T03:52:12.4611317Z         if error_index is None:
2025-04-11T03:52:12.4611602Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4611906Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4612146Z     
2025-04-11T03:52:12.4612383Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4612689Z         for process in self.processes:
2025-04-11T03:52:12.4612946Z             if process.is_alive():
2025-04-11T03:52:12.4613194Z                 process.terminate()
2025-04-11T03:52:12.4613436Z             process.join()
2025-04-11T03:52:12.4613645Z     
2025-04-11T03:52:12.4613891Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4614235Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4614537Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4614845Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4615282Z             if exitcode < 0:
2025-04-11T03:52:12.4615531Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4615822Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4616152Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4616478Z                     error_index=error_index,
2025-04-11T03:52:12.4616746Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4617013Z                     exit_code=exitcode,
2025-04-11T03:52:12.4617254Z                     signal_name=name,
2025-04-11T03:52:12.4617487Z                 )
2025-04-11T03:52:12.4617679Z             else:
2025-04-11T03:52:12.4617903Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4618360Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4618686Z                     error_index=error_index,
2025-04-11T03:52:12.4618951Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4619213Z                     exit_code=exitcode,
2025-04-11T03:52:12.4619448Z                 )
2025-04-11T03:52:12.4619635Z     
2025-04-11T03:52:12.4619860Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4620230Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4620562Z         msg += original_trace
2025-04-11T03:52:12.4620882Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4621288Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4621580Z E       
2025-04-11T03:52:12.4621815Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4622109Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4622585Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4623050Z E           fn(i, *args)
2025-04-11T03:52:12.4623455Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_checkpoint_io.py", line 212, in run_dist
2025-04-11T03:52:12.4623916Z E           exam_state_dict()
2025-04-11T03:52:12.4624277Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4624669Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4625097Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4625532Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4625978Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4626443Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4626730Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4627195Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4627699Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4628062Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4628284Z 
2025-04-11T03:52:12.4628643Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4629202Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4629591Z [04/11/25 03:42:01] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4629950Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4630277Z                              :75 launch                                         
2025-04-11T03:52:12.4630596Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4631046Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.4631437Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4631849Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4633196Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4634662Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4635992Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4637311Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4638631Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4639953Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4641263Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4642568Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4643470Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4644296Z   warnings.warn(
2025-04-11T03:52:12.4645077Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4645889Z   warnings.warn(
2025-04-11T03:52:12.4646667Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4647473Z   warnings.warn(
2025-04-11T03:52:12.4648248Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4649065Z   warnings.warn(
2025-04-11T03:52:12.4650107Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4651062Z   warnings.warn(
2025-04-11T03:52:12.4651959Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4652890Z   warnings.warn(
2025-04-11T03:52:12.4653789Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4654844Z   warnings.warn(
2025-04-11T03:52:12.4655757Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4656682Z   warnings.warn(
2025-04-11T03:52:12.4657619Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4658563Z   warnings.warn(
2025-04-11T03:52:12.4659461Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4660382Z   warnings.warn(
2025-04-11T03:52:12.4661264Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4662186Z   warnings.warn(
2025-04-11T03:52:12.4663080Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4664010Z   warnings.warn(
2025-04-11T03:52:12.4664903Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4665826Z   warnings.warn(
2025-04-11T03:52:12.4666723Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4667655Z   warnings.warn(
2025-04-11T03:52:12.4668592Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4669527Z   warnings.warn(
2025-04-11T03:52:12.4670557Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4671487Z   warnings.warn(
2025-04-11T03:52:12.4671907Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:62155 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4672821Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4673667Z   warnings.warn(
2025-04-11T03:52:12.4674320Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4674998Z   warnings.warn(
2025-04-11T03:52:12.4675645Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4676361Z   warnings.warn(
2025-04-11T03:52:12.4677002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4677678Z   warnings.warn(
2025-04-11T03:52:12.4678333Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4679022Z   warnings.warn(
2025-04-11T03:52:12.4679659Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4680322Z   warnings.warn(
2025-04-11T03:52:12.4680967Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4681641Z   warnings.warn(
2025-04-11T03:52:12.4682287Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4682964Z   warnings.warn(
2025-04-11T03:52:12.4683248Z Exception ignored in: <function GeminiDDP.__del__ at 0x7f8d36345ea0>
2025-04-11T03:52:12.4683573Z Traceback (most recent call last):
2025-04-11T03:52:12.4683983Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T03:52:12.4684386Z     self.remove_hooks()
2025-04-11T03:52:12.4684794Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T03:52:12.4685270Z     for p in self.module.parameters():
2025-04-11T03:52:12.4685754Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T03:52:12.4686280Z Exception ignored in: <function GeminiDDP.__del__ at 0x7f1d6e56df30>
2025-04-11T03:52:12.4686605Z Traceback (most recent call last):
2025-04-11T03:52:12.4686997Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T03:52:12.4687496Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T03:52:12.4687907Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T03:52:12.4688215Z     self.remove_hooks()
2025-04-11T03:52:12.4688699Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T03:52:12.4689118Z     for p in self.module.parameters():
2025-04-11T03:52:12.4689577Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T03:52:12.4690123Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T03:52:12.4690515Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T03:52:12.4690876Z Exception ignored in: <function GeminiDDP.__del__ at 0x7f9b14815ea0>
2025-04-11T03:52:12.4691193Z Traceback (most recent call last):
2025-04-11T03:52:12.4691577Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T03:52:12.4692087Z     self.remove_hooks()
2025-04-11T03:52:12.4692462Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T03:52:12.4692881Z     for p in self.module.parameters():
2025-04-11T03:52:12.4693340Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T03:52:12.4693890Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T03:52:12.4694321Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T03:52:12.4694676Z _____________________________ test_gemini_ckpIO[2] _____________________________
2025-04-11T03:52:12.4694882Z 
2025-04-11T03:52:12.4695009Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T03:52:12.4695794Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4696452Z 
2025-04-11T03:52:12.4696568Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4696817Z         try_count = 0
2025-04-11T03:52:12.4697051Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4697308Z             max_try, int
2025-04-11T03:52:12.4697590Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4697882Z     
2025-04-11T03:52:12.4698090Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4698345Z             try:
2025-04-11T03:52:12.4698546Z                 try_count += 1
2025-04-11T03:52:12.4698785Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4699028Z                 return ret
2025-04-11T03:52:12.4699253Z             except exception_type as e:
2025-04-11T03:52:12.4699519Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4699879Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4700257Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4700596Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4700961Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4701272Z                     continue
2025-04-11T03:52:12.4701489Z                 else:
2025-04-11T03:52:12.4701830Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4702203Z >                   raise e
2025-04-11T03:52:12.4702335Z 
2025-04-11T03:52:12.4702440Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4702708Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4703035Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4703332Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4703711Z tests/test_checkpoint_io/test_gemini_torch_compability.py:175: in test_gemini_ckpIO
2025-04-11T03:52:12.4704077Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.4704435Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4704716Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4705142Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4705646Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4706177Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4706625Z     while not context.join():
2025-04-11T03:52:12.4706873Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4707059Z 
2025-04-11T03:52:12.4707262Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0020f70>
2025-04-11T03:52:12.4707790Z timeout = None
2025-04-11T03:52:12.4707905Z 
2025-04-11T03:52:12.4708008Z     def join(self, timeout=None):
2025-04-11T03:52:12.4708295Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4708596Z     
2025-04-11T03:52:12.4708847Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4709209Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4709600Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4709944Z         of the first process exiting.
2025-04-11T03:52:12.4710181Z     
2025-04-11T03:52:12.4710424Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4710789Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4711071Z     
2025-04-11T03:52:12.4711252Z         Args:
2025-04-11T03:52:12.4711512Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4711793Z         """
2025-04-11T03:52:12.4712038Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4712367Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4712617Z             return True
2025-04-11T03:52:12.4712828Z     
2025-04-11T03:52:12.4713054Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4713375Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4713656Z             self.sentinels.keys(),
2025-04-11T03:52:12.4713901Z             timeout=timeout,
2025-04-11T03:52:12.4714118Z         )
2025-04-11T03:52:12.4714290Z     
2025-04-11T03:52:12.4714474Z         error_index = None
2025-04-11T03:52:12.4714701Z         for sentinel in ready:
2025-04-11T03:52:12.4714962Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4715235Z             process = self.processes[index]
2025-04-11T03:52:12.4715500Z             process.join()
2025-04-11T03:52:12.4715739Z             if process.exitcode != 0:
2025-04-11T03:52:12.4715991Z                 error_index = index
2025-04-11T03:52:12.4716224Z                 break
2025-04-11T03:52:12.4716414Z     
2025-04-11T03:52:12.4716613Z         # Return if there was no error.
2025-04-11T03:52:12.4716865Z         if error_index is None:
2025-04-11T03:52:12.4717146Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4717448Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4717681Z     
2025-04-11T03:52:12.4717922Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4718229Z         for process in self.processes:
2025-04-11T03:52:12.4718485Z             if process.is_alive():
2025-04-11T03:52:12.4718732Z                 process.terminate()
2025-04-11T03:52:12.4718965Z             process.join()
2025-04-11T03:52:12.4719175Z     
2025-04-11T03:52:12.4719418Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4719756Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4720054Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4720353Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4720755Z             if exitcode < 0:
2025-04-11T03:52:12.4721010Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4721298Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4721647Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4721977Z                     error_index=error_index,
2025-04-11T03:52:12.4722242Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4722509Z                     exit_code=exitcode,
2025-04-11T03:52:12.4722764Z                     signal_name=name,
2025-04-11T03:52:12.4722996Z                 )
2025-04-11T03:52:12.4723189Z             else:
2025-04-11T03:52:12.4723533Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4723868Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4724202Z                     error_index=error_index,
2025-04-11T03:52:12.4724468Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4724732Z                     exit_code=exitcode,
2025-04-11T03:52:12.4724963Z                 )
2025-04-11T03:52:12.4725159Z     
2025-04-11T03:52:12.4725397Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4725776Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4726112Z         msg += original_trace
2025-04-11T03:52:12.4726430Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4726845Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4727164Z E       
2025-04-11T03:52:12.4727402Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4727704Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4728170Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4728642Z E           fn(i, *args)
2025-04-11T03:52:12.4729070Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_torch_compability.py", line 167, in run_dist
2025-04-11T03:52:12.4729539Z E           exam_torch_load_from_gemini()
2025-04-11T03:52:12.4729932Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4730328Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4730761Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4731224Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4731680Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4732138Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4732420Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4732886Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4733389Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4733760Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4733992Z 
2025-04-11T03:52:12.4734293Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4734839Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4735229Z [04/11/25 03:42:09] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4735586Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4735894Z                              :75 launch                                         
2025-04-11T03:52:12.4736215Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4736665Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4737064Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4737479Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4738844Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4740342Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4741698Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4743041Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4743958Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4744815Z   warnings.warn(
2025-04-11T03:52:12.4745634Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4746459Z   warnings.warn(
2025-04-11T03:52:12.4747407Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4748354Z   warnings.warn(
2025-04-11T03:52:12.4749316Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4750258Z   warnings.warn(
2025-04-11T03:52:12.4751162Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4752096Z   warnings.warn(
2025-04-11T03:52:12.4752996Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4753929Z   warnings.warn(
2025-04-11T03:52:12.4754830Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4755772Z   warnings.warn(
2025-04-11T03:52:12.4756805Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4757740Z   warnings.warn(
2025-04-11T03:52:12.4758154Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60258 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4759059Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4759858Z   warnings.warn(
2025-04-11T03:52:12.4760504Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4761184Z   warnings.warn(
2025-04-11T03:52:12.4761831Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4762521Z   warnings.warn(
2025-04-11T03:52:12.4763156Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4763827Z   warnings.warn(
2025-04-11T03:52:12.4764101Z Exception ignored in: <function GeminiDDP.__del__ at 0x7fc11587d750>
2025-04-11T03:52:12.4764436Z Traceback (most recent call last):
2025-04-11T03:52:12.4764843Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T03:52:12.4765236Z     self.remove_hooks()
2025-04-11T03:52:12.4765622Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T03:52:12.4766045Z     for p in self.module.parameters():
2025-04-11T03:52:12.4766554Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T03:52:12.4767111Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T03:52:12.4767533Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T03:52:12.4767900Z __________________________ test_unsharded_checkpoint ___________________________
2025-04-11T03:52:12.4768118Z 
2025-04-11T03:52:12.4768215Z args = (), kwargs = {}
2025-04-11T03:52:12.4768346Z 
2025-04-11T03:52:12.4768447Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.4768706Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.4768981Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.4769282Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.4769577Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.4769852Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.4770007Z 
2025-04-11T03:52:12.4770108Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.4770380Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4770731Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.4771060Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.4771334Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4771513Z 
2025-04-11T03:52:12.4771594Z device = None
2025-04-11T03:52:12.4771715Z 
2025-04-11T03:52:12.4771839Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.4772184Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.4772490Z     
2025-04-11T03:52:12.4772668Z         Args:
2025-04-11T03:52:12.4773061Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.4773474Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.4773827Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.4774073Z         """
2025-04-11T03:52:12.4774263Z         _lazy_init()
2025-04-11T03:52:12.4774483Z         with torch.cuda.device(device):
2025-04-11T03:52:12.4774743Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4775025Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4775511Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4776110Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4776481Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4776704Z 
2025-04-11T03:52:12.4776954Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.4777415Z ___________________ test_sharded_model_checkpoint[True-True] ___________________
2025-04-11T03:52:12.4777646Z 
2025-04-11T03:52:12.4777751Z use_safetensors = True, use_async = True
2025-04-11T03:52:12.4777926Z 
2025-04-11T03:52:12.4778078Z     @pytest.mark.parametrize("use_safetensors", [True, False])
2025-04-11T03:52:12.4778420Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T03:52:12.4778787Z     def test_sharded_model_checkpoint(use_safetensors: bool, use_async: bool):
2025-04-11T03:52:12.4779125Z         # create a model and optimizer
2025-04-11T03:52:12.4779371Z         model = resnet18()
2025-04-11T03:52:12.4779633Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T03:52:12.4779915Z         # create test data sample
2025-04-11T03:52:12.4780160Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T03:52:12.4780397Z     
2025-04-11T03:52:12.4780579Z         # run fwd and bwd
2025-04-11T03:52:12.4780796Z         y = model(x)
2025-04-11T03:52:12.4780999Z         loss = y.sum()
2025-04-11T03:52:12.4781210Z         loss.backward()
2025-04-11T03:52:12.4781425Z         optimizer.step()
2025-04-11T03:52:12.4781639Z     
2025-04-11T03:52:12.4781873Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4782217Z         optimizer_ckpt_tempfile = tempfile.NamedTemporaryFile()
2025-04-11T03:52:12.4782499Z     
2025-04-11T03:52:12.4782683Z         # save the model and optimizer
2025-04-11T03:52:12.4782946Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T03:52:12.4783187Z     
2025-04-11T03:52:12.4783374Z >       ckpt_io.save_model(
2025-04-11T03:52:12.4783598Z             model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=use_safetensors, use_async=use_async
2025-04-11T03:52:12.4783684Z         )
2025-04-11T03:52:12.4783688Z 
2025-04-11T03:52:12.4783839Z tests/test_checkpoint_io/test_general_checkpoint_io.py:96: 
2025-04-11T03:52:12.4783963Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4784130Z colossalai/checkpoint_io/checkpoint_io_base.py:190: in save_model
2025-04-11T03:52:12.4784222Z     self.save_sharded_model(
2025-04-11T03:52:12.4784420Z colossalai/checkpoint_io/general_checkpoint_io.py:238: in save_sharded_model
2025-04-11T03:52:12.4784613Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T03:52:12.4784808Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T03:52:12.4784958Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T03:52:12.4785144Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T03:52:12.4785387Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4785758Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T03:52:12.4785910Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4786152Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T03:52:12.4786292Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4786416Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T03:52:12.4786646Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4786761Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4786766Z 
2025-04-11T03:52:12.4786931Z tensor = tensor([[[[ 1.1998e-02, -1.2170e-02,  1.5254e-02,  ..., -4.4274e-02,
2025-04-11T03:52:12.4787136Z            -2.0488e-02,  1.2153e-02],
2025-04-11T03:52:12.4787221Z           [...709e-02],
2025-04-11T03:52:12.4787358Z           [-6.2823e-03,  8.1821e-03, -1.6644e-02,  ..., -2.5185e-02,
2025-04-11T03:52:12.4787455Z            -1.2198e-02,  9.3967e-05]]]])
2025-04-11T03:52:12.4787542Z empty = True
2025-04-11T03:52:12.4787546Z 
2025-04-11T03:52:12.4787727Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4787814Z         if empty:
2025-04-11T03:52:12.4787972Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4788081Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4788373Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4788582Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4788755Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4788759Z 
2025-04-11T03:52:12.4788883Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T03:52:12.4789053Z __________________ test_sharded_model_checkpoint[True-False] ___________________
2025-04-11T03:52:12.4789056Z 
2025-04-11T03:52:12.4789163Z use_safetensors = False, use_async = True
2025-04-11T03:52:12.4789167Z 
2025-04-11T03:52:12.4789322Z     @pytest.mark.parametrize("use_safetensors", [True, False])
2025-04-11T03:52:12.4789455Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T03:52:12.4789632Z     def test_sharded_model_checkpoint(use_safetensors: bool, use_async: bool):
2025-04-11T03:52:12.4789738Z         # create a model and optimizer
2025-04-11T03:52:12.4789828Z         model = resnet18()
2025-04-11T03:52:12.4789958Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T03:52:12.4790054Z         # create test data sample
2025-04-11T03:52:12.4790153Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T03:52:12.4790236Z     
2025-04-11T03:52:12.4790322Z         # run fwd and bwd
2025-04-11T03:52:12.4790411Z         y = model(x)
2025-04-11T03:52:12.4790494Z         loss = y.sum()
2025-04-11T03:52:12.4790578Z         loss.backward()
2025-04-11T03:52:12.4790679Z         optimizer.step()
2025-04-11T03:52:12.4790752Z     
2025-04-11T03:52:12.4790885Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4791031Z         optimizer_ckpt_tempfile = tempfile.NamedTemporaryFile()
2025-04-11T03:52:12.4791108Z     
2025-04-11T03:52:12.4791202Z         # save the model and optimizer
2025-04-11T03:52:12.4791299Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T03:52:12.4791383Z     
2025-04-11T03:52:12.4791472Z >       ckpt_io.save_model(
2025-04-11T03:52:12.4791699Z             model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=use_safetensors, use_async=use_async
2025-04-11T03:52:12.4791778Z         )
2025-04-11T03:52:12.4791789Z 
2025-04-11T03:52:12.4791946Z tests/test_checkpoint_io/test_general_checkpoint_io.py:96: 
2025-04-11T03:52:12.4792058Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4792356Z colossalai/checkpoint_io/checkpoint_io_base.py:190: in save_model
2025-04-11T03:52:12.4792461Z     self.save_sharded_model(
2025-04-11T03:52:12.4792653Z colossalai/checkpoint_io/general_checkpoint_io.py:238: in save_sharded_model
2025-04-11T03:52:12.4792843Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T03:52:12.4793026Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T03:52:12.4793176Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T03:52:12.4793345Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T03:52:12.4793578Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4793967Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T03:52:12.4794105Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4794357Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T03:52:12.4794492Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4794627Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T03:52:12.4794856Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4794972Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4794976Z 
2025-04-11T03:52:12.4795138Z tensor = tensor([[[[ 5.6167e-02,  4.7108e-03,  1.3328e-02,  ...,  1.3376e-02,
2025-04-11T03:52:12.4795231Z             3.0058e-02,  2.0246e-02],
2025-04-11T03:52:12.4795321Z           [...166e-03],
2025-04-11T03:52:12.4795455Z           [ 1.1236e-02, -3.2021e-02, -6.4110e-05,  ..., -7.2526e-03,
2025-04-11T03:52:12.4795551Z             2.9210e-03,  1.7852e-02]]]])
2025-04-11T03:52:12.4795633Z empty = True
2025-04-11T03:52:12.4795638Z 
2025-04-11T03:52:12.4795816Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4795906Z         if empty:
2025-04-11T03:52:12.4796060Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4796179Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4796467Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4796621Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4796782Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4796786Z 
2025-04-11T03:52:12.4796919Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T03:52:12.4797076Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4797243Z [04/11/25 03:42:12] WARNING  colossalai - colossalai - WARNING:                 
2025-04-11T03:52:12.4797383Z                              /__w/ColossalAI/ColossalAI/colossalai/checkpoint_io
2025-04-11T03:52:12.4797509Z                              /checkpoint_io_base.py:183 save_model              
2025-04-11T03:52:12.4797663Z                     WARNING  colossalai - colossalai - WARNING: Async save is   
2025-04-11T03:52:12.4797792Z                              only supported when use_safetensors is set to True.
2025-04-11T03:52:12.4797929Z                              Setting use_safetensors to True for async save.    
2025-04-11T03:52:12.4798086Z ___________________ test_sharded_optimizer_checkpoint[False] ___________________
2025-04-11T03:52:12.4798089Z 
2025-04-11T03:52:12.4798175Z use_async = False
2025-04-11T03:52:12.4798179Z 
2025-04-11T03:52:12.4798328Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T03:52:12.4798462Z     def test_sharded_optimizer_checkpoint(use_async: bool):
2025-04-11T03:52:12.4798564Z         # create a model and optimizer
2025-04-11T03:52:12.4798652Z         model = resnet18()
2025-04-11T03:52:12.4798889Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T03:52:12.4798968Z     
2025-04-11T03:52:12.4799066Z         # create test data sample
2025-04-11T03:52:12.4799169Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T03:52:12.4799244Z     
2025-04-11T03:52:12.4799336Z         # run fwd and bwd
2025-04-11T03:52:12.4799417Z         y = model(x)
2025-04-11T03:52:12.4799501Z         loss = y.sum()
2025-04-11T03:52:12.4799598Z         loss.backward()
2025-04-11T03:52:12.4799687Z         optimizer.step()
2025-04-11T03:52:12.4799767Z     
2025-04-11T03:52:12.4799876Z         # create temp directories for checkpoint
2025-04-11T03:52:12.4800003Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4800236Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4800310Z     
2025-04-11T03:52:12.4800411Z         # save the model and optimizer
2025-04-11T03:52:12.4800510Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T03:52:12.4800588Z     
2025-04-11T03:52:12.4800804Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T03:52:12.4801073Z         ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T03:52:12.4801160Z     
2025-04-11T03:52:12.4801250Z         ckpt_io._sync_d2h()
2025-04-11T03:52:12.4801352Z         ckpt_io._sync_io()
2025-04-11T03:52:12.4801426Z     
2025-04-11T03:52:12.4801509Z         # create new model
2025-04-11T03:52:12.4801605Z         new_model = resnet18()
2025-04-11T03:52:12.4801742Z         new_optimizer = Adam(new_model.parameters(), lr=0.001)
2025-04-11T03:52:12.4801822Z     
2025-04-11T03:52:12.4801981Z         ckpt_io.load_model(new_model, str(model_ckpt_dir.name), strict=True)
2025-04-11T03:52:12.4802153Z >       ckpt_io.load_optimizer(new_optimizer, str(optimizer_ckpt_dir.name))
2025-04-11T03:52:12.4802159Z 
2025-04-11T03:52:12.4802310Z tests/test_checkpoint_io/test_general_checkpoint_io.py:149: 
2025-04-11T03:52:12.4802426Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4802610Z colossalai/checkpoint_io/checkpoint_io_base.py:224: in load_optimizer
2025-04-11T03:52:12.4802708Z     self.load_sharded_optimizer(
2025-04-11T03:52:12.4802910Z colossalai/checkpoint_io/general_checkpoint_io.py:100: in load_sharded_optimizer
2025-04-11T03:52:12.4803053Z     load_states_into_optimizer(optimizer, state_dict, id_map)
2025-04-11T03:52:12.4803231Z colossalai/checkpoint_io/utils.py:830: in load_states_into_optimizer
2025-04-11T03:52:12.4803330Z     get_accelerator().synchronize()
2025-04-11T03:52:12.4803489Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.4803598Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.4803709Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4803713Z 
2025-04-11T03:52:12.4803801Z device = None
2025-04-11T03:52:12.4803805Z 
2025-04-11T03:52:12.4803930Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.4804092Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.4804165Z     
2025-04-11T03:52:12.4804242Z         Args:
2025-04-11T03:52:12.4804422Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.4804593Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.4804714Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.4804792Z         """
2025-04-11T03:52:12.4804884Z         _lazy_init()
2025-04-11T03:52:12.4804982Z         with torch.cuda.device(device):
2025-04-11T03:52:12.4805090Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4805205Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4805601Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4805752Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4805915Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4805920Z 
2025-04-11T03:52:12.4806175Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.4806331Z ___________________ test_sharded_optimizer_checkpoint[True] ____________________
2025-04-11T03:52:12.4806335Z 
2025-04-11T03:52:12.4806426Z use_async = True
2025-04-11T03:52:12.4806430Z 
2025-04-11T03:52:12.4806563Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T03:52:12.4806697Z     def test_sharded_optimizer_checkpoint(use_async: bool):
2025-04-11T03:52:12.4806896Z         # create a model and optimizer
2025-04-11T03:52:12.4806987Z         model = resnet18()
2025-04-11T03:52:12.4807112Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T03:52:12.4807190Z     
2025-04-11T03:52:12.4807290Z         # create test data sample
2025-04-11T03:52:12.4807389Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T03:52:12.4807474Z     
2025-04-11T03:52:12.4807573Z         # run fwd and bwd
2025-04-11T03:52:12.4807655Z         y = model(x)
2025-04-11T03:52:12.4807737Z         loss = y.sum()
2025-04-11T03:52:12.4807832Z         loss.backward()
2025-04-11T03:52:12.4807922Z         optimizer.step()
2025-04-11T03:52:12.4808013Z     
2025-04-11T03:52:12.4808159Z         # create temp directories for checkpoint
2025-04-11T03:52:12.4808292Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4808425Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4808502Z     
2025-04-11T03:52:12.4808605Z         # save the model and optimizer
2025-04-11T03:52:12.4808701Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T03:52:12.4808785Z     
2025-04-11T03:52:12.4808991Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T03:52:12.4809254Z >       ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T03:52:12.4809266Z 
2025-04-11T03:52:12.4809418Z tests/test_checkpoint_io/test_general_checkpoint_io.py:139: 
2025-04-11T03:52:12.4809533Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4809711Z colossalai/checkpoint_io/checkpoint_io_base.py:258: in save_optimizer
2025-04-11T03:52:12.4809809Z     self.save_sharded_optimizer(
2025-04-11T03:52:12.4810008Z colossalai/checkpoint_io/general_checkpoint_io.py:144: in save_sharded_optimizer
2025-04-11T03:52:12.4810204Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T03:52:12.4810397Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T03:52:12.4810551Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T03:52:12.4810717Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T03:52:12.4810966Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4811201Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T03:52:12.4811345Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4811582Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T03:52:12.4811718Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4811841Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T03:52:12.4812066Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4812191Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4812195Z 
2025-04-11T03:52:12.4812295Z tensor = tensor(1.), empty = True
2025-04-11T03:52:12.4812399Z 
2025-04-11T03:52:12.4812584Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4812665Z         if empty:
2025-04-11T03:52:12.4812826Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4812935Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4813230Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4813369Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4813529Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4813627Z 
2025-04-11T03:52:12.4813758Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T03:52:12.4813927Z _____________ test_sharded_optimizer_multiple_param_groups[False] ______________
2025-04-11T03:52:12.4813931Z 
2025-04-11T03:52:12.4814025Z use_async = False
2025-04-11T03:52:12.4814029Z 
2025-04-11T03:52:12.4814164Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T03:52:12.4814335Z     def test_sharded_optimizer_multiple_param_groups(use_async: bool):
2025-04-11T03:52:12.4814433Z         # create a model and optimizer
2025-04-11T03:52:12.4814521Z         model = resnet18()
2025-04-11T03:52:12.4814615Z         optimizer = Adam(
2025-04-11T03:52:12.4814842Z             [{"params": model.layer1.parameters()}, {"params": model.layer2.parameters(), "lr": 0.002}], lr=0.001
2025-04-11T03:52:12.4814927Z         )
2025-04-11T03:52:12.4815002Z     
2025-04-11T03:52:12.4815099Z         # create test data sample
2025-04-11T03:52:12.4815192Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T03:52:12.4815269Z     
2025-04-11T03:52:12.4815363Z         # run fwd and bwd
2025-04-11T03:52:12.4815445Z         y = model(x)
2025-04-11T03:52:12.4815531Z         loss = y.sum()
2025-04-11T03:52:12.4815616Z         loss.backward()
2025-04-11T03:52:12.4815707Z         optimizer.step()
2025-04-11T03:52:12.4815790Z     
2025-04-11T03:52:12.4815897Z         # create temp directories for checkpoint
2025-04-11T03:52:12.4816029Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4816164Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4816237Z     
2025-04-11T03:52:12.4816338Z         # save the model and optimizer
2025-04-11T03:52:12.4816436Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T03:52:12.4816515Z     
2025-04-11T03:52:12.4816718Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T03:52:12.4816981Z         ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T03:52:12.4817058Z     
2025-04-11T03:52:12.4817146Z         ckpt_io._sync_d2h()
2025-04-11T03:52:12.4817238Z         ckpt_io._sync_io()
2025-04-11T03:52:12.4817313Z     
2025-04-11T03:52:12.4817406Z         # create new model
2025-04-11T03:52:12.4817496Z         new_model = resnet18()
2025-04-11T03:52:12.4817587Z         new_optimizer = Adam(
2025-04-11T03:52:12.4817841Z             [{"params": new_model.layer1.parameters()}, {"params": new_model.layer2.parameters(), "lr": 0.002}], lr=0.001
2025-04-11T03:52:12.4817919Z         )
2025-04-11T03:52:12.4817998Z     
2025-04-11T03:52:12.4818156Z         ckpt_io.load_model(new_model, str(model_ckpt_dir.name), strict=True)
2025-04-11T03:52:12.4818323Z >       ckpt_io.load_optimizer(new_optimizer, str(optimizer_ckpt_dir.name))
2025-04-11T03:52:12.4818327Z 
2025-04-11T03:52:12.4818480Z tests/test_checkpoint_io/test_general_checkpoint_io.py:222: 
2025-04-11T03:52:12.4818594Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4818779Z colossalai/checkpoint_io/checkpoint_io_base.py:224: in load_optimizer
2025-04-11T03:52:12.4818877Z     self.load_sharded_optimizer(
2025-04-11T03:52:12.4819213Z colossalai/checkpoint_io/general_checkpoint_io.py:100: in load_sharded_optimizer
2025-04-11T03:52:12.4819364Z     load_states_into_optimizer(optimizer, state_dict, id_map)
2025-04-11T03:52:12.4819541Z colossalai/checkpoint_io/utils.py:830: in load_states_into_optimizer
2025-04-11T03:52:12.4819642Z     get_accelerator().synchronize()
2025-04-11T03:52:12.4819799Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.4819905Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.4820014Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4820018Z 
2025-04-11T03:52:12.4820106Z device = None
2025-04-11T03:52:12.4820110Z 
2025-04-11T03:52:12.4820233Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.4820504Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.4820580Z     
2025-04-11T03:52:12.4820657Z         Args:
2025-04-11T03:52:12.4820844Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.4821016Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.4821137Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.4821213Z         """
2025-04-11T03:52:12.4821301Z         _lazy_init()
2025-04-11T03:52:12.4821401Z         with torch.cuda.device(device):
2025-04-11T03:52:12.4821507Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4821624Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4821913Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4822062Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4822224Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4822228Z 
2025-04-11T03:52:12.4822481Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.4822651Z ______________ test_sharded_optimizer_multiple_param_groups[True] ______________
2025-04-11T03:52:12.4822654Z 
2025-04-11T03:52:12.4822743Z use_async = True
2025-04-11T03:52:12.4822747Z 
2025-04-11T03:52:12.4822880Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T03:52:12.4823040Z     def test_sharded_optimizer_multiple_param_groups(use_async: bool):
2025-04-11T03:52:12.4823142Z         # create a model and optimizer
2025-04-11T03:52:12.4823231Z         model = resnet18()
2025-04-11T03:52:12.4823326Z         optimizer = Adam(
2025-04-11T03:52:12.4823552Z             [{"params": model.layer1.parameters()}, {"params": model.layer2.parameters(), "lr": 0.002}], lr=0.001
2025-04-11T03:52:12.4823641Z         )
2025-04-11T03:52:12.4823717Z     
2025-04-11T03:52:12.4823809Z         # create test data sample
2025-04-11T03:52:12.4823910Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T03:52:12.4823985Z     
2025-04-11T03:52:12.4824078Z         # run fwd and bwd
2025-04-11T03:52:12.4824158Z         y = model(x)
2025-04-11T03:52:12.4824239Z         loss = y.sum()
2025-04-11T03:52:12.4824331Z         loss.backward()
2025-04-11T03:52:12.4824420Z         optimizer.step()
2025-04-11T03:52:12.4824495Z     
2025-04-11T03:52:12.4824602Z         # create temp directories for checkpoint
2025-04-11T03:52:12.4824726Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4824868Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4824942Z     
2025-04-11T03:52:12.4825041Z         # save the model and optimizer
2025-04-11T03:52:12.4825139Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T03:52:12.4825214Z     
2025-04-11T03:52:12.4825421Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T03:52:12.4825677Z >       ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T03:52:12.4825796Z 
2025-04-11T03:52:12.4825959Z tests/test_checkpoint_io/test_general_checkpoint_io.py:210: 
2025-04-11T03:52:12.4826074Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4826258Z colossalai/checkpoint_io/checkpoint_io_base.py:258: in save_optimizer
2025-04-11T03:52:12.4826358Z     self.save_sharded_optimizer(
2025-04-11T03:52:12.4826563Z colossalai/checkpoint_io/general_checkpoint_io.py:144: in save_sharded_optimizer
2025-04-11T03:52:12.4826753Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T03:52:12.4826942Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T03:52:12.4827187Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T03:52:12.4827349Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T03:52:12.4827589Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4827830Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T03:52:12.4827978Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4828217Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T03:52:12.4828352Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4828537Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T03:52:12.4828777Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4828906Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4828913Z 
2025-04-11T03:52:12.4829011Z tensor = tensor(1.), empty = True
2025-04-11T03:52:12.4829015Z 
2025-04-11T03:52:12.4829210Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4829296Z         if empty:
2025-04-11T03:52:12.4829456Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4829571Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4829855Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4830003Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4830167Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4830171Z 
2025-04-11T03:52:12.4830308Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T03:52:12.4830452Z _____________________________ test_hybrid_ckpIO[4] _____________________________
2025-04-11T03:52:12.4830459Z 
2025-04-11T03:52:12.4830588Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.4831209Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4831214Z 
2025-04-11T03:52:12.4831335Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4831420Z         try_count = 0
2025-04-11T03:52:12.4831528Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4831620Z             max_try, int
2025-04-11T03:52:12.4831770Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4831851Z     
2025-04-11T03:52:12.4831969Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4832053Z             try:
2025-04-11T03:52:12.4832143Z                 try_count += 1
2025-04-11T03:52:12.4832239Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4832330Z                 return ret
2025-04-11T03:52:12.4832427Z             except exception_type as e:
2025-04-11T03:52:12.4832659Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4832852Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4832972Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4833125Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4833277Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4833365Z                     continue
2025-04-11T03:52:12.4833446Z                 else:
2025-04-11T03:52:12.4833678Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4833862Z >                   raise e
2025-04-11T03:52:12.4833866Z 
2025-04-11T03:52:12.4833965Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4834088Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4834225Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4834325Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4834566Z tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py:155: in test_hybrid_ckpIO
2025-04-11T03:52:12.4834667Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.4834770Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4834877Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4835145Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4835323Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4835617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4835711Z     while not context.join():
2025-04-11T03:52:12.4835828Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4835832Z 
2025-04-11T03:52:12.4836034Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0250430>
2025-04-11T03:52:12.4836121Z timeout = None
2025-04-11T03:52:12.4836125Z 
2025-04-11T03:52:12.4836219Z     def join(self, timeout=None):
2025-04-11T03:52:12.4836350Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4836433Z     
2025-04-11T03:52:12.4836581Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4836732Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4836896Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4836991Z         of the first process exiting.
2025-04-11T03:52:12.4837076Z     
2025-04-11T03:52:12.4837226Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4837370Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4837442Z     
2025-04-11T03:52:12.4837523Z         Args:
2025-04-11T03:52:12.4837667Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4837751Z         """
2025-04-11T03:52:12.4837897Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4837994Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4838104Z             return True
2025-04-11T03:52:12.4838178Z     
2025-04-11T03:52:12.4838311Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4838445Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4838540Z             self.sentinels.keys(),
2025-04-11T03:52:12.4838637Z             timeout=timeout,
2025-04-11T03:52:12.4838714Z         )
2025-04-11T03:52:12.4838806Z     
2025-04-11T03:52:12.4838892Z         error_index = None
2025-04-11T03:52:12.4838977Z         for sentinel in ready:
2025-04-11T03:52:12.4839092Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4839197Z             process = self.processes[index]
2025-04-11T03:52:12.4839389Z             process.join()
2025-04-11T03:52:12.4839494Z             if process.exitcode != 0:
2025-04-11T03:52:12.4839587Z                 error_index = index
2025-04-11T03:52:12.4839675Z                 break
2025-04-11T03:52:12.4839751Z     
2025-04-11T03:52:12.4839853Z         # Return if there was no error.
2025-04-11T03:52:12.4839942Z         if error_index is None:
2025-04-11T03:52:12.4840078Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4840185Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4840262Z     
2025-04-11T03:52:12.4840409Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4840603Z         for process in self.processes:
2025-04-11T03:52:12.4840702Z             if process.is_alive():
2025-04-11T03:52:12.4840800Z                 process.terminate()
2025-04-11T03:52:12.4840887Z             process.join()
2025-04-11T03:52:12.4840969Z     
2025-04-11T03:52:12.4841115Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4841240Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4841353Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4841480Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4841573Z             if exitcode < 0:
2025-04-11T03:52:12.4841682Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4841799Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4841950Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4842059Z                     error_index=error_index,
2025-04-11T03:52:12.4842167Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4842258Z                     exit_code=exitcode,
2025-04-11T03:52:12.4842354Z                     signal_name=name,
2025-04-11T03:52:12.4842430Z                 )
2025-04-11T03:52:12.4842517Z             else:
2025-04-11T03:52:12.4842623Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4842786Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4842888Z                     error_index=error_index,
2025-04-11T03:52:12.4842992Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4843088Z                     exit_code=exitcode,
2025-04-11T03:52:12.4843166Z                 )
2025-04-11T03:52:12.4843245Z     
2025-04-11T03:52:12.4843378Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4843546Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4843645Z         msg += original_trace
2025-04-11T03:52:12.4843816Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4843981Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4844059Z E       
2025-04-11T03:52:12.4844194Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4844297Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4844601Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4844689Z E           fn(i, *args)
2025-04-11T03:52:12.4845010Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py", line 148, in run_dist
2025-04-11T03:52:12.4845104Z E           exam_state_dict()
2025-04-11T03:52:12.4845367Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4845467Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4845719Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4845810Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4846185Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4846278Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4846395Z E         [Previous line repeated 3 more times]
2025-04-11T03:52:12.4846617Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4846726Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4846988Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4847099Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4847398Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4847637Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4847751Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4848041Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4848188Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4848347Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4848351Z 
2025-04-11T03:52:12.4848660Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4848817Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4849004Z [04/11/25 03:42:21] WARNING  colossalai - colossalai.shardformer.modeling.llama 
2025-04-11T03:52:12.4849143Z                              - WARNING: `use_cache=True` is incompatible with   
2025-04-11T03:52:12.4849277Z                              pipeline parallelism. Setting `use_cache=False`... 
2025-04-11T03:52:12.4849433Z [04/11/25 03:42:21] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4849561Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4849678Z                              :75 launch                                         
2025-04-11T03:52:12.4849820Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4849945Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.4850151Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4850303Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4851439Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4851617Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4852718Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4852889Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4854091Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4854259Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4855372Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4855641Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4856339Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4856425Z   warnings.warn(
2025-04-11T03:52:12.4857111Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4857193Z   warnings.warn(
2025-04-11T03:52:12.4857896Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4857980Z   warnings.warn(
2025-04-11T03:52:12.4858668Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4858748Z   warnings.warn(
2025-04-11T03:52:12.4859574Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4859661Z   warnings.warn(
2025-04-11T03:52:12.4860464Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4860549Z   warnings.warn(
2025-04-11T03:52:12.4861354Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4861434Z   warnings.warn(
2025-04-11T03:52:12.4862226Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4862307Z   warnings.warn(
2025-04-11T03:52:12.4863202Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4863286Z   warnings.warn(
2025-04-11T03:52:12.4864079Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4864164Z   warnings.warn(
2025-04-11T03:52:12.4864950Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4865160Z   warnings.warn(
2025-04-11T03:52:12.4865964Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4866057Z   warnings.warn(
2025-04-11T03:52:12.4866855Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4866948Z   warnings.warn(
2025-04-11T03:52:12.4867744Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4867834Z   warnings.warn(
2025-04-11T03:52:12.4868671Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4868763Z   warnings.warn(
2025-04-11T03:52:12.4869542Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4869631Z   warnings.warn(
2025-04-11T03:52:12.4870183Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4870270Z   warnings.warn(
2025-04-11T03:52:12.4870803Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4870888Z   warnings.warn(
2025-04-11T03:52:12.4871425Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4871505Z   warnings.warn(
2025-04-11T03:52:12.4872037Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4872119Z   warnings.warn(
2025-04-11T03:52:12.4872781Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4872872Z   warnings.warn(
2025-04-11T03:52:12.4873406Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4873486Z   warnings.warn(
2025-04-11T03:52:12.4874017Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4874096Z   warnings.warn(
2025-04-11T03:52:12.4874624Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4874859Z   warnings.warn(
2025-04-11T03:52:12.4875020Z _______________________ test_low_level_zero_checkpointIO _______________________
2025-04-11T03:52:12.4875024Z 
2025-04-11T03:52:12.4875131Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4875135Z 
2025-04-11T03:52:12.4875241Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4875336Z         try_count = 0
2025-04-11T03:52:12.4875439Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4875531Z             max_try, int
2025-04-11T03:52:12.4875682Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4875755Z     
2025-04-11T03:52:12.4875880Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4875957Z             try:
2025-04-11T03:52:12.4876055Z                 try_count += 1
2025-04-11T03:52:12.4876153Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.4876157Z 
2025-04-11T03:52:12.4876257Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.4876383Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4876503Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.4876607Z     get_accelerator().synchronize()
2025-04-11T03:52:12.4876765Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.4876871Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.4876983Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4876987Z 
2025-04-11T03:52:12.4877067Z device = None
2025-04-11T03:52:12.4877077Z 
2025-04-11T03:52:12.4877203Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.4877357Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.4877446Z     
2025-04-11T03:52:12.4877522Z         Args:
2025-04-11T03:52:12.4877699Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.4877877Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.4877993Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.4878078Z         """
2025-04-11T03:52:12.4878161Z         _lazy_init()
2025-04-11T03:52:12.4878266Z         with torch.cuda.device(device):
2025-04-11T03:52:12.4878373Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4878489Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4878781Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4878922Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4879090Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4879097Z 
2025-04-11T03:52:12.4879335Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.4879596Z ______________________ test_huggingface_compatibility[2] _______________________
2025-04-11T03:52:12.4879601Z 
2025-04-11T03:52:12.4879729Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T03:52:12.4880342Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4880348Z 
2025-04-11T03:52:12.4880456Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4880549Z         try_count = 0
2025-04-11T03:52:12.4880651Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4880733Z             max_try, int
2025-04-11T03:52:12.4881015Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4881093Z     
2025-04-11T03:52:12.4881215Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4881294Z             try:
2025-04-11T03:52:12.4881387Z                 try_count += 1
2025-04-11T03:52:12.4881489Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4881573Z                 return ret
2025-04-11T03:52:12.4881677Z             except exception_type as e:
2025-04-11T03:52:12.4881797Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4882006Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4882125Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4882278Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4882441Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4882530Z                     continue
2025-04-11T03:52:12.4882626Z                 else:
2025-04-11T03:52:12.4882849Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4882941Z >                   raise e
2025-04-11T03:52:12.4882945Z 
2025-04-11T03:52:12.4883044Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4883165Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4883299Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4883390Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4883660Z tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py:79: in test_huggingface_compatibility
2025-04-11T03:52:12.4883753Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.4883866Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4883973Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4884236Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4884424Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4884714Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4884814Z     while not context.join():
2025-04-11T03:52:12.4884927Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4884931Z 
2025-04-11T03:52:12.4885137Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0251de0>
2025-04-11T03:52:12.4885220Z timeout = None
2025-04-11T03:52:12.4885224Z 
2025-04-11T03:52:12.4885329Z     def join(self, timeout=None):
2025-04-11T03:52:12.4885457Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4885531Z     
2025-04-11T03:52:12.4885689Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4885840Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4886013Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4886111Z         of the first process exiting.
2025-04-11T03:52:12.4886189Z     
2025-04-11T03:52:12.4886439Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4886585Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4886669Z     
2025-04-11T03:52:12.4886744Z         Args:
2025-04-11T03:52:12.4886893Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4886971Z         """
2025-04-11T03:52:12.4887112Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4887213Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4887299Z             return True
2025-04-11T03:52:12.4887381Z     
2025-04-11T03:52:12.4887514Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4887731Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4887827Z             self.sentinels.keys(),
2025-04-11T03:52:12.4887914Z             timeout=timeout,
2025-04-11T03:52:12.4888002Z         )
2025-04-11T03:52:12.4888080Z     
2025-04-11T03:52:12.4888174Z         error_index = None
2025-04-11T03:52:12.4888263Z         for sentinel in ready:
2025-04-11T03:52:12.4888372Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4888481Z             process = self.processes[index]
2025-04-11T03:52:12.4888572Z             process.join()
2025-04-11T03:52:12.4888675Z             if process.exitcode != 0:
2025-04-11T03:52:12.4888767Z                 error_index = index
2025-04-11T03:52:12.4888846Z                 break
2025-04-11T03:52:12.4888925Z     
2025-04-11T03:52:12.4889018Z         # Return if there was no error.
2025-04-11T03:52:12.4889107Z         if error_index is None:
2025-04-11T03:52:12.4889242Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4889348Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4889421Z     
2025-04-11T03:52:12.4889562Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4889673Z         for process in self.processes:
2025-04-11T03:52:12.4889764Z             if process.is_alive():
2025-04-11T03:52:12.4889866Z                 process.terminate()
2025-04-11T03:52:12.4889954Z             process.join()
2025-04-11T03:52:12.4890028Z     
2025-04-11T03:52:12.4890178Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4890296Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4890416Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4890542Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4890628Z             if exitcode < 0:
2025-04-11T03:52:12.4890744Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4890854Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4891017Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4891115Z                     error_index=error_index,
2025-04-11T03:52:12.4891242Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4891334Z                     exit_code=exitcode,
2025-04-11T03:52:12.4891424Z                     signal_name=name,
2025-04-11T03:52:12.4891514Z                 )
2025-04-11T03:52:12.4891593Z             else:
2025-04-11T03:52:12.4891708Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4891874Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4891977Z                     error_index=error_index,
2025-04-11T03:52:12.4892081Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4892171Z                     exit_code=exitcode,
2025-04-11T03:52:12.4892257Z                 )
2025-04-11T03:52:12.4892332Z     
2025-04-11T03:52:12.4892470Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4892646Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4892842Z         msg += original_trace
2025-04-11T03:52:12.4893025Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4893188Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4893276Z E       
2025-04-11T03:52:12.4893407Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4893516Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4893821Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4893906Z E           fn(i, *args)
2025-04-11T03:52:12.4894231Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py", line 72, in run_dist
2025-04-11T03:52:12.4894446Z E           exam_from_pretrained()
2025-04-11T03:52:12.4894676Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4894780Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4895047Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4895152Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4895432Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4895551Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4895662Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4895959Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4896099Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4896275Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4896279Z 
2025-04-11T03:52:12.4896591Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4896755Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4896917Z [04/11/25 03:42:28] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4897049Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4897165Z                              :75 launch                                         
2025-04-11T03:52:12.4897305Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4897438Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4897634Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4897794Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4898968Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4899151Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4900256Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4900449Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4901243Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4901336Z   warnings.warn(
2025-04-11T03:52:12.4902002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4902087Z   warnings.warn(
2025-04-11T03:52:12.4902914Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4903128Z   warnings.warn(
2025-04-11T03:52:12.4903912Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4903993Z   warnings.warn(
2025-04-11T03:52:12.4904789Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4904874Z   warnings.warn(
2025-04-11T03:52:12.4905676Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4905755Z   warnings.warn(
2025-04-11T03:52:12.4906552Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4906632Z   warnings.warn(
2025-04-11T03:52:12.4907429Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4907509Z   warnings.warn(
2025-04-11T03:52:12.4908066Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4908146Z   warnings.warn(
2025-04-11T03:52:12.4908790Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4908877Z   warnings.warn(
2025-04-11T03:52:12.4909433Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4909531Z   warnings.warn(
2025-04-11T03:52:12.4910186Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4910277Z   warnings.warn(
2025-04-11T03:52:12.4910424Z ___________________________ test_create_pin[1-True] ____________________________
2025-04-11T03:52:12.4910428Z 
2025-04-11T03:52:12.4910533Z empty = True, num_threads = 1
2025-04-11T03:52:12.4910537Z 
2025-04-11T03:52:12.4910663Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T03:52:12.4910793Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T03:52:12.4910924Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T03:52:12.4911035Z         model_state_dict = gen_model_state_dict()
2025-04-11T03:52:12.4911287Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T03:52:12.4911412Z 
2025-04-11T03:52:12.4911567Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T03:52:12.4911692Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4911865Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T03:52:12.4912117Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4912358Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T03:52:12.4912508Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4912751Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T03:52:12.4912891Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4913024Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T03:52:12.4913256Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4913376Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4913380Z 
2025-04-11T03:52:12.4913527Z tensor = tensor([[0.7278, 0.6708, 0.5495,  ..., 0.8589, 0.0268, 0.0457],
2025-04-11T03:52:12.4913703Z         [0.2799, 0.6305, 0.0349,  ..., 0.2965, 0.9488,...0.5473, 0.9859, 0.3709,  ..., 0.4942, 0.6802, 0.4158],
2025-04-11T03:52:12.4913817Z         [0.6189, 0.1026, 0.3877,  ..., 0.9755, 0.7854, 0.8188]])
2025-04-11T03:52:12.4913906Z empty = True
2025-04-11T03:52:12.4913910Z 
2025-04-11T03:52:12.4914097Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4914180Z         if empty:
2025-04-11T03:52:12.4914345Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4914456Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4914761Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4914906Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4915085Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4915088Z 
2025-04-11T03:52:12.4915219Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T03:52:12.4915369Z ___________________________ test_create_pin[1-False] ___________________________
2025-04-11T03:52:12.4915373Z 
2025-04-11T03:52:12.4915469Z empty = False, num_threads = 1
2025-04-11T03:52:12.4915472Z 
2025-04-11T03:52:12.4915600Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T03:52:12.4915724Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T03:52:12.4915853Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T03:52:12.4915966Z         model_state_dict = gen_model_state_dict()
2025-04-11T03:52:12.4916220Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T03:52:12.4916224Z 
2025-04-11T03:52:12.4916376Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T03:52:12.4916589Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4916762Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T03:52:12.4917004Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4917243Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T03:52:12.4917384Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4917620Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T03:52:12.4917765Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4918004Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T03:52:12.4918233Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4918347Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4918351Z 
2025-04-11T03:52:12.4918493Z tensor = tensor([[0.9313, 0.0163, 0.9832,  ..., 0.2312, 0.5103, 0.1652],
2025-04-11T03:52:12.4918674Z         [0.5121, 0.5898, 0.1334,  ..., 0.6431, 0.9040,...0.2168, 0.6617, 0.6909,  ..., 0.6668, 0.3573, 0.0393],
2025-04-11T03:52:12.4918789Z         [0.9993, 0.1955, 0.7855,  ..., 0.2109, 0.8531, 0.2361]])
2025-04-11T03:52:12.4918880Z empty = False
2025-04-11T03:52:12.4918884Z 
2025-04-11T03:52:12.4919062Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4919146Z         if empty:
2025-04-11T03:52:12.4919303Z             return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4919405Z >       return tensor.pin_memory()
2025-04-11T03:52:12.4919522Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4919815Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4919960Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4920122Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4920125Z 
2025-04-11T03:52:12.4920258Z colossalai/checkpoint_io/utils.py:968: RuntimeError
2025-04-11T03:52:12.4920400Z ___________________________ test_create_pin[4-True] ____________________________
2025-04-11T03:52:12.4920404Z 
2025-04-11T03:52:12.4920506Z empty = True, num_threads = 4
2025-04-11T03:52:12.4920510Z 
2025-04-11T03:52:12.4920634Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T03:52:12.4920751Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T03:52:12.4920886Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T03:52:12.4920991Z         model_state_dict = gen_model_state_dict()
2025-04-11T03:52:12.4921235Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T03:52:12.4921242Z 
2025-04-11T03:52:12.4921389Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T03:52:12.4921506Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4921671Z colossalai/checkpoint_io/utils.py:987: in create_pinned_state_dict
2025-04-11T03:52:12.4921767Z     elems[idx] = future.result()
2025-04-11T03:52:12.4921981Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:451: in result
2025-04-11T03:52:12.4922074Z     return self.__get_result()
2025-04-11T03:52:12.4922304Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
2025-04-11T03:52:12.4922395Z     raise self._exception
2025-04-11T03:52:12.4922599Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py:58: in run
2025-04-11T03:52:12.4922709Z     result = self.fn(*self.args, **self.kwargs)
2025-04-11T03:52:12.4922818Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4922926Z 
2025-04-11T03:52:12.4923075Z tensor = tensor([[0.9931, 0.0592, 0.6090,  ..., 0.0731, 0.8302, 0.3647],
2025-04-11T03:52:12.4923246Z         [0.8529, 0.9077, 0.4732,  ..., 0.0980, 0.4233,...0.5983, 0.8300, 0.4153,  ..., 0.0877, 0.5103, 0.6271],
2025-04-11T03:52:12.4923367Z         [0.7461, 0.1834, 0.2279,  ..., 0.9305, 0.2178, 0.5575]])
2025-04-11T03:52:12.4923448Z empty = True
2025-04-11T03:52:12.4923453Z 
2025-04-11T03:52:12.4923632Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4923712Z         if empty:
2025-04-11T03:52:12.4923876Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4923983Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4924385Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4924534Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4924696Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4924700Z 
2025-04-11T03:52:12.4924830Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T03:52:12.4924972Z ___________________________ test_create_pin[4-False] ___________________________
2025-04-11T03:52:12.4924978Z 
2025-04-11T03:52:12.4925079Z empty = False, num_threads = 4
2025-04-11T03:52:12.4925083Z 
2025-04-11T03:52:12.4925206Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T03:52:12.4925331Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T03:52:12.4925453Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T03:52:12.4925565Z         model_state_dict = gen_model_state_dict()
2025-04-11T03:52:12.4925815Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T03:52:12.4925820Z 
2025-04-11T03:52:12.4925976Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T03:52:12.4926107Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4926272Z colossalai/checkpoint_io/utils.py:987: in create_pinned_state_dict
2025-04-11T03:52:12.4926393Z     elems[idx] = future.result()
2025-04-11T03:52:12.4926599Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:451: in result
2025-04-11T03:52:12.4926700Z     return self.__get_result()
2025-04-11T03:52:12.4926927Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
2025-04-11T03:52:12.4927020Z     raise self._exception
2025-04-11T03:52:12.4927221Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py:58: in run
2025-04-11T03:52:12.4927332Z     result = self.fn(*self.args, **self.kwargs)
2025-04-11T03:52:12.4927448Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4927452Z 
2025-04-11T03:52:12.4927609Z tensor = tensor([[0.8519, 0.4295, 0.4258,  ..., 0.7715, 0.7338, 0.9187],
2025-04-11T03:52:12.4927780Z         [0.1316, 0.1529, 0.8565,  ..., 0.6041, 0.0071,...0.2519, 0.7662, 0.7709,  ..., 0.9714, 0.1224, 0.5552],
2025-04-11T03:52:12.4927908Z         [0.6948, 0.7876, 0.6498,  ..., 0.6190, 0.9752, 0.7557]])
2025-04-11T03:52:12.4927992Z empty = False
2025-04-11T03:52:12.4927996Z 
2025-04-11T03:52:12.4928181Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4928263Z         if empty:
2025-04-11T03:52:12.4928428Z             return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4928527Z >       return tensor.pin_memory()
2025-04-11T03:52:12.4928637Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4928938Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4929076Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4929377Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4929382Z 
2025-04-11T03:52:12.4929507Z colossalai/checkpoint_io/utils.py:968: RuntimeError
2025-04-11T03:52:12.4929654Z ________________________________ test_save_load ________________________________
2025-04-11T03:52:12.4929658Z 
2025-04-11T03:52:12.4929743Z args = (), kwargs = {}
2025-04-11T03:52:12.4929747Z 
2025-04-11T03:52:12.4929856Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.4929954Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.4930066Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.4930196Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.4930403Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.4930514Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.4930518Z 
2025-04-11T03:52:12.4930624Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.4930751Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4930917Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.4931021Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.4931147Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4931151Z 
2025-04-11T03:52:12.4931238Z device = None
2025-04-11T03:52:12.4931241Z 
2025-04-11T03:52:12.4931384Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.4931548Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.4931637Z     
2025-04-11T03:52:12.4931721Z         Args:
2025-04-11T03:52:12.4931897Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.4932086Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.4932203Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.4932296Z         """
2025-04-11T03:52:12.4932382Z         _lazy_init()
2025-04-11T03:52:12.4932493Z         with torch.cuda.device(device):
2025-04-11T03:52:12.4932601Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4932714Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4933017Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4933164Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4933339Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4933343Z 
2025-04-11T03:52:12.4933589Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.4933751Z _________________________ test_torch_ddp_checkpointIO __________________________
2025-04-11T03:52:12.4933754Z 
2025-04-11T03:52:12.4933858Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4934482Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4934488Z 
2025-04-11T03:52:12.4934598Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4934686Z         try_count = 0
2025-04-11T03:52:12.4934804Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4934903Z             max_try, int
2025-04-11T03:52:12.4935079Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4935160Z     
2025-04-11T03:52:12.4935293Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4935380Z             try:
2025-04-11T03:52:12.4935472Z                 try_count += 1
2025-04-11T03:52:12.4935583Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4935670Z                 return ret
2025-04-11T03:52:12.4935899Z             except exception_type as e:
2025-04-11T03:52:12.4936008Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4936206Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4936340Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4936496Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4936667Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4936756Z                     continue
2025-04-11T03:52:12.4936850Z                 else:
2025-04-11T03:52:12.4937078Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4937281Z >                   raise e
2025-04-11T03:52:12.4937296Z 
2025-04-11T03:52:12.4937398Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4937516Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4937665Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4937759Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4938002Z tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py:81: in test_torch_ddp_checkpointIO
2025-04-11T03:52:12.4938091Z     spawn(run_dist, 2)
2025-04-11T03:52:12.4938203Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4938320Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4938582Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4938772Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4939066Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4939169Z     while not context.join():
2025-04-11T03:52:12.4939285Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4939289Z 
2025-04-11T03:52:12.4939501Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0164af0>
2025-04-11T03:52:12.4939586Z timeout = None
2025-04-11T03:52:12.4939590Z 
2025-04-11T03:52:12.4939686Z     def join(self, timeout=None):
2025-04-11T03:52:12.4939826Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4939903Z     
2025-04-11T03:52:12.4940069Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4940223Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4940407Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4940514Z         of the first process exiting.
2025-04-11T03:52:12.4940592Z     
2025-04-11T03:52:12.4940755Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4940902Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4940988Z     
2025-04-11T03:52:12.4941069Z         Args:
2025-04-11T03:52:12.4941214Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4941304Z         """
2025-04-11T03:52:12.4941446Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4941553Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4941637Z             return True
2025-04-11T03:52:12.4941711Z     
2025-04-11T03:52:12.4941857Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4941980Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4942086Z             self.sentinels.keys(),
2025-04-11T03:52:12.4942174Z             timeout=timeout,
2025-04-11T03:52:12.4942258Z         )
2025-04-11T03:52:12.4942331Z     
2025-04-11T03:52:12.4942418Z         error_index = None
2025-04-11T03:52:12.4942516Z         for sentinel in ready:
2025-04-11T03:52:12.4942726Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4942836Z             process = self.processes[index]
2025-04-11T03:52:12.4942927Z             process.join()
2025-04-11T03:52:12.4943024Z             if process.exitcode != 0:
2025-04-11T03:52:12.4943123Z                 error_index = index
2025-04-11T03:52:12.4943202Z                 break
2025-04-11T03:52:12.4943280Z     
2025-04-11T03:52:12.4943375Z         # Return if there was no error.
2025-04-11T03:52:12.4943466Z         if error_index is None:
2025-04-11T03:52:12.4943609Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4943709Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4943786Z     
2025-04-11T03:52:12.4944024Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4944150Z         for process in self.processes:
2025-04-11T03:52:12.4944247Z             if process.is_alive():
2025-04-11T03:52:12.4944338Z                 process.terminate()
2025-04-11T03:52:12.4944443Z             process.join()
2025-04-11T03:52:12.4944516Z     
2025-04-11T03:52:12.4944665Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4944788Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4944906Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4945043Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4945131Z             if exitcode < 0:
2025-04-11T03:52:12.4945253Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4945367Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4945527Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4945630Z                     error_index=error_index,
2025-04-11T03:52:12.4945734Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4945832Z                     exit_code=exitcode,
2025-04-11T03:52:12.4945924Z                     signal_name=name,
2025-04-11T03:52:12.4946006Z                 )
2025-04-11T03:52:12.4946085Z             else:
2025-04-11T03:52:12.4946198Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4946365Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4946461Z                     error_index=error_index,
2025-04-11T03:52:12.4946569Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4946657Z                     exit_code=exitcode,
2025-04-11T03:52:12.4946741Z                 )
2025-04-11T03:52:12.4946814Z     
2025-04-11T03:52:12.4946949Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4947134Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4947222Z         msg += original_trace
2025-04-11T03:52:12.4947401Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4947567Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4947653Z E       
2025-04-11T03:52:12.4947784Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4947889Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4948197Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4948283Z E           fn(i, *args)
2025-04-11T03:52:12.4948621Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 76, in run_dist
2025-04-11T03:52:12.4948726Z E           check_torch_ddp_checkpointIO()
2025-04-11T03:52:12.4948993Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4949089Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4949352Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4949580Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4949834Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4949935Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4950042Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.4950376Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 26, in check_torch_ddp_checkpointIO
2025-04-11T03:52:12.4950619Z E           model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)
2025-04-11T03:52:12.4950829Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T03:52:12.4951273Z E           model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T03:52:12.4951543Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_ddp_plugin.py", line 283, in configure
2025-04-11T03:52:12.4951664Z E           model = model.to(get_current_device())
2025-04-11T03:52:12.4951929Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:12.4952037Z E           return self._apply(convert)
2025-04-11T03:52:12.4952310Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.4952415Z E           module._apply(fn)
2025-04-11T03:52:12.4952686Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.4952784Z E           param_applied = fn(param)
2025-04-11T03:52:12.4953075Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:12.4953291Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.4953431Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4953730Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4953888Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4954061Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4954065Z 
2025-04-11T03:52:12.4954379Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4954533Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4954696Z [04/11/25 03:42:35] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4954839Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4954949Z                              :75 launch                                         
2025-04-11T03:52:12.4955099Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4955225Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4955424Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4955563Z _____________________________ test_torch_fsdp_ckpt _____________________________
2025-04-11T03:52:12.4955568Z 
2025-04-11T03:52:12.4955669Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4956272Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4956280Z 
2025-04-11T03:52:12.4956391Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4956480Z         try_count = 0
2025-04-11T03:52:12.4956701Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4956798Z             max_try, int
2025-04-11T03:52:12.4956950Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4957033Z     
2025-04-11T03:52:12.4957150Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4957230Z             try:
2025-04-11T03:52:12.4957325Z                 try_count += 1
2025-04-11T03:52:12.4957422Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4957511Z                 return ret
2025-04-11T03:52:12.4957608Z             except exception_type as e:
2025-04-11T03:52:12.4957715Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4957910Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4958152Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4958307Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4958466Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4958556Z                     continue
2025-04-11T03:52:12.4958636Z                 else:
2025-04-11T03:52:12.4958856Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4958943Z >                   raise e
2025-04-11T03:52:12.4958947Z 
2025-04-11T03:52:12.4959045Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4959166Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4959300Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4959402Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4959614Z tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py:162: in test_torch_fsdp_ckpt
2025-04-11T03:52:12.4959699Z     spawn(run_dist, 2)
2025-04-11T03:52:12.4959814Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4959921Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4960182Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4960358Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4960649Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4960740Z     while not context.join():
2025-04-11T03:52:12.4960852Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4960863Z 
2025-04-11T03:52:12.4961065Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0318730>
2025-04-11T03:52:12.4961150Z timeout = None
2025-04-11T03:52:12.4961154Z 
2025-04-11T03:52:12.4961254Z     def join(self, timeout=None):
2025-04-11T03:52:12.4961384Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4961464Z     
2025-04-11T03:52:12.4961613Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4961758Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4961930Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4962028Z         of the first process exiting.
2025-04-11T03:52:12.4962111Z     
2025-04-11T03:52:12.4962258Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4962406Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4962481Z     
2025-04-11T03:52:12.4962558Z         Args:
2025-04-11T03:52:12.4962707Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4962788Z         """
2025-04-11T03:52:12.4962947Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4963048Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4963134Z             return True
2025-04-11T03:52:12.4963330Z     
2025-04-11T03:52:12.4963474Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4963601Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4963696Z             self.sentinels.keys(),
2025-04-11T03:52:12.4963788Z             timeout=timeout,
2025-04-11T03:52:12.4963864Z         )
2025-04-11T03:52:12.4963937Z     
2025-04-11T03:52:12.4964031Z         error_index = None
2025-04-11T03:52:12.4964117Z         for sentinel in ready:
2025-04-11T03:52:12.4964239Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4964340Z             process = self.processes[index]
2025-04-11T03:52:12.4964430Z             process.join()
2025-04-11T03:52:12.4964655Z             if process.exitcode != 0:
2025-04-11T03:52:12.4964748Z                 error_index = index
2025-04-11T03:52:12.4964834Z                 break
2025-04-11T03:52:12.4964910Z     
2025-04-11T03:52:12.4965008Z         # Return if there was no error.
2025-04-11T03:52:12.4965120Z         if error_index is None:
2025-04-11T03:52:12.4965258Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4965367Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4965440Z     
2025-04-11T03:52:12.4965591Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4965689Z         for process in self.processes:
2025-04-11T03:52:12.4965781Z             if process.is_alive():
2025-04-11T03:52:12.4965883Z                 process.terminate()
2025-04-11T03:52:12.4965971Z             process.join()
2025-04-11T03:52:12.4966051Z     
2025-04-11T03:52:12.4966195Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4966318Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4966437Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4966560Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4966657Z             if exitcode < 0:
2025-04-11T03:52:12.4966763Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4966877Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4967028Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4967127Z                     error_index=error_index,
2025-04-11T03:52:12.4967236Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4967330Z                     exit_code=exitcode,
2025-04-11T03:52:12.4967426Z                     signal_name=name,
2025-04-11T03:52:12.4967502Z                 )
2025-04-11T03:52:12.4967578Z             else:
2025-04-11T03:52:12.4967685Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4967853Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4967958Z                     error_index=error_index,
2025-04-11T03:52:12.4968057Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4968158Z                     exit_code=exitcode,
2025-04-11T03:52:12.4968231Z                 )
2025-04-11T03:52:12.4968305Z     
2025-04-11T03:52:12.4968445Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4968618Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4968708Z         msg += original_trace
2025-04-11T03:52:12.4968886Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4969051Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4969126Z E       
2025-04-11T03:52:12.4969254Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4969366Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4969664Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4969755Z E           fn(i, *args)
2025-04-11T03:52:12.4970152Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 156, in run_dist
2025-04-11T03:52:12.4970257Z E           check_torch_fsdp_ckpt()
2025-04-11T03:52:12.4970518Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4970610Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4970937Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 53, in check_torch_fsdp_ckpt
2025-04-11T03:52:12.4971142Z E           fsdp_model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion)
2025-04-11T03:52:12.4971358Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T03:52:12.4971664Z E           model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T03:52:12.4971945Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 533, in configure
2025-04-11T03:52:12.4972176Z E           fsdp_model = TorchFSDPModel(model, device_id=torch.cuda.current_device(), **self.fsdp_kwargs)
2025-04-11T03:52:12.4972451Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 438, in __init__
2025-04-11T03:52:12.4972577Z E           self.module = FSDP(module, *args, **kwargs)
2025-04-11T03:52:12.4972949Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 503, in __init__
2025-04-11T03:52:12.4973058Z E           _init_param_handle_from_module(
2025-04-11T03:52:12.4973426Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 568, in _init_param_handle_from_module
2025-04-11T03:52:12.4973534Z E           _move_module_to_device(
2025-04-11T03:52:12.4973882Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 956, in _move_module_to_device
2025-04-11T03:52:12.4974074Z E           _move_states_to_device(params_to_move, bufs_to_move, device_from_device_id)
2025-04-11T03:52:12.4974420Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 986, in _move_states_to_device
2025-04-11T03:52:12.4974552Z E           param.data = param.to(device_from_device_id)
2025-04-11T03:52:12.4974662Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4974950Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4975096Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4975260Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4975264Z 
2025-04-11T03:52:12.4975579Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4975734Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4975900Z [04/11/25 03:42:39] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4976034Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4976142Z                              :75 launch                                         
2025-04-11T03:52:12.4976287Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4976413Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4976619Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4976760Z _______________________________ test_logical_pg ________________________________
2025-04-11T03:52:12.4976763Z 
2025-04-11T03:52:12.4976867Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4977551Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4977556Z 
2025-04-11T03:52:12.4977668Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4977755Z         try_count = 0
2025-04-11T03:52:12.4977859Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4977975Z             max_try, int
2025-04-11T03:52:12.4978130Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4978218Z     
2025-04-11T03:52:12.4978338Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4978546Z             try:
2025-04-11T03:52:12.4978639Z                 try_count += 1
2025-04-11T03:52:12.4978735Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4978833Z                 return ret
2025-04-11T03:52:12.4978933Z             except exception_type as e:
2025-04-11T03:52:12.4979045Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4979236Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4979366Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4979513Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4979673Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4979768Z                     continue
2025-04-11T03:52:12.4979858Z                 else:
2025-04-11T03:52:12.4980099Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4980188Z >                   raise e
2025-04-11T03:52:12.4980192Z 
2025-04-11T03:52:12.4980301Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4980428Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4980565Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4980673Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4980837Z tests/test_device/test_init_logical_pg.py:33: in test_logical_pg
2025-04-11T03:52:12.4980936Z     spawn(check_layer, 4)
2025-04-11T03:52:12.4981042Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4981145Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4981412Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4981589Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4981888Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4981980Z     while not context.join():
2025-04-11T03:52:12.4982100Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4982107Z 
2025-04-11T03:52:12.4982309Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f024f0a0>
2025-04-11T03:52:12.4982400Z timeout = None
2025-04-11T03:52:12.4982404Z 
2025-04-11T03:52:12.4982499Z     def join(self, timeout=None):
2025-04-11T03:52:12.4982627Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4982711Z     
2025-04-11T03:52:12.4982861Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4983015Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4983179Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4983288Z         of the first process exiting.
2025-04-11T03:52:12.4983363Z     
2025-04-11T03:52:12.4983513Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4983663Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4983833Z     
2025-04-11T03:52:12.4983924Z         Args:
2025-04-11T03:52:12.4984066Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4984142Z         """
2025-04-11T03:52:12.4984293Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4984390Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4984482Z             return True
2025-04-11T03:52:12.4984558Z     
2025-04-11T03:52:12.4984694Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4984825Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4984922Z             self.sentinels.keys(),
2025-04-11T03:52:12.4985019Z             timeout=timeout,
2025-04-11T03:52:12.4985214Z         )
2025-04-11T03:52:12.4985302Z     
2025-04-11T03:52:12.4985392Z         error_index = None
2025-04-11T03:52:12.4985481Z         for sentinel in ready:
2025-04-11T03:52:12.4985604Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4985711Z             process = self.processes[index]
2025-04-11T03:52:12.4985810Z             process.join()
2025-04-11T03:52:12.4985912Z             if process.exitcode != 0:
2025-04-11T03:52:12.4986005Z                 error_index = index
2025-04-11T03:52:12.4986097Z                 break
2025-04-11T03:52:12.4986171Z     
2025-04-11T03:52:12.4986279Z         # Return if there was no error.
2025-04-11T03:52:12.4986372Z         if error_index is None:
2025-04-11T03:52:12.4986512Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4986623Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4986698Z     
2025-04-11T03:52:12.4986852Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4986962Z         for process in self.processes:
2025-04-11T03:52:12.4987073Z             if process.is_alive():
2025-04-11T03:52:12.4987171Z                 process.terminate()
2025-04-11T03:52:12.4987263Z             process.join()
2025-04-11T03:52:12.4987358Z     
2025-04-11T03:52:12.4987506Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4987636Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4987750Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4987879Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4987978Z             if exitcode < 0:
2025-04-11T03:52:12.4988090Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4988209Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4988367Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4988529Z                     error_index=error_index,
2025-04-11T03:52:12.4988637Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4988732Z                     exit_code=exitcode,
2025-04-11T03:52:12.4988835Z                     signal_name=name,
2025-04-11T03:52:12.4988919Z                 )
2025-04-11T03:52:12.4989028Z             else:
2025-04-11T03:52:12.4989137Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4989334Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4989440Z                     error_index=error_index,
2025-04-11T03:52:12.4989556Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4989665Z                     exit_code=exitcode,
2025-04-11T03:52:12.4989748Z                 )
2025-04-11T03:52:12.4989836Z     
2025-04-11T03:52:12.4989972Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4990145Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4990250Z         msg += original_trace
2025-04-11T03:52:12.4990428Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4990606Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4990844Z E       
2025-04-11T03:52:12.4990988Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4991095Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4991396Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4991493Z E           fn(i, *args)
2025-04-11T03:52:12.4991745Z E         File "/__w/ColossalAI/ColossalAI/tests/test_device/test_init_logical_pg.py", line 17, in check_layer
2025-04-11T03:52:12.4991882Z E           tensor_to_check = torch.tensor([2, 2, 2, 2]).cuda()
2025-04-11T03:52:12.4991994Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4992430Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4992569Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4992737Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4992751Z 
2025-04-11T03:52:12.4993061Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4993214Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4993384Z [04/11/25 03:43:00] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4993518Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4993640Z                              :75 launch                                         
2025-04-11T03:52:12.4993781Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4993920Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.4994119Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4994271Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4994580Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43505 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4994721Z ____________________________ test_all_to_all_single ____________________________
2025-04-11T03:52:12.4994725Z 
2025-04-11T03:52:12.4994836Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4995439Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4995447Z 
2025-04-11T03:52:12.4995564Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4995650Z         try_count = 0
2025-04-11T03:52:12.4995764Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4995850Z             max_try, int
2025-04-11T03:52:12.4996005Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4996089Z     
2025-04-11T03:52:12.4996206Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4996293Z             try:
2025-04-11T03:52:12.4996381Z                 try_count += 1
2025-04-11T03:52:12.4996477Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4996571Z                 return ret
2025-04-11T03:52:12.4996671Z             except exception_type as e:
2025-04-11T03:52:12.4996784Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4996974Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4997107Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4997255Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4997414Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4997614Z                     continue
2025-04-11T03:52:12.4997699Z                 else:
2025-04-11T03:52:12.4997938Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4998041Z >                   raise e
2025-04-11T03:52:12.4998045Z 
2025-04-11T03:52:12.4998162Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4998283Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4998418Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4998525Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4998693Z tests/test_fp8/test_all_to_all_single.py:73: in test_all_to_all_single
2025-04-11T03:52:12.4998883Z     spawn(run_dist, 4)
2025-04-11T03:52:12.4998993Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4999110Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4999379Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4999563Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4999866Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4999966Z     while not context.join():
2025-04-11T03:52:12.5000094Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5000098Z 
2025-04-11T03:52:12.5000301Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0319960>
2025-04-11T03:52:12.5000394Z timeout = None
2025-04-11T03:52:12.5000398Z 
2025-04-11T03:52:12.5000495Z     def join(self, timeout=None):
2025-04-11T03:52:12.5000636Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5000713Z     
2025-04-11T03:52:12.5000862Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5001020Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5001188Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5001290Z         of the first process exiting.
2025-04-11T03:52:12.5001369Z     
2025-04-11T03:52:12.5001525Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5001677Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5001756Z     
2025-04-11T03:52:12.5001847Z         Args:
2025-04-11T03:52:12.5001991Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5002080Z         """
2025-04-11T03:52:12.5002223Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5002323Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5002417Z             return True
2025-04-11T03:52:12.5002492Z     
2025-04-11T03:52:12.5002639Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5002767Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5002864Z             self.sentinels.keys(),
2025-04-11T03:52:12.5002963Z             timeout=timeout,
2025-04-11T03:52:12.5003040Z         )
2025-04-11T03:52:12.5003124Z     
2025-04-11T03:52:12.5003211Z         error_index = None
2025-04-11T03:52:12.5003300Z         for sentinel in ready:
2025-04-11T03:52:12.5003423Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5003525Z             process = self.processes[index]
2025-04-11T03:52:12.5003621Z             process.join()
2025-04-11T03:52:12.5003719Z             if process.exitcode != 0:
2025-04-11T03:52:12.5003820Z                 error_index = index
2025-04-11T03:52:12.5003902Z                 break
2025-04-11T03:52:12.5003976Z     
2025-04-11T03:52:12.5004083Z         # Return if there was no error.
2025-04-11T03:52:12.5004173Z         if error_index is None:
2025-04-11T03:52:12.5004417Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5004524Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5004601Z     
2025-04-11T03:52:12.5004755Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5004858Z         for process in self.processes:
2025-04-11T03:52:12.5004959Z             if process.is_alive():
2025-04-11T03:52:12.5005054Z                 process.terminate()
2025-04-11T03:52:12.5005140Z             process.join()
2025-04-11T03:52:12.5005225Z     
2025-04-11T03:52:12.5005368Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5005497Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5005608Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5005868Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5005956Z             if exitcode < 0:
2025-04-11T03:52:12.5006064Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5006187Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5006341Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5006447Z                     error_index=error_index,
2025-04-11T03:52:12.5006552Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5006654Z                     exit_code=exitcode,
2025-04-11T03:52:12.5006745Z                     signal_name=name,
2025-04-11T03:52:12.5006822Z                 )
2025-04-11T03:52:12.5006910Z             else:
2025-04-11T03:52:12.5007015Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5007195Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5007298Z                     error_index=error_index,
2025-04-11T03:52:12.5007403Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5007513Z                     exit_code=exitcode,
2025-04-11T03:52:12.5007593Z                 )
2025-04-11T03:52:12.5007684Z     
2025-04-11T03:52:12.5007825Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5008019Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5008108Z         msg += original_trace
2025-04-11T03:52:12.5008287Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5008460Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5008538Z E       
2025-04-11T03:52:12.5008676Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5008780Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5009096Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5009191Z E           fn(i, *args)
2025-04-11T03:52:12.5009487Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_all_to_all_single.py", line 67, in run_dist
2025-04-11T03:52:12.5009591Z E           check_all2all()
2025-04-11T03:52:12.5009816Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5009931Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5010192Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5010311Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5010591Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5010702Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5010819Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5011110Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5011260Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5011531Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5011537Z 
2025-04-11T03:52:12.5011858Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5012016Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5012175Z [04/11/25 03:43:06] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5012316Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5012430Z                              :75 launch                                         
2025-04-11T03:52:12.5012674Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5012802Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.5013012Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5013161Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5013468Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:48660 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5013756Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:48660 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5014321Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5014424Z   warnings.warn(
2025-04-11T03:52:12.5014968Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5015072Z   warnings.warn(
2025-04-11T03:52:12.5015625Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5015718Z   warnings.warn(
2025-04-11T03:52:12.5016250Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5016346Z   warnings.warn(
2025-04-11T03:52:12.5016882Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5016983Z   warnings.warn(
2025-04-11T03:52:12.5017519Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5017602Z   warnings.warn(
2025-04-11T03:52:12.5018142Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5018224Z   warnings.warn(
2025-04-11T03:52:12.5018756Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5018837Z   warnings.warn(
2025-04-11T03:52:12.5018985Z _______________________________ test_all_to_all ________________________________
2025-04-11T03:52:12.5018989Z 
2025-04-11T03:52:12.5019086Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5019803Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5019809Z 
2025-04-11T03:52:12.5019917Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5020010Z         try_count = 0
2025-04-11T03:52:12.5020116Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5020201Z             max_try, int
2025-04-11T03:52:12.5020361Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5020436Z     
2025-04-11T03:52:12.5020561Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5020639Z             try:
2025-04-11T03:52:12.5020817Z                 try_count += 1
2025-04-11T03:52:12.5020921Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5021008Z                 return ret
2025-04-11T03:52:12.5021115Z             except exception_type as e:
2025-04-11T03:52:12.5021224Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5021422Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5021544Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5021693Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5021859Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5021943Z                     continue
2025-04-11T03:52:12.5022031Z                 else:
2025-04-11T03:52:12.5022254Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5022349Z >                   raise e
2025-04-11T03:52:12.5022353Z 
2025-04-11T03:52:12.5022451Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5022565Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5022715Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5022806Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5022967Z tests/test_fp8/test_fp8_all_to_all.py:36: in test_all_to_all
2025-04-11T03:52:12.5023054Z     spawn(run_dist, 4)
2025-04-11T03:52:12.5023170Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5023275Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5023534Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5023723Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5024011Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5024116Z     while not context.join():
2025-04-11T03:52:12.5024228Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5024232Z 
2025-04-11T03:52:12.5024442Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0165c00>
2025-04-11T03:52:12.5024524Z timeout = None
2025-04-11T03:52:12.5024528Z 
2025-04-11T03:52:12.5024622Z     def join(self, timeout=None):
2025-04-11T03:52:12.5024758Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5024835Z     
2025-04-11T03:52:12.5024990Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5025137Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5025313Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5025413Z         of the first process exiting.
2025-04-11T03:52:12.5025503Z     
2025-04-11T03:52:12.5025666Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5025807Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5025907Z     
2025-04-11T03:52:12.5025986Z         Args:
2025-04-11T03:52:12.5026236Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5026329Z         """
2025-04-11T03:52:12.5026475Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5026582Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5026669Z             return True
2025-04-11T03:52:12.5026753Z     
2025-04-11T03:52:12.5026890Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5027014Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5027120Z             self.sentinels.keys(),
2025-04-11T03:52:12.5027210Z             timeout=timeout,
2025-04-11T03:52:12.5027296Z         )
2025-04-11T03:52:12.5027468Z     
2025-04-11T03:52:12.5027559Z         error_index = None
2025-04-11T03:52:12.5027673Z         for sentinel in ready:
2025-04-11T03:52:12.5027788Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5027904Z             process = self.processes[index]
2025-04-11T03:52:12.5027998Z             process.join()
2025-04-11T03:52:12.5028107Z             if process.exitcode != 0:
2025-04-11T03:52:12.5028202Z                 error_index = index
2025-04-11T03:52:12.5028283Z                 break
2025-04-11T03:52:12.5028370Z     
2025-04-11T03:52:12.5028510Z         # Return if there was no error.
2025-04-11T03:52:12.5028613Z         if error_index is None:
2025-04-11T03:52:12.5028751Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5028850Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5028934Z     
2025-04-11T03:52:12.5029079Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5029189Z         for process in self.processes:
2025-04-11T03:52:12.5029286Z             if process.is_alive():
2025-04-11T03:52:12.5029384Z                 process.terminate()
2025-04-11T03:52:12.5029484Z             process.join()
2025-04-11T03:52:12.5029559Z     
2025-04-11T03:52:12.5029713Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5029834Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5029957Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5030082Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5030172Z             if exitcode < 0:
2025-04-11T03:52:12.5030291Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5030404Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5030566Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5030668Z                     error_index=error_index,
2025-04-11T03:52:12.5030790Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5030883Z                     exit_code=exitcode,
2025-04-11T03:52:12.5030975Z                     signal_name=name,
2025-04-11T03:52:12.5031064Z                 )
2025-04-11T03:52:12.5031144Z             else:
2025-04-11T03:52:12.5031266Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5031434Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5031532Z                     error_index=error_index,
2025-04-11T03:52:12.5031646Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5031738Z                     exit_code=exitcode,
2025-04-11T03:52:12.5031824Z                 )
2025-04-11T03:52:12.5031900Z     
2025-04-11T03:52:12.5032037Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5032218Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5032308Z         msg += original_trace
2025-04-11T03:52:12.5032502Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5032668Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5032785Z E       
2025-04-11T03:52:12.5033055Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5033163Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5033471Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5033559Z E           fn(i, *args)
2025-04-11T03:52:12.5033798Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all.py", line 31, in run_dist
2025-04-11T03:52:12.5033889Z E           check_4gpu()
2025-04-11T03:52:12.5034128Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5034231Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5034628Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5034749Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5035035Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5035153Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5035262Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5035557Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5035696Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5035868Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5035872Z 
2025-04-11T03:52:12.5036199Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5036359Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5036524Z [04/11/25 03:43:13] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5036660Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5036782Z                              :75 launch                                         
2025-04-11T03:52:12.5036924Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5037060Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.5037259Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5037408Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5037716Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:48337 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5038279Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5038388Z   warnings.warn(
2025-04-11T03:52:12.5038931Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5039030Z   warnings.warn(
2025-04-11T03:52:12.5039573Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5039665Z   warnings.warn(
2025-04-11T03:52:12.5040199Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5040295Z   warnings.warn(
2025-04-11T03:52:12.5040942Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5041028Z   warnings.warn(
2025-04-11T03:52:12.5041561Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5041644Z   warnings.warn(
2025-04-11T03:52:12.5042195Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5042277Z   warnings.warn(
2025-04-11T03:52:12.5042938Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5043021Z   warnings.warn(
2025-04-11T03:52:12.5043178Z ____________________________ test_all_to_all_single ____________________________
2025-04-11T03:52:12.5043182Z 
2025-04-11T03:52:12.5043283Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5043891Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5043896Z 
2025-04-11T03:52:12.5044006Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5044094Z         try_count = 0
2025-04-11T03:52:12.5044217Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5044312Z             max_try, int
2025-04-11T03:52:12.5044474Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5044550Z     
2025-04-11T03:52:12.5044676Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5044756Z             try:
2025-04-11T03:52:12.5044844Z                 try_count += 1
2025-04-11T03:52:12.5044950Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5045035Z                 return ret
2025-04-11T03:52:12.5045143Z             except exception_type as e:
2025-04-11T03:52:12.5045248Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5045440Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5045570Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5045721Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5045889Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5045978Z                     continue
2025-04-11T03:52:12.5046068Z                 else:
2025-04-11T03:52:12.5046298Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5046384Z >                   raise e
2025-04-11T03:52:12.5046397Z 
2025-04-11T03:52:12.5046496Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5046612Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5046758Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5046848Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5047035Z tests/test_fp8/test_fp8_all_to_all_single.py:34: in test_all_to_all_single
2025-04-11T03:52:12.5047122Z     spawn(run_dist, 4)
2025-04-11T03:52:12.5047228Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5047345Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5047606Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5047794Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5048190Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5048296Z     while not context.join():
2025-04-11T03:52:12.5048412Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5048416Z 
2025-04-11T03:52:12.5048623Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f031bca0>
2025-04-11T03:52:12.5048708Z timeout = None
2025-04-11T03:52:12.5048712Z 
2025-04-11T03:52:12.5048807Z     def join(self, timeout=None):
2025-04-11T03:52:12.5048948Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5049023Z     
2025-04-11T03:52:12.5049182Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5049421Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5049588Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5049696Z         of the first process exiting.
2025-04-11T03:52:12.5049775Z     
2025-04-11T03:52:12.5049934Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5050073Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5050159Z     
2025-04-11T03:52:12.5050238Z         Args:
2025-04-11T03:52:12.5050381Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5050472Z         """
2025-04-11T03:52:12.5050616Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5050723Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5050808Z             return True
2025-04-11T03:52:12.5050883Z     
2025-04-11T03:52:12.5051033Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5051164Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5051267Z             self.sentinels.keys(),
2025-04-11T03:52:12.5051356Z             timeout=timeout,
2025-04-11T03:52:12.5051442Z         )
2025-04-11T03:52:12.5051527Z     
2025-04-11T03:52:12.5051614Z         error_index = None
2025-04-11T03:52:12.5051715Z         for sentinel in ready:
2025-04-11T03:52:12.5051824Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5051936Z             process = self.processes[index]
2025-04-11T03:52:12.5052025Z             process.join()
2025-04-11T03:52:12.5052124Z             if process.exitcode != 0:
2025-04-11T03:52:12.5052225Z                 error_index = index
2025-04-11T03:52:12.5052307Z                 break
2025-04-11T03:52:12.5052394Z     
2025-04-11T03:52:12.5052490Z         # Return if there was no error.
2025-04-11T03:52:12.5052580Z         if error_index is None:
2025-04-11T03:52:12.5052729Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5052829Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5052912Z     
2025-04-11T03:52:12.5053057Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5053170Z         for process in self.processes:
2025-04-11T03:52:12.5053265Z             if process.is_alive():
2025-04-11T03:52:12.5053361Z                 process.terminate()
2025-04-11T03:52:12.5053462Z             process.join()
2025-04-11T03:52:12.5053537Z     
2025-04-11T03:52:12.5053694Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5053815Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5053927Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5054065Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5054154Z             if exitcode < 0:
2025-04-11T03:52:12.5054277Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5054390Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5054552Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5054653Z                     error_index=error_index,
2025-04-11T03:52:12.5054862Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5054969Z                     exit_code=exitcode,
2025-04-11T03:52:12.5055061Z                     signal_name=name,
2025-04-11T03:52:12.5055148Z                 )
2025-04-11T03:52:12.5055229Z             else:
2025-04-11T03:52:12.5055335Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5055510Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5055609Z                     error_index=error_index,
2025-04-11T03:52:12.5055721Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5055811Z                     exit_code=exitcode,
2025-04-11T03:52:12.5055997Z                 )
2025-04-11T03:52:12.5056073Z     
2025-04-11T03:52:12.5056212Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5056396Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5056490Z         msg += original_trace
2025-04-11T03:52:12.5056677Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5056842Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5056929Z E       
2025-04-11T03:52:12.5057059Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5057163Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5057474Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5057559Z E           fn(i, *args)
2025-04-11T03:52:12.5057818Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all_single.py", line 29, in run_dist
2025-04-11T03:52:12.5057910Z E           check_4gpu()
2025-04-11T03:52:12.5058142Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5058251Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5058516Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5058632Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5058916Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5059035Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5059146Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5059441Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5059582Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5059749Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5059753Z 
2025-04-11T03:52:12.5060071Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5060227Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5060406Z [04/11/25 03:43:19] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5060544Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5060665Z                              :75 launch                                         
2025-04-11T03:52:12.5060808Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5060944Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.5061141Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5061293Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5062087Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5062178Z   warnings.warn(
2025-04-11T03:52:12.5062724Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5062811Z   warnings.warn(
2025-04-11T03:52:12.5063363Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5063564Z   warnings.warn(
2025-04-11T03:52:12.5064110Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5064197Z   warnings.warn(
2025-04-11T03:52:12.5064737Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5064833Z   warnings.warn(
2025-04-11T03:52:12.5065360Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5065455Z   warnings.warn(
2025-04-11T03:52:12.5065992Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5066092Z   warnings.warn(
2025-04-11T03:52:12.5066625Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5066717Z   warnings.warn(
2025-04-11T03:52:12.5066857Z _______________________________ test_all_gather ________________________________
2025-04-11T03:52:12.5066862Z 
2025-04-11T03:52:12.5066970Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5067566Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5067570Z 
2025-04-11T03:52:12.5067691Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5067780Z         try_count = 0
2025-04-11T03:52:12.5067887Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5067983Z             max_try, int
2025-04-11T03:52:12.5068137Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5068225Z     
2025-04-11T03:52:12.5068342Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5068465Z             try:
2025-04-11T03:52:12.5068571Z                 try_count += 1
2025-04-11T03:52:12.5068668Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5068762Z                 return ret
2025-04-11T03:52:12.5068861Z             except exception_type as e:
2025-04-11T03:52:12.5068966Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5069168Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5069293Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5069461Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5069625Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5069735Z                     continue
2025-04-11T03:52:12.5069946Z                 else:
2025-04-11T03:52:12.5070180Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5070276Z >                   raise e
2025-04-11T03:52:12.5070280Z 
2025-04-11T03:52:12.5070379Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5070502Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5070637Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5070739Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5070889Z tests/test_fp8/test_fp8_allgather.py:42: in test_all_gather
2025-04-11T03:52:12.5070977Z     spawn(run_dist, 4)
2025-04-11T03:52:12.5071202Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5071308Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5071582Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5071766Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5072072Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5072167Z     while not context.join():
2025-04-11T03:52:12.5072283Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5072299Z 
2025-04-11T03:52:12.5072506Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0164100>
2025-04-11T03:52:12.5072590Z timeout = None
2025-04-11T03:52:12.5072594Z 
2025-04-11T03:52:12.5072697Z     def join(self, timeout=None):
2025-04-11T03:52:12.5072830Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5072916Z     
2025-04-11T03:52:12.5073068Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5073227Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5073397Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5073496Z         of the first process exiting.
2025-04-11T03:52:12.5073582Z     
2025-04-11T03:52:12.5073734Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5073886Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5076982Z     
2025-04-11T03:52:12.5077060Z         Args:
2025-04-11T03:52:12.5077214Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5077291Z         """
2025-04-11T03:52:12.5077443Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5077541Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5077636Z             return True
2025-04-11T03:52:12.5077710Z     
2025-04-11T03:52:12.5077850Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5077983Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5078083Z             self.sentinels.keys(),
2025-04-11T03:52:12.5078179Z             timeout=timeout,
2025-04-11T03:52:12.5078257Z         )
2025-04-11T03:52:12.5078332Z     
2025-04-11T03:52:12.5078427Z         error_index = None
2025-04-11T03:52:12.5078519Z         for sentinel in ready:
2025-04-11T03:52:12.5078638Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5078741Z             process = self.processes[index]
2025-04-11T03:52:12.5078829Z             process.join()
2025-04-11T03:52:12.5078941Z             if process.exitcode != 0:
2025-04-11T03:52:12.5079033Z                 error_index = index
2025-04-11T03:52:12.5079121Z                 break
2025-04-11T03:52:12.5079219Z     
2025-04-11T03:52:12.5079328Z         # Return if there was no error.
2025-04-11T03:52:12.5079418Z         if error_index is None:
2025-04-11T03:52:12.5079562Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5079672Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5079866Z     
2025-04-11T03:52:12.5080022Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5080124Z         for process in self.processes:
2025-04-11T03:52:12.5080218Z             if process.is_alive():
2025-04-11T03:52:12.5080326Z                 process.terminate()
2025-04-11T03:52:12.5080415Z             process.join()
2025-04-11T03:52:12.5080504Z     
2025-04-11T03:52:12.5080650Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5080787Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5080902Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5081033Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5081260Z             if exitcode < 0:
2025-04-11T03:52:12.5081371Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5081492Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5081651Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5081760Z                     error_index=error_index,
2025-04-11T03:52:12.5081875Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5081969Z                     exit_code=exitcode,
2025-04-11T03:52:12.5082072Z                     signal_name=name,
2025-04-11T03:52:12.5082152Z                 )
2025-04-11T03:52:12.5082247Z             else:
2025-04-11T03:52:12.5082358Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5082530Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5082644Z                     error_index=error_index,
2025-04-11T03:52:12.5082755Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5082863Z                     exit_code=exitcode,
2025-04-11T03:52:12.5082944Z                 )
2025-04-11T03:52:12.5083024Z     
2025-04-11T03:52:12.5083182Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5083355Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5083459Z         msg += original_trace
2025-04-11T03:52:12.5083635Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5083805Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5083883Z E       
2025-04-11T03:52:12.5084015Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5084129Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5084429Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5084527Z E           fn(i, *args)
2025-04-11T03:52:12.5084758Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allgather.py", line 37, in run_dist
2025-04-11T03:52:12.5084853Z E           check_4gpu()
2025-04-11T03:52:12.5085078Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5085182Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5085452Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5085559Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5085865Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5085976Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5086108Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5086394Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5086537Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5086712Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5086820Z 
2025-04-11T03:52:12.5087131Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5087298Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5087462Z [04/11/25 03:43:25] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5087602Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5087710Z                              :75 launch                                         
2025-04-11T03:52:12.5087860Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5088099Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.5088301Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5088461Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5088765Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56858 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5089070Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56858 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5089628Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5089722Z   warnings.warn(
2025-04-11T03:52:12.5090251Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5090350Z   warnings.warn(
2025-04-11T03:52:12.5090892Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5090980Z   warnings.warn(
2025-04-11T03:52:12.5091510Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5091592Z   warnings.warn(
2025-04-11T03:52:12.5092149Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5092233Z   warnings.warn(
2025-04-11T03:52:12.5092785Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5092872Z   warnings.warn(
2025-04-11T03:52:12.5093428Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5093509Z   warnings.warn(
2025-04-11T03:52:12.5094058Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5094141Z   warnings.warn(
2025-04-11T03:52:12.5094281Z _______________________________ test_all_reduce ________________________________
2025-04-11T03:52:12.5094296Z 
2025-04-11T03:52:12.5094393Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5095103Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5095108Z 
2025-04-11T03:52:12.5095242Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5095326Z         try_count = 0
2025-04-11T03:52:12.5095449Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5095537Z             max_try, int
2025-04-11T03:52:12.5095707Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5095786Z     
2025-04-11T03:52:12.5095905Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5095995Z             try:
2025-04-11T03:52:12.5096085Z                 try_count += 1
2025-04-11T03:52:12.5096191Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5096380Z                 return ret
2025-04-11T03:52:12.5096486Z             except exception_type as e:
2025-04-11T03:52:12.5096604Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5096801Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5096932Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5097081Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5097246Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5097332Z                     continue
2025-04-11T03:52:12.5097415Z                 else:
2025-04-11T03:52:12.5097651Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5097737Z >                   raise e
2025-04-11T03:52:12.5097744Z 
2025-04-11T03:52:12.5097855Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5097975Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5098122Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5098220Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5098375Z tests/test_fp8/test_fp8_allreduce.py:52: in test_all_reduce
2025-04-11T03:52:12.5098475Z     spawn(run_dist, 4)
2025-04-11T03:52:12.5098584Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5098701Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5098960Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5099150Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5099433Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5099527Z     while not context.join():
2025-04-11T03:52:12.5099654Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5099658Z 
2025-04-11T03:52:12.5099858Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0318a30>
2025-04-11T03:52:12.5099952Z timeout = None
2025-04-11T03:52:12.5099961Z 
2025-04-11T03:52:12.5100058Z     def join(self, timeout=None):
2025-04-11T03:52:12.5100197Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5100274Z     
2025-04-11T03:52:12.5100425Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5100580Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5100747Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5100854Z         of the first process exiting.
2025-04-11T03:52:12.5100931Z     
2025-04-11T03:52:12.5101090Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5101232Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5101308Z     
2025-04-11T03:52:12.5101397Z         Args:
2025-04-11T03:52:12.5101540Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5101726Z         """
2025-04-11T03:52:12.5101873Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5101972Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5102065Z             return True
2025-04-11T03:52:12.5102140Z     
2025-04-11T03:52:12.5102285Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5102409Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5102517Z             self.sentinels.keys(),
2025-04-11T03:52:12.5102607Z             timeout=timeout,
2025-04-11T03:52:12.5102685Z         )
2025-04-11T03:52:12.5102769Z     
2025-04-11T03:52:12.5102855Z         error_index = None
2025-04-11T03:52:12.5103063Z         for sentinel in ready:
2025-04-11T03:52:12.5103176Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5103280Z             process = self.processes[index]
2025-04-11T03:52:12.5103378Z             process.join()
2025-04-11T03:52:12.5103479Z             if process.exitcode != 0:
2025-04-11T03:52:12.5103578Z                 error_index = index
2025-04-11T03:52:12.5103659Z                 break
2025-04-11T03:52:12.5103732Z     
2025-04-11T03:52:12.5103838Z         # Return if there was no error.
2025-04-11T03:52:12.5103928Z         if error_index is None:
2025-04-11T03:52:12.5104074Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5104174Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5104257Z     
2025-04-11T03:52:12.5104407Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5104507Z         for process in self.processes:
2025-04-11T03:52:12.5104620Z             if process.is_alive():
2025-04-11T03:52:12.5104721Z                 process.terminate()
2025-04-11T03:52:12.5104829Z             process.join()
2025-04-11T03:52:12.5104907Z     
2025-04-11T03:52:12.5105051Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5105185Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5105298Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5105431Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5105518Z             if exitcode < 0:
2025-04-11T03:52:12.5105637Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5105747Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5105901Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5106009Z                     error_index=error_index,
2025-04-11T03:52:12.5106112Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5106214Z                     exit_code=exitcode,
2025-04-11T03:52:12.5106309Z                     signal_name=name,
2025-04-11T03:52:12.5106387Z                 )
2025-04-11T03:52:12.5106477Z             else:
2025-04-11T03:52:12.5106583Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5106762Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5106860Z                     error_index=error_index,
2025-04-11T03:52:12.5106972Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5107064Z                     exit_code=exitcode,
2025-04-11T03:52:12.5107141Z                 )
2025-04-11T03:52:12.5107227Z     
2025-04-11T03:52:12.5107364Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5107543Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5107633Z         msg += original_trace
2025-04-11T03:52:12.5107817Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5107980Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5108061Z E       
2025-04-11T03:52:12.5108202Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5108408Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5108751Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5108840Z E           fn(i, *args)
2025-04-11T03:52:12.5109078Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allreduce.py", line 47, in run_dist
2025-04-11T03:52:12.5109184Z E           check_4gpu()
2025-04-11T03:52:12.5109448Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.5109561Z E           partial_func(**kwargs)
2025-04-11T03:52:12.5109789Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5110079Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5110342Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5110466Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5110753Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5110865Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5110988Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5111275Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5111427Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5111591Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5111596Z 
2025-04-11T03:52:12.5111910Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5112072Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5112236Z [04/11/25 03:43:31] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5112381Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5112492Z                              :75 launch                                         
2025-04-11T03:52:12.5112643Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5112771Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.5112979Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5113128Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5113444Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30330 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5113741Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30330 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5114019Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30330 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5114581Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5114668Z   warnings.warn(
2025-04-11T03:52:12.5115202Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5115288Z   warnings.warn(
2025-04-11T03:52:12.5115831Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5115913Z   warnings.warn(
2025-04-11T03:52:12.5116561Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5116648Z   warnings.warn(
2025-04-11T03:52:12.5117183Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5117264Z   warnings.warn(
2025-04-11T03:52:12.5117786Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5118009Z   warnings.warn(
2025-04-11T03:52:12.5118560Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5118655Z   warnings.warn(
2025-04-11T03:52:12.5119178Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5119272Z   warnings.warn(
2025-04-11T03:52:12.5119409Z ________________________________ test_fp8_cast _________________________________
2025-04-11T03:52:12.5119413Z 
2025-04-11T03:52:12.5119507Z args = (), kwargs = {}
2025-04-11T03:52:12.5119511Z 
2025-04-11T03:52:12.5119612Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.5119716Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.5119842Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.5119962Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.5120086Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.5120186Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.5120190Z 
2025-04-11T03:52:12.5120297Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.5120414Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5120577Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.5120691Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.5120805Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5120809Z 
2025-04-11T03:52:12.5120906Z device = None
2025-04-11T03:52:12.5120910Z 
2025-04-11T03:52:12.5121039Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5121216Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5121294Z     
2025-04-11T03:52:12.5121372Z         Args:
2025-04-11T03:52:12.5121557Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5121736Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5121861Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5121939Z         """
2025-04-11T03:52:12.5122031Z         _lazy_init()
2025-04-11T03:52:12.5122133Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5122242Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5122363Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5122665Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5122842Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5123009Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5123014Z 
2025-04-11T03:52:12.5123364Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5123506Z __________________________________ test_fsdp ___________________________________
2025-04-11T03:52:12.5123510Z 
2025-04-11T03:52:12.5123619Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5124216Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5124220Z 
2025-04-11T03:52:12.5124336Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5124422Z         try_count = 0
2025-04-11T03:52:12.5124529Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5124750Z             max_try, int
2025-04-11T03:52:12.5124904Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5124990Z     
2025-04-11T03:52:12.5125107Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5125190Z             try:
2025-04-11T03:52:12.5125290Z                 try_count += 1
2025-04-11T03:52:12.5125386Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5125481Z                 return ret
2025-04-11T03:52:12.5125578Z             except exception_type as e:
2025-04-11T03:52:12.5125695Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5125889Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5126015Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5126178Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5126337Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5126430Z                     continue
2025-04-11T03:52:12.5126510Z                 else:
2025-04-11T03:52:12.5126746Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5126830Z >                   raise e
2025-04-11T03:52:12.5126834Z 
2025-04-11T03:52:12.5126929Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5127051Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5127185Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5127283Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5127433Z tests/test_fp8/test_fp8_fsdp_comm_hook.py:104: in test_fsdp
2025-04-11T03:52:12.5127526Z     spawn(demo_basic, n_gpus)
2025-04-11T03:52:12.5127640Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5127744Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5128019Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5128198Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5128501Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5128594Z     while not context.join():
2025-04-11T03:52:12.5128707Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5128721Z 
2025-04-11T03:52:12.5128923Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f031a590>
2025-04-11T03:52:12.5129006Z timeout = None
2025-04-11T03:52:12.5129010Z 
2025-04-11T03:52:12.5129112Z     def join(self, timeout=None):
2025-04-11T03:52:12.5129242Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5129326Z     
2025-04-11T03:52:12.5129478Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5129636Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5129800Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5129897Z         of the first process exiting.
2025-04-11T03:52:12.5130077Z     
2025-04-11T03:52:12.5130229Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5130378Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5130455Z     
2025-04-11T03:52:12.5130533Z         Args:
2025-04-11T03:52:12.5130683Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5130760Z         """
2025-04-11T03:52:12.5130911Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5131007Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5131099Z             return True
2025-04-11T03:52:12.5131176Z     
2025-04-11T03:52:12.5131424Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5131556Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5131652Z             self.sentinels.keys(),
2025-04-11T03:52:12.5131760Z             timeout=timeout,
2025-04-11T03:52:12.5131840Z         )
2025-04-11T03:52:12.5131922Z     
2025-04-11T03:52:12.5132018Z         error_index = None
2025-04-11T03:52:12.5132114Z         for sentinel in ready:
2025-04-11T03:52:12.5132232Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5132334Z             process = self.processes[index]
2025-04-11T03:52:12.5132423Z             process.join()
2025-04-11T03:52:12.5132530Z             if process.exitcode != 0:
2025-04-11T03:52:12.5132621Z                 error_index = index
2025-04-11T03:52:12.5132703Z                 break
2025-04-11T03:52:12.5132778Z     
2025-04-11T03:52:12.5132888Z         # Return if there was no error.
2025-04-11T03:52:12.5132981Z         if error_index is None:
2025-04-11T03:52:12.5133122Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5133234Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5133311Z     
2025-04-11T03:52:12.5133463Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5133569Z         for process in self.processes:
2025-04-11T03:52:12.5133665Z             if process.is_alive():
2025-04-11T03:52:12.5133770Z                 process.terminate()
2025-04-11T03:52:12.5133860Z             process.join()
2025-04-11T03:52:12.5133944Z     
2025-04-11T03:52:12.5134088Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5134216Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5134328Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5134453Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5134553Z             if exitcode < 0:
2025-04-11T03:52:12.5134669Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5134787Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5134941Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5135042Z                     error_index=error_index,
2025-04-11T03:52:12.5135163Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5135257Z                     exit_code=exitcode,
2025-04-11T03:52:12.5135358Z                     signal_name=name,
2025-04-11T03:52:12.5135440Z                 )
2025-04-11T03:52:12.5135531Z             else:
2025-04-11T03:52:12.5135639Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5135807Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5135914Z                     error_index=error_index,
2025-04-11T03:52:12.5136021Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5136122Z                     exit_code=exitcode,
2025-04-11T03:52:12.5136206Z                 )
2025-04-11T03:52:12.5136282Z     
2025-04-11T03:52:12.5136428Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5136598Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5136802Z         msg += original_trace
2025-04-11T03:52:12.5136980Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5137158Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5137237Z E       
2025-04-11T03:52:12.5137370Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5137485Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5137790Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5137882Z E           fn(i, *args)
2025-04-11T03:52:12.5138132Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_fsdp_comm_hook.py", line 95, in demo_basic
2025-04-11T03:52:12.5138338Z E           run_model()
2025-04-11T03:52:12.5138563Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5138672Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5138943Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5139048Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5139335Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5139444Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5139563Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5139849Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5139995Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5140171Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5140175Z 
2025-04-11T03:52:12.5140484Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5140648Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5140761Z Running basic FSDP example on rank 4.
2025-04-11T03:52:12.5140876Z Running basic FSDP example on rank 5.
2025-04-11T03:52:12.5140979Z Running basic FSDP example on rank 1.
2025-04-11T03:52:12.5141076Z Running basic FSDP example on rank 6.
2025-04-11T03:52:12.5141187Z Running basic FSDP example on rank 3.
2025-04-11T03:52:12.5141283Z Running basic FSDP example on rank 0.
2025-04-11T03:52:12.5141449Z [04/11/25 03:43:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5141584Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5141705Z                              :75 launch                                         
2025-04-11T03:52:12.5141844Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5141983Z                              environment is initialized, world size: 8          
2025-04-11T03:52:12.5142081Z Running basic FSDP example on rank 7.
2025-04-11T03:52:12.5142175Z Running basic FSDP example on rank 2.
2025-04-11T03:52:12.5142382Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5142530Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5142829Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43732 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5143113Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43732 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5143674Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5143865Z   warnings.warn(
2025-04-11T03:52:12.5144413Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5144499Z   warnings.warn(
2025-04-11T03:52:12.5145039Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5145132Z   warnings.warn(
2025-04-11T03:52:12.5145659Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5145869Z   warnings.warn(
2025-04-11T03:52:12.5146399Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5146480Z   warnings.warn(
2025-04-11T03:52:12.5146998Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5147081Z   warnings.warn(
2025-04-11T03:52:12.5147605Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5147687Z   warnings.warn(
2025-04-11T03:52:12.5148228Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5148314Z   warnings.warn(
2025-04-11T03:52:12.5148903Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5148989Z   warnings.warn(
2025-04-11T03:52:12.5149529Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5149613Z   warnings.warn(
2025-04-11T03:52:12.5150165Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5150251Z   warnings.warn(
2025-04-11T03:52:12.5150789Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5150874Z   warnings.warn(
2025-04-11T03:52:12.5151413Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5151509Z   warnings.warn(
2025-04-11T03:52:12.5152034Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5152130Z   warnings.warn(
2025-04-11T03:52:12.5152667Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5152763Z   warnings.warn(
2025-04-11T03:52:12.5153409Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5153508Z   warnings.warn(
2025-04-11T03:52:12.5154256Z [rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T03:52:12.5154991Z [rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T03:52:12.5155836Z [rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T03:52:12.5156573Z [rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T03:52:12.5157296Z [rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T03:52:12.5158042Z [rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T03:52:12.5158186Z ________________________________ test_fp8_hook _________________________________
2025-04-11T03:52:12.5158190Z 
2025-04-11T03:52:12.5158488Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T03:52:12.5158581Z     def test_fp8_hook():
2025-04-11T03:52:12.5158676Z         # create tensors
2025-04-11T03:52:12.5158875Z >       w = nn.Parameter(torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE))
2025-04-11T03:52:12.5158998Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5159291Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5159431Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5159605Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5159609Z 
2025-04-11T03:52:12.5159731Z tests/test_fp8/test_fp8_hook.py:41: RuntimeError
2025-04-11T03:52:12.5159890Z __________________________ test_fp8_linear[True-True] __________________________
2025-04-11T03:52:12.5159894Z 
2025-04-11T03:52:12.5159991Z use_bias = True, use_batch = True
2025-04-11T03:52:12.5159994Z 
2025-04-11T03:52:12.5160273Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T03:52:12.5160410Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T03:52:12.5160550Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T03:52:12.5160681Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T03:52:12.5160878Z         # create tensors
2025-04-11T03:52:12.5161094Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T03:52:12.5161205Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5161501Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5161638Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5161805Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5161809Z 
2025-04-11T03:52:12.5161934Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T03:52:12.5162080Z _________________________ test_fp8_linear[True-False] __________________________
2025-04-11T03:52:12.5162212Z 
2025-04-11T03:52:12.5162317Z use_bias = False, use_batch = True
2025-04-11T03:52:12.5162321Z 
2025-04-11T03:52:12.5162594Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T03:52:12.5162736Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T03:52:12.5162868Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T03:52:12.5163007Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T03:52:12.5163093Z         # create tensors
2025-04-11T03:52:12.5163296Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T03:52:12.5163409Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5163690Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5163842Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5164006Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5164010Z 
2025-04-11T03:52:12.5164144Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T03:52:12.5164295Z _________________________ test_fp8_linear[False-True] __________________________
2025-04-11T03:52:12.5164299Z 
2025-04-11T03:52:12.5164404Z use_bias = True, use_batch = False
2025-04-11T03:52:12.5164408Z 
2025-04-11T03:52:12.5164675Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T03:52:12.5164809Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T03:52:12.5164938Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T03:52:12.5165064Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T03:52:12.5165161Z         # create tensors
2025-04-11T03:52:12.5165355Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T03:52:12.5165472Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5165755Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5165895Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5166056Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5166060Z 
2025-04-11T03:52:12.5166182Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T03:52:12.5166355Z _________________________ test_fp8_linear[False-False] _________________________
2025-04-11T03:52:12.5166359Z 
2025-04-11T03:52:12.5166460Z use_bias = False, use_batch = False
2025-04-11T03:52:12.5166464Z 
2025-04-11T03:52:12.5166745Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T03:52:12.5166878Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T03:52:12.5167013Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T03:52:12.5167139Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T03:52:12.5167339Z         # create tensors
2025-04-11T03:52:12.5167533Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T03:52:12.5167643Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5167932Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5168066Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5168231Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5168235Z 
2025-04-11T03:52:12.5168357Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T03:52:12.5168510Z _____________________________ test_reduce_scatter ______________________________
2025-04-11T03:52:12.5168624Z 
2025-04-11T03:52:12.5168724Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5169345Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5169350Z 
2025-04-11T03:52:12.5169457Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5169541Z         try_count = 0
2025-04-11T03:52:12.5169656Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5169740Z             max_try, int
2025-04-11T03:52:12.5169900Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5169978Z     
2025-04-11T03:52:12.5170106Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5170185Z             try:
2025-04-11T03:52:12.5170276Z                 try_count += 1
2025-04-11T03:52:12.5170384Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5170468Z                 return ret
2025-04-11T03:52:12.5170578Z             except exception_type as e:
2025-04-11T03:52:12.5170686Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5170876Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5171014Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5171164Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5171335Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5171423Z                     continue
2025-04-11T03:52:12.5171513Z                 else:
2025-04-11T03:52:12.5171739Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5171827Z >                   raise e
2025-04-11T03:52:12.5171843Z 
2025-04-11T03:52:12.5171940Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5172055Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5172202Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5172294Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5172471Z tests/test_fp8/test_fp8_reduce_scatter.py:41: in test_reduce_scatter
2025-04-11T03:52:12.5172559Z     spawn(run_dist, 4)
2025-04-11T03:52:12.5172666Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5172777Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5173034Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5173223Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5173515Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5173620Z     while not context.join():
2025-04-11T03:52:12.5173735Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5173739Z 
2025-04-11T03:52:12.5174050Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f031b430>
2025-04-11T03:52:12.5174146Z timeout = None
2025-04-11T03:52:12.5174151Z 
2025-04-11T03:52:12.5174247Z     def join(self, timeout=None):
2025-04-11T03:52:12.5174386Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5174462Z     
2025-04-11T03:52:12.5174619Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5174768Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5174935Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5175041Z         of the first process exiting.
2025-04-11T03:52:12.5175117Z     
2025-04-11T03:52:12.5175369Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5175513Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5175638Z     
2025-04-11T03:52:12.5175719Z         Args:
2025-04-11T03:52:12.5175864Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5175959Z         """
2025-04-11T03:52:12.5176103Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5176211Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5176303Z             return True
2025-04-11T03:52:12.5176376Z     
2025-04-11T03:52:12.5176523Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5176646Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5176751Z             self.sentinels.keys(),
2025-04-11T03:52:12.5176840Z             timeout=timeout,
2025-04-11T03:52:12.5176928Z         )
2025-04-11T03:52:12.5177003Z     
2025-04-11T03:52:12.5177092Z         error_index = None
2025-04-11T03:52:12.5177191Z         for sentinel in ready:
2025-04-11T03:52:12.5177300Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5177411Z             process = self.processes[index]
2025-04-11T03:52:12.5177504Z             process.join()
2025-04-11T03:52:12.5177601Z             if process.exitcode != 0:
2025-04-11T03:52:12.5177702Z                 error_index = index
2025-04-11T03:52:12.5177784Z                 break
2025-04-11T03:52:12.5177867Z     
2025-04-11T03:52:12.5177964Z         # Return if there was no error.
2025-04-11T03:52:12.5178055Z         if error_index is None:
2025-04-11T03:52:12.5178202Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5178305Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5178389Z     
2025-04-11T03:52:12.5178534Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5178643Z         for process in self.processes:
2025-04-11T03:52:12.5178741Z             if process.is_alive():
2025-04-11T03:52:12.5178840Z                 process.terminate()
2025-04-11T03:52:12.5178939Z             process.join()
2025-04-11T03:52:12.5179015Z     
2025-04-11T03:52:12.5179173Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5179293Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5179406Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5179542Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5179630Z             if exitcode < 0:
2025-04-11T03:52:12.5179751Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5179863Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5180028Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5180131Z                     error_index=error_index,
2025-04-11T03:52:12.5180238Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5180346Z                     exit_code=exitcode,
2025-04-11T03:52:12.5180439Z                     signal_name=name,
2025-04-11T03:52:12.5180526Z                 )
2025-04-11T03:52:12.5180606Z             else:
2025-04-11T03:52:12.5180912Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5181096Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5181198Z                     error_index=error_index,
2025-04-11T03:52:12.5181315Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5181413Z                     exit_code=exitcode,
2025-04-11T03:52:12.5181514Z                 )
2025-04-11T03:52:12.5181596Z     
2025-04-11T03:52:12.5181736Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5181929Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5182024Z         msg += original_trace
2025-04-11T03:52:12.5182319Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5182487Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5182581Z E       
2025-04-11T03:52:12.5182719Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5182824Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5183133Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5183218Z E           fn(i, *args)
2025-04-11T03:52:12.5183468Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_reduce_scatter.py", line 36, in run_dist
2025-04-11T03:52:12.5183553Z E           check_4gpu()
2025-04-11T03:52:12.5183787Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5183891Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5184156Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5184274Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5184555Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5184683Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5184828Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5185127Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5185263Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5185426Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5185429Z 
2025-04-11T03:52:12.5185744Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5185901Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5186066Z [04/11/25 03:43:46] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5186201Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5186318Z                              :75 launch                                         
2025-04-11T03:52:12.5186458Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5186594Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.5186791Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5186940Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5187504Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5187592Z   warnings.warn(
2025-04-11T03:52:12.5188229Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5188317Z   warnings.warn(
2025-04-11T03:52:12.5188916Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5188999Z   warnings.warn(
2025-04-11T03:52:12.5189537Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5189620Z   warnings.warn(
2025-04-11T03:52:12.5190298Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5190391Z   warnings.warn(
2025-04-11T03:52:12.5190925Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5191019Z   warnings.warn(
2025-04-11T03:52:12.5191562Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5191655Z   warnings.warn(
2025-04-11T03:52:12.5192184Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5192277Z   warnings.warn(
2025-04-11T03:52:12.5192411Z _________________________________ test_bucket __________________________________
2025-04-11T03:52:12.5192416Z 
2025-04-11T03:52:12.5192501Z kwargs = {}
2025-04-11T03:52:12.5192708Z val = {'block_size': 4, 'dtype': torch.float16, 'max_batch_size': 4, 'max_input_len': 32, ...}
2025-04-11T03:52:12.5192945Z arg_map = {'test_config': {'block_size': 4, 'dtype': torch.float16, 'max_batch_size': 4, 'max_input_len': 32, ...}}
2025-04-11T03:52:12.5193412Z partial_func = functools.partial(<function test_bucket at 0x7f68f1d997e0>, test_config={'block_size': 4, 'max_batch_size': 4, 'max_input_len': 32, 'max_output_len': 8, 'dtype': torch.float16, 'tp_size': 1})
2025-04-11T03:52:12.5193417Z 
2025-04-11T03:52:12.5193529Z     def _execute_function_by_param(**kwargs):
2025-04-11T03:52:12.5193626Z         for val in values:
2025-04-11T03:52:12.5193724Z             arg_map = {argument: val}
2025-04-11T03:52:12.5193854Z             partial_func = partial(func, **arg_map)
2025-04-11T03:52:12.5193952Z >           partial_func(**kwargs)
2025-04-11T03:52:12.5193955Z 
2025-04-11T03:52:12.5194065Z colossalai/testing/utils.py:64: 
2025-04-11T03:52:12.5194208Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5194350Z tests/test_infer/test_batch_bucket.py:42: in test_bucket
2025-04-11T03:52:12.5194525Z     cache_manager = KVCacheManager(inference_config, model_config)
2025-04-11T03:52:12.5194695Z colossalai/inference/kv_cache/kvcache_manager.py:105: in __init__
2025-04-11T03:52:12.5194866Z     self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T03:52:12.5194981Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5194985Z 
2025-04-11T03:52:12.5195232Z self = <colossalai.inference.kv_cache.kvcache_manager.KVCacheManager object at 0x7f68f0228fa0>
2025-04-11T03:52:12.5195372Z kalloc_shape = (40, 4, 4, 32), valloc_shape = (40, 4, 4, 32)
2025-04-11T03:52:12.5195380Z 
2025-04-11T03:52:12.5195473Z     def _init_device_caches(
2025-04-11T03:52:12.5195645Z         self, kalloc_shape: Tuple[int, ...], valloc_shape: Tuple[int, ...]
2025-04-11T03:52:12.5195874Z     ) -> Tuple[torch.Tensor, torch.Tensor]:
2025-04-11T03:52:12.5196008Z         """Initialize the physical cache on the device.
2025-04-11T03:52:12.5196085Z     
2025-04-11T03:52:12.5196293Z         For each layer of the model, we allocate two tensors for key and value respectively,
2025-04-11T03:52:12.5196450Z         with shape of [num_blocks, num_kv_heads, block_size, head_size]
2025-04-11T03:52:12.5196529Z         """
2025-04-11T03:52:12.5196640Z         k_cache: List[torch.Tensor] = []
2025-04-11T03:52:12.5196737Z         v_cache: List[torch.Tensor] = []
2025-04-11T03:52:12.5196843Z         for _ in range(self.num_layers):
2025-04-11T03:52:12.5197064Z >           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T03:52:12.5197299Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5197601Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5197756Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5197938Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5197943Z 
2025-04-11T03:52:12.5198115Z colossalai/inference/kv_cache/kvcache_manager.py:519: RuntimeError
2025-04-11T03:52:12.5198276Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5198424Z [04/11/25 03:43:47] INFO     colossalai -                                       
2025-04-11T03:52:12.5198572Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T03:52:12.5198680Z                              INFO:                                              
2025-04-11T03:52:12.5198810Z                              /__w/ColossalAI/ColossalAI/colossalai/inference/kv_
2025-04-11T03:52:12.5198943Z                              cache/kvcache_manager.py:104 __init__              
2025-04-11T03:52:12.5199070Z                     INFO     colossalai -                                       
2025-04-11T03:52:12.5199213Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T03:52:12.5199336Z                              INFO: Allocating KV cache with shape: (40, 4, 4,   
2025-04-11T03:52:12.5199461Z                              32) consisting of 40 blocks.                       
2025-04-11T03:52:12.5199605Z ___________________________ test_continuous_batching ___________________________
2025-04-11T03:52:12.5199609Z 
2025-04-11T03:52:12.5199716Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5200312Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5200319Z 
2025-04-11T03:52:12.5200435Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5200518Z         try_count = 0
2025-04-11T03:52:12.5200626Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5200722Z             max_try, int
2025-04-11T03:52:12.5200870Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5200957Z     
2025-04-11T03:52:12.5201084Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5201172Z             try:
2025-04-11T03:52:12.5201269Z                 try_count += 1
2025-04-11T03:52:12.5201368Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5201467Z                 return ret
2025-04-11T03:52:12.5201569Z             except exception_type as e:
2025-04-11T03:52:12.5201679Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5201879Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5202003Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5202266Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5202429Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5202523Z                     continue
2025-04-11T03:52:12.5202604Z                 else:
2025-04-11T03:52:12.5202837Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5202922Z >                   raise e
2025-04-11T03:52:12.5202925Z 
2025-04-11T03:52:12.5203026Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5203151Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5203286Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5203497Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5203688Z tests/test_infer/test_continuous_batching.py:67: in test_continuous_batching
2025-04-11T03:52:12.5203776Z     spawn(run_dist, 1)
2025-04-11T03:52:12.5203892Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5204001Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5204271Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5204450Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5204749Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5204842Z     while not context.join():
2025-04-11T03:52:12.5204955Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5204968Z 
2025-04-11T03:52:12.5205164Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f202a470>
2025-04-11T03:52:12.5205250Z timeout = None
2025-04-11T03:52:12.5205254Z 
2025-04-11T03:52:12.5205358Z     def join(self, timeout=None):
2025-04-11T03:52:12.5205486Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5205571Z     
2025-04-11T03:52:12.5205724Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5205883Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5206049Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5206150Z         of the first process exiting.
2025-04-11T03:52:12.5206237Z     
2025-04-11T03:52:12.5206385Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5206534Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5206610Z     
2025-04-11T03:52:12.5206689Z         Args:
2025-04-11T03:52:12.5206838Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5206919Z         """
2025-04-11T03:52:12.5207071Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5207167Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5207263Z             return True
2025-04-11T03:52:12.5207339Z     
2025-04-11T03:52:12.5207475Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5207611Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5207707Z             self.sentinels.keys(),
2025-04-11T03:52:12.5207804Z             timeout=timeout,
2025-04-11T03:52:12.5207883Z         )
2025-04-11T03:52:12.5207958Z     
2025-04-11T03:52:12.5208056Z         error_index = None
2025-04-11T03:52:12.5208146Z         for sentinel in ready:
2025-04-11T03:52:12.5208264Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5208365Z             process = self.processes[index]
2025-04-11T03:52:12.5208456Z             process.join()
2025-04-11T03:52:12.5208568Z             if process.exitcode != 0:
2025-04-11T03:52:12.5208661Z                 error_index = index
2025-04-11T03:52:12.5208748Z                 break
2025-04-11T03:52:12.5208823Z     
2025-04-11T03:52:12.5208928Z         # Return if there was no error.
2025-04-11T03:52:12.5209122Z         if error_index is None:
2025-04-11T03:52:12.5209263Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5209377Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5209457Z     
2025-04-11T03:52:12.5209617Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5209724Z         for process in self.processes:
2025-04-11T03:52:12.5209821Z             if process.is_alive():
2025-04-11T03:52:12.5209930Z                 process.terminate()
2025-04-11T03:52:12.5210021Z             process.join()
2025-04-11T03:52:12.5210110Z     
2025-04-11T03:52:12.5210270Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5210567Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5210683Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5210812Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5210917Z             if exitcode < 0:
2025-04-11T03:52:12.5211029Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5211153Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5211309Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5211412Z                     error_index=error_index,
2025-04-11T03:52:12.5211530Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5211625Z                     exit_code=exitcode,
2025-04-11T03:52:12.5211729Z                     signal_name=name,
2025-04-11T03:52:12.5211809Z                 )
2025-04-11T03:52:12.5211899Z             else:
2025-04-11T03:52:12.5212005Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5212174Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5212283Z                     error_index=error_index,
2025-04-11T03:52:12.5212389Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5212494Z                     exit_code=exitcode,
2025-04-11T03:52:12.5212572Z                 )
2025-04-11T03:52:12.5212647Z     
2025-04-11T03:52:12.5212792Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5212966Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5213065Z         msg += original_trace
2025-04-11T03:52:12.5213243Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5213412Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5213490Z E       
2025-04-11T03:52:12.5213620Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5213734Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5214035Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5214127Z E           fn(i, *args)
2025-04-11T03:52:12.5214384Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 61, in run_dist
2025-04-11T03:52:12.5214486Z E           check_inference_engine()
2025-04-11T03:52:12.5214747Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.5214841Z E           partial_func(**kwargs)
2025-04-11T03:52:12.5215106Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.5215198Z E           partial_func(**kwargs)
2025-04-11T03:52:12.5215457Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.5215551Z E           partial_func(**kwargs)
2025-04-11T03:52:12.5215670Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.5216069Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 39, in check_inference_engine
2025-04-11T03:52:12.5216240Z E           model = LlamaForCausalLM(LlamaConfig(num_hidden_layers=2)).cuda()
2025-04-11T03:52:12.5216541Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.5216648Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.5216924Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.5217045Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.5217325Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.5217530Z E           module._apply(fn)
2025-04-11T03:52:12.5217809Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.5217901Z E           module._apply(fn)
2025-04-11T03:52:12.5218170Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.5218281Z E           param_applied = fn(param)
2025-04-11T03:52:12.5218559Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.5218691Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.5218804Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5219104Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5219245Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5219413Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5219417Z 
2025-04-11T03:52:12.5219735Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5219898Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5220062Z [04/11/25 03:43:55] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5220203Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5220325Z                              :75 launch                                         
2025-04-11T03:52:12.5220474Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5220613Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.5220811Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5220965Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5222111Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.5222289Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.5222990Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.5223082Z   warnings.warn(
2025-04-11T03:52:12.5224022Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5224114Z   warnings.warn(
2025-04-11T03:52:12.5224934Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5225022Z   warnings.warn(
2025-04-11T03:52:12.5225845Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5226057Z   warnings.warn(
2025-04-11T03:52:12.5226867Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5226951Z   warnings.warn(
2025-04-11T03:52:12.5227764Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5227848Z   warnings.warn(
2025-04-11T03:52:12.5228698Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5228788Z   warnings.warn(
2025-04-11T03:52:12.5229598Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5229680Z   warnings.warn(
2025-04-11T03:52:12.5230477Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5230563Z   warnings.warn(
2025-04-11T03:52:12.5231365Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5231447Z   warnings.warn(
2025-04-11T03:52:12.5231583Z _______________________________ test_drafter[5] ________________________________
2025-04-11T03:52:12.5231587Z 
2025-04-11T03:52:12.5231969Z tokenizer = LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fas... special=True),
2025-04-11T03:52:12.5232201Z 	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
2025-04-11T03:52:12.5232298Z }
2025-04-11T03:52:12.5232382Z spec_num = 5
2025-04-11T03:52:12.5232386Z 
2025-04-11T03:52:12.5232527Z     @pytest.mark.parametrize("spec_num", [SPEC_NUM])
2025-04-11T03:52:12.5232645Z     def test_drafter(tokenizer, spec_num: int):
2025-04-11T03:52:12.5232739Z         torch.manual_seed(123)
2025-04-11T03:52:12.5232827Z     
2025-04-11T03:52:12.5232925Z         device = get_current_device()
2025-04-11T03:52:12.5233183Z         toy_config = LlamaConfig(num_hidden_layers=NUM_LAYERS)
2025-04-11T03:52:12.5233314Z         toy_config.pad_token_id = tokenizer.eos_token_id
2025-04-11T03:52:12.5233441Z         drafter_model = LlamaForCausalLM(toy_config)
2025-04-11T03:52:12.5233555Z >       drafter_model = drafter_model.eval().cuda()
2025-04-11T03:52:12.5233559Z 
2025-04-11T03:52:12.5233662Z tests/test_infer/test_drafter.py:27: 
2025-04-11T03:52:12.5233786Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5234038Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2548: in cuda
2025-04-11T03:52:12.5234147Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.5234486Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: in cuda
2025-04-11T03:52:12.5234608Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.5234851Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.5234944Z     module._apply(fn)
2025-04-11T03:52:12.5235193Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.5235281Z     module._apply(fn)
2025-04-11T03:52:12.5235525Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.5235622Z     param_applied = fn(param)
2025-04-11T03:52:12.5235748Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5235752Z 
2025-04-11T03:52:12.5235847Z t = Parameter containing:
2025-04-11T03:52:12.5235991Z tensor([[-0.0259,  0.0026,  0.0006,  ...,  0.0104,  0.0194,  0.0062],
2025-04-11T03:52:12.5236102Z         [-0.0076,  0.0020,...5,  0.0329,  0.0046],
2025-04-11T03:52:12.5236232Z         [-0.0124,  0.0230, -0.0264,  ..., -0.0224, -0.0274, -0.0157]],
2025-04-11T03:52:12.5236332Z        requires_grad=True)
2025-04-11T03:52:12.5236335Z 
2025-04-11T03:52:12.5236453Z >   return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.5236569Z E   RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5236863Z E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5237000Z E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5237173Z E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5237177Z 
2025-04-11T03:52:12.5237429Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: RuntimeError
2025-04-11T03:52:12.5237575Z ________________________________ test_spec_dec _________________________________
2025-04-11T03:52:12.5237582Z 
2025-04-11T03:52:12.5237953Z tokenizer = LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fas... special=True),
2025-04-11T03:52:12.5238196Z 	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
2025-04-11T03:52:12.5238282Z }
2025-04-11T03:52:12.5238287Z 
2025-04-11T03:52:12.5238393Z     def test_spec_dec(tokenizer):
2025-04-11T03:52:12.5238489Z         spec_num = SPEC_NUM
2025-04-11T03:52:12.5238587Z         device = get_current_device()
2025-04-11T03:52:12.5238715Z         tokenizer.pad_token = tokenizer.eos_token
2025-04-11T03:52:12.5238791Z     
2025-04-11T03:52:12.5238903Z         # Dummy config for Glide Model
2025-04-11T03:52:12.5239005Z         glide_config = GlideLlamaConfig(
2025-04-11T03:52:12.5239107Z             intermediate_size=8192,
2025-04-11T03:52:12.5239199Z             large_hidden_size=4096,
2025-04-11T03:52:12.5239300Z             large_num_attention_heads=32,
2025-04-11T03:52:12.5239410Z             num_hidden_layers=NUM_LAYERS,
2025-04-11T03:52:12.5239490Z         )
2025-04-11T03:52:12.5239638Z         drafter_model = GlideLlamaForCausalLM(glide_config)
2025-04-11T03:52:12.5239714Z     
2025-04-11T03:52:12.5239929Z         assert hasattr(drafter_model, "model")
2025-04-11T03:52:12.5240064Z         assert hasattr(drafter_model.model, "layers")
2025-04-11T03:52:12.5240203Z         for _, layer in enumerate(drafter_model.model.layers):
2025-04-11T03:52:12.5240319Z             assert hasattr(layer, "cross_attn")
2025-04-11T03:52:12.5240396Z     
2025-04-11T03:52:12.5240546Z         # Init the Drafter by providing the sharded drafter model
2025-04-11T03:52:12.5240745Z >       drafter = Drafter(drafter_model, tokenizer, device=device, dtype=torch.float16)
2025-04-11T03:52:12.5240749Z 
2025-04-11T03:52:12.5240852Z tests/test_infer/test_drafter.py:65: 
2025-04-11T03:52:12.5240978Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5241201Z colossalai/inference/spec/drafter.py:31: in __init__
2025-04-11T03:52:12.5241324Z     self._drafter_model = model.to(self._device)
2025-04-11T03:52:12.5241573Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.5241690Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.5241925Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.5242029Z     return self._apply(convert)
2025-04-11T03:52:12.5242285Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.5242373Z     module._apply(fn)
2025-04-11T03:52:12.5242621Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.5242708Z     module._apply(fn)
2025-04-11T03:52:12.5242951Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.5243051Z     param_applied = fn(param)
2025-04-11T03:52:12.5243168Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5243171Z 
2025-04-11T03:52:12.5243275Z t = Parameter containing:
2025-04-11T03:52:12.5243422Z tensor([[-0.0389,  0.0039, -0.0004,  ...,  0.0133,  0.0029, -0.0177],
2025-04-11T03:52:12.5243534Z         [-0.0144,  0.0054,...4,  0.0227,  0.0264],
2025-04-11T03:52:12.5243662Z         [ 0.0320, -0.0080,  0.0294,  ...,  0.0173,  0.0005, -0.0045]],
2025-04-11T03:52:12.5243758Z        requires_grad=True)
2025-04-11T03:52:12.5243762Z 
2025-04-11T03:52:12.5243847Z     def convert(t):
2025-04-11T03:52:12.5243983Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.5244179Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.5244303Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.5244523Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.5244639Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5244944Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5245084Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5245252Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5245265Z 
2025-04-11T03:52:12.5245527Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.5245667Z ______________________________ test_cache_manager ______________________________
2025-04-11T03:52:12.5245671Z 
2025-04-11T03:52:12.5245776Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5246387Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5246395Z 
2025-04-11T03:52:12.5246513Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5246596Z         try_count = 0
2025-04-11T03:52:12.5246805Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5246893Z             max_try, int
2025-04-11T03:52:12.5247055Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5247131Z     
2025-04-11T03:52:12.5247246Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5247335Z             try:
2025-04-11T03:52:12.5247423Z                 try_count += 1
2025-04-11T03:52:12.5247539Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5247635Z                 return ret
2025-04-11T03:52:12.5247735Z             except exception_type as e:
2025-04-11T03:52:12.5247873Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5248160Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5248295Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5248450Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5248618Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5248706Z                     continue
2025-04-11T03:52:12.5248787Z                 else:
2025-04-11T03:52:12.5249020Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5249105Z >                   raise e
2025-04-11T03:52:12.5249109Z 
2025-04-11T03:52:12.5249220Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5249336Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5249480Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5249574Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5249741Z tests/test_infer/test_kvcache_manager.py:174: in test_cache_manager
2025-04-11T03:52:12.5249840Z     spawn(run_dist, 1)
2025-04-11T03:52:12.5249947Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5250062Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5250319Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5250505Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5250791Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5250884Z     while not context.join():
2025-04-11T03:52:12.5251006Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5251010Z 
2025-04-11T03:52:12.5251208Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f202a500>
2025-04-11T03:52:12.5251306Z timeout = None
2025-04-11T03:52:12.5251310Z 
2025-04-11T03:52:12.5251405Z     def join(self, timeout=None):
2025-04-11T03:52:12.5251543Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5251620Z     
2025-04-11T03:52:12.5251775Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5251933Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5252098Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5252210Z         of the first process exiting.
2025-04-11T03:52:12.5252286Z     
2025-04-11T03:52:12.5252445Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5252586Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5252662Z     
2025-04-11T03:52:12.5252752Z         Args:
2025-04-11T03:52:12.5252894Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5252987Z         """
2025-04-11T03:52:12.5253131Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5253229Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5253448Z             return True
2025-04-11T03:52:12.5253526Z     
2025-04-11T03:52:12.5253671Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5253794Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5253901Z             self.sentinels.keys(),
2025-04-11T03:52:12.5253991Z             timeout=timeout,
2025-04-11T03:52:12.5254068Z         )
2025-04-11T03:52:12.5254167Z     
2025-04-11T03:52:12.5254259Z         error_index = None
2025-04-11T03:52:12.5254371Z         for sentinel in ready:
2025-04-11T03:52:12.5254488Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5254592Z             process = self.processes[index]
2025-04-11T03:52:12.5254702Z             process.join()
2025-04-11T03:52:12.5254920Z             if process.exitcode != 0:
2025-04-11T03:52:12.5255022Z                 error_index = index
2025-04-11T03:52:12.5255104Z                 break
2025-04-11T03:52:12.5255180Z     
2025-04-11T03:52:12.5255288Z         # Return if there was no error.
2025-04-11T03:52:12.5255382Z         if error_index is None:
2025-04-11T03:52:12.5255528Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5255630Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5255713Z     
2025-04-11T03:52:12.5255856Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5255960Z         for process in self.processes:
2025-04-11T03:52:12.5256065Z             if process.is_alive():
2025-04-11T03:52:12.5256163Z                 process.terminate()
2025-04-11T03:52:12.5256260Z             process.join()
2025-04-11T03:52:12.5256335Z     
2025-04-11T03:52:12.5256483Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5256615Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5256737Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5256874Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5256975Z             if exitcode < 0:
2025-04-11T03:52:12.5257095Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5257211Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5257367Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5257476Z                     error_index=error_index,
2025-04-11T03:52:12.5257584Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5257687Z                     exit_code=exitcode,
2025-04-11T03:52:12.5257778Z                     signal_name=name,
2025-04-11T03:52:12.5257857Z                 )
2025-04-11T03:52:12.5257944Z             else:
2025-04-11T03:52:12.5258055Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5258238Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5258336Z                     error_index=error_index,
2025-04-11T03:52:12.5258456Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5258548Z                     exit_code=exitcode,
2025-04-11T03:52:12.5258626Z                 )
2025-04-11T03:52:12.5258712Z     
2025-04-11T03:52:12.5258849Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5259031Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5259120Z         msg += original_trace
2025-04-11T03:52:12.5259299Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5259473Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5259550Z E       
2025-04-11T03:52:12.5259691Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5259796Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5260112Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5260198Z E           fn(i, *args)
2025-04-11T03:52:12.5260544Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 168, in run_dist
2025-04-11T03:52:12.5260650Z E           check_cache_manager()
2025-04-11T03:52:12.5260912Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.5261015Z E           partial_func(**kwargs)
2025-04-11T03:52:12.5261277Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 89, in check_cache_manager
2025-04-11T03:52:12.5261445Z E           cache_manager = KVCacheManager(inference_config, model_config)
2025-04-11T03:52:12.5261711Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T03:52:12.5261996Z E           self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T03:52:12.5262296Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T03:52:12.5262517Z E           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T03:52:12.5262640Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5262922Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5263071Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5263237Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5263241Z 
2025-04-11T03:52:12.5263560Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5263721Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5263892Z [04/11/25 03:44:22] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5264035Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5264148Z                              :75 launch                                         
2025-04-11T03:52:12.5264306Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5264445Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.5264609Z [04/11/25 03:44:22] INFO     colossalai -                                       
2025-04-11T03:52:12.5264747Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T03:52:12.5264861Z                              INFO:                                              
2025-04-11T03:52:12.5264993Z                              /__w/ColossalAI/ColossalAI/colossalai/inference/kv_
2025-04-11T03:52:12.5265117Z                              cache/kvcache_manager.py:104 __init__              
2025-04-11T03:52:12.5265254Z                     INFO     colossalai -                                       
2025-04-11T03:52:12.5265392Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T03:52:12.5265523Z                              INFO: Allocating KV cache with shape: (80, 16, 8,  
2025-04-11T03:52:12.5265642Z                              32) consisting of 80 blocks.                       
2025-04-11T03:52:12.5265848Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5265999Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5267257Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.5267436Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.5267600Z ____________________ test_running_list_and_request_handler _____________________
2025-04-11T03:52:12.5267604Z 
2025-04-11T03:52:12.5267704Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5268307Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5268323Z 
2025-04-11T03:52:12.5268469Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5268697Z         try_count = 0
2025-04-11T03:52:12.5268818Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5268908Z             max_try, int
2025-04-11T03:52:12.5269077Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5269159Z     
2025-04-11T03:52:12.5269275Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5269370Z             try:
2025-04-11T03:52:12.5269458Z                 try_count += 1
2025-04-11T03:52:12.5269569Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5269653Z                 return ret
2025-04-11T03:52:12.5269764Z             except exception_type as e:
2025-04-11T03:52:12.5269869Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5270056Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5270187Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5270341Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5270507Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5270593Z                     continue
2025-04-11T03:52:12.5270685Z                 else:
2025-04-11T03:52:12.5270909Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5270995Z >                   raise e
2025-04-11T03:52:12.5270999Z 
2025-04-11T03:52:12.5271107Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5271223Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5271367Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5271457Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5271668Z tests/test_infer/test_request_handler.py:101: in test_running_list_and_request_handler
2025-04-11T03:52:12.5271755Z     spawn(run_dist, 1)
2025-04-11T03:52:12.5271861Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5271972Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5272229Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5272419Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5272705Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5272806Z     while not context.join():
2025-04-11T03:52:12.5272917Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5272920Z 
2025-04-11T03:52:12.5273118Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be79fc70>
2025-04-11T03:52:12.5273227Z timeout = None
2025-04-11T03:52:12.5273232Z 
2025-04-11T03:52:12.5273325Z     def join(self, timeout=None):
2025-04-11T03:52:12.5273468Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5273547Z     
2025-04-11T03:52:12.5273701Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5273853Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5274158Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5274268Z         of the first process exiting.
2025-04-11T03:52:12.5274345Z     
2025-04-11T03:52:12.5274507Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5274648Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5274732Z     
2025-04-11T03:52:12.5274813Z         Args:
2025-04-11T03:52:12.5274955Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5275042Z         """
2025-04-11T03:52:12.5275186Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5275295Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5275490Z             return True
2025-04-11T03:52:12.5275569Z     
2025-04-11T03:52:12.5275718Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5275842Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5275954Z             self.sentinels.keys(),
2025-04-11T03:52:12.5276044Z             timeout=timeout,
2025-04-11T03:52:12.5276120Z         )
2025-04-11T03:52:12.5276205Z     
2025-04-11T03:52:12.5276291Z         error_index = None
2025-04-11T03:52:12.5276389Z         for sentinel in ready:
2025-04-11T03:52:12.5276500Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5276610Z             process = self.processes[index]
2025-04-11T03:52:12.5276699Z             process.join()
2025-04-11T03:52:12.5276798Z             if process.exitcode != 0:
2025-04-11T03:52:12.5276902Z                 error_index = index
2025-04-11T03:52:12.5276984Z                 break
2025-04-11T03:52:12.5277068Z     
2025-04-11T03:52:12.5277169Z         # Return if there was no error.
2025-04-11T03:52:12.5277259Z         if error_index is None:
2025-04-11T03:52:12.5277406Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5277506Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5277592Z     
2025-04-11T03:52:12.5277736Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5277841Z         for process in self.processes:
2025-04-11T03:52:12.5277944Z             if process.is_alive():
2025-04-11T03:52:12.5278040Z                 process.terminate()
2025-04-11T03:52:12.5278139Z             process.join()
2025-04-11T03:52:12.5278216Z     
2025-04-11T03:52:12.5278369Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5278491Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5278602Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5278740Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5278831Z             if exitcode < 0:
2025-04-11T03:52:12.5278951Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5279062Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5279216Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5279327Z                     error_index=error_index,
2025-04-11T03:52:12.5279431Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5279534Z                     exit_code=exitcode,
2025-04-11T03:52:12.5279629Z                     signal_name=name,
2025-04-11T03:52:12.5279717Z                 )
2025-04-11T03:52:12.5279796Z             else:
2025-04-11T03:52:12.5279901Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5280076Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5280172Z                     error_index=error_index,
2025-04-11T03:52:12.5280286Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5280375Z                     exit_code=exitcode,
2025-04-11T03:52:12.5280460Z                 )
2025-04-11T03:52:12.5280536Z     
2025-04-11T03:52:12.5280798Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5280983Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5281077Z         msg += original_trace
2025-04-11T03:52:12.5281260Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5281422Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5281500Z E       
2025-04-11T03:52:12.5281640Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5281744Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5282053Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5282251Z E           fn(i, *args)
2025-04-11T03:52:12.5282506Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 95, in run_dist
2025-04-11T03:52:12.5282603Z E           check_request_handler()
2025-04-11T03:52:12.5282880Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 70, in check_request_handler
2025-04-11T03:52:12.5283052Z E           request_handler = RequestHandler(inference_config, model_config)
2025-04-11T03:52:12.5283313Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 160, in __init__
2025-04-11T03:52:12.5283427Z E           self._init_cache(model_config)
2025-04-11T03:52:12.5283690Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 222, in _init_cache
2025-04-11T03:52:12.5283895Z E           self.cache_manager = KVCacheManager(self.inference_config, model_config)
2025-04-11T03:52:12.5284158Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T03:52:12.5284333Z E           self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T03:52:12.5284631Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T03:52:12.5284848Z E           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T03:52:12.5284970Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5285255Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5285401Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5285563Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5285567Z 
2025-04-11T03:52:12.5285880Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5286038Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5286204Z [04/11/25 03:44:26] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5286339Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5286450Z                              :75 launch                                         
2025-04-11T03:52:12.5286601Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5286727Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.5286874Z [04/11/25 03:44:26] INFO     colossalai -                                       
2025-04-11T03:52:12.5287010Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T03:52:12.5287122Z                              INFO:                                              
2025-04-11T03:52:12.5287251Z                              /__w/ColossalAI/ColossalAI/colossalai/inference/kv_
2025-04-11T03:52:12.5287377Z                              cache/kvcache_manager.py:104 __init__              
2025-04-11T03:52:12.5287620Z                     INFO     colossalai -                                       
2025-04-11T03:52:12.5287757Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T03:52:12.5287890Z                              INFO: Allocating KV cache with shape: (24, 4, 8, 8)
2025-04-11T03:52:12.5288010Z                              consisting of 24 blocks.                           
2025-04-11T03:52:12.5288217Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5288368Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5289510Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.5289880Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.5290030Z _________________________________ test_engine __________________________________
2025-04-11T03:52:12.5290034Z 
2025-04-11T03:52:12.5290135Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5290732Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5290747Z 
2025-04-11T03:52:12.5290860Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5290955Z         try_count = 0
2025-04-11T03:52:12.5291079Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5291172Z             max_try, int
2025-04-11T03:52:12.5291341Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5291430Z     
2025-04-11T03:52:12.5291560Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5291668Z             try:
2025-04-11T03:52:12.5291763Z                 try_count += 1
2025-04-11T03:52:12.5291880Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5291971Z                 return ret
2025-04-11T03:52:12.5292086Z             except exception_type as e:
2025-04-11T03:52:12.5292198Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5292394Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5292538Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5292696Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5292869Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5292962Z                     continue
2025-04-11T03:52:12.5293058Z                 else:
2025-04-11T03:52:12.5293286Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5293376Z >                   raise e
2025-04-11T03:52:12.5293380Z 
2025-04-11T03:52:12.5293495Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5293614Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5293762Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5293858Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5294015Z tests/test_infer/test_streamingllm.py:117: in test_engine
2025-04-11T03:52:12.5294181Z     spawn(run_dist, 1, func_to_run=check_streamingllm, ret=result_list)
2025-04-11T03:52:12.5294294Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5294416Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5294786Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5294975Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5295264Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5295367Z     while not context.join():
2025-04-11T03:52:12.5295481Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5295485Z 
2025-04-11T03:52:12.5295687Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f007f100>
2025-04-11T03:52:12.5295779Z timeout = None
2025-04-11T03:52:12.5295783Z 
2025-04-11T03:52:12.5295877Z     def join(self, timeout=None):
2025-04-11T03:52:12.5296117Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5296194Z     
2025-04-11T03:52:12.5296354Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5296506Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5296674Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5296783Z         of the first process exiting.
2025-04-11T03:52:12.5296859Z     
2025-04-11T03:52:12.5297019Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5297160Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5297246Z     
2025-04-11T03:52:12.5297327Z         Args:
2025-04-11T03:52:12.5297470Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5297558Z         """
2025-04-11T03:52:12.5297700Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5297809Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5297893Z             return True
2025-04-11T03:52:12.5297968Z     
2025-04-11T03:52:12.5298116Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5298245Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5298351Z             self.sentinels.keys(),
2025-04-11T03:52:12.5298439Z             timeout=timeout,
2025-04-11T03:52:12.5298516Z         )
2025-04-11T03:52:12.5298601Z     
2025-04-11T03:52:12.5298688Z         error_index = None
2025-04-11T03:52:12.5298786Z         for sentinel in ready:
2025-04-11T03:52:12.5298898Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5299010Z             process = self.processes[index]
2025-04-11T03:52:12.5299101Z             process.join()
2025-04-11T03:52:12.5299200Z             if process.exitcode != 0:
2025-04-11T03:52:12.5299301Z                 error_index = index
2025-04-11T03:52:12.5299386Z                 break
2025-04-11T03:52:12.5299469Z     
2025-04-11T03:52:12.5299567Z         # Return if there was no error.
2025-04-11T03:52:12.5299657Z         if error_index is None:
2025-04-11T03:52:12.5299802Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5299907Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5299991Z     
2025-04-11T03:52:12.5300136Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5300238Z         for process in self.processes:
2025-04-11T03:52:12.5300344Z             if process.is_alive():
2025-04-11T03:52:12.5300441Z                 process.terminate()
2025-04-11T03:52:12.5300540Z             process.join()
2025-04-11T03:52:12.5300615Z     
2025-04-11T03:52:12.5300776Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5300904Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5301016Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5301162Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5301249Z             if exitcode < 0:
2025-04-11T03:52:12.5301379Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5301594Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5301763Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5301866Z                     error_index=error_index,
2025-04-11T03:52:12.5301975Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5302084Z                     exit_code=exitcode,
2025-04-11T03:52:12.5302182Z                     signal_name=name,
2025-04-11T03:52:12.5302274Z                 )
2025-04-11T03:52:12.5302358Z             else:
2025-04-11T03:52:12.5302469Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5302651Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5302858Z                     error_index=error_index,
2025-04-11T03:52:12.5302975Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5303070Z                     exit_code=exitcode,
2025-04-11T03:52:12.5303165Z                 )
2025-04-11T03:52:12.5303245Z     
2025-04-11T03:52:12.5303388Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5303586Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5303675Z         msg += original_trace
2025-04-11T03:52:12.5303858Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5304021Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5304098Z E       
2025-04-11T03:52:12.5304235Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5304336Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5304644Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5304733Z E           fn(i, *args)
2025-04-11T03:52:12.5304977Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 107, in run_dist
2025-04-11T03:52:12.5305081Z E           ret[rank] = func_to_run(**kwargs)
2025-04-11T03:52:12.5305336Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 39, in check_streamingllm
2025-04-11T03:52:12.5305421Z E           ).cuda()
2025-04-11T03:52:12.5305711Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.5305822Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.5306088Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.5306214Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.5306482Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.5306584Z E           module._apply(fn)
2025-04-11T03:52:12.5306856Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.5306946Z E           module._apply(fn)
2025-04-11T03:52:12.5307219Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.5307316Z E           param_applied = fn(param)
2025-04-11T03:52:12.5307593Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.5307712Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.5307826Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5308110Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5308253Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5308455Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5308459Z 
2025-04-11T03:52:12.5308886Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5309050Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5309209Z [04/11/25 03:44:30] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5309350Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5309460Z                              :75 launch                                         
2025-04-11T03:52:12.5309608Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5309733Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.5310058Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5310238Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5311445Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.5311628Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.5312325Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.5312420Z   warnings.warn(
2025-04-11T03:52:12.5313234Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5352483Z   warnings.warn(
2025-04-11T03:52:12.5352716Z ________________________ test_flash_decoding_attention _________________________
2025-04-11T03:52:12.5352725Z 
2025-04-11T03:52:12.5352836Z args = (), kwargs = {}
2025-04-11T03:52:12.5352841Z 
2025-04-11T03:52:12.5352953Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.5353081Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.5353203Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.5353328Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.5353470Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.5353576Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.5353581Z 
2025-04-11T03:52:12.5353695Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.5353828Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5354009Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.5354113Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.5354232Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5354244Z 
2025-04-11T03:52:12.5354325Z device = None
2025-04-11T03:52:12.5354330Z 
2025-04-11T03:52:12.5354461Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5354641Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5354719Z     
2025-04-11T03:52:12.5354805Z         Args:
2025-04-11T03:52:12.5355009Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5355201Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5355556Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5355643Z         """
2025-04-11T03:52:12.5355781Z         _lazy_init()
2025-04-11T03:52:12.5355883Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5356001Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5356120Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5356440Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5356609Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5356788Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5357027Z 
2025-04-11T03:52:12.5357293Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5357451Z ______________________ test_vllm_flash_decoding_attention ______________________
2025-04-11T03:52:12.5357455Z 
2025-04-11T03:52:12.5357553Z args = (), kwargs = {}
2025-04-11T03:52:12.5357557Z 
2025-04-11T03:52:12.5357656Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.5357770Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.5357883Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.5358001Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.5358118Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.5358214Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.5358217Z 
2025-04-11T03:52:12.5358325Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.5358440Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5358614Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.5358715Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.5358829Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5358833Z 
2025-04-11T03:52:12.5358928Z device = None
2025-04-11T03:52:12.5358933Z 
2025-04-11T03:52:12.5359053Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5359219Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5359295Z     
2025-04-11T03:52:12.5359381Z         Args:
2025-04-11T03:52:12.5359556Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5359724Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5359844Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5359920Z         """
2025-04-11T03:52:12.5360010Z         _lazy_init()
2025-04-11T03:52:12.5360110Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5360222Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5360332Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5360627Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5360775Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5360939Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5360943Z 
2025-04-11T03:52:12.5361194Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5361344Z _____________________ test_get_cos_and_sin[dtype0-64-64-4] _____________________
2025-04-11T03:52:12.5361347Z 
2025-04-11T03:52:12.5361511Z BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float16
2025-04-11T03:52:12.5361515Z 
2025-04-11T03:52:12.5361636Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.5361767Z     @pytest.mark.parametrize("MAX_SEQ_LEN", [64])
2025-04-11T03:52:12.5361878Z     @pytest.mark.parametrize("HEAD_DIM", [64])
2025-04-11T03:52:12.5362178Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T03:52:12.5362346Z     def test_get_cos_and_sin(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
2025-04-11T03:52:12.5362454Z         MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
2025-04-11T03:52:12.5362655Z >       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
2025-04-11T03:52:12.5362770Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5363075Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5363214Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5363377Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5363588Z 
2025-04-11T03:52:12.5363790Z tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py:24: RuntimeError
2025-04-11T03:52:12.5363944Z _____________________ test_get_cos_and_sin[dtype1-64-64-4] _____________________
2025-04-11T03:52:12.5363948Z 
2025-04-11T03:52:12.5364113Z BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float32
2025-04-11T03:52:12.5364117Z 
2025-04-11T03:52:12.5364229Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.5364351Z     @pytest.mark.parametrize("MAX_SEQ_LEN", [64])
2025-04-11T03:52:12.5364463Z     @pytest.mark.parametrize("HEAD_DIM", [64])
2025-04-11T03:52:12.5364637Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T03:52:12.5364789Z     def test_get_cos_and_sin(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
2025-04-11T03:52:12.5364894Z         MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
2025-04-11T03:52:12.5365087Z >       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
2025-04-11T03:52:12.5365197Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5365493Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5365635Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5365803Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5365807Z 
2025-04-11T03:52:12.5365987Z tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py:24: RuntimeError
2025-04-11T03:52:12.5366138Z ____________________ test_kv_cache_memcopy[True-16-8-16-4] _____________________
2025-04-11T03:52:12.5366151Z 
2025-04-11T03:52:12.5366306Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5366395Z same_context_len = True
2025-04-11T03:52:12.5366398Z 
2025-04-11T03:52:12.5366516Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5366654Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5366810Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5366926Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5367080Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5367182Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5367263Z         bsz: int,
2025-04-11T03:52:12.5367360Z         block_size: int,
2025-04-11T03:52:12.5367457Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5367550Z         num_kv_heads: int,
2025-04-11T03:52:12.5367642Z         same_context_len: bool,
2025-04-11T03:52:12.5367719Z     ):
2025-04-11T03:52:12.5367956Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5367961Z 
2025-04-11T03:52:12.5368118Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5368239Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5368246Z 
2025-04-11T03:52:12.5368394Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5368489Z same_context_len = True
2025-04-11T03:52:12.5368492Z 
2025-04-11T03:52:12.5368697Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5368781Z         bsz: int,
2025-04-11T03:52:12.5368879Z         block_size: int,
2025-04-11T03:52:12.5368974Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5369065Z         num_kv_heads: int,
2025-04-11T03:52:12.5369156Z         same_context_len: bool,
2025-04-11T03:52:12.5369240Z     ):
2025-04-11T03:52:12.5369329Z         torch.manual_seed(123)
2025-04-11T03:52:12.5369405Z     
2025-04-11T03:52:12.5369619Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5369742Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5369841Z         dtype = torch.float16
2025-04-11T03:52:12.5370207Z         device = get_current_device()
2025-04-11T03:52:12.5370280Z     
2025-04-11T03:52:12.5370374Z         if same_context_len:
2025-04-11T03:52:12.5370614Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5370734Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5371020Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5371168Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5371332Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5371336Z 
2025-04-11T03:52:12.5371523Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5371674Z ____________________ test_kv_cache_memcopy[True-16-8-16-7] _____________________
2025-04-11T03:52:12.5371681Z 
2025-04-11T03:52:12.5371829Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5371926Z same_context_len = True
2025-04-11T03:52:12.5371930Z 
2025-04-11T03:52:12.5372038Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5372178Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5372321Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5372443Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5372586Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5372679Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5372770Z         bsz: int,
2025-04-11T03:52:12.5372857Z         block_size: int,
2025-04-11T03:52:12.5372957Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5373042Z         num_kv_heads: int,
2025-04-11T03:52:12.5373134Z         same_context_len: bool,
2025-04-11T03:52:12.5373214Z     ):
2025-04-11T03:52:12.5373445Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5373448Z 
2025-04-11T03:52:12.5373608Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5373724Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5373728Z 
2025-04-11T03:52:12.5373880Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5373966Z same_context_len = True
2025-04-11T03:52:12.5373969Z 
2025-04-11T03:52:12.5374080Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5374163Z         bsz: int,
2025-04-11T03:52:12.5374264Z         block_size: int,
2025-04-11T03:52:12.5374363Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5374447Z         num_kv_heads: int,
2025-04-11T03:52:12.5374550Z         same_context_len: bool,
2025-04-11T03:52:12.5374623Z     ):
2025-04-11T03:52:12.5374710Z         torch.manual_seed(123)
2025-04-11T03:52:12.5374797Z     
2025-04-11T03:52:12.5374998Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5375130Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5375220Z         dtype = torch.float16
2025-04-11T03:52:12.5375434Z         device = get_current_device()
2025-04-11T03:52:12.5375514Z     
2025-04-11T03:52:12.5375604Z         if same_context_len:
2025-04-11T03:52:12.5375841Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5375949Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5376236Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5376378Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5376540Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5376645Z 
2025-04-11T03:52:12.5376829Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5376978Z ____________________ test_kv_cache_memcopy[True-16-8-16-32] ____________________
2025-04-11T03:52:12.5376982Z 
2025-04-11T03:52:12.5377142Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5377228Z same_context_len = True
2025-04-11T03:52:12.5377232Z 
2025-04-11T03:52:12.5377350Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5377480Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5377627Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5377743Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5377883Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5377984Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5378061Z         bsz: int,
2025-04-11T03:52:12.5378153Z         block_size: int,
2025-04-11T03:52:12.5378245Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5378334Z         num_kv_heads: int,
2025-04-11T03:52:12.5378422Z         same_context_len: bool,
2025-04-11T03:52:12.5378497Z     ):
2025-04-11T03:52:12.5378731Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5378735Z 
2025-04-11T03:52:12.5378888Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5379009Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5379013Z 
2025-04-11T03:52:12.5379158Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5379247Z same_context_len = True
2025-04-11T03:52:12.5379251Z 
2025-04-11T03:52:12.5379348Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5379426Z         bsz: int,
2025-04-11T03:52:12.5379516Z         block_size: int,
2025-04-11T03:52:12.5379611Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5379701Z         num_kv_heads: int,
2025-04-11T03:52:12.5379787Z         same_context_len: bool,
2025-04-11T03:52:12.5379861Z     ):
2025-04-11T03:52:12.5379954Z         torch.manual_seed(123)
2025-04-11T03:52:12.5380028Z     
2025-04-11T03:52:12.5380234Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5380355Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5380448Z         dtype = torch.float16
2025-04-11T03:52:12.5380540Z         device = get_current_device()
2025-04-11T03:52:12.5380613Z     
2025-04-11T03:52:12.5380708Z         if same_context_len:
2025-04-11T03:52:12.5380930Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5381045Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5381325Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5381471Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5381628Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5381759Z 
2025-04-11T03:52:12.5381942Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5382096Z ____________________ test_kv_cache_memcopy[True-16-8-32-4] _____________________
2025-04-11T03:52:12.5382100Z 
2025-04-11T03:52:12.5382244Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5382335Z same_context_len = True
2025-04-11T03:52:12.5382339Z 
2025-04-11T03:52:12.5382453Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5382587Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5382733Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5382939Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5383086Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5383176Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5383261Z         bsz: int,
2025-04-11T03:52:12.5383347Z         block_size: int,
2025-04-11T03:52:12.5383442Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5383527Z         num_kv_heads: int,
2025-04-11T03:52:12.5383612Z         same_context_len: bool,
2025-04-11T03:52:12.5383692Z     ):
2025-04-11T03:52:12.5383913Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5383917Z 
2025-04-11T03:52:12.5384076Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5384188Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5384192Z 
2025-04-11T03:52:12.5384339Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5384426Z same_context_len = True
2025-04-11T03:52:12.5384429Z 
2025-04-11T03:52:12.5384525Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5384608Z         bsz: int,
2025-04-11T03:52:12.5384692Z         block_size: int,
2025-04-11T03:52:12.5384792Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5384876Z         num_kv_heads: int,
2025-04-11T03:52:12.5384970Z         same_context_len: bool,
2025-04-11T03:52:12.5385046Z     ):
2025-04-11T03:52:12.5385133Z         torch.manual_seed(123)
2025-04-11T03:52:12.5385212Z     
2025-04-11T03:52:12.5385411Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5385548Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5385635Z         dtype = torch.float16
2025-04-11T03:52:12.5385733Z         device = get_current_device()
2025-04-11T03:52:12.5385812Z     
2025-04-11T03:52:12.5385899Z         if same_context_len:
2025-04-11T03:52:12.5386134Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5386241Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5386533Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5386673Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5386830Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5386840Z 
2025-04-11T03:52:12.5387019Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5387163Z ____________________ test_kv_cache_memcopy[True-16-8-32-7] _____________________
2025-04-11T03:52:12.5387167Z 
2025-04-11T03:52:12.5387316Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5387401Z same_context_len = True
2025-04-11T03:52:12.5387407Z 
2025-04-11T03:52:12.5387521Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5387647Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5387797Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5388020Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5388162Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5388258Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5388334Z         bsz: int,
2025-04-11T03:52:12.5389006Z         block_size: int,
2025-04-11T03:52:12.5389100Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5389183Z         num_kv_heads: int,
2025-04-11T03:52:12.5389278Z         same_context_len: bool,
2025-04-11T03:52:12.5389353Z     ):
2025-04-11T03:52:12.5389582Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5389586Z 
2025-04-11T03:52:12.5389877Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5389996Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5389999Z 
2025-04-11T03:52:12.5390149Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5390243Z same_context_len = True
2025-04-11T03:52:12.5390247Z 
2025-04-11T03:52:12.5390341Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5390418Z         bsz: int,
2025-04-11T03:52:12.5390507Z         block_size: int,
2025-04-11T03:52:12.5390598Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5390687Z         num_kv_heads: int,
2025-04-11T03:52:12.5390774Z         same_context_len: bool,
2025-04-11T03:52:12.5390847Z     ):
2025-04-11T03:52:12.5390945Z         torch.manual_seed(123)
2025-04-11T03:52:12.5391018Z     
2025-04-11T03:52:12.5391226Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5391348Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5391441Z         dtype = torch.float16
2025-04-11T03:52:12.5391535Z         device = get_current_device()
2025-04-11T03:52:12.5391609Z     
2025-04-11T03:52:12.5391702Z         if same_context_len:
2025-04-11T03:52:12.5391929Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5392042Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5392321Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5392457Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5392620Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5392624Z 
2025-04-11T03:52:12.5392802Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5392961Z ____________________ test_kv_cache_memcopy[True-16-8-32-32] ____________________
2025-04-11T03:52:12.5392964Z 
2025-04-11T03:52:12.5393114Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5393204Z same_context_len = True
2025-04-11T03:52:12.5393207Z 
2025-04-11T03:52:12.5393318Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5393449Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5393589Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5393704Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5393848Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5393937Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5394017Z         bsz: int,
2025-04-11T03:52:12.5394098Z         block_size: int,
2025-04-11T03:52:12.5394194Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5394276Z         num_kv_heads: int,
2025-04-11T03:52:12.5394365Z         same_context_len: bool,
2025-04-11T03:52:12.5394446Z     ):
2025-04-11T03:52:12.5394665Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5394669Z 
2025-04-11T03:52:12.5394947Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5395064Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5395068Z 
2025-04-11T03:52:12.5395217Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5395304Z same_context_len = True
2025-04-11T03:52:12.5395307Z 
2025-04-11T03:52:12.5395401Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5395486Z         bsz: int,
2025-04-11T03:52:12.5395569Z         block_size: int,
2025-04-11T03:52:12.5395665Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5395750Z         num_kv_heads: int,
2025-04-11T03:52:12.5395838Z         same_context_len: bool,
2025-04-11T03:52:12.5396013Z     ):
2025-04-11T03:52:12.5396101Z         torch.manual_seed(123)
2025-04-11T03:52:12.5396181Z     
2025-04-11T03:52:12.5396378Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5396512Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5396606Z         dtype = torch.float16
2025-04-11T03:52:12.5396704Z         device = get_current_device()
2025-04-11T03:52:12.5396789Z     
2025-04-11T03:52:12.5396873Z         if same_context_len:
2025-04-11T03:52:12.5397103Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5397211Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5397510Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5397648Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5397812Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5397816Z 
2025-04-11T03:52:12.5398000Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5398148Z ____________________ test_kv_cache_memcopy[True-16-8-64-4] _____________________
2025-04-11T03:52:12.5398152Z 
2025-04-11T03:52:12.5398305Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5398391Z same_context_len = True
2025-04-11T03:52:12.5398394Z 
2025-04-11T03:52:12.5398509Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5398636Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5398786Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5398903Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5399043Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5399143Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5399222Z         bsz: int,
2025-04-11T03:52:12.5399312Z         block_size: int,
2025-04-11T03:52:12.5399405Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5399491Z         num_kv_heads: int,
2025-04-11T03:52:12.5399592Z         same_context_len: bool,
2025-04-11T03:52:12.5399666Z     ):
2025-04-11T03:52:12.5399898Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5399902Z 
2025-04-11T03:52:12.5400055Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5400177Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5400181Z 
2025-04-11T03:52:12.5400325Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5400410Z same_context_len = True
2025-04-11T03:52:12.5400420Z 
2025-04-11T03:52:12.5400516Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5400598Z         bsz: int,
2025-04-11T03:52:12.5400692Z         block_size: int,
2025-04-11T03:52:12.5400785Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5400875Z         num_kv_heads: int,
2025-04-11T03:52:12.5400963Z         same_context_len: bool,
2025-04-11T03:52:12.5401141Z     ):
2025-04-11T03:52:12.5401242Z         torch.manual_seed(123)
2025-04-11T03:52:12.5401318Z     
2025-04-11T03:52:12.5401523Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5401644Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5401733Z         dtype = torch.float16
2025-04-11T03:52:12.5401837Z         device = get_current_device()
2025-04-11T03:52:12.5401912Z     
2025-04-11T03:52:12.5402005Z         if same_context_len:
2025-04-11T03:52:12.5402232Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5402349Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5402746Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5402883Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5403055Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5403059Z 
2025-04-11T03:52:12.5403238Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5403394Z ____________________ test_kv_cache_memcopy[True-16-8-64-7] _____________________
2025-04-11T03:52:12.5403398Z 
2025-04-11T03:52:12.5403541Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5403635Z same_context_len = True
2025-04-11T03:52:12.5403638Z 
2025-04-11T03:52:12.5403748Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5403881Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5404026Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5404143Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5404295Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5404390Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5404478Z         bsz: int,
2025-04-11T03:52:12.5404560Z         block_size: int,
2025-04-11T03:52:12.5404651Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5404744Z         num_kv_heads: int,
2025-04-11T03:52:12.5404834Z         same_context_len: bool,
2025-04-11T03:52:12.5404917Z     ):
2025-04-11T03:52:12.5405141Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5405145Z 
2025-04-11T03:52:12.5405303Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5405414Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5405421Z 
2025-04-11T03:52:12.5405577Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5405662Z same_context_len = True
2025-04-11T03:52:12.5405665Z 
2025-04-11T03:52:12.5405760Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5405853Z         bsz: int,
2025-04-11T03:52:12.5405938Z         block_size: int,
2025-04-11T03:52:12.5406037Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5406122Z         num_kv_heads: int,
2025-04-11T03:52:12.5406209Z         same_context_len: bool,
2025-04-11T03:52:12.5406294Z     ):
2025-04-11T03:52:12.5406384Z         torch.manual_seed(123)
2025-04-11T03:52:12.5406466Z     
2025-04-11T03:52:12.5406666Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5406796Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5406885Z         dtype = torch.float16
2025-04-11T03:52:12.5406980Z         device = get_current_device()
2025-04-11T03:52:12.5407069Z     
2025-04-11T03:52:12.5407157Z         if same_context_len:
2025-04-11T03:52:12.5407393Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5407607Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5407902Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5408051Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5408211Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5408215Z 
2025-04-11T03:52:12.5408401Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5408547Z ____________________ test_kv_cache_memcopy[True-16-8-64-32] ____________________
2025-04-11T03:52:12.5408550Z 
2025-04-11T03:52:12.5408704Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5408897Z same_context_len = True
2025-04-11T03:52:12.5408901Z 
2025-04-11T03:52:12.5409020Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5409152Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5409302Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5409440Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5409582Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5409680Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5409758Z         bsz: int,
2025-04-11T03:52:12.5409850Z         block_size: int,
2025-04-11T03:52:12.5409941Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5410025Z         num_kv_heads: int,
2025-04-11T03:52:12.5410123Z         same_context_len: bool,
2025-04-11T03:52:12.5410197Z     ):
2025-04-11T03:52:12.5410427Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5410434Z 
2025-04-11T03:52:12.5410588Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5410708Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5410715Z 
2025-04-11T03:52:12.5410863Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5410950Z same_context_len = True
2025-04-11T03:52:12.5410953Z 
2025-04-11T03:52:12.5411058Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5411136Z         bsz: int,
2025-04-11T03:52:12.5411229Z         block_size: int,
2025-04-11T03:52:12.5411319Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5411411Z         num_kv_heads: int,
2025-04-11T03:52:12.5411498Z         same_context_len: bool,
2025-04-11T03:52:12.5411580Z     ):
2025-04-11T03:52:12.5411712Z         torch.manual_seed(123)
2025-04-11T03:52:12.5411802Z     
2025-04-11T03:52:12.5412011Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5412135Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5412223Z         dtype = torch.float16
2025-04-11T03:52:12.5412324Z         device = get_current_device()
2025-04-11T03:52:12.5412401Z     
2025-04-11T03:52:12.5412495Z         if same_context_len:
2025-04-11T03:52:12.5412717Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5412826Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5413119Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5413260Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5413425Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5413429Z 
2025-04-11T03:52:12.5413611Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5413765Z ____________________ test_kv_cache_memcopy[True-16-32-16-4] ____________________
2025-04-11T03:52:12.5413768Z 
2025-04-11T03:52:12.5414025Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5414124Z same_context_len = True
2025-04-11T03:52:12.5414127Z 
2025-04-11T03:52:12.5414238Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5414367Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5414519Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5414637Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5414784Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5414872Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5414956Z         bsz: int,
2025-04-11T03:52:12.5415041Z         block_size: int,
2025-04-11T03:52:12.5415237Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5415332Z         num_kv_heads: int,
2025-04-11T03:52:12.5415420Z         same_context_len: bool,
2025-04-11T03:52:12.5415500Z     ):
2025-04-11T03:52:12.5415727Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5415731Z 
2025-04-11T03:52:12.5415890Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5416004Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5416008Z 
2025-04-11T03:52:12.5416153Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5416247Z same_context_len = True
2025-04-11T03:52:12.5416251Z 
2025-04-11T03:52:12.5416347Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5416433Z         bsz: int,
2025-04-11T03:52:12.5416516Z         block_size: int,
2025-04-11T03:52:12.5416611Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5416699Z         num_kv_heads: int,
2025-04-11T03:52:12.5416787Z         same_context_len: bool,
2025-04-11T03:52:12.5416870Z     ):
2025-04-11T03:52:12.5416959Z         torch.manual_seed(123)
2025-04-11T03:52:12.5417039Z     
2025-04-11T03:52:12.5417242Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5417360Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5417459Z         dtype = torch.float16
2025-04-11T03:52:12.5417555Z         device = get_current_device()
2025-04-11T03:52:12.5417634Z     
2025-04-11T03:52:12.5417719Z         if same_context_len:
2025-04-11T03:52:12.5417953Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5418061Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5418346Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5418501Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5418660Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5418663Z 
2025-04-11T03:52:12.5418859Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5419014Z ____________________ test_kv_cache_memcopy[True-16-32-16-7] ____________________
2025-04-11T03:52:12.5419018Z 
2025-04-11T03:52:12.5419194Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5419279Z same_context_len = True
2025-04-11T03:52:12.5419283Z 
2025-04-11T03:52:12.5419396Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5419522Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5419663Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5419788Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5419930Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5420023Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5420100Z         bsz: int,
2025-04-11T03:52:12.5420184Z         block_size: int,
2025-04-11T03:52:12.5420399Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5420483Z         num_kv_heads: int,
2025-04-11T03:52:12.5420583Z         same_context_len: bool,
2025-04-11T03:52:12.5420657Z     ):
2025-04-11T03:52:12.5420886Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5420890Z 
2025-04-11T03:52:12.5421045Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5421159Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5421169Z 
2025-04-11T03:52:12.5421315Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5421496Z same_context_len = True
2025-04-11T03:52:12.5421500Z 
2025-04-11T03:52:12.5421609Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5421687Z         bsz: int,
2025-04-11T03:52:12.5421778Z         block_size: int,
2025-04-11T03:52:12.5421868Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5421956Z         num_kv_heads: int,
2025-04-11T03:52:12.5422053Z         same_context_len: bool,
2025-04-11T03:52:12.5422128Z     ):
2025-04-11T03:52:12.5422224Z         torch.manual_seed(123)
2025-04-11T03:52:12.5422303Z     
2025-04-11T03:52:12.5422508Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5422630Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5422718Z         dtype = torch.float16
2025-04-11T03:52:12.5422819Z         device = get_current_device()
2025-04-11T03:52:12.5422891Z     
2025-04-11T03:52:12.5422982Z         if same_context_len:
2025-04-11T03:52:12.5423207Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5423319Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5423615Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5423753Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5423921Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5423924Z 
2025-04-11T03:52:12.5424103Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5424259Z ___________________ test_kv_cache_memcopy[True-16-32-16-32] ____________________
2025-04-11T03:52:12.5424262Z 
2025-04-11T03:52:12.5424413Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5424506Z same_context_len = True
2025-04-11T03:52:12.5424509Z 
2025-04-11T03:52:12.5424622Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5424750Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5424903Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5425020Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5425169Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5425257Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5425344Z         bsz: int,
2025-04-11T03:52:12.5425428Z         block_size: int,
2025-04-11T03:52:12.5425518Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5425611Z         num_kv_heads: int,
2025-04-11T03:52:12.5425698Z         same_context_len: bool,
2025-04-11T03:52:12.5425778Z     ):
2025-04-11T03:52:12.5425998Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5426002Z 
2025-04-11T03:52:12.5426163Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5426280Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5426284Z 
2025-04-11T03:52:12.5426432Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5426639Z same_context_len = True
2025-04-11T03:52:12.5426644Z 
2025-04-11T03:52:12.5426742Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5426827Z         bsz: int,
2025-04-11T03:52:12.5426911Z         block_size: int,
2025-04-11T03:52:12.5427010Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5427094Z         num_kv_heads: int,
2025-04-11T03:52:12.5427179Z         same_context_len: bool,
2025-04-11T03:52:12.5427263Z     ):
2025-04-11T03:52:12.5427351Z         torch.manual_seed(123)
2025-04-11T03:52:12.5427431Z     
2025-04-11T03:52:12.5427632Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5427751Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5427947Z         dtype = torch.float16
2025-04-11T03:52:12.5428041Z         device = get_current_device()
2025-04-11T03:52:12.5428133Z     
2025-04-11T03:52:12.5428247Z         if same_context_len:
2025-04-11T03:52:12.5428533Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5428654Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5428940Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5429088Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5429247Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5429251Z 
2025-04-11T03:52:12.5429435Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5429584Z ____________________ test_kv_cache_memcopy[True-16-32-32-4] ____________________
2025-04-11T03:52:12.5429590Z 
2025-04-11T03:52:12.5429747Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5429836Z same_context_len = True
2025-04-11T03:52:12.5429839Z 
2025-04-11T03:52:12.5429962Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5430093Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5430236Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5430363Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5430502Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5430604Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5430683Z         bsz: int,
2025-04-11T03:52:12.5430770Z         block_size: int,
2025-04-11T03:52:12.5430876Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5430963Z         num_kv_heads: int,
2025-04-11T03:52:12.5431056Z         same_context_len: bool,
2025-04-11T03:52:12.5431136Z     ):
2025-04-11T03:52:12.5431362Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5431366Z 
2025-04-11T03:52:12.5431519Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5431633Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5431637Z 
2025-04-11T03:52:12.5431791Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5431877Z same_context_len = True
2025-04-11T03:52:12.5431880Z 
2025-04-11T03:52:12.5431984Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5432063Z         bsz: int,
2025-04-11T03:52:12.5432152Z         block_size: int,
2025-04-11T03:52:12.5432245Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5432328Z         num_kv_heads: int,
2025-04-11T03:52:12.5432425Z         same_context_len: bool,
2025-04-11T03:52:12.5432500Z     ):
2025-04-11T03:52:12.5432597Z         torch.manual_seed(123)
2025-04-11T03:52:12.5432670Z     
2025-04-11T03:52:12.5432872Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5432998Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5433201Z         dtype = torch.float16
2025-04-11T03:52:12.5433306Z         device = get_current_device()
2025-04-11T03:52:12.5433380Z     
2025-04-11T03:52:12.5433471Z         if same_context_len:
2025-04-11T03:52:12.5433697Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5433809Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5434105Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5434243Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5434614Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5434618Z 
2025-04-11T03:52:12.5434796Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5434953Z ____________________ test_kv_cache_memcopy[True-16-32-32-7] ____________________
2025-04-11T03:52:12.5434956Z 
2025-04-11T03:52:12.5435102Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5435195Z same_context_len = True
2025-04-11T03:52:12.5435198Z 
2025-04-11T03:52:12.5435306Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5435433Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5435583Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5435701Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5435847Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5435936Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5436025Z         bsz: int,
2025-04-11T03:52:12.5436110Z         block_size: int,
2025-04-11T03:52:12.5436200Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5436291Z         num_kv_heads: int,
2025-04-11T03:52:12.5436380Z         same_context_len: bool,
2025-04-11T03:52:12.5436466Z     ):
2025-04-11T03:52:12.5436686Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5436690Z 
2025-04-11T03:52:12.5436846Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5436970Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5436973Z 
2025-04-11T03:52:12.5437116Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5437212Z same_context_len = True
2025-04-11T03:52:12.5437215Z 
2025-04-11T03:52:12.5437310Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5437394Z         bsz: int,
2025-04-11T03:52:12.5437480Z         block_size: int,
2025-04-11T03:52:12.5437572Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5437665Z         num_kv_heads: int,
2025-04-11T03:52:12.5437753Z         same_context_len: bool,
2025-04-11T03:52:12.5437834Z     ):
2025-04-11T03:52:12.5437924Z         torch.manual_seed(123)
2025-04-11T03:52:12.5438005Z     
2025-04-11T03:52:12.5438205Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5438322Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5438416Z         dtype = torch.float16
2025-04-11T03:52:12.5438510Z         device = get_current_device()
2025-04-11T03:52:12.5438590Z     
2025-04-11T03:52:12.5438678Z         if same_context_len:
2025-04-11T03:52:12.5438904Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5439022Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5439315Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5439464Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5439735Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5439739Z 
2025-04-11T03:52:12.5439933Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5440084Z ___________________ test_kv_cache_memcopy[True-16-32-32-32] ____________________
2025-04-11T03:52:12.5440088Z 
2025-04-11T03:52:12.5440247Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5440335Z same_context_len = True
2025-04-11T03:52:12.5440339Z 
2025-04-11T03:52:12.5440449Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5440589Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5440872Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5440998Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5441139Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5441235Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5441320Z         bsz: int,
2025-04-11T03:52:12.5441404Z         block_size: int,
2025-04-11T03:52:12.5441502Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5441588Z         num_kv_heads: int,
2025-04-11T03:52:12.5441680Z         same_context_len: bool,
2025-04-11T03:52:12.5441754Z     ):
2025-04-11T03:52:12.5441986Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5442003Z 
2025-04-11T03:52:12.5442160Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5442277Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5442281Z 
2025-04-11T03:52:12.5442452Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5442542Z same_context_len = True
2025-04-11T03:52:12.5442545Z 
2025-04-11T03:52:12.5442652Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5442729Z         bsz: int,
2025-04-11T03:52:12.5442828Z         block_size: int,
2025-04-11T03:52:12.5442923Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5443011Z         num_kv_heads: int,
2025-04-11T03:52:12.5443109Z         same_context_len: bool,
2025-04-11T03:52:12.5443185Z     ):
2025-04-11T03:52:12.5443283Z         torch.manual_seed(123)
2025-04-11T03:52:12.5443361Z     
2025-04-11T03:52:12.5443565Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5443690Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5443778Z         dtype = torch.float16
2025-04-11T03:52:12.5443879Z         device = get_current_device()
2025-04-11T03:52:12.5443958Z     
2025-04-11T03:52:12.5444046Z         if same_context_len:
2025-04-11T03:52:12.5444277Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5444387Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5444679Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5444815Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5444982Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5444986Z 
2025-04-11T03:52:12.5445162Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5445319Z ____________________ test_kv_cache_memcopy[True-16-32-64-4] ____________________
2025-04-11T03:52:12.5445323Z 
2025-04-11T03:52:12.5445473Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5445566Z same_context_len = True
2025-04-11T03:52:12.5445579Z 
2025-04-11T03:52:12.5445688Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5445817Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5446073Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5446190Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5446338Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5446429Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5446507Z         bsz: int,
2025-04-11T03:52:12.5446601Z         block_size: int,
2025-04-11T03:52:12.5446695Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5446788Z         num_kv_heads: int,
2025-04-11T03:52:12.5446878Z         same_context_len: bool,
2025-04-11T03:52:12.5446953Z     ):
2025-04-11T03:52:12.5447185Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5447283Z 
2025-04-11T03:52:12.5447436Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5447560Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5447564Z 
2025-04-11T03:52:12.5447709Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5447804Z same_context_len = True
2025-04-11T03:52:12.5447807Z 
2025-04-11T03:52:12.5447903Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5447988Z         bsz: int,
2025-04-11T03:52:12.5448072Z         block_size: int,
2025-04-11T03:52:12.5448163Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5448258Z         num_kv_heads: int,
2025-04-11T03:52:12.5448344Z         same_context_len: bool,
2025-04-11T03:52:12.5448425Z     ):
2025-04-11T03:52:12.5448511Z         torch.manual_seed(123)
2025-04-11T03:52:12.5448585Z     
2025-04-11T03:52:12.5448788Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5448911Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5449009Z         dtype = torch.float16
2025-04-11T03:52:12.5449103Z         device = get_current_device()
2025-04-11T03:52:12.5449185Z     
2025-04-11T03:52:12.5449275Z         if same_context_len:
2025-04-11T03:52:12.5449503Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5449620Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5449900Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5450036Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5450191Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5450196Z 
2025-04-11T03:52:12.5450373Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5450522Z ____________________ test_kv_cache_memcopy[True-16-32-64-7] ____________________
2025-04-11T03:52:12.5450525Z 
2025-04-11T03:52:12.5450678Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5450780Z same_context_len = True
2025-04-11T03:52:12.5450784Z 
2025-04-11T03:52:12.5450895Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5451041Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5451186Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5451317Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5451457Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5451555Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5451633Z         bsz: int,
2025-04-11T03:52:12.5451716Z         block_size: int,
2025-04-11T03:52:12.5451815Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5451902Z         num_kv_heads: int,
2025-04-11T03:52:12.5451995Z         same_context_len: bool,
2025-04-11T03:52:12.5452071Z     ):
2025-04-11T03:52:12.5452291Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5452411Z 
2025-04-11T03:52:12.5452575Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5452690Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5452694Z 
2025-04-11T03:52:12.5452844Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5452934Z same_context_len = True
2025-04-11T03:52:12.5452937Z 
2025-04-11T03:52:12.5453051Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5453128Z         bsz: int,
2025-04-11T03:52:12.5453212Z         block_size: int,
2025-04-11T03:52:12.5453308Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5453392Z         num_kv_heads: int,
2025-04-11T03:52:12.5453571Z         same_context_len: bool,
2025-04-11T03:52:12.5453646Z     ):
2025-04-11T03:52:12.5453740Z         torch.manual_seed(123)
2025-04-11T03:52:12.5453814Z     
2025-04-11T03:52:12.5454015Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5454148Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5454240Z         dtype = torch.float16
2025-04-11T03:52:12.5454345Z         device = get_current_device()
2025-04-11T03:52:12.5454419Z     
2025-04-11T03:52:12.5454506Z         if same_context_len:
2025-04-11T03:52:12.5454735Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5454844Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5455134Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5455275Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5455440Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5455444Z 
2025-04-11T03:52:12.5455623Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5455778Z ___________________ test_kv_cache_memcopy[True-16-32-64-32] ____________________
2025-04-11T03:52:12.5455782Z 
2025-04-11T03:52:12.5455929Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5456015Z same_context_len = True
2025-04-11T03:52:12.5456019Z 
2025-04-11T03:52:12.5456138Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5456265Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5456415Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5456533Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5456684Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5456773Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5456850Z         bsz: int,
2025-04-11T03:52:12.5456941Z         block_size: int,
2025-04-11T03:52:12.5457032Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5457123Z         num_kv_heads: int,
2025-04-11T03:52:12.5457211Z         same_context_len: bool,
2025-04-11T03:52:12.5457286Z     ):
2025-04-11T03:52:12.5457516Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5457520Z 
2025-04-11T03:52:12.5457672Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5457794Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5457797Z 
2025-04-11T03:52:12.5457940Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5458034Z same_context_len = True
2025-04-11T03:52:12.5458041Z 
2025-04-11T03:52:12.5458138Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5458222Z         bsz: int,
2025-04-11T03:52:12.5458305Z         block_size: int,
2025-04-11T03:52:12.5458396Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5458593Z         num_kv_heads: int,
2025-04-11T03:52:12.5458685Z         same_context_len: bool,
2025-04-11T03:52:12.5458769Z     ):
2025-04-11T03:52:12.5458857Z         torch.manual_seed(123)
2025-04-11T03:52:12.5458931Z     
2025-04-11T03:52:12.5459139Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5459258Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5459353Z         dtype = torch.float16
2025-04-11T03:52:12.5459447Z         device = get_current_device()
2025-04-11T03:52:12.5459522Z     
2025-04-11T03:52:12.5459616Z         if same_context_len:
2025-04-11T03:52:12.5459844Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5460071Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5460358Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5460504Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5460666Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5460670Z 
2025-04-11T03:52:12.5460854Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5461001Z ____________________ test_kv_cache_memcopy[False-16-8-16-4] ____________________
2025-04-11T03:52:12.5461005Z 
2025-04-11T03:52:12.5461155Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5461260Z same_context_len = False
2025-04-11T03:52:12.5461264Z 
2025-04-11T03:52:12.5461382Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5461526Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5461668Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5461799Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5461943Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5462033Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5462119Z         bsz: int,
2025-04-11T03:52:12.5462202Z         block_size: int,
2025-04-11T03:52:12.5462305Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5462388Z         num_kv_heads: int,
2025-04-11T03:52:12.5462484Z         same_context_len: bool,
2025-04-11T03:52:12.5462559Z     ):
2025-04-11T03:52:12.5462779Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5462783Z 
2025-04-11T03:52:12.5462942Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5463055Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5463059Z 
2025-04-11T03:52:12.5463215Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5463303Z same_context_len = False
2025-04-11T03:52:12.5463307Z 
2025-04-11T03:52:12.5463413Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5463490Z         bsz: int,
2025-04-11T03:52:12.5463573Z         block_size: int,
2025-04-11T03:52:12.5463673Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5463756Z         num_kv_heads: int,
2025-04-11T03:52:12.5463847Z         same_context_len: bool,
2025-04-11T03:52:12.5463923Z     ):
2025-04-11T03:52:12.5464010Z         torch.manual_seed(123)
2025-04-11T03:52:12.5464091Z     
2025-04-11T03:52:12.5464288Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5464416Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5464510Z         dtype = torch.float16
2025-04-11T03:52:12.5464611Z         device = get_current_device()
2025-04-11T03:52:12.5464684Z     
2025-04-11T03:52:12.5464772Z         if same_context_len:
2025-04-11T03:52:12.5465139Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5465219Z         else:
2025-04-11T03:52:12.5465468Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5465580Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5465872Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5466012Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5466172Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5466176Z 
2025-04-11T03:52:12.5466471Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5466619Z ____________________ test_kv_cache_memcopy[False-16-8-16-7] ____________________
2025-04-11T03:52:12.5466623Z 
2025-04-11T03:52:12.5466784Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5466871Z same_context_len = False
2025-04-11T03:52:12.5466875Z 
2025-04-11T03:52:12.5466995Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5467122Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5467274Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5467389Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5467529Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5467631Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5467709Z         bsz: int,
2025-04-11T03:52:12.5467798Z         block_size: int,
2025-04-11T03:52:12.5467894Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5467978Z         num_kv_heads: int,
2025-04-11T03:52:12.5468074Z         same_context_len: bool,
2025-04-11T03:52:12.5468149Z     ):
2025-04-11T03:52:12.5468378Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5468382Z 
2025-04-11T03:52:12.5468579Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5468701Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5468705Z 
2025-04-11T03:52:12.5468849Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5468937Z same_context_len = False
2025-04-11T03:52:12.5468949Z 
2025-04-11T03:52:12.5469056Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5469142Z         bsz: int,
2025-04-11T03:52:12.5469232Z         block_size: int,
2025-04-11T03:52:12.5469331Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5469427Z         num_kv_heads: int,
2025-04-11T03:52:12.5469519Z         same_context_len: bool,
2025-04-11T03:52:12.5469596Z     ):
2025-04-11T03:52:12.5469692Z         torch.manual_seed(123)
2025-04-11T03:52:12.5469767Z     
2025-04-11T03:52:12.5469973Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5470092Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5470188Z         dtype = torch.float16
2025-04-11T03:52:12.5470282Z         device = get_current_device()
2025-04-11T03:52:12.5470356Z     
2025-04-11T03:52:12.5470452Z         if same_context_len:
2025-04-11T03:52:12.5470677Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5470762Z         else:
2025-04-11T03:52:12.5470996Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5471112Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5471402Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5471651Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5471819Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5471823Z 
2025-04-11T03:52:12.5472003Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5472161Z ___________________ test_kv_cache_memcopy[False-16-8-16-32] ____________________
2025-04-11T03:52:12.5472165Z 
2025-04-11T03:52:12.5472315Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5472410Z same_context_len = False
2025-04-11T03:52:12.5472414Z 
2025-04-11T03:52:12.5472523Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5472653Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5472911Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5473030Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5473181Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5473273Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5473362Z         bsz: int,
2025-04-11T03:52:12.5473447Z         block_size: int,
2025-04-11T03:52:12.5473539Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5473635Z         num_kv_heads: int,
2025-04-11T03:52:12.5473725Z         same_context_len: bool,
2025-04-11T03:52:12.5473808Z     ):
2025-04-11T03:52:12.5474036Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5474040Z 
2025-04-11T03:52:12.5474207Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5474325Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5474332Z 
2025-04-11T03:52:12.5474484Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5474577Z same_context_len = False
2025-04-11T03:52:12.5474581Z 
2025-04-11T03:52:12.5474677Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5474764Z         bsz: int,
2025-04-11T03:52:12.5474845Z         block_size: int,
2025-04-11T03:52:12.5474943Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5475026Z         num_kv_heads: int,
2025-04-11T03:52:12.5475111Z         same_context_len: bool,
2025-04-11T03:52:12.5475197Z     ):
2025-04-11T03:52:12.5475285Z         torch.manual_seed(123)
2025-04-11T03:52:12.5475366Z     
2025-04-11T03:52:12.5475563Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5475683Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5475781Z         dtype = torch.float16
2025-04-11T03:52:12.5475874Z         device = get_current_device()
2025-04-11T03:52:12.5475958Z     
2025-04-11T03:52:12.5476046Z         if same_context_len:
2025-04-11T03:52:12.5476272Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5476353Z         else:
2025-04-11T03:52:12.5476588Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5476707Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5476994Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5477140Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5477304Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5477307Z 
2025-04-11T03:52:12.5477493Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5477648Z ____________________ test_kv_cache_memcopy[False-16-8-32-4] ____________________
2025-04-11T03:52:12.5477652Z 
2025-04-11T03:52:12.5477799Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5477891Z same_context_len = False
2025-04-11T03:52:12.5477997Z 
2025-04-11T03:52:12.5478108Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5478244Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5478385Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5478509Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5478649Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5478739Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5478824Z         bsz: int,
2025-04-11T03:52:12.5478907Z         block_size: int,
2025-04-11T03:52:12.5479004Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5479090Z         num_kv_heads: int,
2025-04-11T03:52:12.5479280Z         same_context_len: bool,
2025-04-11T03:52:12.5479357Z     ):
2025-04-11T03:52:12.5479582Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5479586Z 
2025-04-11T03:52:12.5479751Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5479865Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5479868Z 
2025-04-11T03:52:12.5480023Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5480109Z same_context_len = False
2025-04-11T03:52:12.5480112Z 
2025-04-11T03:52:12.5480219Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5480296Z         bsz: int,
2025-04-11T03:52:12.5480380Z         block_size: int,
2025-04-11T03:52:12.5480481Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5480564Z         num_kv_heads: int,
2025-04-11T03:52:12.5480664Z         same_context_len: bool,
2025-04-11T03:52:12.5480740Z     ):
2025-04-11T03:52:12.5480827Z         torch.manual_seed(123)
2025-04-11T03:52:12.5480914Z     
2025-04-11T03:52:12.5481113Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5481241Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5481337Z         dtype = torch.float16
2025-04-11T03:52:12.5481442Z         device = get_current_device()
2025-04-11T03:52:12.5481529Z     
2025-04-11T03:52:12.5481614Z         if same_context_len:
2025-04-11T03:52:12.5481852Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5481930Z         else:
2025-04-11T03:52:12.5482168Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5482278Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5482570Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5482704Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5482865Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5482877Z 
2025-04-11T03:52:12.5483055Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5483204Z ____________________ test_kv_cache_memcopy[False-16-8-32-7] ____________________
2025-04-11T03:52:12.5483208Z 
2025-04-11T03:52:12.5483366Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5483451Z same_context_len = False
2025-04-11T03:52:12.5483455Z 
2025-04-11T03:52:12.5483574Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5483703Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5483855Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5483972Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5484112Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5484211Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5484390Z         bsz: int,
2025-04-11T03:52:12.5484483Z         block_size: int,
2025-04-11T03:52:12.5484576Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5484660Z         num_kv_heads: int,
2025-04-11T03:52:12.5484755Z         same_context_len: bool,
2025-04-11T03:52:12.5484829Z     ):
2025-04-11T03:52:12.5485059Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5485063Z 
2025-04-11T03:52:12.5485217Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5485339Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5485343Z 
2025-04-11T03:52:12.5485487Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5485674Z same_context_len = False
2025-04-11T03:52:12.5485678Z 
2025-04-11T03:52:12.5485773Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5485850Z         bsz: int,
2025-04-11T03:52:12.5485944Z         block_size: int,
2025-04-11T03:52:12.5486050Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5486141Z         num_kv_heads: int,
2025-04-11T03:52:12.5486229Z         same_context_len: bool,
2025-04-11T03:52:12.5486304Z     ):
2025-04-11T03:52:12.5486399Z         torch.manual_seed(123)
2025-04-11T03:52:12.5486474Z     
2025-04-11T03:52:12.5486683Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5486802Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5486900Z         dtype = torch.float16
2025-04-11T03:52:12.5486995Z         device = get_current_device()
2025-04-11T03:52:12.5487070Z     
2025-04-11T03:52:12.5487171Z         if same_context_len:
2025-04-11T03:52:12.5487394Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5487479Z         else:
2025-04-11T03:52:12.5487713Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5487822Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5488114Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5488250Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5488417Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5488421Z 
2025-04-11T03:52:12.5488602Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5488761Z ___________________ test_kv_cache_memcopy[False-16-8-32-32] ____________________
2025-04-11T03:52:12.5488768Z 
2025-04-11T03:52:12.5488917Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5489011Z same_context_len = False
2025-04-11T03:52:12.5489015Z 
2025-04-11T03:52:12.5489126Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5489255Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5489405Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5489519Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5489667Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5489755Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5489840Z         bsz: int,
2025-04-11T03:52:12.5489922Z         block_size: int,
2025-04-11T03:52:12.5490012Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5490102Z         num_kv_heads: int,
2025-04-11T03:52:12.5490189Z         same_context_len: bool,
2025-04-11T03:52:12.5490271Z     ):
2025-04-11T03:52:12.5490492Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5490495Z 
2025-04-11T03:52:12.5490657Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5490896Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5490900Z 
2025-04-11T03:52:12.5491048Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5491144Z same_context_len = False
2025-04-11T03:52:12.5491147Z 
2025-04-11T03:52:12.5491244Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5491330Z         bsz: int,
2025-04-11T03:52:12.5491413Z         block_size: int,
2025-04-11T03:52:12.5491512Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5491598Z         num_kv_heads: int,
2025-04-11T03:52:12.5491684Z         same_context_len: bool,
2025-04-11T03:52:12.5491767Z     ):
2025-04-11T03:52:12.5491950Z         torch.manual_seed(123)
2025-04-11T03:52:12.5492034Z     
2025-04-11T03:52:12.5492234Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5492368Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5492476Z         dtype = torch.float16
2025-04-11T03:52:12.5492574Z         device = get_current_device()
2025-04-11T03:52:12.5492655Z     
2025-04-11T03:52:12.5492749Z         if same_context_len:
2025-04-11T03:52:12.5492981Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5493068Z         else:
2025-04-11T03:52:12.5493302Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5493419Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5493703Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5493852Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5494014Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5494017Z 
2025-04-11T03:52:12.5494213Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5494360Z ____________________ test_kv_cache_memcopy[False-16-8-64-4] ____________________
2025-04-11T03:52:12.5494364Z 
2025-04-11T03:52:12.5494511Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5494605Z same_context_len = False
2025-04-11T03:52:12.5494608Z 
2025-04-11T03:52:12.5494717Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5494853Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5494994Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5495116Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5495260Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5495349Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5495434Z         bsz: int,
2025-04-11T03:52:12.5495519Z         block_size: int,
2025-04-11T03:52:12.5495621Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5495703Z         num_kv_heads: int,
2025-04-11T03:52:12.5495798Z         same_context_len: bool,
2025-04-11T03:52:12.5495872Z     ):
2025-04-11T03:52:12.5496090Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5496094Z 
2025-04-11T03:52:12.5496256Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5496372Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5496376Z 
2025-04-11T03:52:12.5496530Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5496620Z same_context_len = False
2025-04-11T03:52:12.5496624Z 
2025-04-11T03:52:12.5496726Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5496804Z         bsz: int,
2025-04-11T03:52:12.5496889Z         block_size: int,
2025-04-11T03:52:12.5497098Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5497187Z         num_kv_heads: int,
2025-04-11T03:52:12.5497285Z         same_context_len: bool,
2025-04-11T03:52:12.5497362Z     ):
2025-04-11T03:52:12.5497450Z         torch.manual_seed(123)
2025-04-11T03:52:12.5497533Z     
2025-04-11T03:52:12.5497732Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5497858Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5497949Z         dtype = torch.float16
2025-04-11T03:52:12.5498052Z         device = get_current_device()
2025-04-11T03:52:12.5498126Z     
2025-04-11T03:52:12.5498212Z         if same_context_len:
2025-04-11T03:52:12.5498445Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5498636Z         else:
2025-04-11T03:52:12.5498879Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5498991Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5499285Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5499426Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5499586Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5499589Z 
2025-04-11T03:52:12.5499778Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5499926Z ____________________ test_kv_cache_memcopy[False-16-8-64-7] ____________________
2025-04-11T03:52:12.5499930Z 
2025-04-11T03:52:12.5500087Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5500174Z same_context_len = False
2025-04-11T03:52:12.5500178Z 
2025-04-11T03:52:12.5500296Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5500427Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5500577Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5500693Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5500834Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5500933Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5501012Z         bsz: int,
2025-04-11T03:52:12.5501106Z         block_size: int,
2025-04-11T03:52:12.5501198Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5501283Z         num_kv_heads: int,
2025-04-11T03:52:12.5501382Z         same_context_len: bool,
2025-04-11T03:52:12.5501456Z     ):
2025-04-11T03:52:12.5501685Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5501692Z 
2025-04-11T03:52:12.5501849Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5501976Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5501979Z 
2025-04-11T03:52:12.5502124Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5502209Z same_context_len = False
2025-04-11T03:52:12.5502220Z 
2025-04-11T03:52:12.5502314Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5502392Z         bsz: int,
2025-04-11T03:52:12.5502482Z         block_size: int,
2025-04-11T03:52:12.5502574Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5502667Z         num_kv_heads: int,
2025-04-11T03:52:12.5502753Z         same_context_len: bool,
2025-04-11T03:52:12.5502829Z     ):
2025-04-11T03:52:12.5502924Z         torch.manual_seed(123)
2025-04-11T03:52:12.5503003Z     
2025-04-11T03:52:12.5503211Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5503328Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5503416Z         dtype = torch.float16
2025-04-11T03:52:12.5503615Z         device = get_current_device()
2025-04-11T03:52:12.5503692Z     
2025-04-11T03:52:12.5503788Z         if same_context_len:
2025-04-11T03:52:12.5504022Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5504119Z         else:
2025-04-11T03:52:12.5504351Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5504460Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5504749Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5505039Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5505210Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5505213Z 
2025-04-11T03:52:12.5505392Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5505551Z ___________________ test_kv_cache_memcopy[False-16-8-64-32] ____________________
2025-04-11T03:52:12.5505555Z 
2025-04-11T03:52:12.5505703Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5505798Z same_context_len = False
2025-04-11T03:52:12.5505802Z 
2025-04-11T03:52:12.5505911Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5506040Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5506191Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5506308Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5506456Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5506552Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5506639Z         bsz: int,
2025-04-11T03:52:12.5506722Z         block_size: int,
2025-04-11T03:52:12.5506817Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5506912Z         num_kv_heads: int,
2025-04-11T03:52:12.5507001Z         same_context_len: bool,
2025-04-11T03:52:12.5507087Z     ):
2025-04-11T03:52:12.5507310Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5507314Z 
2025-04-11T03:52:12.5507468Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5507591Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5507595Z 
2025-04-11T03:52:12.5507741Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5507837Z same_context_len = False
2025-04-11T03:52:12.5507843Z 
2025-04-11T03:52:12.5507940Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5508024Z         bsz: int,
2025-04-11T03:52:12.5508107Z         block_size: int,
2025-04-11T03:52:12.5508198Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5508290Z         num_kv_heads: int,
2025-04-11T03:52:12.5508380Z         same_context_len: bool,
2025-04-11T03:52:12.5508512Z     ):
2025-04-11T03:52:12.5508600Z         torch.manual_seed(123)
2025-04-11T03:52:12.5508684Z     
2025-04-11T03:52:12.5508883Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5509003Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5509100Z         dtype = torch.float16
2025-04-11T03:52:12.5509193Z         device = get_current_device()
2025-04-11T03:52:12.5509275Z     
2025-04-11T03:52:12.5509361Z         if same_context_len:
2025-04-11T03:52:12.5509582Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5509672Z         else:
2025-04-11T03:52:12.5509901Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5510020Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5510414Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5510566Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5510726Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5510730Z 
2025-04-11T03:52:12.5510919Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5511070Z ___________________ test_kv_cache_memcopy[False-16-32-16-4] ____________________
2025-04-11T03:52:12.5511073Z 
2025-04-11T03:52:12.5511221Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5511417Z same_context_len = False
2025-04-11T03:52:12.5511421Z 
2025-04-11T03:52:12.5511533Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5511672Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5511817Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5511940Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5512082Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5512174Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5512287Z         bsz: int,
2025-04-11T03:52:12.5512393Z         block_size: int,
2025-04-11T03:52:12.5512496Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5512579Z         num_kv_heads: int,
2025-04-11T03:52:12.5512665Z         same_context_len: bool,
2025-04-11T03:52:12.5512746Z     ):
2025-04-11T03:52:12.5512970Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5512979Z 
2025-04-11T03:52:12.5513138Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5513255Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5513258Z 
2025-04-11T03:52:12.5513412Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5513498Z same_context_len = False
2025-04-11T03:52:12.5513502Z 
2025-04-11T03:52:12.5513604Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5513681Z         bsz: int,
2025-04-11T03:52:12.5513763Z         block_size: int,
2025-04-11T03:52:12.5513877Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5513969Z         num_kv_heads: int,
2025-04-11T03:52:12.5514061Z         same_context_len: bool,
2025-04-11T03:52:12.5514137Z     ):
2025-04-11T03:52:12.5514226Z         torch.manual_seed(123)
2025-04-11T03:52:12.5514312Z     
2025-04-11T03:52:12.5514513Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5514645Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5514732Z         dtype = torch.float16
2025-04-11T03:52:12.5514834Z         device = get_current_device()
2025-04-11T03:52:12.5514909Z     
2025-04-11T03:52:12.5514996Z         if same_context_len:
2025-04-11T03:52:12.5515227Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5515306Z         else:
2025-04-11T03:52:12.5515542Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5515651Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5515939Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5516077Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5516241Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5516245Z 
2025-04-11T03:52:12.5516432Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5516704Z ___________________ test_kv_cache_memcopy[False-16-32-16-7] ____________________
2025-04-11T03:52:12.5516709Z 
2025-04-11T03:52:12.5516863Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5516951Z same_context_len = False
2025-04-11T03:52:12.5516955Z 
2025-04-11T03:52:12.5517075Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5517202Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5517350Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5517465Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5517608Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5517807Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5517886Z         bsz: int,
2025-04-11T03:52:12.5517981Z         block_size: int,
2025-04-11T03:52:12.5518074Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5518159Z         num_kv_heads: int,
2025-04-11T03:52:12.5518260Z         same_context_len: bool,
2025-04-11T03:52:12.5518337Z     ):
2025-04-11T03:52:12.5518568Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5518572Z 
2025-04-11T03:52:12.5518725Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5518852Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5518856Z 
2025-04-11T03:52:12.5518999Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5519088Z same_context_len = False
2025-04-11T03:52:12.5519098Z 
2025-04-11T03:52:12.5519195Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5519276Z         bsz: int,
2025-04-11T03:52:12.5519365Z         block_size: int,
2025-04-11T03:52:12.5519453Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5519544Z         num_kv_heads: int,
2025-04-11T03:52:12.5519632Z         same_context_len: bool,
2025-04-11T03:52:12.5519712Z     ):
2025-04-11T03:52:12.5519815Z         torch.manual_seed(123)
2025-04-11T03:52:12.5519890Z     
2025-04-11T03:52:12.5520102Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5520223Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5520315Z         dtype = torch.float16
2025-04-11T03:52:12.5520417Z         device = get_current_device()
2025-04-11T03:52:12.5520495Z     
2025-04-11T03:52:12.5520588Z         if same_context_len:
2025-04-11T03:52:12.5520814Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5520897Z         else:
2025-04-11T03:52:12.5521130Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5521240Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5521536Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5521671Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5521837Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5521841Z 
2025-04-11T03:52:12.5522022Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5522179Z ___________________ test_kv_cache_memcopy[False-16-32-16-32] ___________________
2025-04-11T03:52:12.5522182Z 
2025-04-11T03:52:12.5522330Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5522424Z same_context_len = False
2025-04-11T03:52:12.5522430Z 
2025-04-11T03:52:12.5522540Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5522668Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5522819Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5523043Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5523194Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5523284Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5523371Z         bsz: int,
2025-04-11T03:52:12.5523454Z         block_size: int,
2025-04-11T03:52:12.5523545Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5523637Z         num_kv_heads: int,
2025-04-11T03:52:12.5523725Z         same_context_len: bool,
2025-04-11T03:52:12.5523806Z     ):
2025-04-11T03:52:12.5524026Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5524029Z 
2025-04-11T03:52:12.5524274Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5524398Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5524402Z 
2025-04-11T03:52:12.5524555Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5524651Z same_context_len = False
2025-04-11T03:52:12.5524655Z 
2025-04-11T03:52:12.5524751Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5524834Z         bsz: int,
2025-04-11T03:52:12.5524922Z         block_size: int,
2025-04-11T03:52:12.5525012Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5525111Z         num_kv_heads: int,
2025-04-11T03:52:12.5525197Z         same_context_len: bool,
2025-04-11T03:52:12.5525286Z     ):
2025-04-11T03:52:12.5525371Z         torch.manual_seed(123)
2025-04-11T03:52:12.5525456Z     
2025-04-11T03:52:12.5525655Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5525780Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5525879Z         dtype = torch.float16
2025-04-11T03:52:12.5525974Z         device = get_current_device()
2025-04-11T03:52:12.5526055Z     
2025-04-11T03:52:12.5526140Z         if same_context_len:
2025-04-11T03:52:12.5526368Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5526456Z         else:
2025-04-11T03:52:12.5526682Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5526800Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5527085Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5527232Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5527394Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5527401Z 
2025-04-11T03:52:12.5527586Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5527737Z ___________________ test_kv_cache_memcopy[False-16-32-32-4] ____________________
2025-04-11T03:52:12.5527743Z 
2025-04-11T03:52:12.5527891Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5527989Z same_context_len = False
2025-04-11T03:52:12.5527992Z 
2025-04-11T03:52:12.5528103Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5528234Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5528377Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5528504Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5528645Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5528733Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5528824Z         bsz: int,
2025-04-11T03:52:12.5528908Z         block_size: int,
2025-04-11T03:52:12.5529008Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5529093Z         num_kv_heads: int,
2025-04-11T03:52:12.5529180Z         same_context_len: bool,
2025-04-11T03:52:12.5529259Z     ):
2025-04-11T03:52:12.5529643Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5529648Z 
2025-04-11T03:52:12.5529808Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5529923Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5529927Z 
2025-04-11T03:52:12.5530078Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5530166Z same_context_len = False
2025-04-11T03:52:12.5530170Z 
2025-04-11T03:52:12.5530272Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5530353Z         bsz: int,
2025-04-11T03:52:12.5530535Z         block_size: int,
2025-04-11T03:52:12.5530634Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5530719Z         num_kv_heads: int,
2025-04-11T03:52:12.5530813Z         same_context_len: bool,
2025-04-11T03:52:12.5530899Z     ):
2025-04-11T03:52:12.5530991Z         torch.manual_seed(123)
2025-04-11T03:52:12.5531074Z     
2025-04-11T03:52:12.5531270Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5531397Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5531485Z         dtype = torch.float16
2025-04-11T03:52:12.5531588Z         device = get_current_device()
2025-04-11T03:52:12.5531665Z     
2025-04-11T03:52:12.5531751Z         if same_context_len:
2025-04-11T03:52:12.5531982Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5532059Z         else:
2025-04-11T03:52:12.5532295Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5532408Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5532700Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5532838Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5532995Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5532999Z 
2025-04-11T03:52:12.5533185Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5533335Z ___________________ test_kv_cache_memcopy[False-16-32-32-7] ____________________
2025-04-11T03:52:12.5533338Z 
2025-04-11T03:52:12.5533490Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5533575Z same_context_len = False
2025-04-11T03:52:12.5533579Z 
2025-04-11T03:52:12.5533694Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5533825Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5533972Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5534087Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5534229Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5534329Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5534406Z         bsz: int,
2025-04-11T03:52:12.5534493Z         block_size: int,
2025-04-11T03:52:12.5534587Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5534670Z         num_kv_heads: int,
2025-04-11T03:52:12.5534765Z         same_context_len: bool,
2025-04-11T03:52:12.5534841Z     ):
2025-04-11T03:52:12.5535069Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5535073Z 
2025-04-11T03:52:12.5535226Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5535350Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5535354Z 
2025-04-11T03:52:12.5535497Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5535781Z same_context_len = False
2025-04-11T03:52:12.5535795Z 
2025-04-11T03:52:12.5535892Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5535971Z         bsz: int,
2025-04-11T03:52:12.5536069Z         block_size: int,
2025-04-11T03:52:12.5536163Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5536274Z         num_kv_heads: int,
2025-04-11T03:52:12.5536360Z         same_context_len: bool,
2025-04-11T03:52:12.5536441Z     ):
2025-04-11T03:52:12.5536535Z         torch.manual_seed(123)
2025-04-11T03:52:12.5536609Z     
2025-04-11T03:52:12.5536820Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5536940Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5537157Z         dtype = torch.float16
2025-04-11T03:52:12.5537256Z         device = get_current_device()
2025-04-11T03:52:12.5537331Z     
2025-04-11T03:52:12.5537424Z         if same_context_len:
2025-04-11T03:52:12.5537650Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5537734Z         else:
2025-04-11T03:52:12.5537965Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5538078Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5538371Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5538506Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5538672Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5538680Z 
2025-04-11T03:52:12.5538861Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5539019Z ___________________ test_kv_cache_memcopy[False-16-32-32-32] ___________________
2025-04-11T03:52:12.5539023Z 
2025-04-11T03:52:12.5539174Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5539271Z same_context_len = False
2025-04-11T03:52:12.5539275Z 
2025-04-11T03:52:12.5539388Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5539515Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5539667Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5539787Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5539935Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5540027Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5540113Z         bsz: int,
2025-04-11T03:52:12.5540198Z         block_size: int,
2025-04-11T03:52:12.5540295Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5540388Z         num_kv_heads: int,
2025-04-11T03:52:12.5540475Z         same_context_len: bool,
2025-04-11T03:52:12.5540553Z     ):
2025-04-11T03:52:12.5540775Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5540779Z 
2025-04-11T03:52:12.5540935Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5541058Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5541061Z 
2025-04-11T03:52:12.5541210Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5541304Z same_context_len = False
2025-04-11T03:52:12.5541307Z 
2025-04-11T03:52:12.5541401Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5541485Z         bsz: int,
2025-04-11T03:52:12.5541568Z         block_size: int,
2025-04-11T03:52:12.5541659Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5541755Z         num_kv_heads: int,
2025-04-11T03:52:12.5541843Z         same_context_len: bool,
2025-04-11T03:52:12.5541926Z     ):
2025-04-11T03:52:12.5542014Z         torch.manual_seed(123)
2025-04-11T03:52:12.5542094Z     
2025-04-11T03:52:12.5542400Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5542523Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5542622Z         dtype = torch.float16
2025-04-11T03:52:12.5542720Z         device = get_current_device()
2025-04-11T03:52:12.5542803Z     
2025-04-11T03:52:12.5542890Z         if same_context_len:
2025-04-11T03:52:12.5543115Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5543204Z         else:
2025-04-11T03:52:12.5543433Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5543647Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5543933Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5544080Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5544241Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5544245Z 
2025-04-11T03:52:12.5544431Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5544583Z ___________________ test_kv_cache_memcopy[False-16-32-64-4] ____________________
2025-04-11T03:52:12.5544586Z 
2025-04-11T03:52:12.5544734Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5544833Z same_context_len = False
2025-04-11T03:52:12.5544836Z 
2025-04-11T03:52:12.5544948Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5545089Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5545231Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5545358Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5545500Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5545599Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5545692Z         bsz: int,
2025-04-11T03:52:12.5545775Z         block_size: int,
2025-04-11T03:52:12.5545879Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5545964Z         num_kv_heads: int,
2025-04-11T03:52:12.5546051Z         same_context_len: bool,
2025-04-11T03:52:12.5546141Z     ):
2025-04-11T03:52:12.5546361Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5546365Z 
2025-04-11T03:52:12.5546526Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5546640Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5546646Z 
2025-04-11T03:52:12.5546799Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5546885Z same_context_len = False
2025-04-11T03:52:12.5546889Z 
2025-04-11T03:52:12.5546994Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5547071Z         bsz: int,
2025-04-11T03:52:12.5547151Z         block_size: int,
2025-04-11T03:52:12.5547248Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5547332Z         num_kv_heads: int,
2025-04-11T03:52:12.5547424Z         same_context_len: bool,
2025-04-11T03:52:12.5547497Z     ):
2025-04-11T03:52:12.5547584Z         torch.manual_seed(123)
2025-04-11T03:52:12.5547667Z     
2025-04-11T03:52:12.5547864Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5547991Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5548079Z         dtype = torch.float16
2025-04-11T03:52:12.5548185Z         device = get_current_device()
2025-04-11T03:52:12.5548261Z     
2025-04-11T03:52:12.5548349Z         if same_context_len:
2025-04-11T03:52:12.5548631Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5548822Z         else:
2025-04-11T03:52:12.5549066Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5549176Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5549467Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5549605Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5549764Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5549768Z 
2025-04-11T03:52:12.5549956Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5550214Z ___________________ test_kv_cache_memcopy[False-16-32-64-7] ____________________
2025-04-11T03:52:12.5550218Z 
2025-04-11T03:52:12.5550370Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5550460Z same_context_len = False
2025-04-11T03:52:12.5550464Z 
2025-04-11T03:52:12.5550581Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5550709Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5550851Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5550973Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5551114Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5551211Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5551292Z         bsz: int,
2025-04-11T03:52:12.5551383Z         block_size: int,
2025-04-11T03:52:12.5551475Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5551563Z         num_kv_heads: int,
2025-04-11T03:52:12.5551662Z         same_context_len: bool,
2025-04-11T03:52:12.5551737Z     ):
2025-04-11T03:52:12.5551964Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5551970Z 
2025-04-11T03:52:12.5552123Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5552245Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5552249Z 
2025-04-11T03:52:12.5552394Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5552480Z same_context_len = False
2025-04-11T03:52:12.5552484Z 
2025-04-11T03:52:12.5552590Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5552667Z         bsz: int,
2025-04-11T03:52:12.5552758Z         block_size: int,
2025-04-11T03:52:12.5552847Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5552940Z         num_kv_heads: int,
2025-04-11T03:52:12.5553031Z         same_context_len: bool,
2025-04-11T03:52:12.5553105Z     ):
2025-04-11T03:52:12.5553203Z         torch.manual_seed(123)
2025-04-11T03:52:12.5553276Z     
2025-04-11T03:52:12.5553485Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5553605Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5553694Z         dtype = torch.float16
2025-04-11T03:52:12.5553794Z         device = get_current_device()
2025-04-11T03:52:12.5553868Z     
2025-04-11T03:52:12.5553960Z         if same_context_len:
2025-04-11T03:52:12.5554183Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5554269Z         else:
2025-04-11T03:52:12.5554502Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5554612Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5554904Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5555043Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5555327Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5555332Z 
2025-04-11T03:52:12.5555520Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5555683Z ___________________ test_kv_cache_memcopy[False-16-32-64-32] ___________________
2025-04-11T03:52:12.5555688Z 
2025-04-11T03:52:12.5555839Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5555936Z same_context_len = False
2025-04-11T03:52:12.5555940Z 
2025-04-11T03:52:12.5556051Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5556178Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5556429Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5556546Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5556698Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5556792Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5556870Z         bsz: int,
2025-04-11T03:52:12.5556964Z         block_size: int,
2025-04-11T03:52:12.5557054Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5557147Z         num_kv_heads: int,
2025-04-11T03:52:12.5557231Z         same_context_len: bool,
2025-04-11T03:52:12.5557311Z     ):
2025-04-11T03:52:12.5557533Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5557536Z 
2025-04-11T03:52:12.5557688Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5557809Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5557816Z 
2025-04-11T03:52:12.5557962Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5558057Z same_context_len = False
2025-04-11T03:52:12.5558060Z 
2025-04-11T03:52:12.5558156Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5558242Z         bsz: int,
2025-04-11T03:52:12.5558328Z         block_size: int,
2025-04-11T03:52:12.5558419Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5558509Z         num_kv_heads: int,
2025-04-11T03:52:12.5558595Z         same_context_len: bool,
2025-04-11T03:52:12.5558674Z     ):
2025-04-11T03:52:12.5558761Z         torch.manual_seed(123)
2025-04-11T03:52:12.5558841Z     
2025-04-11T03:52:12.5559039Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5559156Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5559252Z         dtype = torch.float16
2025-04-11T03:52:12.5559344Z         device = get_current_device()
2025-04-11T03:52:12.5559426Z     
2025-04-11T03:52:12.5559512Z         if same_context_len:
2025-04-11T03:52:12.5559735Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5559818Z         else:
2025-04-11T03:52:12.5560052Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5560168Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5560448Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5560592Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5560751Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5560754Z 
2025-04-11T03:52:12.5560945Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5561088Z ___________________________ test_rms_layernorm[64-2] ___________________________
2025-04-11T03:52:12.5561092Z 
2025-04-11T03:52:12.5561170Z M = 2, N = 64
2025-04-11T03:52:12.5561174Z 
2025-04-11T03:52:12.5561295Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5561518Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5561634Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5561723Z         torch.manual_seed(123)
2025-04-11T03:52:12.5561829Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5561925Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5561929Z 
2025-04-11T03:52:12.5562083Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5562205Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5562470Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
2025-04-11T03:52:12.5562578Z     with torch.cuda.device(device):
2025-04-11T03:52:12.5562812Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5562817Z 
2025-04-11T03:52:12.5562954Z self = <torch.cuda.device object at 0x7f68f05afd90>
2025-04-11T03:52:12.5562959Z 
2025-04-11T03:52:12.5563047Z     def __enter__(self):
2025-04-11T03:52:12.5563192Z >       self.prev_idx = torch.cuda._exchange_device(self.idx)
2025-04-11T03:52:12.5563317Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5563614Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5563760Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5563920Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5563924Z 
2025-04-11T03:52:12.5564179Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
2025-04-11T03:52:12.5564319Z ___________________________ test_rms_layernorm[64-4] ___________________________
2025-04-11T03:52:12.5564325Z 
2025-04-11T03:52:12.5564411Z M = 4, N = 64
2025-04-11T03:52:12.5564415Z 
2025-04-11T03:52:12.5564527Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5564651Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5564766Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5564856Z         torch.manual_seed(123)
2025-04-11T03:52:12.5564960Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5565058Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5565062Z 
2025-04-11T03:52:12.5565218Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5565330Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5565334Z 
2025-04-11T03:52:12.5565414Z device = None
2025-04-11T03:52:12.5565427Z 
2025-04-11T03:52:12.5565553Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5565712Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5565794Z     
2025-04-11T03:52:12.5565869Z         Args:
2025-04-11T03:52:12.5566053Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5566226Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5566340Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5566424Z         """
2025-04-11T03:52:12.5566508Z         _lazy_init()
2025-04-11T03:52:12.5566616Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5566723Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5566839Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5567122Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5567257Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5567426Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5567430Z 
2025-04-11T03:52:12.5567672Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5567953Z ___________________________ test_rms_layernorm[64-8] ___________________________
2025-04-11T03:52:12.5567958Z 
2025-04-11T03:52:12.5568036Z M = 8, N = 64
2025-04-11T03:52:12.5568040Z 
2025-04-11T03:52:12.5568158Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5568282Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5568391Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5568479Z         torch.manual_seed(123)
2025-04-11T03:52:12.5568572Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5568676Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5568679Z 
2025-04-11T03:52:12.5568829Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5569045Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5569049Z 
2025-04-11T03:52:12.5569135Z device = None
2025-04-11T03:52:12.5569139Z 
2025-04-11T03:52:12.5569276Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5569439Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5569517Z     
2025-04-11T03:52:12.5569608Z         Args:
2025-04-11T03:52:12.5569788Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5569972Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5570089Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5570180Z         """
2025-04-11T03:52:12.5570268Z         _lazy_init()
2025-04-11T03:52:12.5570372Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5570491Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5570612Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5570910Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5571057Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5571231Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5571235Z 
2025-04-11T03:52:12.5571478Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5571624Z __________________________ test_rms_layernorm[64-16] ___________________________
2025-04-11T03:52:12.5571628Z 
2025-04-11T03:52:12.5571716Z M = 16, N = 64
2025-04-11T03:52:12.5571720Z 
2025-04-11T03:52:12.5571833Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5571968Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5572079Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5572189Z         torch.manual_seed(123)
2025-04-11T03:52:12.5572287Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5572393Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5572396Z 
2025-04-11T03:52:12.5572569Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5572689Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5572693Z 
2025-04-11T03:52:12.5572785Z device = None
2025-04-11T03:52:12.5572788Z 
2025-04-11T03:52:12.5572917Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5573081Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5573162Z     
2025-04-11T03:52:12.5573242Z         Args:
2025-04-11T03:52:12.5573423Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5573598Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5573721Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5573800Z         """
2025-04-11T03:52:12.5573895Z         _lazy_init()
2025-04-11T03:52:12.5574104Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5574212Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5574333Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5574617Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5574767Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5574931Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5574935Z 
2025-04-11T03:52:12.5575187Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5575326Z __________________________ test_rms_layernorm[128-2] ___________________________
2025-04-11T03:52:12.5575436Z 
2025-04-11T03:52:12.5575527Z M = 2, N = 128
2025-04-11T03:52:12.5575531Z 
2025-04-11T03:52:12.5575643Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5575770Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5575883Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5575974Z         torch.manual_seed(123)
2025-04-11T03:52:12.5576074Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5576167Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5576171Z 
2025-04-11T03:52:12.5576329Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5576447Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5576451Z 
2025-04-11T03:52:12.5576538Z device = None
2025-04-11T03:52:12.5576542Z 
2025-04-11T03:52:12.5576674Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5576829Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5576915Z     
2025-04-11T03:52:12.5576990Z         Args:
2025-04-11T03:52:12.5577168Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5577337Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5577443Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5577528Z         """
2025-04-11T03:52:12.5577608Z         _lazy_init()
2025-04-11T03:52:12.5577717Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5577822Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5577936Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5578218Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5578354Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5578524Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5578528Z 
2025-04-11T03:52:12.5578768Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5578919Z __________________________ test_rms_layernorm[128-4] ___________________________
2025-04-11T03:52:12.5578922Z 
2025-04-11T03:52:12.5578999Z M = 4, N = 128
2025-04-11T03:52:12.5579003Z 
2025-04-11T03:52:12.5579119Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5579240Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5579341Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5579437Z         torch.manual_seed(123)
2025-04-11T03:52:12.5579531Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5579636Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5579640Z 
2025-04-11T03:52:12.5579787Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5579910Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5579914Z 
2025-04-11T03:52:12.5579993Z device = None
2025-04-11T03:52:12.5579996Z 
2025-04-11T03:52:12.5580226Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5580381Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5580459Z     
2025-04-11T03:52:12.5580542Z         Args:
2025-04-11T03:52:12.5580713Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5580884Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5580993Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5581071Z         """
2025-04-11T03:52:12.5581157Z         _lazy_init()
2025-04-11T03:52:12.5581257Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5581455Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5581563Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5581860Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5582004Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5582186Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5582199Z 
2025-04-11T03:52:12.5582438Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5582582Z __________________________ test_rms_layernorm[128-8] ___________________________
2025-04-11T03:52:12.5582585Z 
2025-04-11T03:52:12.5582672Z M = 8, N = 128
2025-04-11T03:52:12.5582676Z 
2025-04-11T03:52:12.5582785Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5582913Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5583017Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5583113Z         torch.manual_seed(123)
2025-04-11T03:52:12.5583205Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5583298Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5583302Z 
2025-04-11T03:52:12.5583458Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5583570Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5583574Z 
2025-04-11T03:52:12.5583663Z device = None
2025-04-11T03:52:12.5583667Z 
2025-04-11T03:52:12.5583787Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5583945Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5584021Z     
2025-04-11T03:52:12.5584097Z         Args:
2025-04-11T03:52:12.5584277Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5584441Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5584560Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5584637Z         """
2025-04-11T03:52:12.5584724Z         _lazy_init()
2025-04-11T03:52:12.5584824Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5584929Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5585046Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5585329Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5585470Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5585629Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5585633Z 
2025-04-11T03:52:12.5585876Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5586018Z __________________________ test_rms_layernorm[128-16] __________________________
2025-04-11T03:52:12.5586021Z 
2025-04-11T03:52:12.5586099Z M = 16, N = 128
2025-04-11T03:52:12.5586108Z 
2025-04-11T03:52:12.5586217Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5586467Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5586579Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5586675Z         torch.manual_seed(123)
2025-04-11T03:52:12.5586776Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5586874Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5586878Z 
2025-04-11T03:52:12.5587029Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5587148Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5587151Z 
2025-04-11T03:52:12.5587233Z device = None
2025-04-11T03:52:12.5587236Z 
2025-04-11T03:52:12.5587364Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5587611Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5587698Z     
2025-04-11T03:52:12.5587775Z         Args:
2025-04-11T03:52:12.5587953Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5588125Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5588238Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5588333Z         """
2025-04-11T03:52:12.5588466Z         _lazy_init()
2025-04-11T03:52:12.5588577Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5588681Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5588789Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5589086Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5589222Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5589393Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5589396Z 
2025-04-11T03:52:12.5589640Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5589790Z __________________________ test_rms_layernorm[512-2] ___________________________
2025-04-11T03:52:12.5589794Z 
2025-04-11T03:52:12.5589873Z M = 2, N = 512
2025-04-11T03:52:12.5589877Z 
2025-04-11T03:52:12.5589994Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5590115Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5590214Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5590313Z         torch.manual_seed(123)
2025-04-11T03:52:12.5590406Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5590506Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5590510Z 
2025-04-11T03:52:12.5590658Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5590780Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5590783Z 
2025-04-11T03:52:12.5590863Z device = None
2025-04-11T03:52:12.5590867Z 
2025-04-11T03:52:12.5590991Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5591158Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5591242Z     
2025-04-11T03:52:12.5591329Z         Args:
2025-04-11T03:52:12.5591505Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5591686Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5591793Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5591869Z         """
2025-04-11T03:52:12.5591961Z         _lazy_init()
2025-04-11T03:52:12.5592060Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5592173Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5592282Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5592571Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5592820Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5592986Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5592990Z 
2025-04-11T03:52:12.5593236Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5593375Z __________________________ test_rms_layernorm[512-4] ___________________________
2025-04-11T03:52:12.5593379Z 
2025-04-11T03:52:12.5593467Z M = 4, N = 512
2025-04-11T03:52:12.5593471Z 
2025-04-11T03:52:12.5593581Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5593709Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5593917Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5594015Z         torch.manual_seed(123)
2025-04-11T03:52:12.5594112Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5594206Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5594210Z 
2025-04-11T03:52:12.5594373Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5594485Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5594489Z 
2025-04-11T03:52:12.5594575Z device = None
2025-04-11T03:52:12.5594578Z 
2025-04-11T03:52:12.5594700Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5594862Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5594938Z     
2025-04-11T03:52:12.5595015Z         Args:
2025-04-11T03:52:12.5595191Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5595356Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5595475Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5595553Z         """
2025-04-11T03:52:12.5595634Z         _lazy_init()
2025-04-11T03:52:12.5595742Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5595844Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5595961Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5596241Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5596388Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5596549Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5596552Z 
2025-04-11T03:52:12.5596799Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5596937Z __________________________ test_rms_layernorm[512-8] ___________________________
2025-04-11T03:52:12.5596940Z 
2025-04-11T03:52:12.5597020Z M = 8, N = 512
2025-04-11T03:52:12.5597024Z 
2025-04-11T03:52:12.5597138Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5597265Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5597378Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5597466Z         torch.manual_seed(123)
2025-04-11T03:52:12.5597565Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5597656Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5597659Z 
2025-04-11T03:52:12.5597812Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5597933Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5597937Z 
2025-04-11T03:52:12.5598016Z device = None
2025-04-11T03:52:12.5598020Z 
2025-04-11T03:52:12.5598149Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5598305Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5598384Z     
2025-04-11T03:52:12.5598459Z         Args:
2025-04-11T03:52:12.5598627Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5598912Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5599034Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5599128Z         """
2025-04-11T03:52:12.5599213Z         _lazy_init()
2025-04-11T03:52:12.5599321Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5599429Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5599550Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5599840Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5599975Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5600246Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5600250Z 
2025-04-11T03:52:12.5600493Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5600643Z __________________________ test_rms_layernorm[512-16] __________________________
2025-04-11T03:52:12.5600646Z 
2025-04-11T03:52:12.5600723Z M = 16, N = 512
2025-04-11T03:52:12.5600728Z 
2025-04-11T03:52:12.5600842Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5600961Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5601061Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5601159Z         torch.manual_seed(123)
2025-04-11T03:52:12.5601253Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5601354Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5601358Z 
2025-04-11T03:52:12.5601507Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5601633Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5601637Z 
2025-04-11T03:52:12.5601720Z device = None
2025-04-11T03:52:12.5601724Z 
2025-04-11T03:52:12.5601847Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5602007Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5602083Z     
2025-04-11T03:52:12.5602166Z         Args:
2025-04-11T03:52:12.5602339Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5602513Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5602622Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5602696Z         """
2025-04-11T03:52:12.5602788Z         _lazy_init()
2025-04-11T03:52:12.5602884Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5602995Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5603103Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5603384Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5603531Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5603691Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5603695Z 
2025-04-11T03:52:12.5603941Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5604083Z __________________________ test_rms_layernorm[5120-2] __________________________
2025-04-11T03:52:12.5604087Z 
2025-04-11T03:52:12.5604173Z M = 2, N = 5120
2025-04-11T03:52:12.5604177Z 
2025-04-11T03:52:12.5604286Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5604412Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5604518Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5604606Z         torch.manual_seed(123)
2025-04-11T03:52:12.5604707Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5604799Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5604803Z 
2025-04-11T03:52:12.5605072Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5605190Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5605194Z 
2025-04-11T03:52:12.5605284Z device = None
2025-04-11T03:52:12.5605288Z 
2025-04-11T03:52:12.5605408Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5605568Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5605643Z     
2025-04-11T03:52:12.5605722Z         Args:
2025-04-11T03:52:12.5605900Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5606067Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5606277Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5606354Z         """
2025-04-11T03:52:12.5606436Z         _lazy_init()
2025-04-11T03:52:12.5606545Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5606650Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5606765Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5607055Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5607195Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5607353Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5607356Z 
2025-04-11T03:52:12.5607597Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5607747Z __________________________ test_rms_layernorm[5120-4] __________________________
2025-04-11T03:52:12.5607750Z 
2025-04-11T03:52:12.5607828Z M = 4, N = 5120
2025-04-11T03:52:12.5607833Z 
2025-04-11T03:52:12.5607951Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5608074Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5608190Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5608288Z         torch.manual_seed(123)
2025-04-11T03:52:12.5608396Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5608493Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5608497Z 
2025-04-11T03:52:12.5608648Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5608771Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5608774Z 
2025-04-11T03:52:12.5608854Z device = None
2025-04-11T03:52:12.5608859Z 
2025-04-11T03:52:12.5608987Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5609139Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5609220Z     
2025-04-11T03:52:12.5609296Z         Args:
2025-04-11T03:52:12.5609467Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5609639Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5609749Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5609831Z         """
2025-04-11T03:52:12.5609913Z         _lazy_init()
2025-04-11T03:52:12.5610019Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5610122Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5610230Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5610518Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5610653Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5610821Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5610824Z 
2025-04-11T03:52:12.5611175Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5611324Z __________________________ test_rms_layernorm[5120-8] __________________________
2025-04-11T03:52:12.5611328Z 
2025-04-11T03:52:12.5611406Z M = 8, N = 5120
2025-04-11T03:52:12.5611410Z 
2025-04-11T03:52:12.5611530Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5611651Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5611756Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5611852Z         torch.manual_seed(123)
2025-04-11T03:52:12.5611944Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5612041Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5612045Z 
2025-04-11T03:52:12.5612195Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5612412Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5612423Z 
2025-04-11T03:52:12.5612504Z device = None
2025-04-11T03:52:12.5612508Z 
2025-04-11T03:52:12.5612635Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5612800Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5612894Z     
2025-04-11T03:52:12.5613001Z         Args:
2025-04-11T03:52:12.5613173Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5613339Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5613455Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5613531Z         """
2025-04-11T03:52:12.5613619Z         _lazy_init()
2025-04-11T03:52:12.5613718Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5613833Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5613943Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5614232Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5614374Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5614537Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5614541Z 
2025-04-11T03:52:12.5614793Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5614937Z _________________________ test_rms_layernorm[5120-16] __________________________
2025-04-11T03:52:12.5614941Z 
2025-04-11T03:52:12.5615028Z M = 16, N = 5120
2025-04-11T03:52:12.5615032Z 
2025-04-11T03:52:12.5615145Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5615277Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5615386Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5615476Z         torch.manual_seed(123)
2025-04-11T03:52:12.5615581Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5615678Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5615685Z 
2025-04-11T03:52:12.5615847Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5615961Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5615965Z 
2025-04-11T03:52:12.5616052Z device = None
2025-04-11T03:52:12.5616055Z 
2025-04-11T03:52:12.5616176Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5616329Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5616409Z     
2025-04-11T03:52:12.5616484Z         Args:
2025-04-11T03:52:12.5616661Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5616830Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5616944Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5617020Z         """
2025-04-11T03:52:12.5617102Z         _lazy_init()
2025-04-11T03:52:12.5617324Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5617438Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5617555Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5617846Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5617997Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5618157Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5618161Z 
2025-04-11T03:52:12.5618399Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5618657Z ____________________ test_rotary_emb[dtype0-64-16-32-64-4] _____________________
2025-04-11T03:52:12.5618661Z 
2025-04-11T03:52:12.5618816Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 16, D = 64, dtype = torch.float16
2025-04-11T03:52:12.5618820Z 
2025-04-11T03:52:12.5618942Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.5619052Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T03:52:12.5619161Z     @pytest.mark.parametrize("H", [32])
2025-04-11T03:52:12.5619267Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T03:52:12.5619375Z     @pytest.mark.parametrize("D", [64])
2025-04-11T03:52:12.5619547Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T03:52:12.5619685Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T03:52:12.5619787Z         torch.manual_seed(10)
2025-04-11T03:52:12.5619893Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T03:52:12.5620012Z         # our crafted op equals to Transformers
2025-04-11T03:52:12.5620149Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5620284Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5620367Z     
2025-04-11T03:52:12.5620541Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T03:52:12.5620628Z     
2025-04-11T03:52:12.5620731Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T03:52:12.5620810Z     
2025-04-11T03:52:12.5620909Z         cos, sin = emb(x0, position_ids)
2025-04-11T03:52:12.5621033Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T03:52:12.5621146Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5621247Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5621347Z         cos_2 = cos[:, : D // 2]
2025-04-11T03:52:12.5621433Z         sin_2 = sin[:, : D // 2]
2025-04-11T03:52:12.5621556Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5621710Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T03:52:12.5621923Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T03:52:12.5622056Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T03:52:12.5622132Z     
2025-04-11T03:52:12.5622223Z         # create data
2025-04-11T03:52:12.5622314Z         block_size = 32
2025-04-11T03:52:12.5622483Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T03:52:12.5622584Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5622735Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.5622851Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5623143Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5623285Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5623451Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5623455Z 
2025-04-11T03:52:12.5623650Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T03:52:12.5623913Z ____________________ test_rotary_emb[dtype0-64-32-32-64-4] _____________________
2025-04-11T03:52:12.5623918Z 
2025-04-11T03:52:12.5624070Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 32, D = 64, dtype = torch.float16
2025-04-11T03:52:12.5624075Z 
2025-04-11T03:52:12.5624192Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.5624303Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T03:52:12.5624405Z     @pytest.mark.parametrize("H", [32])
2025-04-11T03:52:12.5624506Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T03:52:12.5624608Z     @pytest.mark.parametrize("D", [64])
2025-04-11T03:52:12.5624768Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T03:52:12.5624910Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T03:52:12.5625111Z         torch.manual_seed(10)
2025-04-11T03:52:12.5625207Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T03:52:12.5625325Z         # our crafted op equals to Transformers
2025-04-11T03:52:12.5625464Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5625590Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5625668Z     
2025-04-11T03:52:12.5625839Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T03:52:12.5625919Z     
2025-04-11T03:52:12.5626019Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T03:52:12.5626094Z     
2025-04-11T03:52:12.5626191Z         cos, sin = emb(x0, position_ids)
2025-04-11T03:52:12.5626312Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T03:52:12.5626417Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5626512Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5626604Z         cos_2 = cos[:, : D // 2]
2025-04-11T03:52:12.5626687Z         sin_2 = sin[:, : D // 2]
2025-04-11T03:52:12.5626808Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5626949Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T03:52:12.5627164Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T03:52:12.5627316Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T03:52:12.5627387Z     
2025-04-11T03:52:12.5627472Z         # create data
2025-04-11T03:52:12.5627556Z         block_size = 32
2025-04-11T03:52:12.5627715Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T03:52:12.5627821Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5627960Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.5628076Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5628363Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5628535Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5628705Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5628709Z 
2025-04-11T03:52:12.5628902Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T03:52:12.5629058Z ____________________ test_rotary_emb[dtype1-64-16-32-64-4] _____________________
2025-04-11T03:52:12.5629062Z 
2025-04-11T03:52:12.5629209Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 16, D = 64, dtype = torch.float32
2025-04-11T03:52:12.5629213Z 
2025-04-11T03:52:12.5629327Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.5629431Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T03:52:12.5629535Z     @pytest.mark.parametrize("H", [32])
2025-04-11T03:52:12.5629638Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T03:52:12.5629739Z     @pytest.mark.parametrize("D", [64])
2025-04-11T03:52:12.5629902Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T03:52:12.5630181Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T03:52:12.5630279Z         torch.manual_seed(10)
2025-04-11T03:52:12.5630376Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T03:52:12.5630482Z         # our crafted op equals to Transformers
2025-04-11T03:52:12.5630610Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5630741Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5630827Z     
2025-04-11T03:52:12.5631001Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T03:52:12.5631086Z     
2025-04-11T03:52:12.5631183Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T03:52:12.5631268Z     
2025-04-11T03:52:12.5631363Z         cos, sin = emb(x0, position_ids)
2025-04-11T03:52:12.5631643Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T03:52:12.5631755Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5631862Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5631955Z         cos_2 = cos[:, : D // 2]
2025-04-11T03:52:12.5632040Z         sin_2 = sin[:, : D // 2]
2025-04-11T03:52:12.5632159Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5632327Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T03:52:12.5632543Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T03:52:12.5632678Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T03:52:12.5632752Z     
2025-04-11T03:52:12.5632857Z         # create data
2025-04-11T03:52:12.5632945Z         block_size = 32
2025-04-11T03:52:12.5633105Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T03:52:12.5633207Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5633345Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.5633454Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5633738Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5633874Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5634032Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5634036Z 
2025-04-11T03:52:12.5634230Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T03:52:12.5634373Z ____________________ test_rotary_emb[dtype1-64-32-32-64-4] _____________________
2025-04-11T03:52:12.5634377Z 
2025-04-11T03:52:12.5634524Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 32, D = 64, dtype = torch.float32
2025-04-11T03:52:12.5634537Z 
2025-04-11T03:52:12.5634645Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.5634750Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T03:52:12.5634853Z     @pytest.mark.parametrize("H", [32])
2025-04-11T03:52:12.5634955Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T03:52:12.5635060Z     @pytest.mark.parametrize("D", [64])
2025-04-11T03:52:12.5635227Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T03:52:12.5635368Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T03:52:12.5635463Z         torch.manual_seed(10)
2025-04-11T03:52:12.5635578Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T03:52:12.5635701Z         # our crafted op equals to Transformers
2025-04-11T03:52:12.5635824Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5635949Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5636021Z     
2025-04-11T03:52:12.5636188Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T03:52:12.5636267Z     
2025-04-11T03:52:12.5636363Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T03:52:12.5636441Z     
2025-04-11T03:52:12.5636534Z         cos, sin = emb(x0, position_ids)
2025-04-11T03:52:12.5636844Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T03:52:12.5636951Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5637047Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5637133Z         cos_2 = cos[:, : D // 2]
2025-04-11T03:52:12.5637213Z         sin_2 = sin[:, : D // 2]
2025-04-11T03:52:12.5637335Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5637461Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T03:52:12.5637669Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T03:52:12.5637797Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T03:52:12.5637973Z     
2025-04-11T03:52:12.5638059Z         # create data
2025-04-11T03:52:12.5638142Z         block_size = 32
2025-04-11T03:52:12.5638303Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T03:52:12.5638404Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5638537Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.5638647Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5638928Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5639063Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5639219Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5639223Z 
2025-04-11T03:52:12.5639417Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T03:52:12.5639565Z _____________________ test_silu_and_mul[dtype0-11008-64-2] _____________________
2025-04-11T03:52:12.5639569Z 
2025-04-11T03:52:12.5639711Z SHAPE_X = 2, SHAPE_Y = 64, SHAPE_Z = 11008, dtype = torch.float32
2025-04-11T03:52:12.5639720Z 
2025-04-11T03:52:12.5639832Z     @pytest.mark.parametrize("SHAPE_X", [2])
2025-04-11T03:52:12.5639938Z     @pytest.mark.parametrize("SHAPE_Y", [64])
2025-04-11T03:52:12.5640053Z     @pytest.mark.parametrize("SHAPE_Z", [11008])
2025-04-11T03:52:12.5640213Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16])
2025-04-11T03:52:12.5640351Z     def test_silu_and_mul(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype):
2025-04-11T03:52:12.5640439Z         torch.manual_seed(5)
2025-04-11T03:52:12.5640534Z         device = get_current_device()
2025-04-11T03:52:12.5640715Z >       ref_input = torch.randn(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype=dtype, device=device)
2025-04-11T03:52:12.5640818Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5641109Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5641240Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5641402Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5641405Z 
2025-04-11T03:52:12.5641574Z tests/test_infer/test_kernels/cuda/test_silu_and_mul.py:17: RuntimeError
2025-04-11T03:52:12.5641721Z _____________________ test_silu_and_mul[dtype1-11008-64-2] _____________________
2025-04-11T03:52:12.5641725Z 
2025-04-11T03:52:12.5641863Z SHAPE_X = 2, SHAPE_Y = 64, SHAPE_Z = 11008, dtype = torch.float16
2025-04-11T03:52:12.5641867Z 
2025-04-11T03:52:12.5641983Z     @pytest.mark.parametrize("SHAPE_X", [2])
2025-04-11T03:52:12.5642097Z     @pytest.mark.parametrize("SHAPE_Y", [64])
2025-04-11T03:52:12.5642210Z     @pytest.mark.parametrize("SHAPE_Z", [11008])
2025-04-11T03:52:12.5642376Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16])
2025-04-11T03:52:12.5642516Z     def test_silu_and_mul(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype):
2025-04-11T03:52:12.5642609Z         torch.manual_seed(5)
2025-04-11T03:52:12.5642705Z         device = get_current_device()
2025-04-11T03:52:12.5642983Z >       ref_input = torch.randn(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype=dtype, device=device)
2025-04-11T03:52:12.5643098Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5643379Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5643529Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5643687Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5643691Z 
2025-04-11T03:52:12.5643867Z tests/test_infer/test_kernels/cuda/test_silu_and_mul.py:17: RuntimeError
2025-04-11T03:52:12.5644036Z _____________ test_context_attention[True-False-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.5644160Z 
2025-04-11T03:52:12.5644323Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5644477Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5644567Z use_new_kcache_layout = True
2025-04-11T03:52:12.5644573Z 
2025-04-11T03:52:12.5644789Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5644894Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5645022Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5645161Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5645286Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5645404Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5645540Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5645680Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5645838Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5645935Z     def test_context_attention(
2025-04-11T03:52:12.5646013Z         bsz: int,
2025-04-11T03:52:12.5646099Z         block_size: int,
2025-04-11T03:52:12.5646201Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5646285Z         num_attn_heads: int,
2025-04-11T03:52:12.5646380Z         kv_group_num: int,
2025-04-11T03:52:12.5646468Z         same_context_len: bool,
2025-04-11T03:52:12.5646559Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5646651Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5646724Z     ):
2025-04-11T03:52:12.5646848Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5647045Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5647234Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5647419Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5647602Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5647680Z             return
2025-04-11T03:52:12.5647765Z     
2025-04-11T03:52:12.5647859Z         torch.manual_seed(123)
2025-04-11T03:52:12.5647957Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5648053Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5648147Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5648150Z 
2025-04-11T03:52:12.5648321Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5648438Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5648685Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
2025-04-11T03:52:12.5648785Z     with torch.cuda.device(device):
2025-04-11T03:52:12.5648898Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5648902Z 
2025-04-11T03:52:12.5649029Z self = <torch.cuda.device object at 0x7f68f0220610>
2025-04-11T03:52:12.5649034Z 
2025-04-11T03:52:12.5649117Z     def __enter__(self):
2025-04-11T03:52:12.5649428Z >       self.prev_idx = torch.cuda._exchange_device(self.idx)
2025-04-11T03:52:12.5649541Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5649821Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5649960Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5650119Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5650123Z 
2025-04-11T03:52:12.5650367Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
2025-04-11T03:52:12.5650534Z _____________ test_context_attention[True-False-True-1-16-8-16-32] _____________
2025-04-11T03:52:12.5650665Z 
2025-04-11T03:52:12.5650828Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5650978Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5651076Z use_new_kcache_layout = True
2025-04-11T03:52:12.5651080Z 
2025-04-11T03:52:12.5651281Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5651390Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5651517Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5651656Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5651779Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5651893Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5652037Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5652173Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5652345Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5652468Z     def test_context_attention(
2025-04-11T03:52:12.5652548Z         bsz: int,
2025-04-11T03:52:12.5652652Z         block_size: int,
2025-04-11T03:52:12.5652748Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5652833Z         num_attn_heads: int,
2025-04-11T03:52:12.5652925Z         kv_group_num: int,
2025-04-11T03:52:12.5653011Z         same_context_len: bool,
2025-04-11T03:52:12.5653099Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5653188Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5653263Z     ):
2025-04-11T03:52:12.5653380Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5653578Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5653766Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5653942Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5654114Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5654197Z             return
2025-04-11T03:52:12.5654275Z     
2025-04-11T03:52:12.5654364Z         torch.manual_seed(123)
2025-04-11T03:52:12.5654464Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5654563Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5654657Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5654660Z 
2025-04-11T03:52:12.5654834Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5654948Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5654951Z 
2025-04-11T03:52:12.5655039Z device = None
2025-04-11T03:52:12.5655042Z 
2025-04-11T03:52:12.5655165Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5655324Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5655406Z     
2025-04-11T03:52:12.5655481Z         Args:
2025-04-11T03:52:12.5655771Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5655945Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5656063Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5656140Z         """
2025-04-11T03:52:12.5656221Z         _lazy_init()
2025-04-11T03:52:12.5656327Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5656432Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5656549Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5656835Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5656973Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5657257Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5657262Z 
2025-04-11T03:52:12.5657509Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5657687Z _____________ test_context_attention[True-False-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.5657691Z 
2025-04-11T03:52:12.5657841Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5657995Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5658084Z use_new_kcache_layout = True
2025-04-11T03:52:12.5658087Z 
2025-04-11T03:52:12.5658293Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5658401Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5658521Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5658667Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5658784Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5658905Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5659046Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5659184Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5659337Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5659428Z     def test_context_attention(
2025-04-11T03:52:12.5659513Z         bsz: int,
2025-04-11T03:52:12.5659595Z         block_size: int,
2025-04-11T03:52:12.5659690Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5659778Z         num_attn_heads: int,
2025-04-11T03:52:12.5659864Z         kv_group_num: int,
2025-04-11T03:52:12.5659958Z         same_context_len: bool,
2025-04-11T03:52:12.5660042Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5660143Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5660217Z     ):
2025-04-11T03:52:12.5660336Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5660525Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5660711Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5660889Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5661051Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5661136Z             return
2025-04-11T03:52:12.5661212Z     
2025-04-11T03:52:12.5661305Z         torch.manual_seed(123)
2025-04-11T03:52:12.5661407Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5661498Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5661599Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5661605Z 
2025-04-11T03:52:12.5661774Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5661892Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5661896Z 
2025-04-11T03:52:12.5661976Z device = None
2025-04-11T03:52:12.5662083Z 
2025-04-11T03:52:12.5662221Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5662377Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5662456Z     
2025-04-11T03:52:12.5662542Z         Args:
2025-04-11T03:52:12.5662712Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5662890Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5662999Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5663083Z         """
2025-04-11T03:52:12.5663164Z         _lazy_init()
2025-04-11T03:52:12.5663389Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5663506Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5663617Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5663914Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5664056Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5664226Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5664230Z 
2025-04-11T03:52:12.5664472Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5664645Z _____________ test_context_attention[True-False-True-1-16-8-32-32] _____________
2025-04-11T03:52:12.5664648Z 
2025-04-11T03:52:12.5664808Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5664955Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5665054Z use_new_kcache_layout = True
2025-04-11T03:52:12.5665059Z 
2025-04-11T03:52:12.5665260Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5665378Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5665499Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5665641Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5665766Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5665881Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5666025Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5666161Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5666318Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5666407Z     def test_context_attention(
2025-04-11T03:52:12.5666488Z         bsz: int,
2025-04-11T03:52:12.5666577Z         block_size: int,
2025-04-11T03:52:12.5666668Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5666757Z         num_attn_heads: int,
2025-04-11T03:52:12.5666842Z         kv_group_num: int,
2025-04-11T03:52:12.5666939Z         same_context_len: bool,
2025-04-11T03:52:12.5667025Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5667119Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5667199Z     ):
2025-04-11T03:52:12.5667310Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5667530Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5667718Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5667890Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5668058Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5668137Z             return
2025-04-11T03:52:12.5668214Z     
2025-04-11T03:52:12.5668300Z         torch.manual_seed(123)
2025-04-11T03:52:12.5668403Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5668654Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5668748Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5668751Z 
2025-04-11T03:52:12.5668925Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5669036Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5669040Z 
2025-04-11T03:52:12.5669125Z device = None
2025-04-11T03:52:12.5669128Z 
2025-04-11T03:52:12.5669246Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5669400Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5669471Z     
2025-04-11T03:52:12.5669545Z         Args:
2025-04-11T03:52:12.5669720Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5669994Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5670106Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5670182Z         """
2025-04-11T03:52:12.5670267Z         _lazy_init()
2025-04-11T03:52:12.5670364Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5670466Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5670577Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5670859Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5671004Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5671161Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5671165Z 
2025-04-11T03:52:12.5671408Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5671574Z _____________ test_context_attention[True-False-True-1-16-16-16-7] _____________
2025-04-11T03:52:12.5671578Z 
2025-04-11T03:52:12.5671735Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5671887Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5671983Z use_new_kcache_layout = True
2025-04-11T03:52:12.5671987Z 
2025-04-11T03:52:12.5672189Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5672296Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5672417Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5672557Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5672677Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5672792Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5672931Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5673069Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5673225Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5673319Z     def test_context_attention(
2025-04-11T03:52:12.5673394Z         bsz: int,
2025-04-11T03:52:12.5673477Z         block_size: int,
2025-04-11T03:52:12.5673573Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5673657Z         num_attn_heads: int,
2025-04-11T03:52:12.5673743Z         kv_group_num: int,
2025-04-11T03:52:12.5673827Z         same_context_len: bool,
2025-04-11T03:52:12.5673918Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5674007Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5674082Z     ):
2025-04-11T03:52:12.5674199Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5674392Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5674577Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5674855Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5675027Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5675103Z             return
2025-04-11T03:52:12.5675178Z     
2025-04-11T03:52:12.5675270Z         torch.manual_seed(123)
2025-04-11T03:52:12.5675371Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5675466Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5675558Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5675561Z 
2025-04-11T03:52:12.5675730Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5675856Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5675955Z 
2025-04-11T03:52:12.5676037Z device = None
2025-04-11T03:52:12.5676041Z 
2025-04-11T03:52:12.5676173Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5676323Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5676402Z     
2025-04-11T03:52:12.5676483Z         Args:
2025-04-11T03:52:12.5676655Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5676835Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5676941Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5677025Z         """
2025-04-11T03:52:12.5677106Z         _lazy_init()
2025-04-11T03:52:12.5677207Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5677311Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5677417Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5677708Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5677849Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5678017Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5678021Z 
2025-04-11T03:52:12.5678260Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5678438Z ____________ test_context_attention[True-False-True-1-16-16-16-32] _____________
2025-04-11T03:52:12.5678442Z 
2025-04-11T03:52:12.5678593Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5678743Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5678834Z use_new_kcache_layout = True
2025-04-11T03:52:12.5678837Z 
2025-04-11T03:52:12.5679037Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5679151Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5679270Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5679413Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5679539Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5679660Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5679797Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5679934Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5680091Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5680181Z     def test_context_attention(
2025-04-11T03:52:12.5680262Z         bsz: int,
2025-04-11T03:52:12.5680343Z         block_size: int,
2025-04-11T03:52:12.5680435Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5680522Z         num_attn_heads: int,
2025-04-11T03:52:12.5680608Z         kv_group_num: int,
2025-04-11T03:52:12.5680697Z         same_context_len: bool,
2025-04-11T03:52:12.5680783Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5680877Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5680951Z     ):
2025-04-11T03:52:12.5681182Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5681399Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5681587Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5681761Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5681925Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5682002Z             return
2025-04-11T03:52:12.5682074Z     
2025-04-11T03:52:12.5682158Z         torch.manual_seed(123)
2025-04-11T03:52:12.5682262Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5682463Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5682559Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5682562Z 
2025-04-11T03:52:12.5682730Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5682850Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5682854Z 
2025-04-11T03:52:12.5682932Z device = None
2025-04-11T03:52:12.5682936Z 
2025-04-11T03:52:12.5683055Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5683210Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5683283Z     
2025-04-11T03:52:12.5683362Z         Args:
2025-04-11T03:52:12.5683528Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5683701Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5683809Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5683887Z         """
2025-04-11T03:52:12.5683971Z         _lazy_init()
2025-04-11T03:52:12.5684068Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5684174Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5684290Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5684572Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5684714Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5684877Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5684881Z 
2025-04-11T03:52:12.5685125Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5685291Z _____________ test_context_attention[True-False-True-1-16-16-32-7] _____________
2025-04-11T03:52:12.5685295Z 
2025-04-11T03:52:12.5685454Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5685601Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5685695Z use_new_kcache_layout = True
2025-04-11T03:52:12.5685699Z 
2025-04-11T03:52:12.5685901Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5686012Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5686138Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5686281Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5686401Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5686514Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5686653Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5686791Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5686946Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5687041Z     def test_context_attention(
2025-04-11T03:52:12.5687121Z         bsz: int,
2025-04-11T03:52:12.5687211Z         block_size: int,
2025-04-11T03:52:12.5687410Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5687503Z         num_attn_heads: int,
2025-04-11T03:52:12.5687591Z         kv_group_num: int,
2025-04-11T03:52:12.5687680Z         same_context_len: bool,
2025-04-11T03:52:12.5687771Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5687861Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5687941Z     ):
2025-04-11T03:52:12.5688054Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5688244Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5688431Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5688603Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5688907Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5688984Z             return
2025-04-11T03:52:12.5689060Z     
2025-04-11T03:52:12.5689152Z         torch.manual_seed(123)
2025-04-11T03:52:12.5689251Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5689347Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5689441Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5689445Z 
2025-04-11T03:52:12.5689619Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5689732Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5689736Z 
2025-04-11T03:52:12.5689818Z device = None
2025-04-11T03:52:12.5689822Z 
2025-04-11T03:52:12.5689941Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5690092Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5690169Z     
2025-04-11T03:52:12.5690244Z         Args:
2025-04-11T03:52:12.5690419Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5690586Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5690701Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5690781Z         """
2025-04-11T03:52:12.5690860Z         _lazy_init()
2025-04-11T03:52:12.5690967Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5691071Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5691180Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5691459Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5691599Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5691762Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5691766Z 
2025-04-11T03:52:12.5692006Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5692186Z ____________ test_context_attention[True-False-True-1-16-16-32-32] _____________
2025-04-11T03:52:12.5692190Z 
2025-04-11T03:52:12.5692345Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5692496Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5692585Z use_new_kcache_layout = True
2025-04-11T03:52:12.5692589Z 
2025-04-11T03:52:12.5692794Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5692898Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5693022Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5693158Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5693279Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5693396Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5693533Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5693784Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5693940Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5694029Z     def test_context_attention(
2025-04-11T03:52:12.5694112Z         bsz: int,
2025-04-11T03:52:12.5694198Z         block_size: int,
2025-04-11T03:52:12.5694294Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5694378Z         num_attn_heads: int,
2025-04-11T03:52:12.5694470Z         kv_group_num: int,
2025-04-11T03:52:12.5694557Z         same_context_len: bool,
2025-04-11T03:52:12.5694644Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5694739Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5694906Z     ):
2025-04-11T03:52:12.5695023Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5695219Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5695406Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5695585Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5695752Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5695834Z             return
2025-04-11T03:52:12.5695905Z     
2025-04-11T03:52:12.5695999Z         torch.manual_seed(123)
2025-04-11T03:52:12.5696100Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5696191Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5696295Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5696299Z 
2025-04-11T03:52:12.5696469Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5696603Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5696606Z 
2025-04-11T03:52:12.5696685Z device = None
2025-04-11T03:52:12.5696688Z 
2025-04-11T03:52:12.5696811Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5696965Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5697043Z     
2025-04-11T03:52:12.5697123Z         Args:
2025-04-11T03:52:12.5697293Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5697466Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5697572Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5697655Z         """
2025-04-11T03:52:12.5697733Z         _lazy_init()
2025-04-11T03:52:12.5697832Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5697947Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5698052Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5698349Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5698493Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5698661Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5698665Z 
2025-04-11T03:52:12.5698905Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5699073Z _____________ test_context_attention[True-False-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.5699084Z 
2025-04-11T03:52:12.5699233Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5699382Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5699477Z use_new_kcache_layout = True
2025-04-11T03:52:12.5699485Z 
2025-04-11T03:52:12.5699689Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5699800Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5700024Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5700175Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5700297Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5700413Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5700556Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5700690Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5700846Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5700936Z     def test_context_attention(
2025-04-11T03:52:12.5701017Z         bsz: int,
2025-04-11T03:52:12.5701101Z         block_size: int,
2025-04-11T03:52:12.5701298Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5701388Z         num_attn_heads: int,
2025-04-11T03:52:12.5701472Z         kv_group_num: int,
2025-04-11T03:52:12.5701565Z         same_context_len: bool,
2025-04-11T03:52:12.5701654Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5701744Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5701826Z     ):
2025-04-11T03:52:12.5701939Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5702135Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5702317Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5702489Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5702649Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5702725Z             return
2025-04-11T03:52:12.5702804Z     
2025-04-11T03:52:12.5702888Z         torch.manual_seed(123)
2025-04-11T03:52:12.5702992Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5703082Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5703176Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5703182Z 
2025-04-11T03:52:12.5703356Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5703469Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5703472Z 
2025-04-11T03:52:12.5703557Z device = None
2025-04-11T03:52:12.5703561Z 
2025-04-11T03:52:12.5703677Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5703832Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5703905Z     
2025-04-11T03:52:12.5703979Z         Args:
2025-04-11T03:52:12.5704153Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5704323Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5704437Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5704514Z         """
2025-04-11T03:52:12.5704598Z         _lazy_init()
2025-04-11T03:52:12.5704698Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5704803Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5704916Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5705198Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5705339Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5705499Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5705503Z 
2025-04-11T03:52:12.5705745Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5705915Z _____________ test_context_attention[True-False-True-4-16-8-16-32] _____________
2025-04-11T03:52:12.5705918Z 
2025-04-11T03:52:12.5706088Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5706361Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5706458Z use_new_kcache_layout = True
2025-04-11T03:52:12.5706463Z 
2025-04-11T03:52:12.5706674Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5706781Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5706908Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5707050Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5707173Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5707289Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5707425Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5707659Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5707816Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5707917Z     def test_context_attention(
2025-04-11T03:52:12.5708006Z         bsz: int,
2025-04-11T03:52:12.5708099Z         block_size: int,
2025-04-11T03:52:12.5708197Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5708285Z         num_attn_heads: int,
2025-04-11T03:52:12.5708379Z         kv_group_num: int,
2025-04-11T03:52:12.5708507Z         same_context_len: bool,
2025-04-11T03:52:12.5708602Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5708695Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5708769Z     ):
2025-04-11T03:52:12.5708888Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5709080Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5709272Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5709451Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5709625Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5709701Z             return
2025-04-11T03:52:12.5709774Z     
2025-04-11T03:52:12.5709873Z         torch.manual_seed(123)
2025-04-11T03:52:12.5709974Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5710071Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5710166Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5710170Z 
2025-04-11T03:52:12.5710346Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5710460Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5710465Z 
2025-04-11T03:52:12.5710544Z device = None
2025-04-11T03:52:12.5710553Z 
2025-04-11T03:52:12.5710675Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5710825Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5710904Z     
2025-04-11T03:52:12.5710979Z         Args:
2025-04-11T03:52:12.5711165Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5711338Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5711449Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5711531Z         """
2025-04-11T03:52:12.5711613Z         _lazy_init()
2025-04-11T03:52:12.5711717Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5711820Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5711927Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5712221Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5712369Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5712531Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5712535Z 
2025-04-11T03:52:12.5712895Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5713072Z _____________ test_context_attention[True-False-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.5713076Z 
2025-04-11T03:52:12.5713225Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5713376Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5713490Z use_new_kcache_layout = True
2025-04-11T03:52:12.5713494Z 
2025-04-11T03:52:12.5713720Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5713833Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5714066Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5714210Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5714329Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5714456Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5714598Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5714731Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5714887Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5714978Z     def test_context_attention(
2025-04-11T03:52:12.5715061Z         bsz: int,
2025-04-11T03:52:12.5715143Z         block_size: int,
2025-04-11T03:52:12.5715240Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5715323Z         num_attn_heads: int,
2025-04-11T03:52:12.5715406Z         kv_group_num: int,
2025-04-11T03:52:12.5715498Z         same_context_len: bool,
2025-04-11T03:52:12.5715586Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5715678Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5715751Z     ):
2025-04-11T03:52:12.5715872Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5716083Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5716272Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5716453Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5716615Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5716695Z             return
2025-04-11T03:52:12.5716768Z     
2025-04-11T03:52:12.5716853Z         torch.manual_seed(123)
2025-04-11T03:52:12.5716957Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5717047Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5717146Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5717149Z 
2025-04-11T03:52:12.5717317Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5717432Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5717438Z 
2025-04-11T03:52:12.5717517Z device = None
2025-04-11T03:52:12.5717520Z 
2025-04-11T03:52:12.5717637Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5717793Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5717865Z     
2025-04-11T03:52:12.5717943Z         Args:
2025-04-11T03:52:12.5718109Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5718277Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5718383Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5718460Z         """
2025-04-11T03:52:12.5718546Z         _lazy_init()
2025-04-11T03:52:12.5718643Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5718748Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5718855Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5719243Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5719385Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5719545Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5719549Z 
2025-04-11T03:52:12.5719794Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5719959Z _____________ test_context_attention[True-False-True-4-16-8-32-32] _____________
2025-04-11T03:52:12.5719963Z 
2025-04-11T03:52:12.5720120Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5720359Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5720457Z use_new_kcache_layout = True
2025-04-11T03:52:12.5720461Z 
2025-04-11T03:52:12.5720675Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5720798Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5720916Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5721068Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5721192Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5721313Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5721454Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5721591Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5721748Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5721842Z     def test_context_attention(
2025-04-11T03:52:12.5721918Z         bsz: int,
2025-04-11T03:52:12.5722005Z         block_size: int,
2025-04-11T03:52:12.5722094Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5722181Z         num_attn_heads: int,
2025-04-11T03:52:12.5722266Z         kv_group_num: int,
2025-04-11T03:52:12.5722353Z         same_context_len: bool,
2025-04-11T03:52:12.5722443Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5722532Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5722606Z     ):
2025-04-11T03:52:12.5722718Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5722911Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5723097Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5723266Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5723437Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5723512Z             return
2025-04-11T03:52:12.5723588Z     
2025-04-11T03:52:12.5723673Z         torch.manual_seed(123)
2025-04-11T03:52:12.5723772Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5723868Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5723959Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5723962Z 
2025-04-11T03:52:12.5724138Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5724250Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5724254Z 
2025-04-11T03:52:12.5724334Z device = None
2025-04-11T03:52:12.5724338Z 
2025-04-11T03:52:12.5724456Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5724612Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5724687Z     
2025-04-11T03:52:12.5724760Z         Args:
2025-04-11T03:52:12.5724934Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5725100Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5725321Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5725407Z         """
2025-04-11T03:52:12.5725497Z         _lazy_init()
2025-04-11T03:52:12.5725602Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5725710Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5725822Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5726115Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5726259Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5726419Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5726541Z 
2025-04-11T03:52:12.5726777Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5726948Z _____________ test_context_attention[True-False-True-4-16-16-16-7] _____________
2025-04-11T03:52:12.5726954Z 
2025-04-11T03:52:12.5727106Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5727254Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5727343Z use_new_kcache_layout = True
2025-04-11T03:52:12.5727347Z 
2025-04-11T03:52:12.5727551Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5727659Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5727780Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5727919Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5728035Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5728157Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5728291Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5728432Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5728587Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5728680Z     def test_context_attention(
2025-04-11T03:52:12.5728759Z         bsz: int,
2025-04-11T03:52:12.5728840Z         block_size: int,
2025-04-11T03:52:12.5728937Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5729023Z         num_attn_heads: int,
2025-04-11T03:52:12.5729110Z         kv_group_num: int,
2025-04-11T03:52:12.5729197Z         same_context_len: bool,
2025-04-11T03:52:12.5729286Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5729381Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5729453Z     ):
2025-04-11T03:52:12.5729567Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5729763Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5729948Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5730124Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5730286Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5730376Z             return
2025-04-11T03:52:12.5730448Z     
2025-04-11T03:52:12.5730538Z         torch.manual_seed(123)
2025-04-11T03:52:12.5730639Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5730727Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5730830Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5730834Z 
2025-04-11T03:52:12.5731000Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5731112Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5731119Z 
2025-04-11T03:52:12.5731195Z device = None
2025-04-11T03:52:12.5731199Z 
2025-04-11T03:52:12.5731320Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5731581Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5731660Z     
2025-04-11T03:52:12.5731736Z         Args:
2025-04-11T03:52:12.5731906Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5732076Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5732183Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5732262Z         """
2025-04-11T03:52:12.5732339Z         _lazy_init()
2025-04-11T03:52:12.5732435Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5732540Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5732834Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5733122Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5733263Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5733429Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5733433Z 
2025-04-11T03:52:12.5733672Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5733847Z ____________ test_context_attention[True-False-True-4-16-16-16-32] _____________
2025-04-11T03:52:12.5733851Z 
2025-04-11T03:52:12.5734002Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5734146Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5734239Z use_new_kcache_layout = True
2025-04-11T03:52:12.5734243Z 
2025-04-11T03:52:12.5734445Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5734559Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5734677Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5734832Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5734958Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5735078Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5735222Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5735367Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5735524Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5735616Z     def test_context_attention(
2025-04-11T03:52:12.5735696Z         bsz: int,
2025-04-11T03:52:12.5735776Z         block_size: int,
2025-04-11T03:52:12.5735867Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5735960Z         num_attn_heads: int,
2025-04-11T03:52:12.5736043Z         kv_group_num: int,
2025-04-11T03:52:12.5736131Z         same_context_len: bool,
2025-04-11T03:52:12.5736215Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5736306Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5736389Z     ):
2025-04-11T03:52:12.5736500Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5736696Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5736878Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5737057Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5737221Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5737296Z             return
2025-04-11T03:52:12.5737373Z     
2025-04-11T03:52:12.5737463Z         torch.manual_seed(123)
2025-04-11T03:52:12.5737570Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5737660Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5737753Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5737757Z 
2025-04-11T03:52:12.5738036Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5738151Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5738154Z 
2025-04-11T03:52:12.5738237Z device = None
2025-04-11T03:52:12.5738241Z 
2025-04-11T03:52:12.5738358Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5738510Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5738582Z     
2025-04-11T03:52:12.5738661Z         Args:
2025-04-11T03:52:12.5738827Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5738990Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5739197Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5739273Z         """
2025-04-11T03:52:12.5739355Z         _lazy_init()
2025-04-11T03:52:12.5739451Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5739558Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5739668Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5739951Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5740096Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5740259Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5740263Z 
2025-04-11T03:52:12.5740504Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5740670Z _____________ test_context_attention[True-False-True-4-16-16-32-7] _____________
2025-04-11T03:52:12.5740678Z 
2025-04-11T03:52:12.5740834Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5740979Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5741070Z use_new_kcache_layout = True
2025-04-11T03:52:12.5741079Z 
2025-04-11T03:52:12.5741288Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5741402Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5741540Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5741682Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5741804Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5741917Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5742056Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5742198Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5742350Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5742447Z     def test_context_attention(
2025-04-11T03:52:12.5742524Z         bsz: int,
2025-04-11T03:52:12.5742611Z         block_size: int,
2025-04-11T03:52:12.5742700Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5742783Z         num_attn_heads: int,
2025-04-11T03:52:12.5742870Z         kv_group_num: int,
2025-04-11T03:52:12.5742956Z         same_context_len: bool,
2025-04-11T03:52:12.5743046Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5743132Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5743202Z     ):
2025-04-11T03:52:12.5743318Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5743515Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5743705Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5743878Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5744043Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5744214Z             return
2025-04-11T03:52:12.5744289Z     
2025-04-11T03:52:12.5744382Z         torch.manual_seed(123)
2025-04-11T03:52:12.5744484Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5744579Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5744671Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5744675Z 
2025-04-11T03:52:12.5744851Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5744965Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5744969Z 
2025-04-11T03:52:12.5745047Z device = None
2025-04-11T03:52:12.5745055Z 
2025-04-11T03:52:12.5745174Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5745421Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5745498Z     
2025-04-11T03:52:12.5745572Z         Args:
2025-04-11T03:52:12.5745751Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5745919Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5746024Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5746107Z         """
2025-04-11T03:52:12.5746188Z         _lazy_init()
2025-04-11T03:52:12.5746294Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5746398Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5746532Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5746828Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5746977Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5747143Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5747147Z 
2025-04-11T03:52:12.5747390Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5747561Z ____________ test_context_attention[True-False-True-4-16-16-32-32] _____________
2025-04-11T03:52:12.5747565Z 
2025-04-11T03:52:12.5747715Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5747864Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5747951Z use_new_kcache_layout = True
2025-04-11T03:52:12.5747955Z 
2025-04-11T03:52:12.5748160Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5748269Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5748388Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5748574Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5748693Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5748808Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5748948Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5749089Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5749238Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5749327Z     def test_context_attention(
2025-04-11T03:52:12.5749408Z         bsz: int,
2025-04-11T03:52:12.5749493Z         block_size: int,
2025-04-11T03:52:12.5749587Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5749675Z         num_attn_heads: int,
2025-04-11T03:52:12.5749762Z         kv_group_num: int,
2025-04-11T03:52:12.5749852Z         same_context_len: bool,
2025-04-11T03:52:12.5749937Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5750036Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5750110Z     ):
2025-04-11T03:52:12.5750225Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5750541Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5750729Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5750906Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5751069Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5751149Z             return
2025-04-11T03:52:12.5751222Z     
2025-04-11T03:52:12.5751309Z         torch.manual_seed(123)
2025-04-11T03:52:12.5751414Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5751509Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5751604Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5751757Z 
2025-04-11T03:52:12.5751935Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5752051Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5752055Z 
2025-04-11T03:52:12.5752138Z device = None
2025-04-11T03:52:12.5752142Z 
2025-04-11T03:52:12.5752271Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5752424Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5752497Z     
2025-04-11T03:52:12.5752576Z         Args:
2025-04-11T03:52:12.5752744Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5752920Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5753033Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5753111Z         """
2025-04-11T03:52:12.5753205Z         _lazy_init()
2025-04-11T03:52:12.5753306Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5753413Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5753523Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5753817Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5753953Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5754113Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5754122Z 
2025-04-11T03:52:12.5754361Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5754525Z _____________ test_context_attention[True-False-False-1-16-8-16-7] _____________
2025-04-11T03:52:12.5754529Z 
2025-04-11T03:52:12.5754688Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5754834Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5754929Z use_new_kcache_layout = True
2025-04-11T03:52:12.5754932Z 
2025-04-11T03:52:12.5755131Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5755244Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5755360Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5755499Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5755625Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5755741Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5755883Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5756017Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5756172Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5756264Z     def test_context_attention(
2025-04-11T03:52:12.5756342Z         bsz: int,
2025-04-11T03:52:12.5756434Z         block_size: int,
2025-04-11T03:52:12.5756537Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5756625Z         num_attn_heads: int,
2025-04-11T03:52:12.5756716Z         kv_group_num: int,
2025-04-11T03:52:12.5756918Z         same_context_len: bool,
2025-04-11T03:52:12.5757022Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5757112Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5757189Z     ):
2025-04-11T03:52:12.5757302Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5757497Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5757677Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5757847Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5758017Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5758195Z             return
2025-04-11T03:52:12.5758273Z     
2025-04-11T03:52:12.5758361Z         torch.manual_seed(123)
2025-04-11T03:52:12.5758467Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5758561Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5758655Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5758659Z 
2025-04-11T03:52:12.5758832Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5758946Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5758949Z 
2025-04-11T03:52:12.5759034Z device = None
2025-04-11T03:52:12.5759038Z 
2025-04-11T03:52:12.5759156Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5759315Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5759387Z     
2025-04-11T03:52:12.5759460Z         Args:
2025-04-11T03:52:12.5759637Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5759805Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5759914Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5759991Z         """
2025-04-11T03:52:12.5760077Z         _lazy_init()
2025-04-11T03:52:12.5760176Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5760278Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5760394Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5760675Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5760817Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5760981Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5760985Z 
2025-04-11T03:52:12.5761234Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5761406Z ____________ test_context_attention[True-False-False-1-16-8-16-32] _____________
2025-04-11T03:52:12.5761410Z 
2025-04-11T03:52:12.5761566Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5761718Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5761808Z use_new_kcache_layout = True
2025-04-11T03:52:12.5761812Z 
2025-04-11T03:52:12.5762019Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5762123Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5762246Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5762385Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5762503Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5762623Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5762765Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5762909Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5763177Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5763277Z     def test_context_attention(
2025-04-11T03:52:12.5763354Z         bsz: int,
2025-04-11T03:52:12.5763436Z         block_size: int,
2025-04-11T03:52:12.5763535Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5763625Z         num_attn_heads: int,
2025-04-11T03:52:12.5763711Z         kv_group_num: int,
2025-04-11T03:52:12.5763796Z         same_context_len: bool,
2025-04-11T03:52:12.5763880Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5763975Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5764047Z     ):
2025-04-11T03:52:12.5764163Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5764355Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5764655Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5764829Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5764996Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5765079Z             return
2025-04-11T03:52:12.5765151Z     
2025-04-11T03:52:12.5765245Z         torch.manual_seed(123)
2025-04-11T03:52:12.5765345Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5765439Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5765532Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5765535Z 
2025-04-11T03:52:12.5765701Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5765819Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5765826Z 
2025-04-11T03:52:12.5765908Z device = None
2025-04-11T03:52:12.5765911Z 
2025-04-11T03:52:12.5766034Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5766186Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5766265Z     
2025-04-11T03:52:12.5766339Z         Args:
2025-04-11T03:52:12.5766507Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5766676Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5766781Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5766856Z         """
2025-04-11T03:52:12.5766935Z         _lazy_init()
2025-04-11T03:52:12.5767036Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5767139Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5767244Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5767538Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5767675Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5767841Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5767845Z 
2025-04-11T03:52:12.5768085Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5768263Z _____________ test_context_attention[True-False-False-1-16-8-32-7] _____________
2025-04-11T03:52:12.5768267Z 
2025-04-11T03:52:12.5768419Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5768572Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5768663Z use_new_kcache_layout = True
2025-04-11T03:52:12.5768666Z 
2025-04-11T03:52:12.5768867Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5768981Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5769099Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5769242Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5769471Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5769591Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5769750Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5769886Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5770046Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5770138Z     def test_context_attention(
2025-04-11T03:52:12.5770237Z         bsz: int,
2025-04-11T03:52:12.5770320Z         block_size: int,
2025-04-11T03:52:12.5770409Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5770507Z         num_attn_heads: int,
2025-04-11T03:52:12.5770681Z         kv_group_num: int,
2025-04-11T03:52:12.5770771Z         same_context_len: bool,
2025-04-11T03:52:12.5770858Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5770951Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5771027Z     ):
2025-04-11T03:52:12.5771142Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5771339Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5771523Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5771697Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5771860Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5771943Z             return
2025-04-11T03:52:12.5772015Z     
2025-04-11T03:52:12.5772101Z         torch.manual_seed(123)
2025-04-11T03:52:12.5772208Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5772301Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5772396Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5772400Z 
2025-04-11T03:52:12.5772574Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5772695Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5772704Z 
2025-04-11T03:52:12.5772784Z device = None
2025-04-11T03:52:12.5772787Z 
2025-04-11T03:52:12.5772905Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5773070Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5773150Z     
2025-04-11T03:52:12.5773231Z         Args:
2025-04-11T03:52:12.5773409Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5773583Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5773696Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5773769Z         """
2025-04-11T03:52:12.5773851Z         _lazy_init()
2025-04-11T03:52:12.5773946Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5774053Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5774161Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5774447Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5774590Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5774748Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5774751Z 
2025-04-11T03:52:12.5774997Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5775164Z ____________ test_context_attention[True-False-False-1-16-8-32-32] _____________
2025-04-11T03:52:12.5775171Z 
2025-04-11T03:52:12.5775326Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5775473Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5775567Z use_new_kcache_layout = True
2025-04-11T03:52:12.5775571Z 
2025-04-11T03:52:12.5775879Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5775990Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5776118Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5776261Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5776387Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5776505Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5776647Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5776782Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5777024Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5777119Z     def test_context_attention(
2025-04-11T03:52:12.5777195Z         bsz: int,
2025-04-11T03:52:12.5777292Z         block_size: int,
2025-04-11T03:52:12.5777383Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5777473Z         num_attn_heads: int,
2025-04-11T03:52:12.5777561Z         kv_group_num: int,
2025-04-11T03:52:12.5777644Z         same_context_len: bool,
2025-04-11T03:52:12.5777741Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5777829Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5777912Z     ):
2025-04-11T03:52:12.5778026Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5778223Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5778423Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5778603Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5778776Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5778853Z             return
2025-04-11T03:52:12.5778927Z     
2025-04-11T03:52:12.5779017Z         torch.manual_seed(123)
2025-04-11T03:52:12.5779117Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5779215Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5779306Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5779309Z 
2025-04-11T03:52:12.5779484Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5779598Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5779602Z 
2025-04-11T03:52:12.5779685Z device = None
2025-04-11T03:52:12.5779689Z 
2025-04-11T03:52:12.5779805Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5779957Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5780037Z     
2025-04-11T03:52:12.5780112Z         Args:
2025-04-11T03:52:12.5780281Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5780451Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5780562Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5780634Z         """
2025-04-11T03:52:12.5780713Z         _lazy_init()
2025-04-11T03:52:12.5780811Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5780914Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5781022Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5781303Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5781445Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5781609Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5781613Z 
2025-04-11T03:52:12.5781854Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5782136Z ____________ test_context_attention[True-False-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.5782141Z 
2025-04-11T03:52:12.5782295Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5782451Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5782539Z use_new_kcache_layout = True
2025-04-11T03:52:12.5782542Z 
2025-04-11T03:52:12.5782743Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5782847Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5782965Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5783111Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5783330Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5783454Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5783593Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5783740Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5783892Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5783980Z     def test_context_attention(
2025-04-11T03:52:12.5784066Z         bsz: int,
2025-04-11T03:52:12.5784147Z         block_size: int,
2025-04-11T03:52:12.5784240Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5784326Z         num_attn_heads: int,
2025-04-11T03:52:12.5784416Z         kv_group_num: int,
2025-04-11T03:52:12.5784502Z         same_context_len: bool,
2025-04-11T03:52:12.5784585Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5784679Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5784756Z     ):
2025-04-11T03:52:12.5784872Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5785062Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5785247Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5785419Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5785584Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5785668Z             return
2025-04-11T03:52:12.5785739Z     
2025-04-11T03:52:12.5785830Z         torch.manual_seed(123)
2025-04-11T03:52:12.5785930Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5786019Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5786115Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5786119Z 
2025-04-11T03:52:12.5786283Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5786401Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5786405Z 
2025-04-11T03:52:12.5786481Z device = None
2025-04-11T03:52:12.5786485Z 
2025-04-11T03:52:12.5786611Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5786758Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5786829Z     
2025-04-11T03:52:12.5786912Z         Args:
2025-04-11T03:52:12.5787078Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5787250Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5787356Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5787435Z         """
2025-04-11T03:52:12.5787514Z         _lazy_init()
2025-04-11T03:52:12.5787611Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5787723Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5787829Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5788226Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5788367Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5788583Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5788587Z 
2025-04-11T03:52:12.5788827Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5788994Z ____________ test_context_attention[True-False-False-1-16-16-16-32] ____________
2025-04-11T03:52:12.5789003Z 
2025-04-11T03:52:12.5789160Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5789308Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5789510Z use_new_kcache_layout = True
2025-04-11T03:52:12.5789514Z 
2025-04-11T03:52:12.5789717Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5789829Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5789951Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5790096Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5790215Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5790329Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5790470Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5790606Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5790770Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5790865Z     def test_context_attention(
2025-04-11T03:52:12.5790943Z         bsz: int,
2025-04-11T03:52:12.5791035Z         block_size: int,
2025-04-11T03:52:12.5791130Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5791224Z         num_attn_heads: int,
2025-04-11T03:52:12.5791311Z         kv_group_num: int,
2025-04-11T03:52:12.5791409Z         same_context_len: bool,
2025-04-11T03:52:12.5791501Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5791589Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5791669Z     ):
2025-04-11T03:52:12.5791781Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5791979Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5792163Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5792334Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5792501Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5792579Z             return
2025-04-11T03:52:12.5792655Z     
2025-04-11T03:52:12.5792742Z         torch.manual_seed(123)
2025-04-11T03:52:12.5792845Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5792935Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5793029Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5793033Z 
2025-04-11T03:52:12.5793203Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5793320Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5793324Z 
2025-04-11T03:52:12.5793413Z device = None
2025-04-11T03:52:12.5793416Z 
2025-04-11T03:52:12.5793538Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5793693Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5793766Z     
2025-04-11T03:52:12.5793840Z         Args:
2025-04-11T03:52:12.5794011Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5794179Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5794291Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5794363Z         """
2025-04-11T03:52:12.5794553Z         _lazy_init()
2025-04-11T03:52:12.5794654Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5794758Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5794871Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5795154Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5795299Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5795455Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5795459Z 
2025-04-11T03:52:12.5795703Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5795970Z ____________ test_context_attention[True-False-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.5795974Z 
2025-04-11T03:52:12.5796128Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5796277Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5796365Z use_new_kcache_layout = True
2025-04-11T03:52:12.5796369Z 
2025-04-11T03:52:12.5796577Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5796682Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5796810Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5796950Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5797072Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5797187Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5797326Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5797467Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5797621Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5797721Z     def test_context_attention(
2025-04-11T03:52:12.5797798Z         bsz: int,
2025-04-11T03:52:12.5797880Z         block_size: int,
2025-04-11T03:52:12.5797974Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5798059Z         num_attn_heads: int,
2025-04-11T03:52:12.5798147Z         kv_group_num: int,
2025-04-11T03:52:12.5798235Z         same_context_len: bool,
2025-04-11T03:52:12.5798326Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5798415Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5798487Z     ):
2025-04-11T03:52:12.5798607Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5798799Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5798993Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5799164Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5799340Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5799416Z             return
2025-04-11T03:52:12.5799488Z     
2025-04-11T03:52:12.5799582Z         torch.manual_seed(123)
2025-04-11T03:52:12.5799681Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5799775Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5799866Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5799870Z 
2025-04-11T03:52:12.5800040Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5800158Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5800162Z 
2025-04-11T03:52:12.5800238Z device = None
2025-04-11T03:52:12.5800245Z 
2025-04-11T03:52:12.5800367Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5800514Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5800591Z     
2025-04-11T03:52:12.5800666Z         Args:
2025-04-11T03:52:12.5800963Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5801138Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5801247Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5801330Z         """
2025-04-11T03:52:12.5801409Z         _lazy_init()
2025-04-11T03:52:12.5801512Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5801614Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5801721Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5802009Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5802241Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5802406Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5802413Z 
2025-04-11T03:52:12.5802654Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5802829Z ____________ test_context_attention[True-False-False-1-16-16-32-32] ____________
2025-04-11T03:52:12.5802833Z 
2025-04-11T03:52:12.5802986Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5803139Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5803229Z use_new_kcache_layout = True
2025-04-11T03:52:12.5803233Z 
2025-04-11T03:52:12.5803431Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5803541Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5803664Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5803839Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5803966Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5804099Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5804237Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5804373Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5804528Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5804618Z     def test_context_attention(
2025-04-11T03:52:12.5804699Z         bsz: int,
2025-04-11T03:52:12.5804782Z         block_size: int,
2025-04-11T03:52:12.5804875Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5804961Z         num_attn_heads: int,
2025-04-11T03:52:12.5805042Z         kv_group_num: int,
2025-04-11T03:52:12.5805138Z         same_context_len: bool,
2025-04-11T03:52:12.5805223Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5805316Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5805388Z     ):
2025-04-11T03:52:12.5805500Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5805702Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5805886Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5806062Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5806225Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5806303Z             return
2025-04-11T03:52:12.5806374Z     
2025-04-11T03:52:12.5806462Z         torch.manual_seed(123)
2025-04-11T03:52:12.5806568Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5806660Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5806758Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5806762Z 
2025-04-11T03:52:12.5806928Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5807150Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5807155Z 
2025-04-11T03:52:12.5807239Z device = None
2025-04-11T03:52:12.5807243Z 
2025-04-11T03:52:12.5807363Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5807521Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5807595Z     
2025-04-11T03:52:12.5807676Z         Args:
2025-04-11T03:52:12.5807843Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5808014Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5808120Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5808295Z         """
2025-04-11T03:52:12.5808379Z         _lazy_init()
2025-04-11T03:52:12.5808478Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5808588Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5808697Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5808980Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5809125Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5809285Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5809289Z 
2025-04-11T03:52:12.5809534Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5809701Z _____________ test_context_attention[True-False-False-4-16-8-16-7] _____________
2025-04-11T03:52:12.5809704Z 
2025-04-11T03:52:12.5809862Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5810010Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5810105Z use_new_kcache_layout = True
2025-04-11T03:52:12.5810108Z 
2025-04-11T03:52:12.5810312Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5810416Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5810539Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5810678Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5810803Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5810916Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5811055Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5811192Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5811345Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5811445Z     def test_context_attention(
2025-04-11T03:52:12.5811522Z         bsz: int,
2025-04-11T03:52:12.5811611Z         block_size: int,
2025-04-11T03:52:12.5811702Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5811797Z         num_attn_heads: int,
2025-04-11T03:52:12.5811885Z         kv_group_num: int,
2025-04-11T03:52:12.5811974Z         same_context_len: bool,
2025-04-11T03:52:12.5812065Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5812154Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5812232Z     ):
2025-04-11T03:52:12.5812343Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5812534Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5812722Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5812890Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5813064Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5813142Z             return
2025-04-11T03:52:12.5813218Z     
2025-04-11T03:52:12.5813304Z         torch.manual_seed(123)
2025-04-11T03:52:12.5813549Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5813656Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5813753Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5813756Z 
2025-04-11T03:52:12.5813925Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5814081Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5814086Z 
2025-04-11T03:52:12.5814196Z device = None
2025-04-11T03:52:12.5814201Z 
2025-04-11T03:52:12.5814327Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5814478Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5814663Z     
2025-04-11T03:52:12.5814739Z         Args:
2025-04-11T03:52:12.5814910Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5815074Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5815188Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5815263Z         """
2025-04-11T03:52:12.5815342Z         _lazy_init()
2025-04-11T03:52:12.5815448Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5815551Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5815667Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5815944Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5816083Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5816244Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5816251Z 
2025-04-11T03:52:12.5816490Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5816666Z ____________ test_context_attention[True-False-False-4-16-8-16-32] _____________
2025-04-11T03:52:12.5816670Z 
2025-04-11T03:52:12.5816821Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5816971Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5817061Z use_new_kcache_layout = True
2025-04-11T03:52:12.5817064Z 
2025-04-11T03:52:12.5817268Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5817371Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5817495Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5817637Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5817757Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5817879Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5818015Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5818163Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5818322Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5818421Z     def test_context_attention(
2025-04-11T03:52:12.5818505Z         bsz: int,
2025-04-11T03:52:12.5818586Z         block_size: int,
2025-04-11T03:52:12.5818679Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5818766Z         num_attn_heads: int,
2025-04-11T03:52:12.5818853Z         kv_group_num: int,
2025-04-11T03:52:12.5818939Z         same_context_len: bool,
2025-04-11T03:52:12.5819025Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5819120Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5819194Z     ):
2025-04-11T03:52:12.5819314Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5819502Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5819681Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5819962Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5820130Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5820216Z             return
2025-04-11T03:52:12.5820291Z     
2025-04-11T03:52:12.5820385Z         torch.manual_seed(123)
2025-04-11T03:52:12.5820485Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5820575Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5820673Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5820677Z 
2025-04-11T03:52:12.5820845Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5821075Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5821078Z 
2025-04-11T03:52:12.5821157Z device = None
2025-04-11T03:52:12.5821161Z 
2025-04-11T03:52:12.5821283Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5821439Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5821514Z     
2025-04-11T03:52:12.5821590Z         Args:
2025-04-11T03:52:12.5821757Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5821925Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5822032Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5822112Z         """
2025-04-11T03:52:12.5822190Z         _lazy_init()
2025-04-11T03:52:12.5822286Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5822393Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5822504Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5822794Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5822933Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5823104Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5823108Z 
2025-04-11T03:52:12.5823364Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5823555Z _____________ test_context_attention[True-False-False-4-16-8-32-7] _____________
2025-04-11T03:52:12.5823559Z 
2025-04-11T03:52:12.5823710Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5823855Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5823949Z use_new_kcache_layout = True
2025-04-11T03:52:12.5823955Z 
2025-04-11T03:52:12.5824152Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5824260Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5824379Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5824526Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5824642Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5824757Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5824897Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5825033Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5825188Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5825278Z     def test_context_attention(
2025-04-11T03:52:12.5825358Z         bsz: int,
2025-04-11T03:52:12.5825442Z         block_size: int,
2025-04-11T03:52:12.5825531Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5825622Z         num_attn_heads: int,
2025-04-11T03:52:12.5825705Z         kv_group_num: int,
2025-04-11T03:52:12.5825796Z         same_context_len: bool,
2025-04-11T03:52:12.5825879Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5826080Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5826165Z     ):
2025-04-11T03:52:12.5826278Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5826473Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5826656Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5826832Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5826995Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5827070Z             return
2025-04-11T03:52:12.5827148Z     
2025-04-11T03:52:12.5827334Z         torch.manual_seed(123)
2025-04-11T03:52:12.5827445Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5827537Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5827628Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5827637Z 
2025-04-11T03:52:12.5827810Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5827925Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5827928Z 
2025-04-11T03:52:12.5828013Z device = None
2025-04-11T03:52:12.5828017Z 
2025-04-11T03:52:12.5828135Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5828292Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5828367Z     
2025-04-11T03:52:12.5828478Z         Args:
2025-04-11T03:52:12.5828647Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5828813Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5828931Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5829004Z         """
2025-04-11T03:52:12.5829089Z         _lazy_init()
2025-04-11T03:52:12.5829191Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5829293Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5829402Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5829685Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5829839Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5830003Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5830007Z 
2025-04-11T03:52:12.5830253Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5830425Z ____________ test_context_attention[True-False-False-4-16-8-32-32] _____________
2025-04-11T03:52:12.5830428Z 
2025-04-11T03:52:12.5830584Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5830731Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5830821Z use_new_kcache_layout = True
2025-04-11T03:52:12.5830834Z 
2025-04-11T03:52:12.5831037Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5831143Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5831271Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5831412Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5831536Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5831651Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5831786Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5831929Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5832082Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5832183Z     def test_context_attention(
2025-04-11T03:52:12.5832260Z         bsz: int,
2025-04-11T03:52:12.5832462Z         block_size: int,
2025-04-11T03:52:12.5832560Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5832648Z         num_attn_heads: int,
2025-04-11T03:52:12.5832740Z         kv_group_num: int,
2025-04-11T03:52:12.5832825Z         same_context_len: bool,
2025-04-11T03:52:12.5832917Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5833008Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5833081Z     ):
2025-04-11T03:52:12.5833203Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5833400Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5833587Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5833992Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5834161Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5834240Z             return
2025-04-11T03:52:12.5834312Z     
2025-04-11T03:52:12.5834410Z         torch.manual_seed(123)
2025-04-11T03:52:12.5834516Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5834620Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5834727Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5834730Z 
2025-04-11T03:52:12.5834915Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5835027Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5835030Z 
2025-04-11T03:52:12.5835109Z device = None
2025-04-11T03:52:12.5835113Z 
2025-04-11T03:52:12.5835238Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5835390Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5835466Z     
2025-04-11T03:52:12.5835540Z         Args:
2025-04-11T03:52:12.5835717Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5835887Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5835994Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5836073Z         """
2025-04-11T03:52:12.5836152Z         _lazy_init()
2025-04-11T03:52:12.5836253Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5836356Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5836468Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5836756Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5836897Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5837060Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5837064Z 
2025-04-11T03:52:12.5837307Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5837481Z ____________ test_context_attention[True-False-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.5837484Z 
2025-04-11T03:52:12.5837635Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5837785Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5837873Z use_new_kcache_layout = True
2025-04-11T03:52:12.5837877Z 
2025-04-11T03:52:12.5838078Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5838185Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5838304Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5838452Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5838568Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5838793Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5838935Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5839081Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5839246Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5839351Z     def test_context_attention(
2025-04-11T03:52:12.5839442Z         bsz: int,
2025-04-11T03:52:12.5839527Z         block_size: int,
2025-04-11T03:52:12.5839621Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5839707Z         num_attn_heads: int,
2025-04-11T03:52:12.5839793Z         kv_group_num: int,
2025-04-11T03:52:12.5839888Z         same_context_len: bool,
2025-04-11T03:52:12.5840091Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5840186Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5840261Z     ):
2025-04-11T03:52:12.5840370Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5840573Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5840754Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5840931Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5841105Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5841186Z             return
2025-04-11T03:52:12.5841258Z     
2025-04-11T03:52:12.5841344Z         torch.manual_seed(123)
2025-04-11T03:52:12.5841453Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5841544Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5841645Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5841652Z 
2025-04-11T03:52:12.5841823Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5841944Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5841948Z 
2025-04-11T03:52:12.5842029Z device = None
2025-04-11T03:52:12.5842032Z 
2025-04-11T03:52:12.5842158Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5842308Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5842381Z     
2025-04-11T03:52:12.5842459Z         Args:
2025-04-11T03:52:12.5842628Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5842800Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5842910Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5842985Z         """
2025-04-11T03:52:12.5843068Z         _lazy_init()
2025-04-11T03:52:12.5843167Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5843279Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5843382Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5843682Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5843823Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5844015Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5844033Z 
2025-04-11T03:52:12.5844284Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5844455Z ____________ test_context_attention[True-False-False-4-16-16-16-32] ____________
2025-04-11T03:52:12.5844459Z 
2025-04-11T03:52:12.5844619Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5844768Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5844864Z use_new_kcache_layout = True
2025-04-11T03:52:12.5844868Z 
2025-04-11T03:52:12.5845071Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5845284Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5845408Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5845550Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5845675Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5845790Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5845939Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5846077Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5846234Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5846326Z     def test_context_attention(
2025-04-11T03:52:12.5846515Z         bsz: int,
2025-04-11T03:52:12.5846604Z         block_size: int,
2025-04-11T03:52:12.5846693Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5846785Z         num_attn_heads: int,
2025-04-11T03:52:12.5846868Z         kv_group_num: int,
2025-04-11T03:52:12.5846962Z         same_context_len: bool,
2025-04-11T03:52:12.5847059Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5847146Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5847227Z     ):
2025-04-11T03:52:12.5847338Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5847538Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5847717Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5847885Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5848053Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5848133Z             return
2025-04-11T03:52:12.5848212Z     
2025-04-11T03:52:12.5848300Z         torch.manual_seed(123)
2025-04-11T03:52:12.5848400Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5848502Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5848595Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5848598Z 
2025-04-11T03:52:12.5848770Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5848884Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5848888Z 
2025-04-11T03:52:12.5848970Z device = None
2025-04-11T03:52:12.5848974Z 
2025-04-11T03:52:12.5849091Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5849257Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5849336Z     
2025-04-11T03:52:12.5849411Z         Args:
2025-04-11T03:52:12.5849591Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5849759Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5849876Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5849950Z         """
2025-04-11T03:52:12.5850027Z         _lazy_init()
2025-04-11T03:52:12.5850128Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5850228Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5850341Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5850621Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5850761Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5850923Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5850929Z 
2025-04-11T03:52:12.5851172Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5851343Z ____________ test_context_attention[True-False-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.5851346Z 
2025-04-11T03:52:12.5851598Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5851752Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5851839Z use_new_kcache_layout = True
2025-04-11T03:52:12.5851843Z 
2025-04-11T03:52:12.5852044Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5852153Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5852274Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5852411Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5852527Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5852738Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5852878Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5853019Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5853171Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5853265Z     def test_context_attention(
2025-04-11T03:52:12.5853346Z         bsz: int,
2025-04-11T03:52:12.5853429Z         block_size: int,
2025-04-11T03:52:12.5853526Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5853609Z         num_attn_heads: int,
2025-04-11T03:52:12.5853700Z         kv_group_num: int,
2025-04-11T03:52:12.5853787Z         same_context_len: bool,
2025-04-11T03:52:12.5853874Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5853969Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5854049Z     ):
2025-04-11T03:52:12.5854171Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5854369Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5854563Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5854744Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5854909Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5854992Z             return
2025-04-11T03:52:12.5855063Z     
2025-04-11T03:52:12.5855157Z         torch.manual_seed(123)
2025-04-11T03:52:12.5855256Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5855355Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5855447Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5855451Z 
2025-04-11T03:52:12.5855619Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5855736Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5855742Z 
2025-04-11T03:52:12.5855821Z device = None
2025-04-11T03:52:12.5855825Z 
2025-04-11T03:52:12.5855946Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5856103Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5856181Z     
2025-04-11T03:52:12.5856256Z         Args:
2025-04-11T03:52:12.5856423Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5856594Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5856699Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5856780Z         """
2025-04-11T03:52:12.5856858Z         _lazy_init()
2025-04-11T03:52:12.5856959Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5857062Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5857168Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5857459Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5857598Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5857867Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5857872Z 
2025-04-11T03:52:12.5858112Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5858287Z ____________ test_context_attention[True-False-False-4-16-16-32-32] ____________
2025-04-11T03:52:12.5858291Z 
2025-04-11T03:52:12.5858445Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5858590Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5858683Z use_new_kcache_layout = True
2025-04-11T03:52:12.5858686Z 
2025-04-11T03:52:12.5858888Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5859125Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5859245Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5859402Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5859523Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5859645Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5859784Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5859923Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5860079Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5860169Z     def test_context_attention(
2025-04-11T03:52:12.5860250Z         bsz: int,
2025-04-11T03:52:12.5860333Z         block_size: int,
2025-04-11T03:52:12.5860423Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5860511Z         num_attn_heads: int,
2025-04-11T03:52:12.5860598Z         kv_group_num: int,
2025-04-11T03:52:12.5860685Z         same_context_len: bool,
2025-04-11T03:52:12.5860770Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5860864Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5860936Z     ):
2025-04-11T03:52:12.5861050Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5861251Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5861435Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5861611Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5861776Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5861853Z             return
2025-04-11T03:52:12.5861927Z     
2025-04-11T03:52:12.5862013Z         torch.manual_seed(123)
2025-04-11T03:52:12.5862121Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5862214Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5862310Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5862314Z 
2025-04-11T03:52:12.5862482Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5862594Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5862603Z 
2025-04-11T03:52:12.5862682Z device = None
2025-04-11T03:52:12.5862686Z 
2025-04-11T03:52:12.5862806Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5862964Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5863036Z     
2025-04-11T03:52:12.5863119Z         Args:
2025-04-11T03:52:12.5863287Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5863454Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5863577Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5863663Z         """
2025-04-11T03:52:12.5863755Z         _lazy_init()
2025-04-11T03:52:12.5863859Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5864092Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5864203Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5864488Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5864631Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5864790Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5864794Z 
2025-04-11T03:52:12.5865042Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5865210Z _____________ test_context_attention[False-True-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.5865310Z 
2025-04-11T03:52:12.5865469Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5865617Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5865719Z use_new_kcache_layout = False
2025-04-11T03:52:12.5865727Z 
2025-04-11T03:52:12.5865929Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5866034Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5866159Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5866300Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5866425Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5866541Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5866683Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5866819Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5866974Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5867070Z     def test_context_attention(
2025-04-11T03:52:12.5867147Z         bsz: int,
2025-04-11T03:52:12.5867233Z         block_size: int,
2025-04-11T03:52:12.5867327Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5867413Z         num_attn_heads: int,
2025-04-11T03:52:12.5867515Z         kv_group_num: int,
2025-04-11T03:52:12.5867611Z         same_context_len: bool,
2025-04-11T03:52:12.5867711Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5867803Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5867889Z     ):
2025-04-11T03:52:12.5868000Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5868194Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5868381Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5868613Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5868781Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5868857Z             return
2025-04-11T03:52:12.5868944Z     
2025-04-11T03:52:12.5869035Z         torch.manual_seed(123)
2025-04-11T03:52:12.5869134Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5869229Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5869318Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5869322Z 
2025-04-11T03:52:12.5869494Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5869607Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5869611Z 
2025-04-11T03:52:12.5869710Z device = None
2025-04-11T03:52:12.5869714Z 
2025-04-11T03:52:12.5869844Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5870000Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5870080Z     
2025-04-11T03:52:12.5870155Z         Args:
2025-04-11T03:52:12.5870329Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5870629Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5870746Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5870823Z         """
2025-04-11T03:52:12.5870904Z         _lazy_init()
2025-04-11T03:52:12.5871009Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5871112Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5871223Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5871504Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5871644Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5871913Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5871917Z 
2025-04-11T03:52:12.5872160Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5872339Z _____________ test_context_attention[False-True-True-1-16-8-16-32] _____________
2025-04-11T03:52:12.5872343Z 
2025-04-11T03:52:12.5872494Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5872645Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5872736Z use_new_kcache_layout = False
2025-04-11T03:52:12.5872740Z 
2025-04-11T03:52:12.5872944Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5873048Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5873169Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5873312Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5873435Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5873568Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5873709Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5873848Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5874001Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5874096Z     def test_context_attention(
2025-04-11T03:52:12.5874177Z         bsz: int,
2025-04-11T03:52:12.5874260Z         block_size: int,
2025-04-11T03:52:12.5874355Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5874438Z         num_attn_heads: int,
2025-04-11T03:52:12.5874527Z         kv_group_num: int,
2025-04-11T03:52:12.5874614Z         same_context_len: bool,
2025-04-11T03:52:12.5874701Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5874797Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5874873Z     ):
2025-04-11T03:52:12.5874991Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5875180Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5875363Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5875539Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5875701Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5875783Z             return
2025-04-11T03:52:12.5875857Z     
2025-04-11T03:52:12.5875952Z         torch.manual_seed(123)
2025-04-11T03:52:12.5876050Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5876138Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5876232Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5876236Z 
2025-04-11T03:52:12.5876408Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5876525Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5876529Z 
2025-04-11T03:52:12.5876607Z device = None
2025-04-11T03:52:12.5876610Z 
2025-04-11T03:52:12.5876854Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5877004Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5877077Z     
2025-04-11T03:52:12.5877158Z         Args:
2025-04-11T03:52:12.5877323Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5877501Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5877607Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5877713Z         """
2025-04-11T03:52:12.5877792Z         _lazy_init()
2025-04-11T03:52:12.5877898Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5878110Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5878214Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5878506Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5878646Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5878810Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5878814Z 
2025-04-11T03:52:12.5879049Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5879216Z _____________ test_context_attention[False-True-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.5879223Z 
2025-04-11T03:52:12.5879373Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5879517Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5879648Z use_new_kcache_layout = False
2025-04-11T03:52:12.5879652Z 
2025-04-11T03:52:12.5879850Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5879959Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5880080Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5880234Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5880359Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5880479Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5880620Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5880757Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5880911Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5881003Z     def test_context_attention(
2025-04-11T03:52:12.5881085Z         bsz: int,
2025-04-11T03:52:12.5881170Z         block_size: int,
2025-04-11T03:52:12.5881258Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5881352Z         num_attn_heads: int,
2025-04-11T03:52:12.5881435Z         kv_group_num: int,
2025-04-11T03:52:12.5881531Z         same_context_len: bool,
2025-04-11T03:52:12.5881619Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5881708Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5881786Z     ):
2025-04-11T03:52:12.5881896Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5882089Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5882270Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5882441Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5882603Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5882680Z             return
2025-04-11T03:52:12.5882760Z     
2025-04-11T03:52:12.5882847Z         torch.manual_seed(123)
2025-04-11T03:52:12.5882950Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5883038Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5883237Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5883242Z 
2025-04-11T03:52:12.5883418Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5883534Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5883538Z 
2025-04-11T03:52:12.5883624Z device = None
2025-04-11T03:52:12.5883628Z 
2025-04-11T03:52:12.5883747Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5883904Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5883987Z     
2025-04-11T03:52:12.5884062Z         Args:
2025-04-11T03:52:12.5884236Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5884517Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5884639Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5884720Z         """
2025-04-11T03:52:12.5884816Z         _lazy_init()
2025-04-11T03:52:12.5884913Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5885022Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5885137Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5885420Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5885562Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5885719Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5885723Z 
2025-04-11T03:52:12.5885967Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5886138Z _____________ test_context_attention[False-True-True-1-16-8-32-32] _____________
2025-04-11T03:52:12.5886142Z 
2025-04-11T03:52:12.5886300Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5886445Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5886537Z use_new_kcache_layout = False
2025-04-11T03:52:12.5886540Z 
2025-04-11T03:52:12.5886746Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5886853Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5886976Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5887115Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5887241Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5887355Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5887495Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5887636Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5887787Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5887888Z     def test_context_attention(
2025-04-11T03:52:12.5887966Z         bsz: int,
2025-04-11T03:52:12.5888050Z         block_size: int,
2025-04-11T03:52:12.5888140Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5888227Z         num_attn_heads: int,
2025-04-11T03:52:12.5888318Z         kv_group_num: int,
2025-04-11T03:52:12.5888404Z         same_context_len: bool,
2025-04-11T03:52:12.5888499Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5888590Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5888664Z     ):
2025-04-11T03:52:12.5888779Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5888973Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5889159Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5889331Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5889732Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5889843Z             return
2025-04-11T03:52:12.5889948Z     
2025-04-11T03:52:12.5890098Z         torch.manual_seed(123)
2025-04-11T03:52:12.5890241Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5890443Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5890625Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5890629Z 
2025-04-11T03:52:12.5890832Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5891024Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5891028Z 
2025-04-11T03:52:12.5891378Z device = None
2025-04-11T03:52:12.5891383Z 
2025-04-11T03:52:12.5891619Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5891828Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5891969Z     
2025-04-11T03:52:12.5892066Z         Args:
2025-04-11T03:52:12.5892298Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5892555Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5892691Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5892832Z         """
2025-04-11T03:52:12.5892942Z         _lazy_init()
2025-04-11T03:52:12.5893099Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5893255Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5893388Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5893745Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5893917Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5894123Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5894130Z 
2025-04-11T03:52:12.5894405Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5894652Z _____________ test_context_attention[False-True-True-1-16-16-16-7] _____________
2025-04-11T03:52:12.5894657Z 
2025-04-11T03:52:12.5894834Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5895039Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5895160Z use_new_kcache_layout = False
2025-04-11T03:52:12.5895165Z 
2025-04-11T03:52:12.5895378Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5895678Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5895843Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5896097Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5896237Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5896461Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5896637Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5896845Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5897057Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5897200Z     def test_context_attention(
2025-04-11T03:52:12.5897355Z         bsz: int,
2025-04-11T03:52:12.5897471Z         block_size: int,
2025-04-11T03:52:12.5897631Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5897748Z         num_attn_heads: int,
2025-04-11T03:52:12.5897851Z         kv_group_num: int,
2025-04-11T03:52:12.5898019Z         same_context_len: bool,
2025-04-11T03:52:12.5898154Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5898350Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5898443Z     ):
2025-04-11T03:52:12.5898755Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5898991Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5899218Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5899590Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5899787Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5899933Z             return
2025-04-11T03:52:12.5900035Z     
2025-04-11T03:52:12.5900173Z         torch.manual_seed(123)
2025-04-11T03:52:12.5900321Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5900578Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5900737Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5900742Z 
2025-04-11T03:52:12.5900940Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5901128Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5901132Z 
2025-04-11T03:52:12.5901224Z device = None
2025-04-11T03:52:12.5901228Z 
2025-04-11T03:52:12.5901429Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5901606Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5901706Z     
2025-04-11T03:52:12.5901848Z         Args:
2025-04-11T03:52:12.5902044Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5902262Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5902409Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5902551Z         """
2025-04-11T03:52:12.5902660Z         _lazy_init()
2025-04-11T03:52:12.5902801Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5903028Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5903175Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5903532Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5903700Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5903935Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5903940Z 
2025-04-11T03:52:12.5904210Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5904438Z ____________ test_context_attention[False-True-True-1-16-16-16-32] _____________
2025-04-11T03:52:12.5904442Z 
2025-04-11T03:52:12.5904641Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5904817Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5904982Z use_new_kcache_layout = False
2025-04-11T03:52:12.5904987Z 
2025-04-11T03:52:12.5905243Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5905460Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5905623Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5905842Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5905993Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5906205Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5906376Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5906543Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5906762Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5906895Z     def test_context_attention(
2025-04-11T03:52:12.5907031Z         bsz: int,
2025-04-11T03:52:12.5907154Z         block_size: int,
2025-04-11T03:52:12.5907393Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5907619Z         num_attn_heads: int,
2025-04-11T03:52:12.5907745Z         kv_group_num: int,
2025-04-11T03:52:12.5907882Z         same_context_len: bool,
2025-04-11T03:52:12.5908009Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5908171Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5908278Z     ):
2025-04-11T03:52:12.5908469Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5908724Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5908920Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5909175Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5909501Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5909652Z             return
2025-04-11T03:52:12.5909756Z     
2025-04-11T03:52:12.5909874Z         torch.manual_seed(123)
2025-04-11T03:52:12.5910068Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5910210Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5910417Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5910421Z 
2025-04-11T03:52:12.5910617Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5910830Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5910835Z 
2025-04-11T03:52:12.5910944Z device = None
2025-04-11T03:52:12.5910949Z 
2025-04-11T03:52:12.5911214Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5911393Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5911512Z     
2025-04-11T03:52:12.5911678Z         Args:
2025-04-11T03:52:12.5911877Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5912107Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5912247Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5912382Z         """
2025-04-11T03:52:12.5912500Z         _lazy_init()
2025-04-11T03:52:12.5912644Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5912807Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5912943Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5913300Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5913451Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5913686Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5913694Z 
2025-04-11T03:52:12.5913963Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5914164Z _____________ test_context_attention[False-True-True-1-16-16-32-7] _____________
2025-04-11T03:52:12.5914201Z 
2025-04-11T03:52:12.5914392Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5914562Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5914747Z use_new_kcache_layout = False
2025-04-11T03:52:12.5914753Z 
2025-04-11T03:52:12.5914998Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5915182Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5915333Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5915670Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5915835Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5915982Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5916188Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5916459Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5916716Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5916842Z     def test_context_attention(
2025-04-11T03:52:12.5916982Z         bsz: int,
2025-04-11T03:52:12.5917094Z         block_size: int,
2025-04-11T03:52:12.5917214Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5917361Z         num_attn_heads: int,
2025-04-11T03:52:12.5917484Z         kv_group_num: int,
2025-04-11T03:52:12.5917632Z         same_context_len: bool,
2025-04-11T03:52:12.5917748Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5917901Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5918000Z     ):
2025-04-11T03:52:12.5918260Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5918525Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5918740Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5918980Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5919256Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5919384Z             return
2025-04-11T03:52:12.5919525Z     
2025-04-11T03:52:12.5919660Z         torch.manual_seed(123)
2025-04-11T03:52:12.5919867Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5920038Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5920211Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5920215Z 
2025-04-11T03:52:12.5920416Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5920600Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5920604Z 
2025-04-11T03:52:12.5920702Z device = None
2025-04-11T03:52:12.5920707Z 
2025-04-11T03:52:12.5920929Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5921114Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5921219Z     
2025-04-11T03:52:12.5921387Z         Args:
2025-04-11T03:52:12.5921587Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5921820Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5921967Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5922070Z         """
2025-04-11T03:52:12.5922209Z         _lazy_init()
2025-04-11T03:52:12.5922338Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5922487Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5922652Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5923099Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5923266Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5923456Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5923491Z 
2025-04-11T03:52:12.5923755Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5923947Z ____________ test_context_attention[False-True-True-1-16-16-32-32] _____________
2025-04-11T03:52:12.5923951Z 
2025-04-11T03:52:12.5924180Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5924356Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5924513Z use_new_kcache_layout = False
2025-04-11T03:52:12.5924521Z 
2025-04-11T03:52:12.5924751Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5924915Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5925071Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5925389Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5925535Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5925679Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5925886Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5926037Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5926268Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5926386Z     def test_context_attention(
2025-04-11T03:52:12.5926531Z         bsz: int,
2025-04-11T03:52:12.5926642Z         block_size: int,
2025-04-11T03:52:12.5926872Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5927082Z         num_attn_heads: int,
2025-04-11T03:52:12.5927205Z         kv_group_num: int,
2025-04-11T03:52:12.5927357Z         same_context_len: bool,
2025-04-11T03:52:12.5927475Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5927598Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5927724Z     ):
2025-04-11T03:52:12.5927873Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5928154Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5928365Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5928603Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5928796Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5928890Z             return
2025-04-11T03:52:12.5929050Z     
2025-04-11T03:52:12.5929176Z         torch.manual_seed(123)
2025-04-11T03:52:12.5929343Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5929469Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5929610Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5929614Z 
2025-04-11T03:52:12.5929822Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5929987Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5930021Z 
2025-04-11T03:52:12.5930129Z device = None
2025-04-11T03:52:12.5930133Z 
2025-04-11T03:52:12.5930282Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5930497Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5930702Z     
2025-04-11T03:52:12.5930873Z         Args:
2025-04-11T03:52:12.5931084Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5931305Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5931458Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5931576Z         """
2025-04-11T03:52:12.5931719Z         _lazy_init()
2025-04-11T03:52:12.5931862Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5932028Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5932160Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5932531Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5932695Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5932895Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5932900Z 
2025-04-11T03:52:12.5933198Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5933399Z _____________ test_context_attention[False-True-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.5933404Z 
2025-04-11T03:52:12.5933615Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5933915Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5934077Z use_new_kcache_layout = False
2025-04-11T03:52:12.5934082Z 
2025-04-11T03:52:12.5934312Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5934480Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5934615Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5934792Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5935061Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5935206Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5935404Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5935684Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5935904Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5936040Z     def test_context_attention(
2025-04-11T03:52:12.5936153Z         bsz: int,
2025-04-11T03:52:12.5936299Z         block_size: int,
2025-04-11T03:52:12.5936420Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5936554Z         num_attn_heads: int,
2025-04-11T03:52:12.5936682Z         kv_group_num: int,
2025-04-11T03:52:12.5936839Z         same_context_len: bool,
2025-04-11T03:52:12.5936952Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5937071Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5937202Z     ):
2025-04-11T03:52:12.5937336Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5937613Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5937834Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5938076Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5938280Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5938398Z             return
2025-04-11T03:52:12.5938606Z     
2025-04-11T03:52:12.5938735Z         torch.manual_seed(123)
2025-04-11T03:52:12.5938896Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5939017Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5939142Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5939188Z 
2025-04-11T03:52:12.5939372Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5939522Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5939527Z 
2025-04-11T03:52:12.5939680Z device = None
2025-04-11T03:52:12.5939684Z 
2025-04-11T03:52:12.5939835Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5940061Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5940167Z     
2025-04-11T03:52:12.5940297Z         Args:
2025-04-11T03:52:12.5940512Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5940702Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5940867Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5940978Z         """
2025-04-11T03:52:12.5941101Z         _lazy_init()
2025-04-11T03:52:12.5941252Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5941421Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5941558Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5941880Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5942172Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5942374Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5942378Z 
2025-04-11T03:52:12.5942822Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5943019Z _____________ test_context_attention[False-True-True-4-16-8-16-32] _____________
2025-04-11T03:52:12.5943023Z 
2025-04-11T03:52:12.5943246Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5943417Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5943554Z use_new_kcache_layout = False
2025-04-11T03:52:12.5943559Z 
2025-04-11T03:52:12.5943798Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5943984Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5944239Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5944406Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5944586Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5944719Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5944948Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5945113Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5945330Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5945453Z     def test_context_attention(
2025-04-11T03:52:12.5945562Z         bsz: int,
2025-04-11T03:52:12.5945698Z         block_size: int,
2025-04-11T03:52:12.5945839Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5945983Z         num_attn_heads: int,
2025-04-11T03:52:12.5946172Z         kv_group_num: int,
2025-04-11T03:52:12.5946288Z         same_context_len: bool,
2025-04-11T03:52:12.5946428Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5946567Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5946709Z     ):
2025-04-11T03:52:12.5946855Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5947108Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5947321Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5947507Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5947764Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5947870Z             return
2025-04-11T03:52:12.5948006Z     
2025-04-11T03:52:12.5948126Z         torch.manual_seed(123)
2025-04-11T03:52:12.5948285Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5948444Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5948583Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5948587Z 
2025-04-11T03:52:12.5948810Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5948950Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5948957Z 
2025-04-11T03:52:12.5949106Z device = None
2025-04-11T03:52:12.5949110Z 
2025-04-11T03:52:12.5949243Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5949482Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5949585Z     
2025-04-11T03:52:12.5949764Z         Args:
2025-04-11T03:52:12.5949994Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5950198Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5985770Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5985933Z         """
2025-04-11T03:52:12.5986041Z         _lazy_init()
2025-04-11T03:52:12.5986150Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5986272Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5986392Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5986931Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5987095Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5987274Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5987281Z 
2025-04-11T03:52:12.5987553Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5987740Z _____________ test_context_attention[False-True-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.5987750Z 
2025-04-11T03:52:12.5987916Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5988230Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5988334Z use_new_kcache_layout = False
2025-04-11T03:52:12.5988338Z 
2025-04-11T03:52:12.5988623Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5988738Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5988868Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5989025Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5989147Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5989263Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5989409Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5989545Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5989704Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5989800Z     def test_context_attention(
2025-04-11T03:52:12.5989889Z         bsz: int,
2025-04-11T03:52:12.5989975Z         block_size: int,
2025-04-11T03:52:12.5990073Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5990165Z         num_attn_heads: int,
2025-04-11T03:52:12.5990255Z         kv_group_num: int,
2025-04-11T03:52:12.5990348Z         same_context_len: bool,
2025-04-11T03:52:12.5990435Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5990528Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5990609Z     ):
2025-04-11T03:52:12.5990728Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5990933Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5991121Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5991298Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5991466Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5991543Z             return
2025-04-11T03:52:12.5991623Z     
2025-04-11T03:52:12.5991713Z         torch.manual_seed(123)
2025-04-11T03:52:12.5991821Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5991915Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5992009Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5992013Z 
2025-04-11T03:52:12.5992193Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5992311Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5992316Z 
2025-04-11T03:52:12.5992402Z device = None
2025-04-11T03:52:12.5992407Z 
2025-04-11T03:52:12.5992532Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5992692Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5992766Z     
2025-04-11T03:52:12.5992848Z         Args:
2025-04-11T03:52:12.5993022Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5993189Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5993418Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5993497Z         """
2025-04-11T03:52:12.5993580Z         _lazy_init()
2025-04-11T03:52:12.5993677Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5993782Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5993896Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5994185Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5994330Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5994490Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5994590Z 
2025-04-11T03:52:12.5994838Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5995007Z _____________ test_context_attention[False-True-True-4-16-8-32-32] _____________
2025-04-11T03:52:12.5995011Z 
2025-04-11T03:52:12.5995174Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5995322Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5995413Z use_new_kcache_layout = False
2025-04-11T03:52:12.5995417Z 
2025-04-11T03:52:12.5995622Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5995730Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5995853Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5995992Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5996113Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5996231Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5996366Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5996506Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5996661Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5996755Z     def test_context_attention(
2025-04-11T03:52:12.5996831Z         bsz: int,
2025-04-11T03:52:12.5996920Z         block_size: int,
2025-04-11T03:52:12.5997013Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5997098Z         num_attn_heads: int,
2025-04-11T03:52:12.5997186Z         kv_group_num: int,
2025-04-11T03:52:12.5997273Z         same_context_len: bool,
2025-04-11T03:52:12.5997363Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5997452Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5997526Z     ):
2025-04-11T03:52:12.5997645Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5997842Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5998031Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5998205Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5998370Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5998449Z             return
2025-04-11T03:52:12.5998520Z     
2025-04-11T03:52:12.5998617Z         torch.manual_seed(123)
2025-04-11T03:52:12.5998718Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5998817Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5998911Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5998915Z 
2025-04-11T03:52:12.5999089Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5999203Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5999210Z 
2025-04-11T03:52:12.5999288Z device = None
2025-04-11T03:52:12.5999292Z 
2025-04-11T03:52:12.5999419Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5999680Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5999758Z     
2025-04-11T03:52:12.5999836Z         Args:
2025-04-11T03:52:12.6000009Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6000176Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6000287Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6000365Z         """
2025-04-11T03:52:12.6000446Z         _lazy_init()
2025-04-11T03:52:12.6000545Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6000649Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6000756Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6001145Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6001282Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6001449Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6001453Z 
2025-04-11T03:52:12.6001695Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6001866Z _____________ test_context_attention[False-True-True-4-16-16-16-7] _____________
2025-04-11T03:52:12.6001870Z 
2025-04-11T03:52:12.6002020Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6002165Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.6002256Z use_new_kcache_layout = False
2025-04-11T03:52:12.6002259Z 
2025-04-11T03:52:12.6002457Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6002575Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6002694Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6002837Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6002954Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6003072Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6003208Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6003342Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6003498Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6003589Z     def test_context_attention(
2025-04-11T03:52:12.6003669Z         bsz: int,
2025-04-11T03:52:12.6003751Z         block_size: int,
2025-04-11T03:52:12.6003846Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6003934Z         num_attn_heads: int,
2025-04-11T03:52:12.6004019Z         kv_group_num: int,
2025-04-11T03:52:12.6004111Z         same_context_len: bool,
2025-04-11T03:52:12.6004195Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6004285Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6004362Z     ):
2025-04-11T03:52:12.6004476Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6004680Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6004862Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6005038Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6005200Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6005279Z             return
2025-04-11T03:52:12.6005350Z     
2025-04-11T03:52:12.6005437Z         torch.manual_seed(123)
2025-04-11T03:52:12.6005545Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6005636Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6005732Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6005735Z 
2025-04-11T03:52:12.6006007Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6006128Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6006132Z 
2025-04-11T03:52:12.6006211Z device = None
2025-04-11T03:52:12.6006215Z 
2025-04-11T03:52:12.6006335Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6006492Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6006563Z     
2025-04-11T03:52:12.6006640Z         Args:
2025-04-11T03:52:12.6006809Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6006979Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6007200Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6007273Z         """
2025-04-11T03:52:12.6007354Z         _lazy_init()
2025-04-11T03:52:12.6007450Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6007559Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6007667Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6007953Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6008093Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6008251Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6008255Z 
2025-04-11T03:52:12.6008495Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6008667Z ____________ test_context_attention[False-True-True-4-16-16-16-32] _____________
2025-04-11T03:52:12.6008675Z 
2025-04-11T03:52:12.6008832Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6008972Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.6009068Z use_new_kcache_layout = False
2025-04-11T03:52:12.6009073Z 
2025-04-11T03:52:12.6009271Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6009381Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6009500Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6009641Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6009763Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6009877Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6010020Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6010156Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6010316Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6010408Z     def test_context_attention(
2025-04-11T03:52:12.6010484Z         bsz: int,
2025-04-11T03:52:12.6010573Z         block_size: int,
2025-04-11T03:52:12.6010669Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6010757Z         num_attn_heads: int,
2025-04-11T03:52:12.6010853Z         kv_group_num: int,
2025-04-11T03:52:12.6010942Z         same_context_len: bool,
2025-04-11T03:52:12.6011035Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6011125Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6011206Z     ):
2025-04-11T03:52:12.6011323Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6011517Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6011707Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6011881Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6012050Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6012125Z             return
2025-04-11T03:52:12.6012317Z     
2025-04-11T03:52:12.6012407Z         torch.manual_seed(123)
2025-04-11T03:52:12.6012507Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6012607Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6012696Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6012700Z 
2025-04-11T03:52:12.6012877Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6012991Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6012995Z 
2025-04-11T03:52:12.6013080Z device = None
2025-04-11T03:52:12.6013083Z 
2025-04-11T03:52:12.6013203Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6013451Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6013532Z     
2025-04-11T03:52:12.6013607Z         Args:
2025-04-11T03:52:12.6013779Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6013947Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6014063Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6014139Z         """
2025-04-11T03:52:12.6014218Z         _lazy_init()
2025-04-11T03:52:12.6014321Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6014424Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6014537Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6014824Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6014958Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6015123Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6015127Z 
2025-04-11T03:52:12.6015451Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6015626Z _____________ test_context_attention[False-True-True-4-16-16-32-7] _____________
2025-04-11T03:52:12.6015630Z 
2025-04-11T03:52:12.6015781Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6015928Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.6016019Z use_new_kcache_layout = False
2025-04-11T03:52:12.6016023Z 
2025-04-11T03:52:12.6016227Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6016333Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6016451Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6016600Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6016719Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6016836Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6016980Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6017123Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6017275Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6017369Z     def test_context_attention(
2025-04-11T03:52:12.6017453Z         bsz: int,
2025-04-11T03:52:12.6017537Z         block_size: int,
2025-04-11T03:52:12.6017635Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6017723Z         num_attn_heads: int,
2025-04-11T03:52:12.6017810Z         kv_group_num: int,
2025-04-11T03:52:12.6017903Z         same_context_len: bool,
2025-04-11T03:52:12.6017991Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6018091Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6018165Z     ):
2025-04-11T03:52:12.6018286Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6018480Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6018771Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6018956Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6019120Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6019202Z             return
2025-04-11T03:52:12.6019275Z     
2025-04-11T03:52:12.6019369Z         torch.manual_seed(123)
2025-04-11T03:52:12.6019470Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6019562Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6019662Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6019665Z 
2025-04-11T03:52:12.6019932Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6020052Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6020056Z 
2025-04-11T03:52:12.6020136Z device = None
2025-04-11T03:52:12.6020143Z 
2025-04-11T03:52:12.6020270Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6020422Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6020496Z     
2025-04-11T03:52:12.6020579Z         Args:
2025-04-11T03:52:12.6020748Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6020916Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6021023Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6021106Z         """
2025-04-11T03:52:12.6021186Z         _lazy_init()
2025-04-11T03:52:12.6021286Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6021399Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6021507Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6021802Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6021940Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6022101Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6022112Z 
2025-04-11T03:52:12.6022351Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6022523Z ____________ test_context_attention[False-True-True-4-16-16-32-32] _____________
2025-04-11T03:52:12.6022527Z 
2025-04-11T03:52:12.6022688Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6022832Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.6022933Z use_new_kcache_layout = False
2025-04-11T03:52:12.6022937Z 
2025-04-11T03:52:12.6023140Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6023254Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6023374Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6023515Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6023637Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6023752Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6023898Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6024035Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6024193Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6024285Z     def test_context_attention(
2025-04-11T03:52:12.6024370Z         bsz: int,
2025-04-11T03:52:12.6024465Z         block_size: int,
2025-04-11T03:52:12.6024555Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6024647Z         num_attn_heads: int,
2025-04-11T03:52:12.6024731Z         kv_group_num: int,
2025-04-11T03:52:12.6024919Z         same_context_len: bool,
2025-04-11T03:52:12.6025014Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6025104Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6025185Z     ):
2025-04-11T03:52:12.6025301Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6025500Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6025683Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6025857Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6026025Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6026209Z             return
2025-04-11T03:52:12.6026291Z     
2025-04-11T03:52:12.6026379Z         torch.manual_seed(123)
2025-04-11T03:52:12.6026486Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6026579Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6026673Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6026677Z 
2025-04-11T03:52:12.6026852Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6026964Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6026968Z 
2025-04-11T03:52:12.6027054Z device = None
2025-04-11T03:52:12.6027058Z 
2025-04-11T03:52:12.6027176Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6027334Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6027406Z     
2025-04-11T03:52:12.6027479Z         Args:
2025-04-11T03:52:12.6027651Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6027819Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6027931Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6028010Z         """
2025-04-11T03:52:12.6028096Z         _lazy_init()
2025-04-11T03:52:12.6028194Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6028295Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6028465Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6028755Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6028901Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6029060Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6029064Z 
2025-04-11T03:52:12.6029310Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6029484Z _____________ test_context_attention[False-True-False-1-16-8-16-7] _____________
2025-04-11T03:52:12.6029488Z 
2025-04-11T03:52:12.6029644Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6029799Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6029889Z use_new_kcache_layout = False
2025-04-11T03:52:12.6029893Z 
2025-04-11T03:52:12.6030100Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6030206Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6030331Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6030472Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6030596Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6030709Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6030849Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6030993Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6031147Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6031358Z     def test_context_attention(
2025-04-11T03:52:12.6031443Z         bsz: int,
2025-04-11T03:52:12.6031529Z         block_size: int,
2025-04-11T03:52:12.6031628Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6031717Z         num_attn_heads: int,
2025-04-11T03:52:12.6031809Z         kv_group_num: int,
2025-04-11T03:52:12.6031895Z         same_context_len: bool,
2025-04-11T03:52:12.6031987Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6032077Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6032151Z     ):
2025-04-11T03:52:12.6032275Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6032471Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6032778Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6032951Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6033121Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6033206Z             return
2025-04-11T03:52:12.6033281Z     
2025-04-11T03:52:12.6033376Z         torch.manual_seed(123)
2025-04-11T03:52:12.6033481Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6033580Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6033674Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6033678Z 
2025-04-11T03:52:12.6033850Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6033972Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6033979Z 
2025-04-11T03:52:12.6034057Z device = None
2025-04-11T03:52:12.6034060Z 
2025-04-11T03:52:12.6034186Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6034339Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6034418Z     
2025-04-11T03:52:12.6034499Z         Args:
2025-04-11T03:52:12.6034667Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6034842Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6034951Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6035037Z         """
2025-04-11T03:52:12.6035120Z         _lazy_init()
2025-04-11T03:52:12.6035230Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6035334Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6035443Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6035743Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6035883Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6036048Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6036053Z 
2025-04-11T03:52:12.6036292Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6036467Z ____________ test_context_attention[False-True-False-1-16-8-16-32] _____________
2025-04-11T03:52:12.6036471Z 
2025-04-11T03:52:12.6036624Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6036783Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6036875Z use_new_kcache_layout = False
2025-04-11T03:52:12.6036879Z 
2025-04-11T03:52:12.6037079Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6037195Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6037315Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6037461Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6037681Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6037806Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6037944Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6038082Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6038244Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6038334Z     def test_context_attention(
2025-04-11T03:52:12.6038420Z         bsz: int,
2025-04-11T03:52:12.6038503Z         block_size: int,
2025-04-11T03:52:12.6038594Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6038687Z         num_attn_heads: int,
2025-04-11T03:52:12.6038770Z         kv_group_num: int,
2025-04-11T03:52:12.6038953Z         same_context_len: bool,
2025-04-11T03:52:12.6039041Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6039138Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6039213Z     ):
2025-04-11T03:52:12.6039331Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6039528Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6039709Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6039888Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6040050Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6040132Z             return
2025-04-11T03:52:12.6040207Z     
2025-04-11T03:52:12.6040296Z         torch.manual_seed(123)
2025-04-11T03:52:12.6040403Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6040497Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6040594Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6040598Z 
2025-04-11T03:52:12.6040764Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6040880Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6040890Z 
2025-04-11T03:52:12.6040967Z device = None
2025-04-11T03:52:12.6040971Z 
2025-04-11T03:52:12.6041090Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6041247Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6041321Z     
2025-04-11T03:52:12.6041402Z         Args:
2025-04-11T03:52:12.6041569Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6041732Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6041844Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6041922Z         """
2025-04-11T03:52:12.6042009Z         _lazy_init()
2025-04-11T03:52:12.6042107Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6042215Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6042324Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6042608Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6042752Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6042909Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6042913Z 
2025-04-11T03:52:12.6043159Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6043329Z _____________ test_context_attention[False-True-False-1-16-8-32-7] _____________
2025-04-11T03:52:12.6043333Z 
2025-04-11T03:52:12.6043492Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6043642Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6043738Z use_new_kcache_layout = False
2025-04-11T03:52:12.6043742Z 
2025-04-11T03:52:12.6044048Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6044159Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6044290Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6044431Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6044557Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6044676Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6044822Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6044959Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6045208Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6045307Z     def test_context_attention(
2025-04-11T03:52:12.6045386Z         bsz: int,
2025-04-11T03:52:12.6045475Z         block_size: int,
2025-04-11T03:52:12.6045566Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6045655Z         num_attn_heads: int,
2025-04-11T03:52:12.6045745Z         kv_group_num: int,
2025-04-11T03:52:12.6045831Z         same_context_len: bool,
2025-04-11T03:52:12.6045921Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6046014Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6046095Z     ):
2025-04-11T03:52:12.6046210Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6046406Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6046593Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6046763Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6046934Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6047011Z             return
2025-04-11T03:52:12.6047090Z     
2025-04-11T03:52:12.6047181Z         torch.manual_seed(123)
2025-04-11T03:52:12.6047283Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6047382Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6047474Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6047478Z 
2025-04-11T03:52:12.6047651Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6047765Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6047769Z 
2025-04-11T03:52:12.6047854Z device = None
2025-04-11T03:52:12.6047858Z 
2025-04-11T03:52:12.6047974Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6048124Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6048208Z     
2025-04-11T03:52:12.6048284Z         Args:
2025-04-11T03:52:12.6048456Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6048623Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6048737Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6048813Z         """
2025-04-11T03:52:12.6048892Z         _lazy_init()
2025-04-11T03:52:12.6049001Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6049105Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6049216Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6049497Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6049632Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6049801Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6049805Z 
2025-04-11T03:52:12.6050041Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6050321Z ____________ test_context_attention[False-True-False-1-16-8-32-32] _____________
2025-04-11T03:52:12.6050326Z 
2025-04-11T03:52:12.6050480Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6050634Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6050726Z use_new_kcache_layout = False
2025-04-11T03:52:12.6050730Z 
2025-04-11T03:52:12.6050935Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6051038Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6051155Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6051298Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6051530Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6051653Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6051793Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6051937Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6052090Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6052180Z     def test_context_attention(
2025-04-11T03:52:12.6052266Z         bsz: int,
2025-04-11T03:52:12.6052350Z         block_size: int,
2025-04-11T03:52:12.6052447Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6052533Z         num_attn_heads: int,
2025-04-11T03:52:12.6052616Z         kv_group_num: int,
2025-04-11T03:52:12.6052711Z         same_context_len: bool,
2025-04-11T03:52:12.6052797Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6052892Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6052971Z     ):
2025-04-11T03:52:12.6053090Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6053283Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6053467Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6053645Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6053807Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6053892Z             return
2025-04-11T03:52:12.6053966Z     
2025-04-11T03:52:12.6054058Z         torch.manual_seed(123)
2025-04-11T03:52:12.6054157Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6054251Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6054350Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6054354Z 
2025-04-11T03:52:12.6054522Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6054645Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6054649Z 
2025-04-11T03:52:12.6054728Z device = None
2025-04-11T03:52:12.6054731Z 
2025-04-11T03:52:12.6054858Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6055008Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6055081Z     
2025-04-11T03:52:12.6055162Z         Args:
2025-04-11T03:52:12.6055327Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6055497Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6055605Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6055685Z         """
2025-04-11T03:52:12.6055767Z         _lazy_init()
2025-04-11T03:52:12.6055864Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6055970Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6056079Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6056367Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6056618Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6056784Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6056788Z 
2025-04-11T03:52:12.6057027Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6057196Z ____________ test_context_attention[False-True-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.6057200Z 
2025-04-11T03:52:12.6057356Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6057502Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6057600Z use_new_kcache_layout = False
2025-04-11T03:52:12.6057692Z 
2025-04-11T03:52:12.6057901Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6058012Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6058134Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6058274Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6058398Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6058513Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6058655Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6058790Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6058946Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6059036Z     def test_context_attention(
2025-04-11T03:52:12.6059115Z         bsz: int,
2025-04-11T03:52:12.6059203Z         block_size: int,
2025-04-11T03:52:12.6059298Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6059386Z         num_attn_heads: int,
2025-04-11T03:52:12.6059469Z         kv_group_num: int,
2025-04-11T03:52:12.6059555Z         same_context_len: bool,
2025-04-11T03:52:12.6059648Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6059741Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6059817Z     ):
2025-04-11T03:52:12.6059927Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6060124Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6060302Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6060473Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6060643Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6060724Z             return
2025-04-11T03:52:12.6060808Z     
2025-04-11T03:52:12.6060895Z         torch.manual_seed(123)
2025-04-11T03:52:12.6061003Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6061094Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6061184Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6061191Z 
2025-04-11T03:52:12.6061365Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6061476Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6061479Z 
2025-04-11T03:52:12.6061566Z device = None
2025-04-11T03:52:12.6061569Z 
2025-04-11T03:52:12.6061686Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6061839Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6061912Z     
2025-04-11T03:52:12.6061987Z         Args:
2025-04-11T03:52:12.6062159Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6062326Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6062438Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6062512Z         """
2025-04-11T03:52:12.6062594Z         _lazy_init()
2025-04-11T03:52:12.6062800Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6062905Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6063015Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6063300Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6063439Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6063598Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6063602Z 
2025-04-11T03:52:12.6063847Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6064108Z ____________ test_context_attention[False-True-False-1-16-16-16-32] ____________
2025-04-11T03:52:12.6064112Z 
2025-04-11T03:52:12.6064268Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6064418Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6064508Z use_new_kcache_layout = False
2025-04-11T03:52:12.6064512Z 
2025-04-11T03:52:12.6064718Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6064821Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6064940Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6065078Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6065201Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6065315Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6065452Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6065596Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6065746Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6065841Z     def test_context_attention(
2025-04-11T03:52:12.6065919Z         bsz: int,
2025-04-11T03:52:12.6066002Z         block_size: int,
2025-04-11T03:52:12.6066099Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6066184Z         num_attn_heads: int,
2025-04-11T03:52:12.6066270Z         kv_group_num: int,
2025-04-11T03:52:12.6066358Z         same_context_len: bool,
2025-04-11T03:52:12.6066449Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6066539Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6066611Z     ):
2025-04-11T03:52:12.6066727Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6066919Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6067110Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6067282Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6067450Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6067528Z             return
2025-04-11T03:52:12.6067600Z     
2025-04-11T03:52:12.6067692Z         torch.manual_seed(123)
2025-04-11T03:52:12.6067792Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6067886Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6067977Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6067980Z 
2025-04-11T03:52:12.6068148Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6068263Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6068267Z 
2025-04-11T03:52:12.6068345Z device = None
2025-04-11T03:52:12.6068351Z 
2025-04-11T03:52:12.6068517Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6068671Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6068745Z     
2025-04-11T03:52:12.6068821Z         Args:
2025-04-11T03:52:12.6069113Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6069286Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6069394Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6069473Z         """
2025-04-11T03:52:12.6069552Z         _lazy_init()
2025-04-11T03:52:12.6069655Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6069756Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6069862Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6070151Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6070485Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6070648Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6070653Z 
2025-04-11T03:52:12.6070899Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6071076Z ____________ test_context_attention[False-True-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.6071080Z 
2025-04-11T03:52:12.6071230Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6071383Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6071475Z use_new_kcache_layout = False
2025-04-11T03:52:12.6071480Z 
2025-04-11T03:52:12.6071683Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6071796Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6071915Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6072060Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6072178Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6072305Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6072446Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6072587Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6072747Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6072837Z     def test_context_attention(
2025-04-11T03:52:12.6072918Z         bsz: int,
2025-04-11T03:52:12.6072999Z         block_size: int,
2025-04-11T03:52:12.6073091Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6073183Z         num_attn_heads: int,
2025-04-11T03:52:12.6073265Z         kv_group_num: int,
2025-04-11T03:52:12.6073351Z         same_context_len: bool,
2025-04-11T03:52:12.6073442Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6073535Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6073607Z     ):
2025-04-11T03:52:12.6073720Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6073923Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6074104Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6074277Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6074439Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6074519Z             return
2025-04-11T03:52:12.6074591Z     
2025-04-11T03:52:12.6074677Z         torch.manual_seed(123)
2025-04-11T03:52:12.6074781Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6074872Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6074973Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6074977Z 
2025-04-11T03:52:12.6075147Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6075259Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6075370Z 
2025-04-11T03:52:12.6075451Z device = None
2025-04-11T03:52:12.6075455Z 
2025-04-11T03:52:12.6075574Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6075731Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6075803Z     
2025-04-11T03:52:12.6075880Z         Args:
2025-04-11T03:52:12.6076049Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6076217Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6076335Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6076409Z         """
2025-04-11T03:52:12.6076607Z         _lazy_init()
2025-04-11T03:52:12.6076702Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6076812Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6076918Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6077209Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6077350Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6077509Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6077513Z 
2025-04-11T03:52:12.6077754Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6077927Z ____________ test_context_attention[False-True-False-1-16-16-32-32] ____________
2025-04-11T03:52:12.6077931Z 
2025-04-11T03:52:12.6078088Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6078237Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6078329Z use_new_kcache_layout = False
2025-04-11T03:52:12.6078333Z 
2025-04-11T03:52:12.6078537Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6078641Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6078761Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6078900Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6079018Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6079131Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6079271Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6079405Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6079558Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6079657Z     def test_context_attention(
2025-04-11T03:52:12.6079734Z         bsz: int,
2025-04-11T03:52:12.6079820Z         block_size: int,
2025-04-11T03:52:12.6079910Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6079996Z         num_attn_heads: int,
2025-04-11T03:52:12.6080089Z         kv_group_num: int,
2025-04-11T03:52:12.6080174Z         same_context_len: bool,
2025-04-11T03:52:12.6080260Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6080349Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6080425Z     ):
2025-04-11T03:52:12.6080538Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6080731Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6080914Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6081083Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6081248Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6081323Z             return
2025-04-11T03:52:12.6081400Z     
2025-04-11T03:52:12.6081484Z         torch.manual_seed(123)
2025-04-11T03:52:12.6081684Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6081783Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6081876Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6081880Z 
2025-04-11T03:52:12.6082061Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6082175Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6082179Z 
2025-04-11T03:52:12.6082259Z device = None
2025-04-11T03:52:12.6082263Z 
2025-04-11T03:52:12.6082383Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6082533Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6082704Z     
2025-04-11T03:52:12.6082779Z         Args:
2025-04-11T03:52:12.6082948Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6083112Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6083227Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6083302Z         """
2025-04-11T03:52:12.6083379Z         _lazy_init()
2025-04-11T03:52:12.6083481Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6083582Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6083695Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6083981Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6084119Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6084284Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6084291Z 
2025-04-11T03:52:12.6084531Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6084706Z _____________ test_context_attention[False-True-False-4-16-8-16-7] _____________
2025-04-11T03:52:12.6084713Z 
2025-04-11T03:52:12.6084864Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6085013Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6085103Z use_new_kcache_layout = False
2025-04-11T03:52:12.6085107Z 
2025-04-11T03:52:12.6085309Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6085415Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6085534Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6085677Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6085793Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6085916Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6086050Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6086192Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6086345Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6086437Z     def test_context_attention(
2025-04-11T03:52:12.6086518Z         bsz: int,
2025-04-11T03:52:12.6086600Z         block_size: int,
2025-04-11T03:52:12.6086693Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6086776Z         num_attn_heads: int,
2025-04-11T03:52:12.6086858Z         kv_group_num: int,
2025-04-11T03:52:12.6086946Z         same_context_len: bool,
2025-04-11T03:52:12.6087029Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6087122Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6087193Z     ):
2025-04-11T03:52:12.6087312Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6087506Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6087685Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6087966Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6088133Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6088215Z             return
2025-04-11T03:52:12.6088289Z     
2025-04-11T03:52:12.6088381Z         torch.manual_seed(123)
2025-04-11T03:52:12.6088483Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6088575Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6088669Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6088673Z 
2025-04-11T03:52:12.6088839Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6088956Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6089050Z 
2025-04-11T03:52:12.6089129Z device = None
2025-04-11T03:52:12.6089133Z 
2025-04-11T03:52:12.6089253Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6089407Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6089479Z     
2025-04-11T03:52:12.6089557Z         Args:
2025-04-11T03:52:12.6089721Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6089890Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6089997Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6090073Z         """
2025-04-11T03:52:12.6090153Z         _lazy_init()
2025-04-11T03:52:12.6090249Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6090354Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6090465Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6090754Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6090893Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6091057Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6091061Z 
2025-04-11T03:52:12.6091302Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6091471Z ____________ test_context_attention[False-True-False-4-16-8-16-32] _____________
2025-04-11T03:52:12.6091474Z 
2025-04-11T03:52:12.6091629Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6091776Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6091870Z use_new_kcache_layout = False
2025-04-11T03:52:12.6091874Z 
2025-04-11T03:52:12.6092076Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6092186Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6092307Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6092450Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6092570Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6092686Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6092832Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6092969Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6093123Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6093214Z     def test_context_attention(
2025-04-11T03:52:12.6093288Z         bsz: int,
2025-04-11T03:52:12.6093377Z         block_size: int,
2025-04-11T03:52:12.6093466Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6093556Z         num_attn_heads: int,
2025-04-11T03:52:12.6093641Z         kv_group_num: int,
2025-04-11T03:52:12.6093730Z         same_context_len: bool,
2025-04-11T03:52:12.6093819Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6093906Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6094079Z     ):
2025-04-11T03:52:12.6094194Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6094391Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6094576Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6094746Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6094910Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6094985Z             return
2025-04-11T03:52:12.6095063Z     
2025-04-11T03:52:12.6095148Z         torch.manual_seed(123)
2025-04-11T03:52:12.6095350Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6095440Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6095529Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6095532Z 
2025-04-11T03:52:12.6095708Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6095819Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6095823Z 
2025-04-11T03:52:12.6095905Z device = None
2025-04-11T03:52:12.6095909Z 
2025-04-11T03:52:12.6096025Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6096182Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6096254Z     
2025-04-11T03:52:12.6096328Z         Args:
2025-04-11T03:52:12.6096504Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6096668Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6096783Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6096855Z         """
2025-04-11T03:52:12.6096940Z         _lazy_init()
2025-04-11T03:52:12.6097035Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6097143Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6097254Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6097538Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6097678Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6097837Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6097841Z 
2025-04-11T03:52:12.6098081Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6098249Z _____________ test_context_attention[False-True-False-4-16-8-32-7] _____________
2025-04-11T03:52:12.6098256Z 
2025-04-11T03:52:12.6098408Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6098557Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6098648Z use_new_kcache_layout = False
2025-04-11T03:52:12.6098652Z 
2025-04-11T03:52:12.6098860Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6098967Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6099089Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6099229Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6099349Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6099464Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6099600Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6099747Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6099898Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6099995Z     def test_context_attention(
2025-04-11T03:52:12.6100071Z         bsz: int,
2025-04-11T03:52:12.6100260Z         block_size: int,
2025-04-11T03:52:12.6100360Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6100444Z         num_attn_heads: int,
2025-04-11T03:52:12.6100533Z         kv_group_num: int,
2025-04-11T03:52:12.6100619Z         same_context_len: bool,
2025-04-11T03:52:12.6100707Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6100795Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6100866Z     ):
2025-04-11T03:52:12.6100981Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6101170Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6101362Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6101645Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6101812Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6101892Z             return
2025-04-11T03:52:12.6101965Z     
2025-04-11T03:52:12.6102055Z         torch.manual_seed(123)
2025-04-11T03:52:12.6102153Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6102247Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6102337Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6102341Z 
2025-04-11T03:52:12.6102510Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6102628Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6102632Z 
2025-04-11T03:52:12.6102709Z device = None
2025-04-11T03:52:12.6102713Z 
2025-04-11T03:52:12.6102835Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6102989Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6103070Z     
2025-04-11T03:52:12.6103142Z         Args:
2025-04-11T03:52:12.6103313Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6103480Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6103587Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6103671Z         """
2025-04-11T03:52:12.6103750Z         _lazy_init()
2025-04-11T03:52:12.6103851Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6103952Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6104059Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6104348Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6104485Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6104651Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6104655Z 
2025-04-11T03:52:12.6104895Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6105069Z ____________ test_context_attention[False-True-False-4-16-8-32-32] _____________
2025-04-11T03:52:12.6105073Z 
2025-04-11T03:52:12.6105224Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6105374Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6105463Z use_new_kcache_layout = False
2025-04-11T03:52:12.6105467Z 
2025-04-11T03:52:12.6105666Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6105775Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6105892Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6106035Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6106149Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6106267Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6106498Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6106636Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6106792Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6106882Z     def test_context_attention(
2025-04-11T03:52:12.6106962Z         bsz: int,
2025-04-11T03:52:12.6107045Z         block_size: int,
2025-04-11T03:52:12.6107133Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6107221Z         num_attn_heads: int,
2025-04-11T03:52:12.6107306Z         kv_group_num: int,
2025-04-11T03:52:12.6107395Z         same_context_len: bool,
2025-04-11T03:52:12.6107479Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6107661Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6107736Z     ):
2025-04-11T03:52:12.6107847Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6108051Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6108234Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6108407Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6108608Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6108688Z             return
2025-04-11T03:52:12.6108759Z     
2025-04-11T03:52:12.6108843Z         torch.manual_seed(123)
2025-04-11T03:52:12.6108949Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6109039Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6109132Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6109139Z 
2025-04-11T03:52:12.6109303Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6109416Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6109423Z 
2025-04-11T03:52:12.6109501Z device = None
2025-04-11T03:52:12.6109508Z 
2025-04-11T03:52:12.6109624Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6109778Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6109853Z     
2025-04-11T03:52:12.6109931Z         Args:
2025-04-11T03:52:12.6110097Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6110268Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6110375Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6110449Z         """
2025-04-11T03:52:12.6110536Z         _lazy_init()
2025-04-11T03:52:12.6110636Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6110742Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6110850Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6111136Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6111276Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6111434Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6111437Z 
2025-04-11T03:52:12.6111677Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6111846Z ____________ test_context_attention[False-True-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.6111850Z 
2025-04-11T03:52:12.6112004Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6112151Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6112248Z use_new_kcache_layout = False
2025-04-11T03:52:12.6112252Z 
2025-04-11T03:52:12.6112450Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6112707Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6112836Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6112973Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6113099Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6113213Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6113356Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6113491Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6113643Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6113737Z     def test_context_attention(
2025-04-11T03:52:12.6113917Z         bsz: int,
2025-04-11T03:52:12.6114003Z         block_size: int,
2025-04-11T03:52:12.6114094Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6114178Z         num_attn_heads: int,
2025-04-11T03:52:12.6114265Z         kv_group_num: int,
2025-04-11T03:52:12.6114354Z         same_context_len: bool,
2025-04-11T03:52:12.6114446Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6114533Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6114609Z     ):
2025-04-11T03:52:12.6114722Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6114912Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6115096Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6115262Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6115431Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6115509Z             return
2025-04-11T03:52:12.6115583Z     
2025-04-11T03:52:12.6115669Z         torch.manual_seed(123)
2025-04-11T03:52:12.6115770Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6115881Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6116006Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6116010Z 
2025-04-11T03:52:12.6116185Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6116295Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6116299Z 
2025-04-11T03:52:12.6116378Z device = None
2025-04-11T03:52:12.6116382Z 
2025-04-11T03:52:12.6116503Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6116652Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6116730Z     
2025-04-11T03:52:12.6116804Z         Args:
2025-04-11T03:52:12.6116982Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6117148Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6117258Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6117336Z         """
2025-04-11T03:52:12.6117415Z         _lazy_init()
2025-04-11T03:52:12.6117518Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6117622Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6117731Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6118013Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6118149Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6118313Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6118317Z 
2025-04-11T03:52:12.6118558Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6118732Z ____________ test_context_attention[False-True-False-4-16-16-16-32] ____________
2025-04-11T03:52:12.6118736Z 
2025-04-11T03:52:12.6119002Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6119159Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6119249Z use_new_kcache_layout = False
2025-04-11T03:52:12.6119253Z 
2025-04-11T03:52:12.6119452Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6119556Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6119678Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6119822Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6119939Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6120058Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6120292Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6120431Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6120584Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6120674Z     def test_context_attention(
2025-04-11T03:52:12.6120757Z         bsz: int,
2025-04-11T03:52:12.6120840Z         block_size: int,
2025-04-11T03:52:12.6120936Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6121023Z         num_attn_heads: int,
2025-04-11T03:52:12.6121111Z         kv_group_num: int,
2025-04-11T03:52:12.6121198Z         same_context_len: bool,
2025-04-11T03:52:12.6121283Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6121376Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6121449Z     ):
2025-04-11T03:52:12.6121569Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6121761Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6121943Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6122123Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6122284Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6122366Z             return
2025-04-11T03:52:12.6122440Z     
2025-04-11T03:52:12.6122529Z         torch.manual_seed(123)
2025-04-11T03:52:12.6122629Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6122718Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6122812Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6122816Z 
2025-04-11T03:52:12.6122979Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6123094Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6123101Z 
2025-04-11T03:52:12.6123178Z device = None
2025-04-11T03:52:12.6123182Z 
2025-04-11T03:52:12.6123303Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6123454Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6123528Z     
2025-04-11T03:52:12.6123606Z         Args:
2025-04-11T03:52:12.6123771Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6123938Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6124043Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6124121Z         """
2025-04-11T03:52:12.6124199Z         _lazy_init()
2025-04-11T03:52:12.6124295Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6124401Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6124505Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6124796Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6124934Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6125221Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6125225Z 
2025-04-11T03:52:12.6125468Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6125635Z ____________ test_context_attention[False-True-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.6125644Z 
2025-04-11T03:52:12.6125797Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6125940Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6126040Z use_new_kcache_layout = False
2025-04-11T03:52:12.6126043Z 
2025-04-11T03:52:12.6126243Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6126447Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6126569Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6126712Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6126831Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6126946Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6127087Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6127224Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6127377Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6127465Z     def test_context_attention(
2025-04-11T03:52:12.6127540Z         bsz: int,
2025-04-11T03:52:12.6127627Z         block_size: int,
2025-04-11T03:52:12.6127716Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6127802Z         num_attn_heads: int,
2025-04-11T03:52:12.6127888Z         kv_group_num: int,
2025-04-11T03:52:12.6127976Z         same_context_len: bool,
2025-04-11T03:52:12.6128061Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6128150Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6128227Z     ):
2025-04-11T03:52:12.6128340Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6128533Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6128713Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6128884Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6129051Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6129126Z             return
2025-04-11T03:52:12.6129202Z     
2025-04-11T03:52:12.6129287Z         torch.manual_seed(123)
2025-04-11T03:52:12.6129389Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6129483Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6129574Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6129577Z 
2025-04-11T03:52:12.6129747Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6129863Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6129866Z 
2025-04-11T03:52:12.6129948Z device = None
2025-04-11T03:52:12.6129952Z 
2025-04-11T03:52:12.6130068Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6130218Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6130289Z     
2025-04-11T03:52:12.6130362Z         Args:
2025-04-11T03:52:12.6130533Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6130696Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6130809Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6130888Z         """
2025-04-11T03:52:12.6130972Z         _lazy_init()
2025-04-11T03:52:12.6131066Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6131174Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6131392Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6131681Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6131821Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6131977Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6131981Z 
2025-04-11T03:52:12.6132219Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6132385Z ____________ test_context_attention[False-True-False-4-16-16-32-32] ____________
2025-04-11T03:52:12.6132478Z 
2025-04-11T03:52:12.6132635Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6132780Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6132869Z use_new_kcache_layout = False
2025-04-11T03:52:12.6132876Z 
2025-04-11T03:52:12.6133080Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6133184Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6133303Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6133442Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6133563Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6133676Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6133814Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6133954Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6134108Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6134201Z     def test_context_attention(
2025-04-11T03:52:12.6134276Z         bsz: int,
2025-04-11T03:52:12.6134358Z         block_size: int,
2025-04-11T03:52:12.6134458Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6134542Z         num_attn_heads: int,
2025-04-11T03:52:12.6134628Z         kv_group_num: int,
2025-04-11T03:52:12.6134712Z         same_context_len: bool,
2025-04-11T03:52:12.6134800Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6134891Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6134965Z     ):
2025-04-11T03:52:12.6135081Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6135272Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6135456Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6135629Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6135795Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6135872Z             return
2025-04-11T03:52:12.6135946Z     
2025-04-11T03:52:12.6136041Z         torch.manual_seed(123)
2025-04-11T03:52:12.6136141Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6136234Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6136324Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6136328Z 
2025-04-11T03:52:12.6136494Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6136612Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6136615Z 
2025-04-11T03:52:12.6136692Z device = None
2025-04-11T03:52:12.6136695Z 
2025-04-11T03:52:12.6136819Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6136967Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6137042Z     
2025-04-11T03:52:12.6137115Z         Args:
2025-04-11T03:52:12.6137282Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6137556Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6137668Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6137745Z         """
2025-04-11T03:52:12.6137825Z         _lazy_init()
2025-04-11T03:52:12.6137931Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6138036Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6138146Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6138444Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6138579Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6138830Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6138835Z 
2025-04-11T03:52:12.6139073Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6139248Z _____________ test_context_attention[False-False-True-1-16-8-16-7] _____________
2025-04-11T03:52:12.6139251Z 
2025-04-11T03:52:12.6139401Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6139550Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6139638Z use_new_kcache_layout = False
2025-04-11T03:52:12.6139642Z 
2025-04-11T03:52:12.6139842Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6139957Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6140079Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6140222Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6140344Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6140462Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6140597Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6140737Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6140891Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6140981Z     def test_context_attention(
2025-04-11T03:52:12.6141059Z         bsz: int,
2025-04-11T03:52:12.6141142Z         block_size: int,
2025-04-11T03:52:12.6141234Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6141324Z         num_attn_heads: int,
2025-04-11T03:52:12.6141408Z         kv_group_num: int,
2025-04-11T03:52:12.6141499Z         same_context_len: bool,
2025-04-11T03:52:12.6141585Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6141679Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6141757Z     ):
2025-04-11T03:52:12.6141869Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6142062Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6142247Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6142418Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6142580Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6142663Z             return
2025-04-11T03:52:12.6142735Z     
2025-04-11T03:52:12.6142823Z         torch.manual_seed(123)
2025-04-11T03:52:12.6142935Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6143025Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6143124Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6143128Z 
2025-04-11T03:52:12.6143295Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6143410Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6143417Z 
2025-04-11T03:52:12.6143495Z device = None
2025-04-11T03:52:12.6143498Z 
2025-04-11T03:52:12.6143716Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6143875Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6143948Z     
2025-04-11T03:52:12.6144025Z         Args:
2025-04-11T03:52:12.6144193Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6144360Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6144468Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6144542Z         """
2025-04-11T03:52:12.6144626Z         _lazy_init()
2025-04-11T03:52:12.6144722Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6144925Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6145032Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6145321Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6145464Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6145624Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6145627Z 
2025-04-11T03:52:12.6145872Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6146041Z ____________ test_context_attention[False-False-True-1-16-8-16-32] _____________
2025-04-11T03:52:12.6146045Z 
2025-04-11T03:52:12.6146200Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6146344Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6146438Z use_new_kcache_layout = False
2025-04-11T03:52:12.6146442Z 
2025-04-11T03:52:12.6146639Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6146740Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6146866Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6147006Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6147126Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6147241Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6147383Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6147519Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6147669Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6147765Z     def test_context_attention(
2025-04-11T03:52:12.6147840Z         bsz: int,
2025-04-11T03:52:12.6147931Z         block_size: int,
2025-04-11T03:52:12.6148021Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6148107Z         num_attn_heads: int,
2025-04-11T03:52:12.6148196Z         kv_group_num: int,
2025-04-11T03:52:12.6148282Z         same_context_len: bool,
2025-04-11T03:52:12.6148376Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6148521Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6148605Z     ):
2025-04-11T03:52:12.6148719Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6148915Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6149103Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6149274Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6149440Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6149519Z             return
2025-04-11T03:52:12.6149596Z     
2025-04-11T03:52:12.6149686Z         torch.manual_seed(123)
2025-04-11T03:52:12.6149787Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6149882Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6150105Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6150110Z 
2025-04-11T03:52:12.6150283Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6150396Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6150400Z 
2025-04-11T03:52:12.6150482Z device = None
2025-04-11T03:52:12.6150486Z 
2025-04-11T03:52:12.6150605Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6150753Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6150830Z     
2025-04-11T03:52:12.6150905Z         Args:
2025-04-11T03:52:12.6151075Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6151346Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6151458Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6151531Z         """
2025-04-11T03:52:12.6151612Z         _lazy_init()
2025-04-11T03:52:12.6151715Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6151816Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6151925Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6152208Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6152343Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6152506Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6152510Z 
2025-04-11T03:52:12.6152748Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6152922Z _____________ test_context_attention[False-False-True-1-16-8-32-7] _____________
2025-04-11T03:52:12.6152926Z 
2025-04-11T03:52:12.6153073Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6153225Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6153317Z use_new_kcache_layout = False
2025-04-11T03:52:12.6153321Z 
2025-04-11T03:52:12.6153526Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6153634Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6153752Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6153894Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6154012Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6154134Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6154274Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6154412Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6154565Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6154660Z     def test_context_attention(
2025-04-11T03:52:12.6154741Z         bsz: int,
2025-04-11T03:52:12.6154823Z         block_size: int,
2025-04-11T03:52:12.6154915Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6155000Z         num_attn_heads: int,
2025-04-11T03:52:12.6155086Z         kv_group_num: int,
2025-04-11T03:52:12.6155171Z         same_context_len: bool,
2025-04-11T03:52:12.6155253Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6155346Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6155417Z     ):
2025-04-11T03:52:12.6155529Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6155718Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6155904Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6156082Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6156349Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6156434Z             return
2025-04-11T03:52:12.6156507Z     
2025-04-11T03:52:12.6156595Z         torch.manual_seed(123)
2025-04-11T03:52:12.6156693Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6156782Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6156878Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6156882Z 
2025-04-11T03:52:12.6157048Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6157162Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6157166Z 
2025-04-11T03:52:12.6157243Z device = None
2025-04-11T03:52:12.6157337Z 
2025-04-11T03:52:12.6157459Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6157611Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6157681Z     
2025-04-11T03:52:12.6157758Z         Args:
2025-04-11T03:52:12.6157930Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6158097Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6158202Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6158279Z         """
2025-04-11T03:52:12.6158358Z         _lazy_init()
2025-04-11T03:52:12.6158457Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6158562Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6158668Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6158956Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6159097Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6159260Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6159267Z 
2025-04-11T03:52:12.6159509Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6159679Z ____________ test_context_attention[False-False-True-1-16-8-32-32] _____________
2025-04-11T03:52:12.6159687Z 
2025-04-11T03:52:12.6159839Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6159984Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6160076Z use_new_kcache_layout = False
2025-04-11T03:52:12.6160080Z 
2025-04-11T03:52:12.6160280Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6160389Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6160511Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6160653Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6160772Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6160888Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6161031Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6161166Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6161323Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6161416Z     def test_context_attention(
2025-04-11T03:52:12.6161490Z         bsz: int,
2025-04-11T03:52:12.6161577Z         block_size: int,
2025-04-11T03:52:12.6161667Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6161754Z         num_attn_heads: int,
2025-04-11T03:52:12.6161838Z         kv_group_num: int,
2025-04-11T03:52:12.6161931Z         same_context_len: bool,
2025-04-11T03:52:12.6162016Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6162104Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6162181Z     ):
2025-04-11T03:52:12.6162292Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6162594Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6162780Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6162955Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6163127Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6163204Z             return
2025-04-11T03:52:12.6163280Z     
2025-04-11T03:52:12.6163368Z         torch.manual_seed(123)
2025-04-11T03:52:12.6163470Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6163562Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6163750Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6163754Z 
2025-04-11T03:52:12.6163923Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6164036Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6164040Z 
2025-04-11T03:52:12.6164121Z device = None
2025-04-11T03:52:12.6164124Z 
2025-04-11T03:52:12.6164242Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6164396Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6164467Z     
2025-04-11T03:52:12.6164540Z         Args:
2025-04-11T03:52:12.6164713Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6164875Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6164984Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6165059Z         """
2025-04-11T03:52:12.6165140Z         _lazy_init()
2025-04-11T03:52:12.6165236Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6165337Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6165449Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6165728Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6165866Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6166022Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6166026Z 
2025-04-11T03:52:12.6166264Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6166431Z ____________ test_context_attention[False-False-True-1-16-16-16-7] _____________
2025-04-11T03:52:12.6166435Z 
2025-04-11T03:52:12.6166588Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6166735Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6166824Z use_new_kcache_layout = False
2025-04-11T03:52:12.6166828Z 
2025-04-11T03:52:12.6167036Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6167140Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6167259Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6167398Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6167518Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6167631Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6167770Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6167910Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6168061Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6168155Z     def test_context_attention(
2025-04-11T03:52:12.6168230Z         bsz: int,
2025-04-11T03:52:12.6168312Z         block_size: int,
2025-04-11T03:52:12.6168406Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6168597Z         num_attn_heads: int,
2025-04-11T03:52:12.6168687Z         kv_group_num: int,
2025-04-11T03:52:12.6168775Z         same_context_len: bool,
2025-04-11T03:52:12.6168861Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6168951Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6169024Z     ):
2025-04-11T03:52:12.6169142Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6169334Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6169520Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6169690Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6170070Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6170151Z             return
2025-04-11T03:52:12.6170222Z     
2025-04-11T03:52:12.6170318Z         torch.manual_seed(123)
2025-04-11T03:52:12.6170421Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6170516Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6170613Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6170616Z 
2025-04-11T03:52:12.6170783Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6170908Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6170912Z 
2025-04-11T03:52:12.6170989Z device = None
2025-04-11T03:52:12.6170993Z 
2025-04-11T03:52:12.6171118Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6171272Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6171357Z     
2025-04-11T03:52:12.6171436Z         Args:
2025-04-11T03:52:12.6171602Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6171769Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6171879Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6171956Z         """
2025-04-11T03:52:12.6172033Z         _lazy_init()
2025-04-11T03:52:12.6172135Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6172238Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6172344Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6172631Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6172765Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6172926Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6172933Z 
2025-04-11T03:52:12.6173171Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6173346Z ____________ test_context_attention[False-False-True-1-16-16-16-32] ____________
2025-04-11T03:52:12.6173350Z 
2025-04-11T03:52:12.6173502Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6173647Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6173736Z use_new_kcache_layout = False
2025-04-11T03:52:12.6173739Z 
2025-04-11T03:52:12.6173937Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6174045Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6174162Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6174302Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6174419Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6174538Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6174672Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6174935Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6175095Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6175183Z     def test_context_attention(
2025-04-11T03:52:12.6175263Z         bsz: int,
2025-04-11T03:52:12.6175344Z         block_size: int,
2025-04-11T03:52:12.6175433Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6175523Z         num_attn_heads: int,
2025-04-11T03:52:12.6175606Z         kv_group_num: int,
2025-04-11T03:52:12.6175694Z         same_context_len: bool,
2025-04-11T03:52:12.6175781Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6175877Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6175949Z     ):
2025-04-11T03:52:12.6176154Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6176352Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6176536Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6176715Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6176881Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6176965Z             return
2025-04-11T03:52:12.6177036Z     
2025-04-11T03:52:12.6177125Z         torch.manual_seed(123)
2025-04-11T03:52:12.6177233Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6177324Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6177418Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6177422Z 
2025-04-11T03:52:12.6177587Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6177698Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6177708Z 
2025-04-11T03:52:12.6177786Z device = None
2025-04-11T03:52:12.6177790Z 
2025-04-11T03:52:12.6177907Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6178063Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6178136Z     
2025-04-11T03:52:12.6178214Z         Args:
2025-04-11T03:52:12.6178380Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6178551Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6178658Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6178731Z         """
2025-04-11T03:52:12.6178815Z         _lazy_init()
2025-04-11T03:52:12.6178912Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6179015Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6179125Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6179407Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6179548Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6179705Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6179709Z 
2025-04-11T03:52:12.6179953Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6180117Z ____________ test_context_attention[False-False-True-1-16-16-32-7] _____________
2025-04-11T03:52:12.6180121Z 
2025-04-11T03:52:12.6180276Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6180422Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6180517Z use_new_kcache_layout = False
2025-04-11T03:52:12.6180523Z 
2025-04-11T03:52:12.6180724Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6180831Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6180950Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6181186Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6181311Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6181423Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6181563Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6181697Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6181846Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6181942Z     def test_context_attention(
2025-04-11T03:52:12.6182017Z         bsz: int,
2025-04-11T03:52:12.6182100Z         block_size: int,
2025-04-11T03:52:12.6182191Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6182370Z         num_attn_heads: int,
2025-04-11T03:52:12.6182454Z         kv_group_num: int,
2025-04-11T03:52:12.6182542Z         same_context_len: bool,
2025-04-11T03:52:12.6182633Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6182725Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6182803Z     ):
2025-04-11T03:52:12.6182915Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6183103Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6183288Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6183457Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6183621Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6183697Z             return
2025-04-11T03:52:12.6183771Z     
2025-04-11T03:52:12.6183860Z         torch.manual_seed(123)
2025-04-11T03:52:12.6183957Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6184051Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6184139Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6184143Z 
2025-04-11T03:52:12.6184313Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6184425Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6184429Z 
2025-04-11T03:52:12.6184511Z device = None
2025-04-11T03:52:12.6184515Z 
2025-04-11T03:52:12.6184629Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6184777Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6184851Z     
2025-04-11T03:52:12.6184924Z         Args:
2025-04-11T03:52:12.6185090Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6185251Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6185362Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6185436Z         """
2025-04-11T03:52:12.6185514Z         _lazy_init()
2025-04-11T03:52:12.6185616Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6185717Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6185825Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6186103Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6186238Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6186398Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6186402Z 
2025-04-11T03:52:12.6186639Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6186817Z ____________ test_context_attention[False-False-True-1-16-16-32-32] ____________
2025-04-11T03:52:12.6186821Z 
2025-04-11T03:52:12.6186972Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6187226Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6187320Z use_new_kcache_layout = False
2025-04-11T03:52:12.6187324Z 
2025-04-11T03:52:12.6187523Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6187626Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6187742Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6187886Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6188001Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6188118Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6188255Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6188533Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6188687Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6188777Z     def test_context_attention(
2025-04-11T03:52:12.6188861Z         bsz: int,
2025-04-11T03:52:12.6188944Z         block_size: int,
2025-04-11T03:52:12.6189040Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6189125Z         num_attn_heads: int,
2025-04-11T03:52:12.6189213Z         kv_group_num: int,
2025-04-11T03:52:12.6189301Z         same_context_len: bool,
2025-04-11T03:52:12.6189385Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6189478Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6189552Z     ):
2025-04-11T03:52:12.6189666Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6189859Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6190042Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6190222Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6190388Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6190471Z             return
2025-04-11T03:52:12.6190542Z     
2025-04-11T03:52:12.6190631Z         torch.manual_seed(123)
2025-04-11T03:52:12.6190730Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6190821Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6190918Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6190921Z 
2025-04-11T03:52:12.6191086Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6191201Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6191205Z 
2025-04-11T03:52:12.6191281Z device = None
2025-04-11T03:52:12.6191285Z 
2025-04-11T03:52:12.6191409Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6191557Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6191627Z     
2025-04-11T03:52:12.6191705Z         Args:
2025-04-11T03:52:12.6191871Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6192037Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6192146Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6192226Z         """
2025-04-11T03:52:12.6192304Z         _lazy_init()
2025-04-11T03:52:12.6192399Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6192509Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6192612Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6192896Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6193036Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6193194Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6193198Z 
2025-04-11T03:52:12.6193547Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6193721Z _____________ test_context_attention[False-False-True-4-16-8-16-7] _____________
2025-04-11T03:52:12.6193729Z 
2025-04-11T03:52:12.6193878Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6194026Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6194127Z use_new_kcache_layout = False
2025-04-11T03:52:12.6194130Z 
2025-04-11T03:52:12.6194331Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6194442Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6194675Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6194817Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6194933Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6195052Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6195196Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6195331Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6195487Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6195576Z     def test_context_attention(
2025-04-11T03:52:12.6195653Z         bsz: int,
2025-04-11T03:52:12.6195736Z         block_size: int,
2025-04-11T03:52:12.6195825Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6195914Z         num_attn_heads: int,
2025-04-11T03:52:12.6195997Z         kv_group_num: int,
2025-04-11T03:52:12.6196085Z         same_context_len: bool,
2025-04-11T03:52:12.6196177Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6196265Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6196342Z     ):
2025-04-11T03:52:12.6196455Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6196654Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6196838Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6197009Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6197175Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6197251Z             return
2025-04-11T03:52:12.6197327Z     
2025-04-11T03:52:12.6197413Z         torch.manual_seed(123)
2025-04-11T03:52:12.6197516Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6197611Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6197701Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6197708Z 
2025-04-11T03:52:12.6197884Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6198003Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6198007Z 
2025-04-11T03:52:12.6198091Z device = None
2025-04-11T03:52:12.6198094Z 
2025-04-11T03:52:12.6198212Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6198364Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6198435Z     
2025-04-11T03:52:12.6198507Z         Args:
2025-04-11T03:52:12.6198677Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6198838Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6198949Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6199021Z         """
2025-04-11T03:52:12.6199100Z         _lazy_init()
2025-04-11T03:52:12.6199200Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6199302Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6199410Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6199800Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6199941Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6200102Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6200107Z 
2025-04-11T03:52:12.6200349Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6200518Z ____________ test_context_attention[False-False-True-4-16-8-16-32] _____________
2025-04-11T03:52:12.6200521Z 
2025-04-11T03:52:12.6200675Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6200920Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6201007Z use_new_kcache_layout = False
2025-04-11T03:52:12.6201011Z 
2025-04-11T03:52:12.6201211Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6201319Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6201437Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6201578Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6201698Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6201813Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6201951Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6202089Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6202240Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6202334Z     def test_context_attention(
2025-04-11T03:52:12.6202413Z         bsz: int,
2025-04-11T03:52:12.6202493Z         block_size: int,
2025-04-11T03:52:12.6202589Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6202672Z         num_attn_heads: int,
2025-04-11T03:52:12.6202760Z         kv_group_num: int,
2025-04-11T03:52:12.6202849Z         same_context_len: bool,
2025-04-11T03:52:12.6202940Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6203032Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6203107Z     ):
2025-04-11T03:52:12.6203222Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6203414Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6203598Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6203768Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6203932Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6204013Z             return
2025-04-11T03:52:12.6204084Z     
2025-04-11T03:52:12.6204173Z         torch.manual_seed(123)
2025-04-11T03:52:12.6204272Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6204368Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6204460Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6204464Z 
2025-04-11T03:52:12.6204631Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6204745Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6204748Z 
2025-04-11T03:52:12.6204825Z device = None
2025-04-11T03:52:12.6204829Z 
2025-04-11T03:52:12.6204948Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6205099Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6205173Z     
2025-04-11T03:52:12.6205247Z         Args:
2025-04-11T03:52:12.6205416Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6205581Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6205789Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6205873Z         """
2025-04-11T03:52:12.6205953Z         _lazy_init()
2025-04-11T03:52:12.6206053Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6206155Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6206261Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6206546Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6206681Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6206845Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6206943Z 
2025-04-11T03:52:12.6207190Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6207361Z _____________ test_context_attention[False-False-True-4-16-8-32-7] _____________
2025-04-11T03:52:12.6207365Z 
2025-04-11T03:52:12.6207516Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6207674Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6207765Z use_new_kcache_layout = False
2025-04-11T03:52:12.6207769Z 
2025-04-11T03:52:12.6207966Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6208085Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6208201Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6208346Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6208460Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6208579Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6208715Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6208849Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6209007Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6209096Z     def test_context_attention(
2025-04-11T03:52:12.6209175Z         bsz: int,
2025-04-11T03:52:12.6209255Z         block_size: int,
2025-04-11T03:52:12.6209343Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6209430Z         num_attn_heads: int,
2025-04-11T03:52:12.6209512Z         kv_group_num: int,
2025-04-11T03:52:12.6209602Z         same_context_len: bool,
2025-04-11T03:52:12.6209687Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6209777Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6209850Z     ):
2025-04-11T03:52:12.6209960Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6210158Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6210340Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6210519Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6210678Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6210756Z             return
2025-04-11T03:52:12.6210826Z     
2025-04-11T03:52:12.6210910Z         torch.manual_seed(123)
2025-04-11T03:52:12.6211012Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6211100Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6211192Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6211196Z 
2025-04-11T03:52:12.6211362Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6211474Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6211483Z 
2025-04-11T03:52:12.6211561Z device = None
2025-04-11T03:52:12.6211565Z 
2025-04-11T03:52:12.6211679Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6211946Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6212020Z     
2025-04-11T03:52:12.6212097Z         Args:
2025-04-11T03:52:12.6212265Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6212431Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6212538Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6212610Z         """
2025-04-11T03:52:12.6212695Z         _lazy_init()
2025-04-11T03:52:12.6212790Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6212898Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6213003Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6213380Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6213523Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6213684Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6213688Z 
2025-04-11T03:52:12.6213929Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6214098Z ____________ test_context_attention[False-False-True-4-16-8-32-32] _____________
2025-04-11T03:52:12.6214102Z 
2025-04-11T03:52:12.6214253Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6214397Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6214487Z use_new_kcache_layout = False
2025-04-11T03:52:12.6214491Z 
2025-04-11T03:52:12.6214691Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6214799Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6214920Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6215061Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6215182Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6215295Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6215434Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6215569Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6215720Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6215813Z     def test_context_attention(
2025-04-11T03:52:12.6215889Z         bsz: int,
2025-04-11T03:52:12.6215972Z         block_size: int,
2025-04-11T03:52:12.6216063Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6216149Z         num_attn_heads: int,
2025-04-11T03:52:12.6216237Z         kv_group_num: int,
2025-04-11T03:52:12.6216322Z         same_context_len: bool,
2025-04-11T03:52:12.6216411Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6216523Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6216622Z     ):
2025-04-11T03:52:12.6216743Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6216935Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6217119Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6217288Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6217461Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6217542Z             return
2025-04-11T03:52:12.6217618Z     
2025-04-11T03:52:12.6217707Z         torch.manual_seed(123)
2025-04-11T03:52:12.6217810Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6217907Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6217999Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6218003Z 
2025-04-11T03:52:12.6218291Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6218409Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6218413Z 
2025-04-11T03:52:12.6218501Z device = None
2025-04-11T03:52:12.6218505Z 
2025-04-11T03:52:12.6218625Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6218778Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6218853Z     
2025-04-11T03:52:12.6218928Z         Args:
2025-04-11T03:52:12.6219101Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6219265Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6219478Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6219552Z         """
2025-04-11T03:52:12.6219632Z         _lazy_init()
2025-04-11T03:52:12.6219734Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6219838Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6219950Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6220231Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6220369Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6220526Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6220530Z 
2025-04-11T03:52:12.6220768Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6220940Z ____________ test_context_attention[False-False-True-4-16-16-16-7] _____________
2025-04-11T03:52:12.6220948Z 
2025-04-11T03:52:12.6221098Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6221247Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6221335Z use_new_kcache_layout = False
2025-04-11T03:52:12.6221343Z 
2025-04-11T03:52:12.6221543Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6221648Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6221766Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6221907Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6222024Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6222142Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6222278Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6222418Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6222572Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6222662Z     def test_context_attention(
2025-04-11T03:52:12.6222746Z         bsz: int,
2025-04-11T03:52:12.6222826Z         block_size: int,
2025-04-11T03:52:12.6222922Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6223013Z         num_attn_heads: int,
2025-04-11T03:52:12.6223097Z         kv_group_num: int,
2025-04-11T03:52:12.6223184Z         same_context_len: bool,
2025-04-11T03:52:12.6223269Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6223361Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6223432Z     ):
2025-04-11T03:52:12.6223549Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6223739Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6223919Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6224098Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6224260Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6224341Z             return
2025-04-11T03:52:12.6224520Z     
2025-04-11T03:52:12.6224612Z         torch.manual_seed(123)
2025-04-11T03:52:12.6224711Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6224801Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6224896Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6224900Z 
2025-04-11T03:52:12.6225068Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6225184Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6225188Z 
2025-04-11T03:52:12.6225264Z device = None
2025-04-11T03:52:12.6225268Z 
2025-04-11T03:52:12.6225391Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6225632Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6225704Z     
2025-04-11T03:52:12.6225783Z         Args:
2025-04-11T03:52:12.6225947Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6226116Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6226224Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6226299Z         """
2025-04-11T03:52:12.6226379Z         _lazy_init()
2025-04-11T03:52:12.6226478Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6226586Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6226691Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6226974Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6227108Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6227272Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6227276Z 
2025-04-11T03:52:12.6227512Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6227683Z ____________ test_context_attention[False-False-True-4-16-16-16-32] ____________
2025-04-11T03:52:12.6227691Z 
2025-04-11T03:52:12.6227842Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6227987Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6228079Z use_new_kcache_layout = False
2025-04-11T03:52:12.6228083Z 
2025-04-11T03:52:12.6228280Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6228389Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6228544Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6228688Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6228805Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6228917Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6229061Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6229201Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6229358Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6229448Z     def test_context_attention(
2025-04-11T03:52:12.6229522Z         bsz: int,
2025-04-11T03:52:12.6229614Z         block_size: int,
2025-04-11T03:52:12.6229708Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6229794Z         num_attn_heads: int,
2025-04-11T03:52:12.6229877Z         kv_group_num: int,
2025-04-11T03:52:12.6229963Z         same_context_len: bool,
2025-04-11T03:52:12.6230049Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6230138Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6230222Z     ):
2025-04-11T03:52:12.6230334Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6230527Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6230818Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6230994Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6231162Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6231237Z             return
2025-04-11T03:52:12.6231315Z     
2025-04-11T03:52:12.6231401Z         torch.manual_seed(123)
2025-04-11T03:52:12.6231502Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6231594Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6231684Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6231688Z 
2025-04-11T03:52:12.6231980Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6232092Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6232096Z 
2025-04-11T03:52:12.6232177Z device = None
2025-04-11T03:52:12.6232180Z 
2025-04-11T03:52:12.6232300Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6232455Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6232525Z     
2025-04-11T03:52:12.6232598Z         Args:
2025-04-11T03:52:12.6232769Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6232931Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6233041Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6233116Z         """
2025-04-11T03:52:12.6233200Z         _lazy_init()
2025-04-11T03:52:12.6233297Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6233405Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6233514Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6233797Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6233935Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6234093Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6234097Z 
2025-04-11T03:52:12.6234336Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6234505Z ____________ test_context_attention[False-False-True-4-16-16-32-7] _____________
2025-04-11T03:52:12.6234508Z 
2025-04-11T03:52:12.6234661Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6234808Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6234898Z use_new_kcache_layout = False
2025-04-11T03:52:12.6234902Z 
2025-04-11T03:52:12.6235106Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6235210Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6235336Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6235475Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6235600Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6235713Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6235847Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6235985Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6236134Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6236227Z     def test_context_attention(
2025-04-11T03:52:12.6236301Z         bsz: int,
2025-04-11T03:52:12.6236386Z         block_size: int,
2025-04-11T03:52:12.6236480Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6236562Z         num_attn_heads: int,
2025-04-11T03:52:12.6236649Z         kv_group_num: int,
2025-04-11T03:52:12.6236735Z         same_context_len: bool,
2025-04-11T03:52:12.6236926Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6237017Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6237089Z     ):
2025-04-11T03:52:12.6237206Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6237399Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6237584Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6237755Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6237920Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6238088Z             return
2025-04-11T03:52:12.6238159Z     
2025-04-11T03:52:12.6238249Z         torch.manual_seed(123)
2025-04-11T03:52:12.6238349Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6238443Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6238537Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6238541Z 
2025-04-11T03:52:12.6238711Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6238828Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6238832Z 
2025-04-11T03:52:12.6238909Z device = None
2025-04-11T03:52:12.6238913Z 
2025-04-11T03:52:12.6239034Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6239184Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6239257Z     
2025-04-11T03:52:12.6239331Z         Args:
2025-04-11T03:52:12.6239498Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6239668Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6239774Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6239855Z         """
2025-04-11T03:52:12.6239936Z         _lazy_init()
2025-04-11T03:52:12.6240034Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6240137Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6240246Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6240537Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6240671Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6240837Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6240840Z 
2025-04-11T03:52:12.6241082Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6241255Z ____________ test_context_attention[False-False-True-4-16-16-32-32] ____________
2025-04-11T03:52:12.6241259Z 
2025-04-11T03:52:12.6241411Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6241558Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6241653Z use_new_kcache_layout = False
2025-04-11T03:52:12.6241656Z 
2025-04-11T03:52:12.6241855Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6241963Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6242081Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6242223Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6242337Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6242455Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6242596Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6242732Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6242891Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6243099Z     def test_context_attention(
2025-04-11T03:52:12.6243183Z         bsz: int,
2025-04-11T03:52:12.6243266Z         block_size: int,
2025-04-11T03:52:12.6243358Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6243441Z         num_attn_heads: int,
2025-04-11T03:52:12.6243523Z         kv_group_num: int,
2025-04-11T03:52:12.6243615Z         same_context_len: bool,
2025-04-11T03:52:12.6243704Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6243795Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6243866Z     ):
2025-04-11T03:52:12.6243975Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6244175Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6244465Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6244640Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6244806Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6244885Z             return
2025-04-11T03:52:12.6244955Z     
2025-04-11T03:52:12.6245040Z         torch.manual_seed(123)
2025-04-11T03:52:12.6245143Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6245232Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6245325Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6245329Z 
2025-04-11T03:52:12.6245494Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6245609Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6245613Z 
2025-04-11T03:52:12.6245694Z device = None
2025-04-11T03:52:12.6245698Z 
2025-04-11T03:52:12.6245813Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6245965Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6246037Z     
2025-04-11T03:52:12.6246117Z         Args:
2025-04-11T03:52:12.6246284Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6246452Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6246558Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6246631Z         """
2025-04-11T03:52:12.6246711Z         _lazy_init()
2025-04-11T03:52:12.6246809Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6246915Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6247019Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6247301Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6247444Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6247607Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6247611Z 
2025-04-11T03:52:12.6247853Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6248021Z ____________ test_context_attention[False-False-False-1-16-8-16-7] _____________
2025-04-11T03:52:12.6248025Z 
2025-04-11T03:52:12.6248181Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6248327Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6248421Z use_new_kcache_layout = False
2025-04-11T03:52:12.6248425Z 
2025-04-11T03:52:12.6248626Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6248733Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6248861Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6248997Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6249224Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6249341Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6249480Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6249613Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6249761Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6249855Z     def test_context_attention(
2025-04-11T03:52:12.6249930Z         bsz: int,
2025-04-11T03:52:12.6250013Z         block_size: int,
2025-04-11T03:52:12.6250101Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6250188Z         num_attn_heads: int,
2025-04-11T03:52:12.6250271Z         kv_group_num: int,
2025-04-11T03:52:12.6250449Z         same_context_len: bool,
2025-04-11T03:52:12.6250538Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6250626Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6250700Z     ):
2025-04-11T03:52:12.6250813Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6251005Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6251191Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6251361Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6251530Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6251605Z             return
2025-04-11T03:52:12.6251680Z     
2025-04-11T03:52:12.6251765Z         torch.manual_seed(123)
2025-04-11T03:52:12.6251865Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6251962Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6252052Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6252056Z 
2025-04-11T03:52:12.6252224Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6252341Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6252345Z 
2025-04-11T03:52:12.6252425Z device = None
2025-04-11T03:52:12.6252428Z 
2025-04-11T03:52:12.6252545Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6252693Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6252769Z     
2025-04-11T03:52:12.6252843Z         Args:
2025-04-11T03:52:12.6253011Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6253173Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6253284Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6253360Z         """
2025-04-11T03:52:12.6253442Z         _lazy_init()
2025-04-11T03:52:12.6253542Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6253646Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6253757Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6254034Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6254175Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6254330Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6254334Z 
2025-04-11T03:52:12.6254571Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6254747Z ____________ test_context_attention[False-False-False-1-16-8-16-32] ____________
2025-04-11T03:52:12.6254751Z 
2025-04-11T03:52:12.6254901Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6255049Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6255137Z use_new_kcache_layout = False
2025-04-11T03:52:12.6255141Z 
2025-04-11T03:52:12.6255439Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6255547Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6255669Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6255807Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6255924Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6256040Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6256175Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6256313Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6256464Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6256644Z     def test_context_attention(
2025-04-11T03:52:12.6256724Z         bsz: int,
2025-04-11T03:52:12.6256805Z         block_size: int,
2025-04-11T03:52:12.6256898Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6256986Z         num_attn_heads: int,
2025-04-11T03:52:12.6257074Z         kv_group_num: int,
2025-04-11T03:52:12.6257158Z         same_context_len: bool,
2025-04-11T03:52:12.6257244Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6257337Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6257410Z     ):
2025-04-11T03:52:12.6257524Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6257712Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6257890Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6258061Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6258223Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6258303Z             return
2025-04-11T03:52:12.6258373Z     
2025-04-11T03:52:12.6258472Z         torch.manual_seed(123)
2025-04-11T03:52:12.6258572Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6258667Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6258767Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6258771Z 
2025-04-11T03:52:12.6258937Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6259049Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6259053Z 
2025-04-11T03:52:12.6259129Z device = None
2025-04-11T03:52:12.6259133Z 
2025-04-11T03:52:12.6259252Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6259400Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6259473Z     
2025-04-11T03:52:12.6259550Z         Args:
2025-04-11T03:52:12.6259715Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6259882Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6259987Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6260063Z         """
2025-04-11T03:52:12.6260141Z         _lazy_init()
2025-04-11T03:52:12.6260236Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6260343Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6260449Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6260730Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6260866Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6261032Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6261035Z 
2025-04-11T03:52:12.6261271Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6261546Z ____________ test_context_attention[False-False-False-1-16-8-32-7] _____________
2025-04-11T03:52:12.6261554Z 
2025-04-11T03:52:12.6261706Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6261853Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6261948Z use_new_kcache_layout = False
2025-04-11T03:52:12.6261952Z 
2025-04-11T03:52:12.6262150Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6262258Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6262374Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6262516Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6262738Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6262851Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6262994Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6263133Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6263286Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6263377Z     def test_context_attention(
2025-04-11T03:52:12.6263455Z         bsz: int,
2025-04-11T03:52:12.6263547Z         block_size: int,
2025-04-11T03:52:12.6263638Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6263732Z         num_attn_heads: int,
2025-04-11T03:52:12.6263822Z         kv_group_num: int,
2025-04-11T03:52:12.6263911Z         same_context_len: bool,
2025-04-11T03:52:12.6263995Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6264084Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6264160Z     ):
2025-04-11T03:52:12.6264278Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6264471Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6264655Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6264823Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6264988Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6265064Z             return
2025-04-11T03:52:12.6265141Z     
2025-04-11T03:52:12.6265229Z         torch.manual_seed(123)
2025-04-11T03:52:12.6265331Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6265421Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6265511Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6265515Z 
2025-04-11T03:52:12.6265686Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6265799Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6265803Z 
2025-04-11T03:52:12.6265886Z device = None
2025-04-11T03:52:12.6265890Z 
2025-04-11T03:52:12.6266008Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6266164Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6266235Z     
2025-04-11T03:52:12.6266306Z         Args:
2025-04-11T03:52:12.6266476Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6266639Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6266747Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6266819Z         """
2025-04-11T03:52:12.6266900Z         _lazy_init()
2025-04-11T03:52:12.6266995Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6267095Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6267207Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6267490Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6267813Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6267975Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6267978Z 
2025-04-11T03:52:12.6268220Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6268390Z ____________ test_context_attention[False-False-False-1-16-8-32-32] ____________
2025-04-11T03:52:12.6268394Z 
2025-04-11T03:52:12.6268590Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6268737Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6268826Z use_new_kcache_layout = False
2025-04-11T03:52:12.6268940Z 
2025-04-11T03:52:12.6269146Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6269250Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6269375Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6269512Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6269632Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6269744Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6269879Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6270021Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6270171Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6270264Z     def test_context_attention(
2025-04-11T03:52:12.6270340Z         bsz: int,
2025-04-11T03:52:12.6270422Z         block_size: int,
2025-04-11T03:52:12.6270521Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6270605Z         num_attn_heads: int,
2025-04-11T03:52:12.6270691Z         kv_group_num: int,
2025-04-11T03:52:12.6270777Z         same_context_len: bool,
2025-04-11T03:52:12.6270865Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6270955Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6271026Z     ):
2025-04-11T03:52:12.6271144Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6271337Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6271522Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6271690Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6271855Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6271930Z             return
2025-04-11T03:52:12.6272006Z     
2025-04-11T03:52:12.6272095Z         torch.manual_seed(123)
2025-04-11T03:52:12.6272190Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6272284Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6272376Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6272383Z 
2025-04-11T03:52:12.6272550Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6272665Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6272669Z 
2025-04-11T03:52:12.6272746Z device = None
2025-04-11T03:52:12.6272749Z 
2025-04-11T03:52:12.6272871Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6273023Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6273097Z     
2025-04-11T03:52:12.6273171Z         Args:
2025-04-11T03:52:12.6273338Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6273508Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6273614Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6273694Z         """
2025-04-11T03:52:12.6273773Z         _lazy_init()
2025-04-11T03:52:12.6273997Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6274103Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6274212Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6274503Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6274642Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6274810Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6274814Z 
2025-04-11T03:52:12.6275060Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6275324Z ____________ test_context_attention[False-False-False-1-16-16-16-7] ____________
2025-04-11T03:52:12.6275328Z 
2025-04-11T03:52:12.6275481Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6275634Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6275725Z use_new_kcache_layout = False
2025-04-11T03:52:12.6275729Z 
2025-04-11T03:52:12.6275926Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6276037Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6276154Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6276298Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6276417Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6276535Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6276675Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6276817Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6276971Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6277061Z     def test_context_attention(
2025-04-11T03:52:12.6277144Z         bsz: int,
2025-04-11T03:52:12.6277223Z         block_size: int,
2025-04-11T03:52:12.6277316Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6277399Z         num_attn_heads: int,
2025-04-11T03:52:12.6277481Z         kv_group_num: int,
2025-04-11T03:52:12.6277568Z         same_context_len: bool,
2025-04-11T03:52:12.6277652Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6277743Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6277815Z     ):
2025-04-11T03:52:12.6277927Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6278121Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6278304Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6278476Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6278641Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6278720Z             return
2025-04-11T03:52:12.6278794Z     
2025-04-11T03:52:12.6278879Z         torch.manual_seed(123)
2025-04-11T03:52:12.6278981Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6279072Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6279168Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6279172Z 
2025-04-11T03:52:12.6279339Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6279455Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6279459Z 
2025-04-11T03:52:12.6279537Z device = None
2025-04-11T03:52:12.6279544Z 
2025-04-11T03:52:12.6279661Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6279818Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6279891Z     
2025-04-11T03:52:12.6279967Z         Args:
2025-04-11T03:52:12.6280234Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6280403Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6280508Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6280579Z         """
2025-04-11T03:52:12.6280663Z         _lazy_init()
2025-04-11T03:52:12.6280761Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6280869Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6280974Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6281258Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6281494Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6281651Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6281655Z 
2025-04-11T03:52:12.6281902Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6282077Z ___________ test_context_attention[False-False-False-1-16-16-16-32] ____________
2025-04-11T03:52:12.6282082Z 
2025-04-11T03:52:12.6282240Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6282386Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6282480Z use_new_kcache_layout = False
2025-04-11T03:52:12.6282484Z 
2025-04-11T03:52:12.6282683Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6282809Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6282932Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6283070Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6283190Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6283305Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6283444Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6283580Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6283729Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6283823Z     def test_context_attention(
2025-04-11T03:52:12.6283898Z         bsz: int,
2025-04-11T03:52:12.6283987Z         block_size: int,
2025-04-11T03:52:12.6284079Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6284173Z         num_attn_heads: int,
2025-04-11T03:52:12.6284255Z         kv_group_num: int,
2025-04-11T03:52:12.6284342Z         same_context_len: bool,
2025-04-11T03:52:12.6284435Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6284522Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6284597Z     ):
2025-04-11T03:52:12.6284708Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6284902Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6285089Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6285258Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6285425Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6285500Z             return
2025-04-11T03:52:12.6285574Z     
2025-04-11T03:52:12.6285658Z         torch.manual_seed(123)
2025-04-11T03:52:12.6285757Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6285853Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6285945Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6285948Z 
2025-04-11T03:52:12.6286118Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6286230Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6286345Z 
2025-04-11T03:52:12.6286431Z device = None
2025-04-11T03:52:12.6286435Z 
2025-04-11T03:52:12.6286554Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6286707Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6286783Z     
2025-04-11T03:52:12.6286856Z         Args:
2025-04-11T03:52:12.6287026Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6287189Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6287303Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6287376Z         """
2025-04-11T03:52:12.6287566Z         _lazy_init()
2025-04-11T03:52:12.6287669Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6287772Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6287884Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6288171Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6288312Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6288470Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6288474Z 
2025-04-11T03:52:12.6288711Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6288888Z ____________ test_context_attention[False-False-False-1-16-16-32-7] ____________
2025-04-11T03:52:12.6288891Z 
2025-04-11T03:52:12.6289040Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6289191Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6289280Z use_new_kcache_layout = False
2025-04-11T03:52:12.6289284Z 
2025-04-11T03:52:12.6289486Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6289590Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6289710Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6289848Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6289965Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6290083Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6290221Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6290360Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6290509Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6290603Z     def test_context_attention(
2025-04-11T03:52:12.6290686Z         bsz: int,
2025-04-11T03:52:12.6290766Z         block_size: int,
2025-04-11T03:52:12.6290858Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6290943Z         num_attn_heads: int,
2025-04-11T03:52:12.6291033Z         kv_group_num: int,
2025-04-11T03:52:12.6291122Z         same_context_len: bool,
2025-04-11T03:52:12.6291207Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6291301Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6291372Z     ):
2025-04-11T03:52:12.6291484Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6291677Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6291858Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6292030Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6292196Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6292277Z             return
2025-04-11T03:52:12.6292348Z     
2025-04-11T03:52:12.6292436Z         torch.manual_seed(123)
2025-04-11T03:52:12.6292632Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6292724Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6292820Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6292824Z 
2025-04-11T03:52:12.6292992Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6293112Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6293116Z 
2025-04-11T03:52:12.6293193Z device = None
2025-04-11T03:52:12.6293197Z 
2025-04-11T03:52:12.6293322Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6293472Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6293544Z     
2025-04-11T03:52:12.6293730Z         Args:
2025-04-11T03:52:12.6293899Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6294066Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6294176Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6294254Z         """
2025-04-11T03:52:12.6294333Z         _lazy_init()
2025-04-11T03:52:12.6294429Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6294536Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6294643Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6294930Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6295066Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6295232Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6295238Z 
2025-04-11T03:52:12.6295476Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6295653Z ___________ test_context_attention[False-False-False-1-16-16-32-32] ____________
2025-04-11T03:52:12.6295665Z 
2025-04-11T03:52:12.6295819Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6295964Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6296060Z use_new_kcache_layout = False
2025-04-11T03:52:12.6296063Z 
2025-04-11T03:52:12.6296260Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6296368Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6296486Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6296629Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6296743Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6296863Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6297007Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6297141Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6297301Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6297393Z     def test_context_attention(
2025-04-11T03:52:12.6297468Z         bsz: int,
2025-04-11T03:52:12.6297560Z         block_size: int,
2025-04-11T03:52:12.6297649Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6297742Z         num_attn_heads: int,
2025-04-11T03:52:12.6297823Z         kv_group_num: int,
2025-04-11T03:52:12.6297909Z         same_context_len: bool,
2025-04-11T03:52:12.6297996Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6298089Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6298164Z     ):
2025-04-11T03:52:12.6298280Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6298482Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6298667Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6298947Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6299117Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6299191Z             return
2025-04-11T03:52:12.6299271Z     
2025-04-11T03:52:12.6299359Z         torch.manual_seed(123)
2025-04-11T03:52:12.6299468Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6299562Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6299655Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6299659Z 
2025-04-11T03:52:12.6299834Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6299943Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6300046Z 
2025-04-11T03:52:12.6300133Z device = None
2025-04-11T03:52:12.6300137Z 
2025-04-11T03:52:12.6300256Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6300416Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6300488Z     
2025-04-11T03:52:12.6300561Z         Args:
2025-04-11T03:52:12.6300732Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6300898Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6301006Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6301079Z         """
2025-04-11T03:52:12.6301160Z         _lazy_init()
2025-04-11T03:52:12.6301256Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6301358Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6301470Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6301756Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6301897Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6302054Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6302058Z 
2025-04-11T03:52:12.6302303Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6302473Z ____________ test_context_attention[False-False-False-4-16-8-16-7] _____________
2025-04-11T03:52:12.6302476Z 
2025-04-11T03:52:12.6302632Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6302777Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6302869Z use_new_kcache_layout = False
2025-04-11T03:52:12.6302873Z 
2025-04-11T03:52:12.6303082Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6303185Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6303304Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6303448Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6303571Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6303685Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6303824Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6303961Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6304110Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6304204Z     def test_context_attention(
2025-04-11T03:52:12.6304279Z         bsz: int,
2025-04-11T03:52:12.6304359Z         block_size: int,
2025-04-11T03:52:12.6304451Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6304540Z         num_attn_heads: int,
2025-04-11T03:52:12.6304626Z         kv_group_num: int,
2025-04-11T03:52:12.6304709Z         same_context_len: bool,
2025-04-11T03:52:12.6304798Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6304886Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6305057Z     ):
2025-04-11T03:52:12.6305175Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6305366Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6305549Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6305717Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6305881Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6305958Z             return
2025-04-11T03:52:12.6306030Z     
2025-04-11T03:52:12.6306120Z         torch.manual_seed(123)
2025-04-11T03:52:12.6306320Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6306414Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6306506Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6306510Z 
2025-04-11T03:52:12.6306680Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6306795Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6306798Z 
2025-04-11T03:52:12.6306874Z device = None
2025-04-11T03:52:12.6306877Z 
2025-04-11T03:52:12.6306996Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6307144Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6307218Z     
2025-04-11T03:52:12.6307293Z         Args:
2025-04-11T03:52:12.6307458Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6307627Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6307736Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6307817Z         """
2025-04-11T03:52:12.6307895Z         _lazy_init()
2025-04-11T03:52:12.6307996Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6308103Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6308209Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6308540Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6308680Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6308844Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6308848Z 
2025-04-11T03:52:12.6309086Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6309259Z ____________ test_context_attention[False-False-False-4-16-8-16-32] ____________
2025-04-11T03:52:12.6309265Z 
2025-04-11T03:52:12.6309414Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6309561Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6309652Z use_new_kcache_layout = False
2025-04-11T03:52:12.6309655Z 
2025-04-11T03:52:12.6309854Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6309962Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6310084Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6310231Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6310346Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6310463Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6310602Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6310739Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6310893Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6310983Z     def test_context_attention(
2025-04-11T03:52:12.6311063Z         bsz: int,
2025-04-11T03:52:12.6311298Z         block_size: int,
2025-04-11T03:52:12.6311395Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6311481Z         num_attn_heads: int,
2025-04-11T03:52:12.6311565Z         kv_group_num: int,
2025-04-11T03:52:12.6311655Z         same_context_len: bool,
2025-04-11T03:52:12.6311740Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6311834Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6311905Z     ):
2025-04-11T03:52:12.6312018Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6312216Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6312395Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6312682Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6312843Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6312926Z             return
2025-04-11T03:52:12.6312997Z     
2025-04-11T03:52:12.6313083Z         torch.manual_seed(123)
2025-04-11T03:52:12.6313189Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6313278Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6313369Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6313373Z 
2025-04-11T03:52:12.6313543Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6313656Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6313660Z 
2025-04-11T03:52:12.6313737Z device = None
2025-04-11T03:52:12.6313741Z 
2025-04-11T03:52:12.6313860Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6314017Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6314089Z     
2025-04-11T03:52:12.6314168Z         Args:
2025-04-11T03:52:12.6314336Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6314506Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6314611Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6314682Z         """
2025-04-11T03:52:12.6314770Z         _lazy_init()
2025-04-11T03:52:12.6314868Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6314979Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6315088Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6315375Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6315520Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6315677Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6315681Z 
2025-04-11T03:52:12.6315927Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6316095Z ____________ test_context_attention[False-False-False-4-16-8-32-7] _____________
2025-04-11T03:52:12.6316099Z 
2025-04-11T03:52:12.6316254Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6316403Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6316495Z use_new_kcache_layout = False
2025-04-11T03:52:12.6316499Z 
2025-04-11T03:52:12.6316699Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6316805Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6316927Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6317079Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6317241Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6317357Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6317605Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6317743Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6317892Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6317986Z     def test_context_attention(
2025-04-11T03:52:12.6318061Z         bsz: int,
2025-04-11T03:52:12.6318145Z         block_size: int,
2025-04-11T03:52:12.6318235Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6318325Z         num_attn_heads: int,
2025-04-11T03:52:12.6318407Z         kv_group_num: int,
2025-04-11T03:52:12.6318492Z         same_context_len: bool,
2025-04-11T03:52:12.6318580Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6318766Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6318844Z     ):
2025-04-11T03:52:12.6318953Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6319149Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6319338Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6319508Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6319675Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6319753Z             return
2025-04-11T03:52:12.6319832Z     
2025-04-11T03:52:12.6319922Z         torch.manual_seed(123)
2025-04-11T03:52:12.6320022Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6320117Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6320206Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6320213Z 
2025-04-11T03:52:12.6320388Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6320499Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6320503Z 
2025-04-11T03:52:12.6320583Z device = None
2025-04-11T03:52:12.6320589Z 
2025-04-11T03:52:12.6320705Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6320853Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6320928Z     
2025-04-11T03:52:12.6321000Z         Args:
2025-04-11T03:52:12.6321168Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6321330Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6321439Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6321512Z         """
2025-04-11T03:52:12.6321589Z         _lazy_init()
2025-04-11T03:52:12.6321694Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6321795Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6321903Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6322188Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6322329Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6322485Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6322489Z 
2025-04-11T03:52:12.6322729Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6322905Z ____________ test_context_attention[False-False-False-4-16-8-32-32] ____________
2025-04-11T03:52:12.6322909Z 
2025-04-11T03:52:12.6323058Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6323210Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6323301Z use_new_kcache_layout = False
2025-04-11T03:52:12.6323305Z 
2025-04-11T03:52:12.6323509Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6323718Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6323842Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6323986Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6324101Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6324218Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6324357Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6324496Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6324646Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6324736Z     def test_context_attention(
2025-04-11T03:52:12.6324908Z         bsz: int,
2025-04-11T03:52:12.6324991Z         block_size: int,
2025-04-11T03:52:12.6325084Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6325168Z         num_attn_heads: int,
2025-04-11T03:52:12.6325261Z         kv_group_num: int,
2025-04-11T03:52:12.6325355Z         same_context_len: bool,
2025-04-11T03:52:12.6325441Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6325538Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6325611Z     ):
2025-04-11T03:52:12.6325724Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6325915Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6326095Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6326266Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6326428Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6326512Z             return
2025-04-11T03:52:12.6326585Z     
2025-04-11T03:52:12.6326673Z         torch.manual_seed(123)
2025-04-11T03:52:12.6326774Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6326867Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6326961Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6326965Z 
2025-04-11T03:52:12.6327131Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6327246Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6327250Z 
2025-04-11T03:52:12.6327325Z device = None
2025-04-11T03:52:12.6327329Z 
2025-04-11T03:52:12.6327449Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6327597Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6327668Z     
2025-04-11T03:52:12.6327749Z         Args:
2025-04-11T03:52:12.6327917Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6328085Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6328191Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6328270Z         """
2025-04-11T03:52:12.6328349Z         _lazy_init()
2025-04-11T03:52:12.6328446Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6328552Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6328654Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6328936Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6329074Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6329234Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6329238Z 
2025-04-11T03:52:12.6329478Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6329647Z ____________ test_context_attention[False-False-False-4-16-16-16-7] ____________
2025-04-11T03:52:12.6329656Z 
2025-04-11T03:52:12.6329906Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6330056Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6330151Z use_new_kcache_layout = False
2025-04-11T03:52:12.6330155Z 
2025-04-11T03:52:12.6330353Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6330465Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6330582Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6330725Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6330842Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6330959Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6331204Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6331340Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6331501Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6331592Z     def test_context_attention(
2025-04-11T03:52:12.6331667Z         bsz: int,
2025-04-11T03:52:12.6331754Z         block_size: int,
2025-04-11T03:52:12.6331845Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6331932Z         num_attn_heads: int,
2025-04-11T03:52:12.6332016Z         kv_group_num: int,
2025-04-11T03:52:12.6332106Z         same_context_len: bool,
2025-04-11T03:52:12.6332189Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6332278Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6332356Z     ):
2025-04-11T03:52:12.6332469Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6332665Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6332849Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6333026Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6333188Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6333264Z             return
2025-04-11T03:52:12.6333350Z     
2025-04-11T03:52:12.6333435Z         torch.manual_seed(123)
2025-04-11T03:52:12.6333536Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6333629Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6333718Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6333722Z 
2025-04-11T03:52:12.6333891Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6334005Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6334012Z 
2025-04-11T03:52:12.6334091Z device = None
2025-04-11T03:52:12.6334095Z 
2025-04-11T03:52:12.6334211Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6334364Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6334438Z     
2025-04-11T03:52:12.6334511Z         Args:
2025-04-11T03:52:12.6334681Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6334844Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6334956Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6335030Z         """
2025-04-11T03:52:12.6335110Z         _lazy_init()
2025-04-11T03:52:12.6335206Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6335306Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6335419Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6335701Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6335839Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6336113Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6336118Z 
2025-04-11T03:52:12.6336363Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6336536Z ___________ test_context_attention[False-False-False-4-16-16-16-32] ____________
2025-04-11T03:52:12.6336540Z 
2025-04-11T03:52:12.6336696Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6336840Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6336929Z use_new_kcache_layout = False
2025-04-11T03:52:12.6336932Z 
2025-04-11T03:52:12.6337135Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6337334Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6337457Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6337594Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6337717Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6337830Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6337968Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6338106Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6338253Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6338348Z     def test_context_attention(
2025-04-11T03:52:12.6338425Z         bsz: int,
2025-04-11T03:52:12.6338508Z         block_size: int,
2025-04-11T03:52:12.6338603Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6338688Z         num_attn_heads: int,
2025-04-11T03:52:12.6338779Z         kv_group_num: int,
2025-04-11T03:52:12.6338866Z         same_context_len: bool,
2025-04-11T03:52:12.6338958Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6339044Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6339116Z     ):
2025-04-11T03:52:12.6339234Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6339428Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6339612Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6339780Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6339947Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6340024Z             return
2025-04-11T03:52:12.6340095Z     
2025-04-11T03:52:12.6340188Z         torch.manual_seed(123)
2025-04-11T03:52:12.6340287Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6340384Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6340472Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6340476Z 
2025-04-11T03:52:12.6340643Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6340759Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6340763Z 
2025-04-11T03:52:12.6340840Z device = None
2025-04-11T03:52:12.6340844Z 
2025-04-11T03:52:12.6340965Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6341116Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6341191Z     
2025-04-11T03:52:12.6341266Z         Args:
2025-04-11T03:52:12.6341437Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6341601Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6341710Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6341788Z         """
2025-04-11T03:52:12.6341867Z         _lazy_init()
2025-04-11T03:52:12.6341969Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6342071Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6342278Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6342573Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6342711Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6342877Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6342881Z 
2025-04-11T03:52:12.6343120Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6343295Z ____________ test_context_attention[False-False-False-4-16-16-32-7] ____________
2025-04-11T03:52:12.6343393Z 
2025-04-11T03:52:12.6343544Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6343695Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6343784Z use_new_kcache_layout = False
2025-04-11T03:52:12.6343791Z 
2025-04-11T03:52:12.6343991Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6344102Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6344218Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6344362Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6344479Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6344597Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6344733Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6344868Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6345026Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6345115Z     def test_context_attention(
2025-04-11T03:52:12.6345193Z         bsz: int,
2025-04-11T03:52:12.6345274Z         block_size: int,
2025-04-11T03:52:12.6345369Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6345452Z         num_attn_heads: int,
2025-04-11T03:52:12.6345534Z         kv_group_num: int,
2025-04-11T03:52:12.6345623Z         same_context_len: bool,
2025-04-11T03:52:12.6345706Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6345797Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6345868Z     ):
2025-04-11T03:52:12.6345979Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6346173Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6346352Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6346532Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6346695Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6346772Z             return
2025-04-11T03:52:12.6346843Z     
2025-04-11T03:52:12.6346931Z         torch.manual_seed(123)
2025-04-11T03:52:12.6347036Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6347125Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6347217Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6347221Z 
2025-04-11T03:52:12.6347388Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6347502Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6347505Z 
2025-04-11T03:52:12.6347585Z device = None
2025-04-11T03:52:12.6347589Z 
2025-04-11T03:52:12.6347710Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6347867Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6347943Z     
2025-04-11T03:52:12.6348019Z         Args:
2025-04-11T03:52:12.6348187Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6348520Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6348636Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6348711Z         """
2025-04-11T03:52:12.6348800Z         _lazy_init()
2025-04-11T03:52:12.6348896Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6349003Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6349108Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6349394Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6349532Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6349803Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6349806Z 
2025-04-11T03:52:12.6350051Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6350229Z ___________ test_context_attention[False-False-False-4-16-16-32-32] ____________
2025-04-11T03:52:12.6350233Z 
2025-04-11T03:52:12.6350386Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6350532Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6350623Z use_new_kcache_layout = False
2025-04-11T03:52:12.6350627Z 
2025-04-11T03:52:12.6350827Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6350933Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6351049Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6351186Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6351312Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6351425Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6351566Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6351699Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6351849Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6351945Z     def test_context_attention(
2025-04-11T03:52:12.6352020Z         bsz: int,
2025-04-11T03:52:12.6352103Z         block_size: int,
2025-04-11T03:52:12.6352191Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6352279Z         num_attn_heads: int,
2025-04-11T03:52:12.6352361Z         kv_group_num: int,
2025-04-11T03:52:12.6352446Z         same_context_len: bool,
2025-04-11T03:52:12.6352533Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6352619Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6352695Z     ):
2025-04-11T03:52:12.6352805Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6352993Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6353181Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6353349Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6353513Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6353587Z             return
2025-04-11T03:52:12.6353661Z     
2025-04-11T03:52:12.6353748Z         torch.manual_seed(123)
2025-04-11T03:52:12.6353847Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6353946Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6354037Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6354040Z 
2025-04-11T03:52:12.6354209Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6354323Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6354326Z 
2025-04-11T03:52:12.6354406Z device = None
2025-04-11T03:52:12.6354410Z 
2025-04-11T03:52:12.6354649Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6354804Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6354880Z     
2025-04-11T03:52:12.6354953Z         Args:
2025-04-11T03:52:12.6355119Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6355283Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6355393Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6355468Z         """
2025-04-11T03:52:12.6355550Z         _lazy_init()
2025-04-11T03:52:12.6355658Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6355876Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6355990Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6356279Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6356420Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6356579Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6356583Z 
2025-04-11T03:52:12.6356826Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6356997Z ______________ test_flash_decoding[True-False-1-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.6357001Z 
2025-04-11T03:52:12.6357151Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6357322Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6357415Z use_new_kcache_layout = True
2025-04-11T03:52:12.6357419Z 
2025-04-11T03:52:12.6357620Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6357724Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6357845Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6357982Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6358096Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6358217Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6358353Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6358459Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6358593Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6358747Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6358834Z     def test_flash_decoding(
2025-04-11T03:52:12.6358912Z         bsz: int,
2025-04-11T03:52:12.6358995Z         block_size: int,
2025-04-11T03:52:12.6359085Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6359172Z         num_attn_heads: int,
2025-04-11T03:52:12.6359254Z         kv_group_num: int,
2025-04-11T03:52:12.6359339Z         same_context_len: bool,
2025-04-11T03:52:12.6359421Z         q_len: int,
2025-04-11T03:52:12.6359505Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6359598Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6359670Z     ):
2025-04-11T03:52:12.6359778Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6359975Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6360160Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6360335Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6360499Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6360662Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6360735Z     
2025-04-11T03:52:12.6360921Z         torch.manual_seed(123)
2025-04-11T03:52:12.6361020Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6361112Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6361116Z 
2025-04-11T03:52:12.6361276Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6361386Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6361389Z 
2025-04-11T03:52:12.6361471Z device = None
2025-04-11T03:52:12.6361474Z 
2025-04-11T03:52:12.6361590Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6361745Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6361815Z     
2025-04-11T03:52:12.6361980Z         Args:
2025-04-11T03:52:12.6362151Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6362315Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6362428Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6362500Z         """
2025-04-11T03:52:12.6362579Z         _lazy_init()
2025-04-11T03:52:12.6362680Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6362785Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6362894Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6363175Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6363315Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6363474Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6363480Z 
2025-04-11T03:52:12.6363720Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6363890Z _____________ test_flash_decoding[True-False-1-True-1-16-8-16-16] ______________
2025-04-11T03:52:12.6363894Z 
2025-04-11T03:52:12.6364046Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6364214Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6364300Z use_new_kcache_layout = True
2025-04-11T03:52:12.6364304Z 
2025-04-11T03:52:12.6364512Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6364616Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6364736Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6364872Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6364988Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6365113Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6365255Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6365364Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6365506Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6365665Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6365755Z     def test_flash_decoding(
2025-04-11T03:52:12.6365829Z         bsz: int,
2025-04-11T03:52:12.6365913Z         block_size: int,
2025-04-11T03:52:12.6366002Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6366087Z         num_attn_heads: int,
2025-04-11T03:52:12.6366169Z         kv_group_num: int,
2025-04-11T03:52:12.6366255Z         same_context_len: bool,
2025-04-11T03:52:12.6366337Z         q_len: int,
2025-04-11T03:52:12.6366423Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6366515Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6366589Z     ):
2025-04-11T03:52:12.6366700Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6366901Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6367265Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6367447Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6367614Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6367775Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6367849Z     
2025-04-11T03:52:12.6367934Z         torch.manual_seed(123)
2025-04-11T03:52:12.6368030Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6368121Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6368125Z 
2025-04-11T03:52:12.6368286Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6368497Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6368501Z 
2025-04-11T03:52:12.6368588Z device = None
2025-04-11T03:52:12.6368591Z 
2025-04-11T03:52:12.6368710Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6368868Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6368940Z     
2025-04-11T03:52:12.6369015Z         Args:
2025-04-11T03:52:12.6369186Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6369349Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6369458Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6369531Z         """
2025-04-11T03:52:12.6369611Z         _lazy_init()
2025-04-11T03:52:12.6369712Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6369818Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6369925Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6370204Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6370348Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6370505Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6370509Z 
2025-04-11T03:52:12.6370751Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6370913Z ______________ test_flash_decoding[True-False-1-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.6370918Z 
2025-04-11T03:52:12.6371067Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6371236Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6371329Z use_new_kcache_layout = True
2025-04-11T03:52:12.6371333Z 
2025-04-11T03:52:12.6371533Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6371638Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6371763Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6371902Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6372020Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6372138Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6372272Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6372381Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6372515Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6372672Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6372757Z     def test_flash_decoding(
2025-04-11T03:52:12.6372836Z         bsz: int,
2025-04-11T03:52:12.6372925Z         block_size: int,
2025-04-11T03:52:12.6373012Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6373098Z         num_attn_heads: int,
2025-04-11T03:52:12.6373181Z         kv_group_num: int,
2025-04-11T03:52:12.6373367Z         same_context_len: bool,
2025-04-11T03:52:12.6373455Z         q_len: int,
2025-04-11T03:52:12.6373541Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6373632Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6373703Z     ):
2025-04-11T03:52:12.6373814Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6374011Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6374193Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6374367Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6374627Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6374794Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6374866Z     
2025-04-11T03:52:12.6374962Z         torch.manual_seed(123)
2025-04-11T03:52:12.6375055Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6375150Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6375154Z 
2025-04-11T03:52:12.6375315Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6375428Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6375432Z 
2025-04-11T03:52:12.6375514Z device = None
2025-04-11T03:52:12.6375517Z 
2025-04-11T03:52:12.6375635Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6375792Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6375866Z     
2025-04-11T03:52:12.6375944Z         Args:
2025-04-11T03:52:12.6376117Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6376282Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6376397Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6376474Z         """
2025-04-11T03:52:12.6376552Z         _lazy_init()
2025-04-11T03:52:12.6376652Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6376753Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6376867Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6377159Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6377299Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6377457Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6377464Z 
2025-04-11T03:52:12.6377712Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6377882Z _____________ test_flash_decoding[True-False-1-True-1-16-8-32-16] ______________
2025-04-11T03:52:12.6377885Z 
2025-04-11T03:52:12.6378039Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6378208Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6378299Z use_new_kcache_layout = True
2025-04-11T03:52:12.6378302Z 
2025-04-11T03:52:12.6378507Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6378614Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6378737Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6378878Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6378996Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6379115Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6379257Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6379365Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6379612Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6379776Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6379866Z     def test_flash_decoding(
2025-04-11T03:52:12.6379944Z         bsz: int,
2025-04-11T03:52:12.6380033Z         block_size: int,
2025-04-11T03:52:12.6380126Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6380212Z         num_attn_heads: int,
2025-04-11T03:52:12.6380298Z         kv_group_num: int,
2025-04-11T03:52:12.6380389Z         same_context_len: bool,
2025-04-11T03:52:12.6380472Z         q_len: int,
2025-04-11T03:52:12.6380559Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6380654Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6380856Z     ):
2025-04-11T03:52:12.6380974Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6381169Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6381354Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6381534Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6381697Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6381859Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6381931Z     
2025-04-11T03:52:12.6382021Z         torch.manual_seed(123)
2025-04-11T03:52:12.6382111Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6382203Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6382206Z 
2025-04-11T03:52:12.6382367Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6382482Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6382485Z 
2025-04-11T03:52:12.6382569Z device = None
2025-04-11T03:52:12.6382573Z 
2025-04-11T03:52:12.6382694Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6382844Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6382917Z     
2025-04-11T03:52:12.6382991Z         Args:
2025-04-11T03:52:12.6383162Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6383324Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6383434Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6383508Z         """
2025-04-11T03:52:12.6383588Z         _lazy_init()
2025-04-11T03:52:12.6383683Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6383788Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6383898Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6384181Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6384323Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6384481Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6384485Z 
2025-04-11T03:52:12.6384731Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6384897Z _____________ test_flash_decoding[True-False-1-True-1-16-16-16-7] ______________
2025-04-11T03:52:12.6384901Z 
2025-04-11T03:52:12.6385054Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6385219Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6385315Z use_new_kcache_layout = True
2025-04-11T03:52:12.6385319Z 
2025-04-11T03:52:12.6385521Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6385626Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6385849Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6385993Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6386113Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6386228Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6386363Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6386472Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6386609Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6386762Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6386848Z     def test_flash_decoding(
2025-04-11T03:52:12.6387026Z         bsz: int,
2025-04-11T03:52:12.6387115Z         block_size: int,
2025-04-11T03:52:12.6387206Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6387294Z         num_attn_heads: int,
2025-04-11T03:52:12.6387377Z         kv_group_num: int,
2025-04-11T03:52:12.6387473Z         same_context_len: bool,
2025-04-11T03:52:12.6387552Z         q_len: int,
2025-04-11T03:52:12.6387636Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6387728Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6387800Z     ):
2025-04-11T03:52:12.6387915Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6388109Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6388292Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6388503Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6388671Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6388833Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6388907Z     
2025-04-11T03:52:12.6388999Z         torch.manual_seed(123)
2025-04-11T03:52:12.6389094Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6389185Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6389189Z 
2025-04-11T03:52:12.6389348Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6389458Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6389461Z 
2025-04-11T03:52:12.6389543Z device = None
2025-04-11T03:52:12.6389547Z 
2025-04-11T03:52:12.6389663Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6389814Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6389885Z     
2025-04-11T03:52:12.6389960Z         Args:
2025-04-11T03:52:12.6390136Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6390301Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6390419Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6390493Z         """
2025-04-11T03:52:12.6390574Z         _lazy_init()
2025-04-11T03:52:12.6390671Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6390775Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6390884Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6391164Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6391306Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6391461Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6391468Z 
2025-04-11T03:52:12.6391710Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6391878Z _____________ test_flash_decoding[True-False-1-True-1-16-16-16-16] _____________
2025-04-11T03:52:12.6391881Z 
2025-04-11T03:52:12.6392156Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6392322Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6392411Z use_new_kcache_layout = True
2025-04-11T03:52:12.6392415Z 
2025-04-11T03:52:12.6392616Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6392721Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6392843Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6392983Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6393101Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6393321Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6393459Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6393573Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6393715Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6393871Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6393960Z     def test_flash_decoding(
2025-04-11T03:52:12.6394039Z         bsz: int,
2025-04-11T03:52:12.6394123Z         block_size: int,
2025-04-11T03:52:12.6394217Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6394306Z         num_attn_heads: int,
2025-04-11T03:52:12.6394390Z         kv_group_num: int,
2025-04-11T03:52:12.6394480Z         same_context_len: bool,
2025-04-11T03:52:12.6394554Z         q_len: int,
2025-04-11T03:52:12.6394639Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6394733Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6394809Z     ):
2025-04-11T03:52:12.6394923Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6395118Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6395302Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6395478Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6395642Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6395804Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6395878Z     
2025-04-11T03:52:12.6395969Z         torch.manual_seed(123)
2025-04-11T03:52:12.6396059Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6396147Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6396151Z 
2025-04-11T03:52:12.6396307Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6396420Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6396423Z 
2025-04-11T03:52:12.6396505Z device = None
2025-04-11T03:52:12.6396509Z 
2025-04-11T03:52:12.6396628Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6396781Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6396853Z     
2025-04-11T03:52:12.6396925Z         Args:
2025-04-11T03:52:12.6397093Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6397254Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6397367Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6397441Z         """
2025-04-11T03:52:12.6397524Z         _lazy_init()
2025-04-11T03:52:12.6397622Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6397727Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6397838Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6398121Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6398375Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6398534Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6398539Z 
2025-04-11T03:52:12.6398782Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6398947Z _____________ test_flash_decoding[True-False-1-True-1-16-16-32-7] ______________
2025-04-11T03:52:12.6398951Z 
2025-04-11T03:52:12.6399103Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6399265Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6399446Z use_new_kcache_layout = True
2025-04-11T03:52:12.6399450Z 
2025-04-11T03:52:12.6399661Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6399763Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6399889Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6400029Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6400147Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6400262Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6400399Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6400507Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6400642Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6400794Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6400882Z     def test_flash_decoding(
2025-04-11T03:52:12.6400964Z         bsz: int,
2025-04-11T03:52:12.6401044Z         block_size: int,
2025-04-11T03:52:12.6401132Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6401221Z         num_attn_heads: int,
2025-04-11T03:52:12.6401303Z         kv_group_num: int,
2025-04-11T03:52:12.6401396Z         same_context_len: bool,
2025-04-11T03:52:12.6401472Z         q_len: int,
2025-04-11T03:52:12.6401560Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6401651Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6401725Z     ):
2025-04-11T03:52:12.6401840Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6402034Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6402224Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6402395Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6402560Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6402723Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6402795Z     
2025-04-11T03:52:12.6402883Z         torch.manual_seed(123)
2025-04-11T03:52:12.6402975Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6403064Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6403072Z 
2025-04-11T03:52:12.6403227Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6403339Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6403343Z 
2025-04-11T03:52:12.6403428Z device = None
2025-04-11T03:52:12.6403432Z 
2025-04-11T03:52:12.6403550Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6403702Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6403775Z     
2025-04-11T03:52:12.6403851Z         Args:
2025-04-11T03:52:12.6404021Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6404187Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6404413Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6404493Z         """
2025-04-11T03:52:12.6404580Z         _lazy_init()
2025-04-11T03:52:12.6404675Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6404777Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6404888Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6405168Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6405309Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6405467Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6405564Z 
2025-04-11T03:52:12.6405809Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6405977Z _____________ test_flash_decoding[True-False-1-True-1-16-16-32-16] _____________
2025-04-11T03:52:12.6405981Z 
2025-04-11T03:52:12.6406138Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6406302Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6406391Z use_new_kcache_layout = True
2025-04-11T03:52:12.6406400Z 
2025-04-11T03:52:12.6406598Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6406701Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6406820Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6406958Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6407080Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6407196Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6407333Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6407442Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6407578Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6407735Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6407824Z     def test_flash_decoding(
2025-04-11T03:52:12.6407902Z         bsz: int,
2025-04-11T03:52:12.6407985Z         block_size: int,
2025-04-11T03:52:12.6408076Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6408163Z         num_attn_heads: int,
2025-04-11T03:52:12.6408247Z         kv_group_num: int,
2025-04-11T03:52:12.6408335Z         same_context_len: bool,
2025-04-11T03:52:12.6408411Z         q_len: int,
2025-04-11T03:52:12.6408496Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6408588Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6408662Z     ):
2025-04-11T03:52:12.6408779Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6408970Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6409157Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6409326Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6409488Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6409649Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6409722Z     
2025-04-11T03:52:12.6409811Z         torch.manual_seed(123)
2025-04-11T03:52:12.6409899Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6409993Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6409996Z 
2025-04-11T03:52:12.6410151Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6410267Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6410271Z 
2025-04-11T03:52:12.6410354Z device = None
2025-04-11T03:52:12.6410358Z 
2025-04-11T03:52:12.6410595Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6410750Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6410824Z     
2025-04-11T03:52:12.6410901Z         Args:
2025-04-11T03:52:12.6411069Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6411235Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6411347Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6411423Z         """
2025-04-11T03:52:12.6411507Z         _lazy_init()
2025-04-11T03:52:12.6411603Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6411707Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6411923Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6412206Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6412357Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6412518Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6412522Z 
2025-04-11T03:52:12.6412769Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6412941Z ______________ test_flash_decoding[True-False-1-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.6412945Z 
2025-04-11T03:52:12.6413105Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6413271Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6413362Z use_new_kcache_layout = True
2025-04-11T03:52:12.6413372Z 
2025-04-11T03:52:12.6413572Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6413678Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6413804Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6413946Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6414068Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6414185Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6414327Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6414434Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6414573Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6414732Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6414822Z     def test_flash_decoding(
2025-04-11T03:52:12.6414907Z         bsz: int,
2025-04-11T03:52:12.6414993Z         block_size: int,
2025-04-11T03:52:12.6415086Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6415177Z         num_attn_heads: int,
2025-04-11T03:52:12.6415263Z         kv_group_num: int,
2025-04-11T03:52:12.6415359Z         same_context_len: bool,
2025-04-11T03:52:12.6415439Z         q_len: int,
2025-04-11T03:52:12.6415526Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6415622Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6415697Z     ):
2025-04-11T03:52:12.6415814Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6416008Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6416197Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6416373Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6416539Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6416709Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6416784Z     
2025-04-11T03:52:12.6416877Z         torch.manual_seed(123)
2025-04-11T03:52:12.6417085Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6417183Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6417188Z 
2025-04-11T03:52:12.6417342Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6417452Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6417456Z 
2025-04-11T03:52:12.6417537Z device = None
2025-04-11T03:52:12.6417542Z 
2025-04-11T03:52:12.6417670Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6417863Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6417936Z     
2025-04-11T03:52:12.6418013Z         Args:
2025-04-11T03:52:12.6418277Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6418442Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6418555Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6418630Z         """
2025-04-11T03:52:12.6418709Z         _lazy_init()
2025-04-11T03:52:12.6418807Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6418914Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6419018Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6419301Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6419441Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6419602Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6419609Z 
2025-04-11T03:52:12.6419853Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6420021Z _____________ test_flash_decoding[True-False-1-True-4-16-8-16-16] ______________
2025-04-11T03:52:12.6420025Z 
2025-04-11T03:52:12.6420180Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6420342Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6420435Z use_new_kcache_layout = True
2025-04-11T03:52:12.6420438Z 
2025-04-11T03:52:12.6420636Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6420740Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6420861Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6421001Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6421120Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6421235Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6421374Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6421478Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6421622Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6421778Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6421868Z     def test_flash_decoding(
2025-04-11T03:52:12.6421949Z         bsz: int,
2025-04-11T03:52:12.6422031Z         block_size: int,
2025-04-11T03:52:12.6422125Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6422213Z         num_attn_heads: int,
2025-04-11T03:52:12.6422297Z         kv_group_num: int,
2025-04-11T03:52:12.6422388Z         same_context_len: bool,
2025-04-11T03:52:12.6422463Z         q_len: int,
2025-04-11T03:52:12.6422555Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6422646Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6422722Z     ):
2025-04-11T03:52:12.6422844Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6423039Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6423326Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6423503Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6423668Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6423831Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6423902Z     
2025-04-11T03:52:12.6423994Z         torch.manual_seed(123)
2025-04-11T03:52:12.6424083Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6424178Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6424182Z 
2025-04-11T03:52:12.6424339Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6424549Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6424556Z 
2025-04-11T03:52:12.6424633Z device = None
2025-04-11T03:52:12.6424637Z 
2025-04-11T03:52:12.6424760Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6424914Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6424987Z     
2025-04-11T03:52:12.6425064Z         Args:
2025-04-11T03:52:12.6425229Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6425394Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6425504Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6425578Z         """
2025-04-11T03:52:12.6425668Z         _lazy_init()
2025-04-11T03:52:12.6425765Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6425872Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6425980Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6426265Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6426410Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6426565Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6426569Z 
2025-04-11T03:52:12.6426809Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6426972Z ______________ test_flash_decoding[True-False-1-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.6426975Z 
2025-04-11T03:52:12.6427128Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6427290Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6427382Z use_new_kcache_layout = True
2025-04-11T03:52:12.6427389Z 
2025-04-11T03:52:12.6427593Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6427696Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6427822Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6427958Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6428081Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6428193Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6428333Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6428466Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6428604Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6428759Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6428845Z     def test_flash_decoding(
2025-04-11T03:52:12.6428932Z         bsz: int,
2025-04-11T03:52:12.6429017Z         block_size: int,
2025-04-11T03:52:12.6429108Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6429199Z         num_attn_heads: int,
2025-04-11T03:52:12.6429282Z         kv_group_num: int,
2025-04-11T03:52:12.6429503Z         same_context_len: bool,
2025-04-11T03:52:12.6429583Z         q_len: int,
2025-04-11T03:52:12.6429674Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6429763Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6429836Z     ):
2025-04-11T03:52:12.6429950Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6430145Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6430338Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6430510Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6430678Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6430961Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6431032Z     
2025-04-11T03:52:12.6431126Z         torch.manual_seed(123)
2025-04-11T03:52:12.6431219Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6431315Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6431319Z 
2025-04-11T03:52:12.6431473Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6431588Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6431592Z 
2025-04-11T03:52:12.6431670Z device = None
2025-04-11T03:52:12.6431673Z 
2025-04-11T03:52:12.6431790Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6431945Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6432015Z     
2025-04-11T03:52:12.6432093Z         Args:
2025-04-11T03:52:12.6432268Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6432441Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6432550Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6432626Z         """
2025-04-11T03:52:12.6432708Z         _lazy_init()
2025-04-11T03:52:12.6432807Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6432915Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6433024Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6433307Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6433444Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6433601Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6433605Z 
2025-04-11T03:52:12.6433852Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6434018Z _____________ test_flash_decoding[True-False-1-True-4-16-8-32-16] ______________
2025-04-11T03:52:12.6434022Z 
2025-04-11T03:52:12.6434178Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6434340Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6434431Z use_new_kcache_layout = True
2025-04-11T03:52:12.6434435Z 
2025-04-11T03:52:12.6434632Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6434737Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6434857Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6434995Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6435117Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6435232Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6435371Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6435475Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6435710Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6435871Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6435958Z     def test_flash_decoding(
2025-04-11T03:52:12.6436038Z         bsz: int,
2025-04-11T03:52:12.6436119Z         block_size: int,
2025-04-11T03:52:12.6436215Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6436297Z         num_attn_heads: int,
2025-04-11T03:52:12.6436379Z         kv_group_num: int,
2025-04-11T03:52:12.6436470Z         same_context_len: bool,
2025-04-11T03:52:12.6436547Z         q_len: int,
2025-04-11T03:52:12.6436637Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6436728Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6436887Z     ):
2025-04-11T03:52:12.6437004Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6437198Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6437389Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6437562Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6437732Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6437889Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6437962Z     
2025-04-11T03:52:12.6438055Z         torch.manual_seed(123)
2025-04-11T03:52:12.6438146Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6438240Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6438244Z 
2025-04-11T03:52:12.6438400Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6438518Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6438522Z 
2025-04-11T03:52:12.6438598Z device = None
2025-04-11T03:52:12.6438602Z 
2025-04-11T03:52:12.6438722Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6438880Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6438953Z     
2025-04-11T03:52:12.6439032Z         Args:
2025-04-11T03:52:12.6439199Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6439368Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6439475Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6439549Z         """
2025-04-11T03:52:12.6439633Z         _lazy_init()
2025-04-11T03:52:12.6439731Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6439837Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6439948Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6440233Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6440375Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6440531Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6440535Z 
2025-04-11T03:52:12.6440779Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6440945Z _____________ test_flash_decoding[True-False-1-True-4-16-16-16-7] ______________
2025-04-11T03:52:12.6440949Z 
2025-04-11T03:52:12.6441106Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6441271Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6441363Z use_new_kcache_layout = True
2025-04-11T03:52:12.6441369Z 
2025-04-11T03:52:12.6441568Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6441677Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6441901Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6442044Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6442163Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6442277Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6442415Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6442521Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6442654Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6442810Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6442897Z     def test_flash_decoding(
2025-04-11T03:52:12.6442978Z         bsz: int,
2025-04-11T03:52:12.6443168Z         block_size: int,
2025-04-11T03:52:12.6443265Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6443349Z         num_attn_heads: int,
2025-04-11T03:52:12.6443434Z         kv_group_num: int,
2025-04-11T03:52:12.6443528Z         same_context_len: bool,
2025-04-11T03:52:12.6443607Z         q_len: int,
2025-04-11T03:52:12.6443697Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6443785Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6443858Z     ):
2025-04-11T03:52:12.6443975Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6444167Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6444355Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6444526Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6444696Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6444859Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6444932Z     
2025-04-11T03:52:12.6445025Z         torch.manual_seed(123)
2025-04-11T03:52:12.6445122Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6445217Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6445221Z 
2025-04-11T03:52:12.6445373Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6445489Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6445492Z 
2025-04-11T03:52:12.6445570Z device = None
2025-04-11T03:52:12.6445574Z 
2025-04-11T03:52:12.6445690Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6445843Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6445915Z     
2025-04-11T03:52:12.6445996Z         Args:
2025-04-11T03:52:12.6446168Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6446338Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6446444Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6446523Z         """
2025-04-11T03:52:12.6446607Z         _lazy_init()
2025-04-11T03:52:12.6446703Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6446809Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6446914Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6447197Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6447330Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6447486Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6447490Z 
2025-04-11T03:52:12.6447732Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6447896Z _____________ test_flash_decoding[True-False-1-True-4-16-16-16-16] _____________
2025-04-11T03:52:12.6447900Z 
2025-04-11T03:52:12.6448165Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6448330Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6448423Z use_new_kcache_layout = True
2025-04-11T03:52:12.6448427Z 
2025-04-11T03:52:12.6448626Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6448737Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6448853Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6448991Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6449111Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6449328Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6449474Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6449581Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6449726Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6449883Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6449973Z     def test_flash_decoding(
2025-04-11T03:52:12.6450061Z         bsz: int,
2025-04-11T03:52:12.6450148Z         block_size: int,
2025-04-11T03:52:12.6450250Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6450337Z         num_attn_heads: int,
2025-04-11T03:52:12.6450425Z         kv_group_num: int,
2025-04-11T03:52:12.6450520Z         same_context_len: bool,
2025-04-11T03:52:12.6450600Z         q_len: int,
2025-04-11T03:52:12.6450693Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6450785Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6450863Z     ):
2025-04-11T03:52:12.6450984Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6451178Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6451373Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6451548Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6451723Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6451883Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6451958Z     
2025-04-11T03:52:12.6452052Z         torch.manual_seed(123)
2025-04-11T03:52:12.6452145Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6452242Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6452246Z 
2025-04-11T03:52:12.6452403Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6452526Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6452529Z 
2025-04-11T03:52:12.6452610Z device = None
2025-04-11T03:52:12.6452614Z 
2025-04-11T03:52:12.6452735Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6452889Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6452963Z     
2025-04-11T03:52:12.6453045Z         Args:
2025-04-11T03:52:12.6453216Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6453387Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6453499Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6453576Z         """
2025-04-11T03:52:12.6453661Z         _lazy_init()
2025-04-11T03:52:12.6453761Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6453871Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6453983Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6454277Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6454523Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6454685Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6454693Z 
2025-04-11T03:52:12.6454934Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6455099Z _____________ test_flash_decoding[True-False-1-True-4-16-16-32-7] ______________
2025-04-11T03:52:12.6455103Z 
2025-04-11T03:52:12.6455254Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6455414Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6455508Z use_new_kcache_layout = True
2025-04-11T03:52:12.6455598Z 
2025-04-11T03:52:12.6455800Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6455908Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6456028Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6456167Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6456289Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6456402Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6456543Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6456648Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6456788Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6456939Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6457025Z     def test_flash_decoding(
2025-04-11T03:52:12.6457107Z         bsz: int,
2025-04-11T03:52:12.6457192Z         block_size: int,
2025-04-11T03:52:12.6457285Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6457367Z         num_attn_heads: int,
2025-04-11T03:52:12.6457449Z         kv_group_num: int,
2025-04-11T03:52:12.6457540Z         same_context_len: bool,
2025-04-11T03:52:12.6457620Z         q_len: int,
2025-04-11T03:52:12.6457711Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6457800Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6457872Z     ):
2025-04-11T03:52:12.6457985Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6458174Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6458365Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6458535Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6458699Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6458861Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6458935Z     
2025-04-11T03:52:12.6459020Z         torch.manual_seed(123)
2025-04-11T03:52:12.6459110Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6459207Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6459211Z 
2025-04-11T03:52:12.6459367Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6459487Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6459491Z 
2025-04-11T03:52:12.6459567Z device = None
2025-04-11T03:52:12.6459571Z 
2025-04-11T03:52:12.6459693Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6459842Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6459913Z     
2025-04-11T03:52:12.6459990Z         Args:
2025-04-11T03:52:12.6460161Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6460332Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6460436Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6460622Z         """
2025-04-11T03:52:12.6460704Z         _lazy_init()
2025-04-11T03:52:12.6460799Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6460908Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6461011Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6461297Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6461432Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6461591Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6461599Z 
2025-04-11T03:52:12.6462022Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6462192Z _____________ test_flash_decoding[True-False-1-True-4-16-16-32-16] _____________
2025-04-11T03:52:12.6462195Z 
2025-04-11T03:52:12.6462355Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6462514Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6462607Z use_new_kcache_layout = True
2025-04-11T03:52:12.6462611Z 
2025-04-11T03:52:12.6462809Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6462920Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6463038Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6463179Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6463299Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6463416Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6463557Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6463662Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6463805Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6463957Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6464044Z     def test_flash_decoding(
2025-04-11T03:52:12.6464123Z         bsz: int,
2025-04-11T03:52:12.6464205Z         block_size: int,
2025-04-11T03:52:12.6464299Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6464382Z         num_attn_heads: int,
2025-04-11T03:52:12.6464464Z         kv_group_num: int,
2025-04-11T03:52:12.6464553Z         same_context_len: bool,
2025-04-11T03:52:12.6464629Z         q_len: int,
2025-04-11T03:52:12.6464717Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6464804Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6464882Z     ):
2025-04-11T03:52:12.6464996Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6465188Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6465375Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6465548Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6465711Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6465872Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6465949Z     
2025-04-11T03:52:12.6466035Z         torch.manual_seed(123)
2025-04-11T03:52:12.6466128Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6466225Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6466229Z 
2025-04-11T03:52:12.6466382Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6466506Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6466510Z 
2025-04-11T03:52:12.6466587Z device = None
2025-04-11T03:52:12.6466591Z 
2025-04-11T03:52:12.6466718Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6466969Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6467046Z     
2025-04-11T03:52:12.6467127Z         Args:
2025-04-11T03:52:12.6467294Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6467465Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6467572Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6467647Z         """
2025-04-11T03:52:12.6467725Z         _lazy_init()
2025-04-11T03:52:12.6467822Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6467930Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6468133Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6468467Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6468606Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6468767Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6468771Z 
2025-04-11T03:52:12.6469008Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6469177Z _____________ test_flash_decoding[True-False-1-False-1-16-8-16-7] ______________
2025-04-11T03:52:12.6469181Z 
2025-04-11T03:52:12.6469339Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6469503Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6469596Z use_new_kcache_layout = True
2025-04-11T03:52:12.6469602Z 
2025-04-11T03:52:12.6469801Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6469910Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6470027Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6470173Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6470292Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6470406Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6470550Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6470659Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6470803Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6470957Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6471042Z     def test_flash_decoding(
2025-04-11T03:52:12.6471122Z         bsz: int,
2025-04-11T03:52:12.6471208Z         block_size: int,
2025-04-11T03:52:12.6471301Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6471383Z         num_attn_heads: int,
2025-04-11T03:52:12.6471468Z         kv_group_num: int,
2025-04-11T03:52:12.6471554Z         same_context_len: bool,
2025-04-11T03:52:12.6471636Z         q_len: int,
2025-04-11T03:52:12.6471729Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6471818Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6471892Z     ):
2025-04-11T03:52:12.6472006Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6472207Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6472400Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6472570Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6472739Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6472897Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6472973Z     
2025-04-11T03:52:12.6473060Z         torch.manual_seed(123)
2025-04-11T03:52:12.6473270Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6473375Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6473379Z 
2025-04-11T03:52:12.6473535Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6473653Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6473657Z 
2025-04-11T03:52:12.6473734Z device = None
2025-04-11T03:52:12.6473738Z 
2025-04-11T03:52:12.6473862Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6474013Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6474086Z     
2025-04-11T03:52:12.6474163Z         Args:
2025-04-11T03:52:12.6474332Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6474648Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6474755Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6474833Z         """
2025-04-11T03:52:12.6474911Z         _lazy_init()
2025-04-11T03:52:12.6475010Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6475117Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6475223Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6475508Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6475644Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6475805Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6475809Z 
2025-04-11T03:52:12.6476050Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6476220Z _____________ test_flash_decoding[True-False-1-False-1-16-8-16-16] _____________
2025-04-11T03:52:12.6476228Z 
2025-04-11T03:52:12.6476380Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6476541Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6476633Z use_new_kcache_layout = True
2025-04-11T03:52:12.6476637Z 
2025-04-11T03:52:12.6476834Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6476943Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6477059Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6477202Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6477318Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6477429Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6477578Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6477682Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6477819Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6477973Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6478066Z     def test_flash_decoding(
2025-04-11T03:52:12.6478142Z         bsz: int,
2025-04-11T03:52:12.6478222Z         block_size: int,
2025-04-11T03:52:12.6478318Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6478401Z         num_attn_heads: int,
2025-04-11T03:52:12.6478487Z         kv_group_num: int,
2025-04-11T03:52:12.6478572Z         same_context_len: bool,
2025-04-11T03:52:12.6478648Z         q_len: int,
2025-04-11T03:52:12.6478740Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6478829Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6478906Z     ):
2025-04-11T03:52:12.6479020Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6479209Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6479498Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6479672Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6479839Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6479996Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6480076Z     
2025-04-11T03:52:12.6480163Z         torch.manual_seed(123)
2025-04-11T03:52:12.6480255Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6480350Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6480353Z 
2025-04-11T03:52:12.6480507Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6480718Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6480722Z 
2025-04-11T03:52:12.6480798Z device = None
2025-04-11T03:52:12.6480802Z 
2025-04-11T03:52:12.6480922Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6481074Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6481154Z     
2025-04-11T03:52:12.6481230Z         Args:
2025-04-11T03:52:12.6481402Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6481572Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6481680Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6481763Z         """
2025-04-11T03:52:12.6481842Z         _lazy_init()
2025-04-11T03:52:12.6481942Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6482050Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6482161Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6482467Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6482605Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6482766Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6482770Z 
2025-04-11T03:52:12.6483010Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6483182Z _____________ test_flash_decoding[True-False-1-False-1-16-8-32-7] ______________
2025-04-11T03:52:12.6483186Z 
2025-04-11T03:52:12.6483335Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6483502Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6483594Z use_new_kcache_layout = True
2025-04-11T03:52:12.6483601Z 
2025-04-11T03:52:12.6483800Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6483908Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6484025Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6484175Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6484291Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6484403Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6484548Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6484651Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6484793Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6484942Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6485032Z     def test_flash_decoding(
2025-04-11T03:52:12.6485107Z         bsz: int,
2025-04-11T03:52:12.6485193Z         block_size: int,
2025-04-11T03:52:12.6485289Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6485374Z         num_attn_heads: int,
2025-04-11T03:52:12.6485465Z         kv_group_num: int,
2025-04-11T03:52:12.6485551Z         same_context_len: bool,
2025-04-11T03:52:12.6485733Z         q_len: int,
2025-04-11T03:52:12.6485831Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6485918Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6485994Z     ):
2025-04-11T03:52:12.6486109Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6486304Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6486494Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6486669Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6486837Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6487105Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6487179Z     
2025-04-11T03:52:12.6487268Z         torch.manual_seed(123)
2025-04-11T03:52:12.6487364Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6487464Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6487468Z 
2025-04-11T03:52:12.6487622Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6487736Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6487740Z 
2025-04-11T03:52:12.6487816Z device = None
2025-04-11T03:52:12.6487820Z 
2025-04-11T03:52:12.6487944Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6488093Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6488168Z     
2025-04-11T03:52:12.6488242Z         Args:
2025-04-11T03:52:12.6488408Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6488582Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6488689Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6488770Z         """
2025-04-11T03:52:12.6488849Z         _lazy_init()
2025-04-11T03:52:12.6488946Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6489054Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6489159Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6489460Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6489595Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6489756Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6489760Z 
2025-04-11T03:52:12.6489995Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6490170Z _____________ test_flash_decoding[True-False-1-False-1-16-8-32-16] _____________
2025-04-11T03:52:12.6490173Z 
2025-04-11T03:52:12.6490327Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6490493Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6490586Z use_new_kcache_layout = True
2025-04-11T03:52:12.6490590Z 
2025-04-11T03:52:12.6490788Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6490897Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6491013Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6491154Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6491270Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6491384Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6491533Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6491635Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6491777Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6492034Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6492129Z     def test_flash_decoding(
2025-04-11T03:52:12.6492205Z         bsz: int,
2025-04-11T03:52:12.6492286Z         block_size: int,
2025-04-11T03:52:12.6492386Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6492469Z         num_attn_heads: int,
2025-04-11T03:52:12.6492558Z         kv_group_num: int,
2025-04-11T03:52:12.6492644Z         same_context_len: bool,
2025-04-11T03:52:12.6492720Z         q_len: int,
2025-04-11T03:52:12.6492814Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6492903Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6492983Z     ):
2025-04-11T03:52:12.6493188Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6493383Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6493570Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6493741Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6493907Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6494067Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6494143Z     
2025-04-11T03:52:12.6494231Z         torch.manual_seed(123)
2025-04-11T03:52:12.6494326Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6494417Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6494420Z 
2025-04-11T03:52:12.6494576Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6494697Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6494701Z 
2025-04-11T03:52:12.6494778Z device = None
2025-04-11T03:52:12.6494782Z 
2025-04-11T03:52:12.6494903Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6495056Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6495132Z     
2025-04-11T03:52:12.6495204Z         Args:
2025-04-11T03:52:12.6495374Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6495541Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6495647Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6495724Z         """
2025-04-11T03:52:12.6495802Z         _lazy_init()
2025-04-11T03:52:12.6495898Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6496007Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6496115Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6496405Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6496541Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6496703Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6496707Z 
2025-04-11T03:52:12.6496946Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6497114Z _____________ test_flash_decoding[True-False-1-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.6497118Z 
2025-04-11T03:52:12.6497267Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6497430Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6497524Z use_new_kcache_layout = True
2025-04-11T03:52:12.6497531Z 
2025-04-11T03:52:12.6497728Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6497836Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6497953Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6498220Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6498341Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6498455Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6498596Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6498701Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6498843Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6498997Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6499087Z     def test_flash_decoding(
2025-04-11T03:52:12.6499166Z         bsz: int,
2025-04-11T03:52:12.6499343Z         block_size: int,
2025-04-11T03:52:12.6499436Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6499519Z         num_attn_heads: int,
2025-04-11T03:52:12.6499611Z         kv_group_num: int,
2025-04-11T03:52:12.6499697Z         same_context_len: bool,
2025-04-11T03:52:12.6499776Z         q_len: int,
2025-04-11T03:52:12.6499867Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6499954Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6500028Z     ):
2025-04-11T03:52:12.6500138Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6500329Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6500512Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6500683Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6500853Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6501010Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6501090Z     
2025-04-11T03:52:12.6501177Z         torch.manual_seed(123)
2025-04-11T03:52:12.6501272Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6501367Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6501370Z 
2025-04-11T03:52:12.6501527Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6501645Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6501648Z 
2025-04-11T03:52:12.6501726Z device = None
2025-04-11T03:52:12.6501730Z 
2025-04-11T03:52:12.6501854Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6502003Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6502079Z     
2025-04-11T03:52:12.6502151Z         Args:
2025-04-11T03:52:12.6502314Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6502484Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6502589Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6502669Z         """
2025-04-11T03:52:12.6502746Z         _lazy_init()
2025-04-11T03:52:12.6502847Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6502948Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6503052Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6503340Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6503472Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6503629Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6503633Z 
2025-04-11T03:52:12.6503867Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6504042Z ____________ test_flash_decoding[True-False-1-False-1-16-16-16-16] _____________
2025-04-11T03:52:12.6504046Z 
2025-04-11T03:52:12.6504299Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6504473Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6504563Z use_new_kcache_layout = True
2025-04-11T03:52:12.6504567Z 
2025-04-11T03:52:12.6504768Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6504880Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6504998Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6505141Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6505262Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6505378Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6505621Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6505725Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6505868Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6506022Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6506118Z     def test_flash_decoding(
2025-04-11T03:52:12.6506194Z         bsz: int,
2025-04-11T03:52:12.6506281Z         block_size: int,
2025-04-11T03:52:12.6506378Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6506463Z         num_attn_heads: int,
2025-04-11T03:52:12.6506553Z         kv_group_num: int,
2025-04-11T03:52:12.6506637Z         same_context_len: bool,
2025-04-11T03:52:12.6506719Z         q_len: int,
2025-04-11T03:52:12.6506804Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6506894Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6506971Z     ):
2025-04-11T03:52:12.6507084Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6507280Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6507464Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6507637Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6507807Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6507963Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6508043Z     
2025-04-11T03:52:12.6508129Z         torch.manual_seed(123)
2025-04-11T03:52:12.6508222Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6508312Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6508316Z 
2025-04-11T03:52:12.6508514Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6508638Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6508642Z 
2025-04-11T03:52:12.6508718Z device = None
2025-04-11T03:52:12.6508722Z 
2025-04-11T03:52:12.6508844Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6508995Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6509070Z     
2025-04-11T03:52:12.6509146Z         Args:
2025-04-11T03:52:12.6509314Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6509482Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6509588Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6509665Z         """
2025-04-11T03:52:12.6509743Z         _lazy_init()
2025-04-11T03:52:12.6509842Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6509947Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6510055Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6510345Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6510602Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6510767Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6510771Z 
2025-04-11T03:52:12.6511010Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6511180Z _____________ test_flash_decoding[True-False-1-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.6511184Z 
2025-04-11T03:52:12.6511335Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6511500Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6511588Z use_new_kcache_layout = True
2025-04-11T03:52:12.6511700Z 
2025-04-11T03:52:12.6511902Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6512016Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6512134Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6512280Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6512398Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6512514Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6512651Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6512754Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6512894Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6513042Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6513133Z     def test_flash_decoding(
2025-04-11T03:52:12.6513210Z         bsz: int,
2025-04-11T03:52:12.6513295Z         block_size: int,
2025-04-11T03:52:12.6513387Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6513470Z         num_attn_heads: int,
2025-04-11T03:52:12.6513560Z         kv_group_num: int,
2025-04-11T03:52:12.6513644Z         same_context_len: bool,
2025-04-11T03:52:12.6513726Z         q_len: int,
2025-04-11T03:52:12.6513813Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6513902Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6513980Z     ):
2025-04-11T03:52:12.6514093Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6514286Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6514468Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6514646Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6514807Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6514966Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6515044Z     
2025-04-11T03:52:12.6515131Z         torch.manual_seed(123)
2025-04-11T03:52:12.6515225Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6515317Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6515320Z 
2025-04-11T03:52:12.6515473Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6515589Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6515593Z 
2025-04-11T03:52:12.6515671Z device = None
2025-04-11T03:52:12.6515675Z 
2025-04-11T03:52:12.6515794Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6515942Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6516020Z     
2025-04-11T03:52:12.6516093Z         Args:
2025-04-11T03:52:12.6516263Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6516430Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6516535Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6516614Z         """
2025-04-11T03:52:12.6516860Z         _lazy_init()
2025-04-11T03:52:12.6516967Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6517070Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6517176Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6517461Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6517598Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6517759Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6517763Z 
2025-04-11T03:52:12.6518002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6518303Z ____________ test_flash_decoding[True-False-1-False-1-16-16-32-16] _____________
2025-04-11T03:52:12.6518308Z 
2025-04-11T03:52:12.6518489Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6518658Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6518744Z use_new_kcache_layout = True
2025-04-11T03:52:12.6518748Z 
2025-04-11T03:52:12.6518945Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6519059Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6519175Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6519320Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6519436Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6519555Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6519694Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6519800Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6519941Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6520094Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6520186Z     def test_flash_decoding(
2025-04-11T03:52:12.6520261Z         bsz: int,
2025-04-11T03:52:12.6520348Z         block_size: int,
2025-04-11T03:52:12.6520438Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6520522Z         num_attn_heads: int,
2025-04-11T03:52:12.6520611Z         kv_group_num: int,
2025-04-11T03:52:12.6520696Z         same_context_len: bool,
2025-04-11T03:52:12.6520776Z         q_len: int,
2025-04-11T03:52:12.6520863Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6520952Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6521032Z     ):
2025-04-11T03:52:12.6521145Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6521341Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6521525Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6521695Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6521854Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6522009Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6522085Z     
2025-04-11T03:52:12.6522171Z         torch.manual_seed(123)
2025-04-11T03:52:12.6522262Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6522352Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6522356Z 
2025-04-11T03:52:12.6522512Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6522625Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6522629Z 
2025-04-11T03:52:12.6522705Z device = None
2025-04-11T03:52:12.6522709Z 
2025-04-11T03:52:12.6522833Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6523106Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6523191Z     
2025-04-11T03:52:12.6523267Z         Args:
2025-04-11T03:52:12.6523438Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6523599Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6523705Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6523782Z         """
2025-04-11T03:52:12.6523863Z         _lazy_init()
2025-04-11T03:52:12.6523964Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6524067Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6524272Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6524565Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6524703Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6524869Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6524872Z 
2025-04-11T03:52:12.6525110Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6525283Z _____________ test_flash_decoding[True-False-1-False-4-16-8-16-7] ______________
2025-04-11T03:52:12.6525287Z 
2025-04-11T03:52:12.6525437Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6525601Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6525689Z use_new_kcache_layout = True
2025-04-11T03:52:12.6525695Z 
2025-04-11T03:52:12.6525901Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6526004Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6526122Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6526270Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6526386Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6526505Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6526642Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6526748Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6526889Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6527040Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6527131Z     def test_flash_decoding(
2025-04-11T03:52:12.6527208Z         bsz: int,
2025-04-11T03:52:12.6527301Z         block_size: int,
2025-04-11T03:52:12.6527393Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6527476Z         num_attn_heads: int,
2025-04-11T03:52:12.6527566Z         kv_group_num: int,
2025-04-11T03:52:12.6527652Z         same_context_len: bool,
2025-04-11T03:52:12.6527741Z         q_len: int,
2025-04-11T03:52:12.6527832Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6527920Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6528000Z     ):
2025-04-11T03:52:12.6528112Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6528315Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6528498Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6528676Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6528837Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6528997Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6529074Z     
2025-04-11T03:52:12.6529164Z         torch.manual_seed(123)
2025-04-11T03:52:12.6529263Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6529464Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6529469Z 
2025-04-11T03:52:12.6529629Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6529747Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6529751Z 
2025-04-11T03:52:12.6529831Z device = None
2025-04-11T03:52:12.6529840Z 
2025-04-11T03:52:12.6529956Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6530107Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6530183Z     
2025-04-11T03:52:12.6530257Z         Args:
2025-04-11T03:52:12.6530428Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6530693Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6530801Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6530884Z         """
2025-04-11T03:52:12.6530965Z         _lazy_init()
2025-04-11T03:52:12.6531069Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6531174Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6531287Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6531571Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6531708Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6531869Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6531873Z 
2025-04-11T03:52:12.6532111Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6532284Z _____________ test_flash_decoding[True-False-1-False-4-16-8-16-16] _____________
2025-04-11T03:52:12.6532288Z 
2025-04-11T03:52:12.6532443Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6532610Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6532698Z use_new_kcache_layout = True
2025-04-11T03:52:12.6532702Z 
2025-04-11T03:52:12.6532903Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6533005Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6533123Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6533268Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6533385Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6533504Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6533643Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6533751Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6533888Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6534040Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6534134Z     def test_flash_decoding(
2025-04-11T03:52:12.6534214Z         bsz: int,
2025-04-11T03:52:12.6534301Z         block_size: int,
2025-04-11T03:52:12.6534391Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6534474Z         num_attn_heads: int,
2025-04-11T03:52:12.6534563Z         kv_group_num: int,
2025-04-11T03:52:12.6534648Z         same_context_len: bool,
2025-04-11T03:52:12.6534730Z         q_len: int,
2025-04-11T03:52:12.6534816Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6534906Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6534984Z     ):
2025-04-11T03:52:12.6535095Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6535294Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6535474Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6535756Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6535923Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6536080Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6536159Z     
2025-04-11T03:52:12.6536244Z         torch.manual_seed(123)
2025-04-11T03:52:12.6536340Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6536430Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6536434Z 
2025-04-11T03:52:12.6536596Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6536709Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6536825Z 
2025-04-11T03:52:12.6536905Z device = None
2025-04-11T03:52:12.6536915Z 
2025-04-11T03:52:12.6537032Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6537183Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6537261Z     
2025-04-11T03:52:12.6537333Z         Args:
2025-04-11T03:52:12.6537503Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6537668Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6537775Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6537854Z         """
2025-04-11T03:52:12.6537933Z         _lazy_init()
2025-04-11T03:52:12.6538036Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6538139Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6538253Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6538534Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6538673Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6538836Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6538840Z 
2025-04-11T03:52:12.6539077Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6539250Z _____________ test_flash_decoding[True-False-1-False-4-16-8-32-7] ______________
2025-04-11T03:52:12.6539254Z 
2025-04-11T03:52:12.6539403Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6539570Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6539655Z use_new_kcache_layout = True
2025-04-11T03:52:12.6539659Z 
2025-04-11T03:52:12.6539864Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6539970Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6540087Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6540232Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6540347Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6540466Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6540600Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6540709Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6540845Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6540994Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6541086Z     def test_flash_decoding(
2025-04-11T03:52:12.6541164Z         bsz: int,
2025-04-11T03:52:12.6541254Z         block_size: int,
2025-04-11T03:52:12.6541351Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6541437Z         num_attn_heads: int,
2025-04-11T03:52:12.6541527Z         kv_group_num: int,
2025-04-11T03:52:12.6541611Z         same_context_len: bool,
2025-04-11T03:52:12.6541798Z         q_len: int,
2025-04-11T03:52:12.6541887Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6541986Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6542060Z     ):
2025-04-11T03:52:12.6542170Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6542371Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6542557Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6542735Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6542898Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6543174Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6543247Z     
2025-04-11T03:52:12.6543334Z         torch.manual_seed(123)
2025-04-11T03:52:12.6543427Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6543521Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6543524Z 
2025-04-11T03:52:12.6543682Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6543796Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6543799Z 
2025-04-11T03:52:12.6543880Z device = None
2025-04-11T03:52:12.6543884Z 
2025-04-11T03:52:12.6544001Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6544148Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6544226Z     
2025-04-11T03:52:12.6544300Z         Args:
2025-04-11T03:52:12.6544475Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6544640Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6544751Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6544826Z         """
2025-04-11T03:52:12.6544908Z         _lazy_init()
2025-04-11T03:52:12.6545012Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6545118Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6545231Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6545511Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6545648Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6545809Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6545813Z 
2025-04-11T03:52:12.6546050Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6546225Z _____________ test_flash_decoding[True-False-1-False-4-16-8-32-16] _____________
2025-04-11T03:52:12.6546229Z 
2025-04-11T03:52:12.6546380Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6546548Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6546638Z use_new_kcache_layout = True
2025-04-11T03:52:12.6546641Z 
2025-04-11T03:52:12.6546842Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6546946Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6547062Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6547207Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6547323Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6547441Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6547581Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6547691Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6547830Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6548083Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6548184Z     def test_flash_decoding(
2025-04-11T03:52:12.6548261Z         bsz: int,
2025-04-11T03:52:12.6548346Z         block_size: int,
2025-04-11T03:52:12.6548479Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6548568Z         num_attn_heads: int,
2025-04-11T03:52:12.6548655Z         kv_group_num: int,
2025-04-11T03:52:12.6548740Z         same_context_len: bool,
2025-04-11T03:52:12.6548822Z         q_len: int,
2025-04-11T03:52:12.6548909Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6549003Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6549077Z     ):
2025-04-11T03:52:12.6549189Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6549490Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6549676Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6549854Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6550018Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6550180Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6550254Z     
2025-04-11T03:52:12.6550340Z         torch.manual_seed(123)
2025-04-11T03:52:12.6550432Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6550522Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6550525Z 
2025-04-11T03:52:12.6550683Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6550798Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6550805Z 
2025-04-11T03:52:12.6550891Z device = None
2025-04-11T03:52:12.6550894Z 
2025-04-11T03:52:12.6551011Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6551166Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6551247Z     
2025-04-11T03:52:12.6551321Z         Args:
2025-04-11T03:52:12.6551493Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6551659Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6551773Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6551847Z         """
2025-04-11T03:52:12.6551926Z         _lazy_init()
2025-04-11T03:52:12.6552031Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6552133Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6552248Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6552535Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6552681Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6552834Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6552839Z 
2025-04-11T03:52:12.6553075Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6553251Z _____________ test_flash_decoding[True-False-1-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.6553255Z 
2025-04-11T03:52:12.6553406Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6553575Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6553661Z use_new_kcache_layout = True
2025-04-11T03:52:12.6553665Z 
2025-04-11T03:52:12.6553869Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6553974Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6554092Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6554352Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6554471Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6554590Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6554729Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6554838Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6554975Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6555127Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6555221Z     def test_flash_decoding(
2025-04-11T03:52:12.6555296Z         bsz: int,
2025-04-11T03:52:12.6555381Z         block_size: int,
2025-04-11T03:52:12.6555572Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6555662Z         num_attn_heads: int,
2025-04-11T03:52:12.6555747Z         kv_group_num: int,
2025-04-11T03:52:12.6555834Z         same_context_len: bool,
2025-04-11T03:52:12.6555919Z         q_len: int,
2025-04-11T03:52:12.6556011Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6556105Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6556178Z     ):
2025-04-11T03:52:12.6556292Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6556491Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6556673Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6556849Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6557013Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6557176Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6557248Z     
2025-04-11T03:52:12.6557336Z         torch.manual_seed(123)
2025-04-11T03:52:12.6557431Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6557526Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6557530Z 
2025-04-11T03:52:12.6557690Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6557801Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6557804Z 
2025-04-11T03:52:12.6557886Z device = None
2025-04-11T03:52:12.6557889Z 
2025-04-11T03:52:12.6558006Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6558156Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6558230Z     
2025-04-11T03:52:12.6558304Z         Args:
2025-04-11T03:52:12.6558474Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6558640Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6558750Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6558823Z         """
2025-04-11T03:52:12.6558903Z         _lazy_init()
2025-04-11T03:52:12.6559003Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6559108Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6559218Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6559501Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6559643Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6559797Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6559801Z 
2025-04-11T03:52:12.6560040Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6560217Z ____________ test_flash_decoding[True-False-1-False-4-16-16-16-16] _____________
2025-04-11T03:52:12.6560221Z 
2025-04-11T03:52:12.6560374Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6560642Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6560733Z use_new_kcache_layout = True
2025-04-11T03:52:12.6560737Z 
2025-04-11T03:52:12.6560943Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6561050Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6561172Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6561312Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6561428Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6561546Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6561894Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6562005Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6562140Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6562300Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6562390Z     def test_flash_decoding(
2025-04-11T03:52:12.6562467Z         bsz: int,
2025-04-11T03:52:12.6562554Z         block_size: int,
2025-04-11T03:52:12.6562647Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6562736Z         num_attn_heads: int,
2025-04-11T03:52:12.6562818Z         kv_group_num: int,
2025-04-11T03:52:12.6562903Z         same_context_len: bool,
2025-04-11T03:52:12.6562987Z         q_len: int,
2025-04-11T03:52:12.6563086Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6563181Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6563255Z     ):
2025-04-11T03:52:12.6563369Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6563573Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6563754Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6563941Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6564107Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6564265Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6564337Z     
2025-04-11T03:52:12.6564423Z         torch.manual_seed(123)
2025-04-11T03:52:12.6564518Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6564610Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6564614Z 
2025-04-11T03:52:12.6564774Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6564889Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6564896Z 
2025-04-11T03:52:12.6564978Z device = None
2025-04-11T03:52:12.6564982Z 
2025-04-11T03:52:12.6565098Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6565266Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6565339Z     
2025-04-11T03:52:12.6565414Z         Args:
2025-04-11T03:52:12.6565588Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6565755Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6565869Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6565944Z         """
2025-04-11T03:52:12.6566022Z         _lazy_init()
2025-04-11T03:52:12.6566124Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6566226Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6566339Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6566629Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6566770Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6567038Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6567043Z 
2025-04-11T03:52:12.6567285Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6567462Z _____________ test_flash_decoding[True-False-1-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.6567466Z 
2025-04-11T03:52:12.6567619Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6567791Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6567884Z use_new_kcache_layout = True
2025-04-11T03:52:12.6567888Z 
2025-04-11T03:52:12.6568198Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6568304Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6568429Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6568571Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6568691Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6568813Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6568950Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6569063Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6569202Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6569360Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6569448Z     def test_flash_decoding(
2025-04-11T03:52:12.6569527Z         bsz: int,
2025-04-11T03:52:12.6569618Z         block_size: int,
2025-04-11T03:52:12.6569715Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6569804Z         num_attn_heads: int,
2025-04-11T03:52:12.6569890Z         kv_group_num: int,
2025-04-11T03:52:12.6569978Z         same_context_len: bool,
2025-04-11T03:52:12.6570064Z         q_len: int,
2025-04-11T03:52:12.6570153Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6570252Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6570326Z     ):
2025-04-11T03:52:12.6570440Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6570638Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6570821Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6570998Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6571160Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6571426Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6571582Z     
2025-04-11T03:52:12.6571702Z         torch.manual_seed(123)
2025-04-11T03:52:12.6571864Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6572006Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6572010Z 
2025-04-11T03:52:12.6572228Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6572385Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6572390Z 
2025-04-11T03:52:12.6572498Z device = None
2025-04-11T03:52:12.6572535Z 
2025-04-11T03:52:12.6572684Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6572872Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6572990Z     
2025-04-11T03:52:12.6573102Z         Args:
2025-04-11T03:52:12.6573346Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6573542Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6573676Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6573821Z         """
2025-04-11T03:52:12.6574014Z         _lazy_init()
2025-04-11T03:52:12.6574198Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6574331Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6574605Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6574927Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6575096Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6575314Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6575318Z 
2025-04-11T03:52:12.6575597Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6575952Z ____________ test_flash_decoding[True-False-1-False-4-16-16-32-16] _____________
2025-04-11T03:52:12.6575957Z 
2025-04-11T03:52:12.6576137Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6576364Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6576469Z use_new_kcache_layout = True
2025-04-11T03:52:12.6576473Z 
2025-04-11T03:52:12.6576763Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6576896Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6577073Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6577244Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6577390Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6577562Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6577751Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6577914Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6578078Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6578290Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6578393Z     def test_flash_decoding(
2025-04-11T03:52:12.6578505Z         bsz: int,
2025-04-11T03:52:12.6578743Z         block_size: int,
2025-04-11T03:52:12.6578858Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6579005Z         num_attn_heads: int,
2025-04-11T03:52:12.6579120Z         kv_group_num: int,
2025-04-11T03:52:12.6579226Z         same_context_len: bool,
2025-04-11T03:52:12.6579394Z         q_len: int,
2025-04-11T03:52:12.6579513Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6579665Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6579769Z     ):
2025-04-11T03:52:12.6579931Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6580176Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6580404Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6580638Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6580833Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6581057Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6581156Z     
2025-04-11T03:52:12.6581318Z         torch.manual_seed(123)
2025-04-11T03:52:12.6581435Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6581553Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6581558Z 
2025-04-11T03:52:12.6581769Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6581922Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6581929Z 
2025-04-11T03:52:12.6582060Z device = None
2025-04-11T03:52:12.6582065Z 
2025-04-11T03:52:12.6582299Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6582626Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6582732Z     
2025-04-11T03:52:12.6582881Z         Args:
2025-04-11T03:52:12.6583066Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6583272Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6583458Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6583562Z         """
2025-04-11T03:52:12.6583712Z         _lazy_init()
2025-04-11T03:52:12.6583841Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6583997Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6584145Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6584572Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6584778Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6584965Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6584969Z 
2025-04-11T03:52:12.6585255Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6585457Z ______________ test_flash_decoding[True-False-5-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.6585461Z 
2025-04-11T03:52:12.6585695Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6585887Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6586035Z use_new_kcache_layout = True
2025-04-11T03:52:12.6586039Z 
2025-04-11T03:52:12.6586349Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6586469Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6586666Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6586845Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6587026Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6587174Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6587360Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6587505Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6587695Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6587910Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6588029Z     def test_flash_decoding(
2025-04-11T03:52:12.6588176Z         bsz: int,
2025-04-11T03:52:12.6588274Z         block_size: int,
2025-04-11T03:52:12.6588496Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6588610Z         num_attn_heads: int,
2025-04-11T03:52:12.6588720Z         kv_group_num: int,
2025-04-11T03:52:12.6588866Z         same_context_len: bool,
2025-04-11T03:52:12.6588971Z         q_len: int,
2025-04-11T03:52:12.6589116Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6589255Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6589355Z     ):
2025-04-11T03:52:12.6589535Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6589762Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6590069Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6590288Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6590519Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6590713Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6590850Z     
2025-04-11T03:52:12.6590972Z         torch.manual_seed(123)
2025-04-11T03:52:12.6591077Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6591384Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6591388Z 
2025-04-11T03:52:12.6591573Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6591748Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6591753Z 
2025-04-11T03:52:12.6591861Z device = None
2025-04-11T03:52:12.6591865Z 
2025-04-11T03:52:12.6592039Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6592225Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6592339Z     
2025-04-11T03:52:12.6592470Z         Args:
2025-04-11T03:52:12.6592668Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6593030Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6593153Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6593305Z         """
2025-04-11T03:52:12.6593419Z         _lazy_init()
2025-04-11T03:52:12.6593625Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6593798Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6593931Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6594276Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6594455Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6594684Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6594689Z 
2025-04-11T03:52:12.6594960Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6595196Z _____________ test_flash_decoding[True-False-5-True-1-16-8-16-16] ______________
2025-04-11T03:52:12.6595200Z 
2025-04-11T03:52:12.6595379Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6595587Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6595758Z use_new_kcache_layout = True
2025-04-11T03:52:12.6595762Z 
2025-04-11T03:52:12.6596001Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6596164Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6596309Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6596506Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6596660Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6596810Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6597013Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6597145Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6597327Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6597517Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6597685Z     def test_flash_decoding(
2025-04-11T03:52:12.6597875Z         bsz: int,
2025-04-11T03:52:12.6597986Z         block_size: int,
2025-04-11T03:52:12.6598138Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6598241Z         num_attn_heads: int,
2025-04-11T03:52:12.6598423Z         kv_group_num: int,
2025-04-11T03:52:12.6598540Z         same_context_len: bool,
2025-04-11T03:52:12.6598680Z         q_len: int,
2025-04-11T03:52:12.6598801Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6598925Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6599055Z     ):
2025-04-11T03:52:12.6599219Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6599479Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6599699Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6600038Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6600219Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6600424Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6600567Z     
2025-04-11T03:52:12.6600685Z         torch.manual_seed(123)
2025-04-11T03:52:12.6600848Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6600967Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6600972Z 
2025-04-11T03:52:12.6601188Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6601447Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6601563Z 
2025-04-11T03:52:12.6601676Z device = None
2025-04-11T03:52:12.6601714Z 
2025-04-11T03:52:12.6601832Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6621699Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6621814Z     
2025-04-11T03:52:12.6621920Z         Args:
2025-04-11T03:52:12.6622113Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6622345Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6622506Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6622590Z         """
2025-04-11T03:52:12.6622682Z         _lazy_init()
2025-04-11T03:52:12.6622789Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6622904Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6623017Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6623347Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6623499Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6623673Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6623680Z 
2025-04-11T03:52:12.6623945Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6624123Z ______________ test_flash_decoding[True-False-5-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.6624128Z 
2025-04-11T03:52:12.6624296Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6624469Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6624571Z use_new_kcache_layout = True
2025-04-11T03:52:12.6624575Z 
2025-04-11T03:52:12.6624788Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6624908Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6625032Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6625183Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6625314Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6625434Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6625580Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6625690Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6625834Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6626000Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6626096Z     def test_flash_decoding(
2025-04-11T03:52:12.6626187Z         bsz: int,
2025-04-11T03:52:12.6626278Z         block_size: int,
2025-04-11T03:52:12.6626389Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6626478Z         num_attn_heads: int,
2025-04-11T03:52:12.6626566Z         kv_group_num: int,
2025-04-11T03:52:12.6626663Z         same_context_len: bool,
2025-04-11T03:52:12.6626755Z         q_len: int,
2025-04-11T03:52:12.6627064Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6627163Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6627243Z     ):
2025-04-11T03:52:12.6627370Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6627574Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6627773Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6627956Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6628127Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6628490Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6628567Z     
2025-04-11T03:52:12.6628673Z         torch.manual_seed(123)
2025-04-11T03:52:12.6628771Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6628877Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6628881Z 
2025-04-11T03:52:12.6629043Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6629167Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6629171Z 
2025-04-11T03:52:12.6629252Z device = None
2025-04-11T03:52:12.6629257Z 
2025-04-11T03:52:12.6629382Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6629543Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6629619Z     
2025-04-11T03:52:12.6629703Z         Args:
2025-04-11T03:52:12.6629879Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6630060Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6630173Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6630250Z         """
2025-04-11T03:52:12.6630350Z         _lazy_init()
2025-04-11T03:52:12.6630450Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6630560Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6630670Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6630968Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6631111Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6631277Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6631281Z 
2025-04-11T03:52:12.6631529Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6631710Z _____________ test_flash_decoding[True-False-5-True-1-16-8-32-16] ______________
2025-04-11T03:52:12.6631714Z 
2025-04-11T03:52:12.6631873Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6632043Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6632139Z use_new_kcache_layout = True
2025-04-11T03:52:12.6632143Z 
2025-04-11T03:52:12.6632348Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6632459Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6632578Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6632721Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6632849Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6632964Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6633110Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6633217Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6633362Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6633639Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6633734Z     def test_flash_decoding(
2025-04-11T03:52:12.6633818Z         bsz: int,
2025-04-11T03:52:12.6633903Z         block_size: int,
2025-04-11T03:52:12.6634002Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6634089Z         num_attn_heads: int,
2025-04-11T03:52:12.6634177Z         kv_group_num: int,
2025-04-11T03:52:12.6634270Z         same_context_len: bool,
2025-04-11T03:52:12.6634349Z         q_len: int,
2025-04-11T03:52:12.6634445Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6634535Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6634611Z     ):
2025-04-11T03:52:12.6634731Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6635048Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6635240Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6655996Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6656210Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6656378Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6656451Z     
2025-04-11T03:52:12.6656551Z         torch.manual_seed(123)
2025-04-11T03:52:12.6656648Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6656744Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6656748Z 
2025-04-11T03:52:12.6656911Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6657033Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6657041Z 
2025-04-11T03:52:12.6657121Z device = None
2025-04-11T03:52:12.6657126Z 
2025-04-11T03:52:12.6657259Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6657423Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6657496Z     
2025-04-11T03:52:12.6657575Z         Args:
2025-04-11T03:52:12.6657745Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6657927Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6658041Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6658113Z         """
2025-04-11T03:52:12.6658197Z         _lazy_init()
2025-04-11T03:52:12.6658295Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6658402Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6658508Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6658810Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6658954Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6659115Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6659123Z 
2025-04-11T03:52:12.6659366Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6659538Z _____________ test_flash_decoding[True-False-5-True-1-16-16-16-7] ______________
2025-04-11T03:52:12.6659542Z 
2025-04-11T03:52:12.6659699Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6659860Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6659955Z use_new_kcache_layout = True
2025-04-11T03:52:12.6659959Z 
2025-04-11T03:52:12.6660163Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6660275Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6660395Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6660657Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6660784Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6660898Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6661042Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6661150Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6661289Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6661441Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6661532Z     def test_flash_decoding(
2025-04-11T03:52:12.6661612Z         bsz: int,
2025-04-11T03:52:12.6661696Z         block_size: int,
2025-04-11T03:52:12.6661882Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6661965Z         num_attn_heads: int,
2025-04-11T03:52:12.6662048Z         kv_group_num: int,
2025-04-11T03:52:12.6662138Z         same_context_len: bool,
2025-04-11T03:52:12.6662215Z         q_len: int,
2025-04-11T03:52:12.6662308Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6662397Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6662470Z     ):
2025-04-11T03:52:12.6662587Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6662782Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6662982Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6663154Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6663322Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6663487Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6663563Z     
2025-04-11T03:52:12.6663651Z         torch.manual_seed(123)
2025-04-11T03:52:12.6663742Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6663841Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6663845Z 
2025-04-11T03:52:12.6664005Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6664121Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6664125Z 
2025-04-11T03:52:12.6664202Z device = None
2025-04-11T03:52:12.6664206Z 
2025-04-11T03:52:12.6664329Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6664480Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6664551Z     
2025-04-11T03:52:12.6664638Z         Args:
2025-04-11T03:52:12.6664806Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6664988Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6665102Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6665186Z         """
2025-04-11T03:52:12.6665266Z         _lazy_init()
2025-04-11T03:52:12.6665367Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6665480Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6665590Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6665886Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6666026Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6666184Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6666194Z 
2025-04-11T03:52:12.6666439Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6666609Z _____________ test_flash_decoding[True-False-5-True-1-16-16-16-16] _____________
2025-04-11T03:52:12.6666613Z 
2025-04-11T03:52:12.6666794Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6667072Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6667185Z use_new_kcache_layout = True
2025-04-11T03:52:12.6667189Z 
2025-04-11T03:52:12.6667391Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6667504Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6667627Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6667769Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6667891Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6668005Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6668264Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6668371Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6668554Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6668710Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6668800Z     def test_flash_decoding(
2025-04-11T03:52:12.6668881Z         bsz: int,
2025-04-11T03:52:12.6668965Z         block_size: int,
2025-04-11T03:52:12.6689610Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6689720Z         num_attn_heads: int,
2025-04-11T03:52:12.6689805Z         kv_group_num: int,
2025-04-11T03:52:12.6689897Z         same_context_len: bool,
2025-04-11T03:52:12.6689973Z         q_len: int,
2025-04-11T03:52:12.6690061Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6690150Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6690224Z     ):
2025-04-11T03:52:12.6690339Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6690538Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6690723Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6690896Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6691062Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6691219Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6691293Z     
2025-04-11T03:52:12.6691382Z         torch.manual_seed(123)
2025-04-11T03:52:12.6691471Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6691567Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6691571Z 
2025-04-11T03:52:12.6691729Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6691846Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6691853Z 
2025-04-11T03:52:12.6691929Z device = None
2025-04-11T03:52:12.6691934Z 
2025-04-11T03:52:12.6692058Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6692210Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6692279Z     
2025-04-11T03:52:12.6692356Z         Args:
2025-04-11T03:52:12.6692524Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6692691Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6692799Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6692875Z         """
2025-04-11T03:52:12.6692954Z         _lazy_init()
2025-04-11T03:52:12.6693053Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6693160Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6693268Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6693560Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6693699Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6693979Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6693984Z 
2025-04-11T03:52:12.6694227Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6694396Z _____________ test_flash_decoding[True-False-5-True-1-16-16-32-7] ______________
2025-04-11T03:52:12.6694400Z 
2025-04-11T03:52:12.6694557Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6694720Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6694812Z use_new_kcache_layout = True
2025-04-11T03:52:12.6694816Z 
2025-04-11T03:52:12.6695015Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6695228Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6695347Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6695491Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6695610Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6695727Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6695872Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6695982Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6696125Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6696280Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6696369Z     def test_flash_decoding(
2025-04-11T03:52:12.6696453Z         bsz: int,
2025-04-11T03:52:12.6696538Z         block_size: int,
2025-04-11T03:52:12.6696638Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6696724Z         num_attn_heads: int,
2025-04-11T03:52:12.6696813Z         kv_group_num: int,
2025-04-11T03:52:12.6696901Z         same_context_len: bool,
2025-04-11T03:52:12.6696981Z         q_len: int,
2025-04-11T03:52:12.6697077Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6697169Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6697247Z     ):
2025-04-11T03:52:12.6697363Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6697560Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6697753Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6697928Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6698097Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6698266Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6698343Z     
2025-04-11T03:52:12.6698441Z         torch.manual_seed(123)
2025-04-11T03:52:12.6698534Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6698634Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6698638Z 
2025-04-11T03:52:12.6698799Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6698915Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6698919Z 
2025-04-11T03:52:12.6699005Z device = None
2025-04-11T03:52:12.6699009Z 
2025-04-11T03:52:12.6699130Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6699286Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6699362Z     
2025-04-11T03:52:12.6699442Z         Args:
2025-04-11T03:52:12.6699612Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6699785Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6699901Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6699979Z         """
2025-04-11T03:52:12.6700062Z         _lazy_init()
2025-04-11T03:52:12.6700262Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6700374Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6700482Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6700772Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6700925Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6701084Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6701088Z 
2025-04-11T03:52:12.6701334Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6701595Z _____________ test_flash_decoding[True-False-5-True-1-16-16-32-16] _____________
2025-04-11T03:52:12.6701599Z 
2025-04-11T03:52:12.6701757Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6701924Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6702017Z use_new_kcache_layout = True
2025-04-11T03:52:12.6702021Z 
2025-04-11T03:52:12.6702229Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6702333Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6702457Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6702594Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6702717Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6702835Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6702974Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6703086Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6703220Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6703380Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6703468Z     def test_flash_decoding(
2025-04-11T03:52:12.6703551Z         bsz: int,
2025-04-11T03:52:12.6703634Z         block_size: int,
2025-04-11T03:52:12.6703722Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6703811Z         num_attn_heads: int,
2025-04-11T03:52:12.6703893Z         kv_group_num: int,
2025-04-11T03:52:12.6703980Z         same_context_len: bool,
2025-04-11T03:52:12.6704056Z         q_len: int,
2025-04-11T03:52:12.6704142Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6704234Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6704306Z     ):
2025-04-11T03:52:12.6704421Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6704620Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6704807Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6704981Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6705143Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6705305Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6705376Z     
2025-04-11T03:52:12.6705470Z         torch.manual_seed(123)
2025-04-11T03:52:12.6705561Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6705651Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6705658Z 
2025-04-11T03:52:12.6705815Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6705926Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6705933Z 
2025-04-11T03:52:12.6706014Z device = None
2025-04-11T03:52:12.6706018Z 
2025-04-11T03:52:12.6706138Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6706399Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6706475Z     
2025-04-11T03:52:12.6706556Z         Args:
2025-04-11T03:52:12.6706725Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6706891Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6707003Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6707078Z         """
2025-04-11T03:52:12.6707161Z         _lazy_init()
2025-04-11T03:52:12.6707258Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6707360Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6707468Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6707850Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6707993Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6708157Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6708161Z 
2025-04-11T03:52:12.6708406Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6708623Z ______________ test_flash_decoding[True-False-5-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.6708627Z 
2025-04-11T03:52:12.6708785Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6708953Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6709043Z use_new_kcache_layout = True
2025-04-11T03:52:12.6709053Z 
2025-04-11T03:52:12.6709252Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6709360Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6709482Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6709626Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6709748Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6709861Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6709997Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6710106Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6710241Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6710398Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6710486Z     def test_flash_decoding(
2025-04-11T03:52:12.6710564Z         bsz: int,
2025-04-11T03:52:12.6710648Z         block_size: int,
2025-04-11T03:52:12.6710741Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6710831Z         num_attn_heads: int,
2025-04-11T03:52:12.6710914Z         kv_group_num: int,
2025-04-11T03:52:12.6711004Z         same_context_len: bool,
2025-04-11T03:52:12.6711078Z         q_len: int,
2025-04-11T03:52:12.6711165Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6711258Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6711331Z     ):
2025-04-11T03:52:12.6711445Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6711637Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6711822Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6711992Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6712153Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6712317Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6712389Z     
2025-04-11T03:52:12.6712479Z         torch.manual_seed(123)
2025-04-11T03:52:12.6712569Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6712665Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6712802Z 
2025-04-11T03:52:12.6712960Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6713074Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6713078Z 
2025-04-11T03:52:12.6713162Z device = None
2025-04-11T03:52:12.6713166Z 
2025-04-11T03:52:12.6713283Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6713443Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6713516Z     
2025-04-11T03:52:12.6713595Z         Args:
2025-04-11T03:52:12.6713762Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6714043Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6714155Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6714231Z         """
2025-04-11T03:52:12.6714316Z         _lazy_init()
2025-04-11T03:52:12.6714419Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6714528Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6714635Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6714919Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6715059Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6715218Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6715222Z 
2025-04-11T03:52:12.6715465Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6715633Z _____________ test_flash_decoding[True-False-5-True-4-16-8-16-16] ______________
2025-04-11T03:52:12.6715637Z 
2025-04-11T03:52:12.6715792Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6715958Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6716045Z use_new_kcache_layout = True
2025-04-11T03:52:12.6716054Z 
2025-04-11T03:52:12.6716257Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6716359Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6716484Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6716625Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6716746Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6716857Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6716997Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6717106Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6717242Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6717402Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6717491Z     def test_flash_decoding(
2025-04-11T03:52:12.6717571Z         bsz: int,
2025-04-11T03:52:12.6717653Z         block_size: int,
2025-04-11T03:52:12.6717743Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6717830Z         num_attn_heads: int,
2025-04-11T03:52:12.6717912Z         kv_group_num: int,
2025-04-11T03:52:12.6718004Z         same_context_len: bool,
2025-04-11T03:52:12.6718082Z         q_len: int,
2025-04-11T03:52:12.6718169Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6718266Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6718337Z     ):
2025-04-11T03:52:12.6718453Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6718648Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6718836Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6719117Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6719283Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6719449Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6719540Z     
2025-04-11T03:52:12.6719656Z         torch.manual_seed(123)
2025-04-11T03:52:12.6719749Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6719848Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6719851Z 
2025-04-11T03:52:12.6720010Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6720124Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6720227Z 
2025-04-11T03:52:12.6720310Z device = None
2025-04-11T03:52:12.6720314Z 
2025-04-11T03:52:12.6720433Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6720591Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6720662Z     
2025-04-11T03:52:12.6720739Z         Args:
2025-04-11T03:52:12.6720905Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6721072Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6721185Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6721257Z         """
2025-04-11T03:52:12.6721337Z         _lazy_init()
2025-04-11T03:52:12.6721435Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6721544Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6721650Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6721942Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6722083Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6722244Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6722248Z 
2025-04-11T03:52:12.6722493Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6722663Z ______________ test_flash_decoding[True-False-5-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.6722667Z 
2025-04-11T03:52:12.6722821Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6722983Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6723076Z use_new_kcache_layout = True
2025-04-11T03:52:12.6723079Z 
2025-04-11T03:52:12.6723278Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6723385Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6723506Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6723643Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6723769Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6723883Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6724027Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6724131Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6724264Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6724417Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6724505Z     def test_flash_decoding(
2025-04-11T03:52:12.6724585Z         bsz: int,
2025-04-11T03:52:12.6724669Z         block_size: int,
2025-04-11T03:52:12.6724759Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6724851Z         num_attn_heads: int,
2025-04-11T03:52:12.6724935Z         kv_group_num: int,
2025-04-11T03:52:12.6725025Z         same_context_len: bool,
2025-04-11T03:52:12.6725102Z         q_len: int,
2025-04-11T03:52:12.6725296Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6725388Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6725461Z     ):
2025-04-11T03:52:12.6725579Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6725774Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6725962Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6726134Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6726300Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6726459Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6726627Z     
2025-04-11T03:52:12.6726720Z         torch.manual_seed(123)
2025-04-11T03:52:12.6726811Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6726910Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6726917Z 
2025-04-11T03:52:12.6727071Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6727186Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6727194Z 
2025-04-11T03:52:12.6727274Z device = None
2025-04-11T03:52:12.6727278Z 
2025-04-11T03:52:12.6727399Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6727552Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6727625Z     
2025-04-11T03:52:12.6727704Z         Args:
2025-04-11T03:52:12.6727870Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6728042Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6728151Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6728223Z         """
2025-04-11T03:52:12.6728308Z         _lazy_init()
2025-04-11T03:52:12.6728410Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6728515Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6728622Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6728907Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6729050Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6729208Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6729213Z 
2025-04-11T03:52:12.6729453Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6729624Z _____________ test_flash_decoding[True-False-5-True-4-16-8-32-16] ______________
2025-04-11T03:52:12.6729628Z 
2025-04-11T03:52:12.6729782Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6729946Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6730038Z use_new_kcache_layout = True
2025-04-11T03:52:12.6730042Z 
2025-04-11T03:52:12.6730241Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6730345Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6730468Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6730607Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6730731Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6730845Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6730987Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6731095Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6731228Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6731513Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6731607Z     def test_flash_decoding(
2025-04-11T03:52:12.6731689Z         bsz: int,
2025-04-11T03:52:12.6731772Z         block_size: int,
2025-04-11T03:52:12.6731862Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6731951Z         num_attn_heads: int,
2025-04-11T03:52:12.6732036Z         kv_group_num: int,
2025-04-11T03:52:12.6732125Z         same_context_len: bool,
2025-04-11T03:52:12.6732203Z         q_len: int,
2025-04-11T03:52:12.6732291Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6732383Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6732455Z     ):
2025-04-11T03:52:12.6732570Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6732867Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6733054Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6733228Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6733393Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6733550Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6733621Z     
2025-04-11T03:52:12.6733714Z         torch.manual_seed(123)
2025-04-11T03:52:12.6733806Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6733903Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6733907Z 
2025-04-11T03:52:12.6734063Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6734180Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6734187Z 
2025-04-11T03:52:12.6734264Z device = None
2025-04-11T03:52:12.6734268Z 
2025-04-11T03:52:12.6734386Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6734540Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6734618Z     
2025-04-11T03:52:12.6734697Z         Args:
2025-04-11T03:52:12.6734864Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6735035Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6735141Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6735212Z         """
2025-04-11T03:52:12.6735298Z         _lazy_init()
2025-04-11T03:52:12.6735394Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6735501Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6735605Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6735891Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6736031Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6736190Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6736194Z 
2025-04-11T03:52:12.6736437Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6736603Z _____________ test_flash_decoding[True-False-5-True-4-16-16-16-7] ______________
2025-04-11T03:52:12.6736607Z 
2025-04-11T03:52:12.6736759Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6736923Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6737013Z use_new_kcache_layout = True
2025-04-11T03:52:12.6737017Z 
2025-04-11T03:52:12.6737218Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6737325Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6737449Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6737590Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6737834Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6737950Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6738088Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6738192Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6738327Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6738487Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6738577Z     def test_flash_decoding(
2025-04-11T03:52:12.6738655Z         bsz: int,
2025-04-11T03:52:12.6738737Z         block_size: int,
2025-04-11T03:52:12.6738832Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6739009Z         num_attn_heads: int,
2025-04-11T03:52:12.6739093Z         kv_group_num: int,
2025-04-11T03:52:12.6739186Z         same_context_len: bool,
2025-04-11T03:52:12.6739263Z         q_len: int,
2025-04-11T03:52:12.6739352Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6739444Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6739518Z     ):
2025-04-11T03:52:12.6739636Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6739831Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6740018Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6740189Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6740353Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6740510Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6740584Z     
2025-04-11T03:52:12.6740676Z         torch.manual_seed(123)
2025-04-11T03:52:12.6740764Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6740857Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6740864Z 
2025-04-11T03:52:12.6741016Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6741132Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6741136Z 
2025-04-11T03:52:12.6741213Z device = None
2025-04-11T03:52:12.6741217Z 
2025-04-11T03:52:12.6741334Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6741487Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6741559Z     
2025-04-11T03:52:12.6741639Z         Args:
2025-04-11T03:52:12.6741804Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6741978Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6742082Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6742155Z         """
2025-04-11T03:52:12.6742238Z         _lazy_init()
2025-04-11T03:52:12.6742337Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6742446Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6742552Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6742839Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6742976Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6743131Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6743135Z 
2025-04-11T03:52:12.6743375Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6743547Z _____________ test_flash_decoding[True-False-5-True-4-16-16-16-16] _____________
2025-04-11T03:52:12.6743551Z 
2025-04-11T03:52:12.6743707Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6743976Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6744071Z use_new_kcache_layout = True
2025-04-11T03:52:12.6744075Z 
2025-04-11T03:52:12.6744276Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6744384Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6744502Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6744638Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6744759Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6744873Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6745009Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6745207Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6745347Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6745507Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6745602Z     def test_flash_decoding(
2025-04-11T03:52:12.6745688Z         bsz: int,
2025-04-11T03:52:12.6745776Z         block_size: int,
2025-04-11T03:52:12.6745875Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6745962Z         num_attn_heads: int,
2025-04-11T03:52:12.6746048Z         kv_group_num: int,
2025-04-11T03:52:12.6746147Z         same_context_len: bool,
2025-04-11T03:52:12.6746227Z         q_len: int,
2025-04-11T03:52:12.6746319Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6746411Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6746487Z     ):
2025-04-11T03:52:12.6746607Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6746809Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6746998Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6747179Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6747353Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6747514Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6747593Z     
2025-04-11T03:52:12.6747689Z         torch.manual_seed(123)
2025-04-11T03:52:12.6747781Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6747881Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6747885Z 
2025-04-11T03:52:12.6748042Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6748159Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6748166Z 
2025-04-11T03:52:12.6748245Z device = None
2025-04-11T03:52:12.6748250Z 
2025-04-11T03:52:12.6748374Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6748565Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6748640Z     
2025-04-11T03:52:12.6748718Z         Args:
2025-04-11T03:52:12.6748883Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6749053Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6749157Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6749230Z         """
2025-04-11T03:52:12.6749313Z         _lazy_init()
2025-04-11T03:52:12.6749408Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6749516Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6749620Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6749908Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6750043Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6750313Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6750318Z 
2025-04-11T03:52:12.6750560Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6750725Z _____________ test_flash_decoding[True-False-5-True-4-16-16-32-7] ______________
2025-04-11T03:52:12.6750729Z 
2025-04-11T03:52:12.6750883Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6751045Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6751136Z use_new_kcache_layout = True
2025-04-11T03:52:12.6751140Z 
2025-04-11T03:52:12.6751338Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6751640Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6751758Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6751899Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6752023Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6752136Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6752277Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6752381Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6752520Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6752672Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6752761Z     def test_flash_decoding(
2025-04-11T03:52:12.6752841Z         bsz: int,
2025-04-11T03:52:12.6752923Z         block_size: int,
2025-04-11T03:52:12.6753015Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6753100Z         num_attn_heads: int,
2025-04-11T03:52:12.6753183Z         kv_group_num: int,
2025-04-11T03:52:12.6753277Z         same_context_len: bool,
2025-04-11T03:52:12.6753351Z         q_len: int,
2025-04-11T03:52:12.6753442Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6753531Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6753603Z     ):
2025-04-11T03:52:12.6753721Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6753912Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6754096Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6754263Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6754426Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6754584Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6754659Z     
2025-04-11T03:52:12.6754748Z         torch.manual_seed(123)
2025-04-11T03:52:12.6754837Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6754933Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6754937Z 
2025-04-11T03:52:12.6755094Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6755211Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6755215Z 
2025-04-11T03:52:12.6755292Z device = None
2025-04-11T03:52:12.6755296Z 
2025-04-11T03:52:12.6755414Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6755561Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6755632Z     
2025-04-11T03:52:12.6755710Z         Args:
2025-04-11T03:52:12.6755874Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6756043Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6756148Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6756221Z         """
2025-04-11T03:52:12.6756302Z         _lazy_init()
2025-04-11T03:52:12.6756510Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6756619Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6756726Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6757013Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6757147Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6757305Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6757313Z 
2025-04-11T03:52:12.6757550Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6757832Z _____________ test_flash_decoding[True-False-5-True-4-16-16-32-16] _____________
2025-04-11T03:52:12.6757836Z 
2025-04-11T03:52:12.6757995Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6758160Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6758250Z use_new_kcache_layout = True
2025-04-11T03:52:12.6758253Z 
2025-04-11T03:52:12.6758454Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6758561Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6758679Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6758815Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6758937Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6759050Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6759186Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6759296Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6759436Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6759586Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6759674Z     def test_flash_decoding(
2025-04-11T03:52:12.6759755Z         bsz: int,
2025-04-11T03:52:12.6759836Z         block_size: int,
2025-04-11T03:52:12.6759928Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6760009Z         num_attn_heads: int,
2025-04-11T03:52:12.6760091Z         kv_group_num: int,
2025-04-11T03:52:12.6760180Z         same_context_len: bool,
2025-04-11T03:52:12.6760256Z         q_len: int,
2025-04-11T03:52:12.6760343Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6760431Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6760508Z     ):
2025-04-11T03:52:12.6760618Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6760817Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6761006Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6761179Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6761348Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6761504Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6761579Z     
2025-04-11T03:52:12.6761665Z         torch.manual_seed(123)
2025-04-11T03:52:12.6761753Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6761849Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6761853Z 
2025-04-11T03:52:12.6762006Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6762121Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6762128Z 
2025-04-11T03:52:12.6762203Z device = None
2025-04-11T03:52:12.6762206Z 
2025-04-11T03:52:12.6762327Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6762477Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6762656Z     
2025-04-11T03:52:12.6762735Z         Args:
2025-04-11T03:52:12.6762903Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6763076Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6763182Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6763258Z         """
2025-04-11T03:52:12.6763336Z         _lazy_init()
2025-04-11T03:52:12.6763434Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6763543Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6763648Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6764034Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6764169Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6764327Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6764334Z 
2025-04-11T03:52:12.6764572Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6764738Z _____________ test_flash_decoding[True-False-5-False-1-16-8-16-7] ______________
2025-04-11T03:52:12.6764742Z 
2025-04-11T03:52:12.6764895Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6765060Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6765153Z use_new_kcache_layout = True
2025-04-11T03:52:12.6765157Z 
2025-04-11T03:52:12.6765354Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6765467Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6765584Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6765721Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6765846Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6765959Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6766097Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6766198Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6766338Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6766488Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6766576Z     def test_flash_decoding(
2025-04-11T03:52:12.6766658Z         bsz: int,
2025-04-11T03:52:12.6766738Z         block_size: int,
2025-04-11T03:52:12.6766832Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6766918Z         num_attn_heads: int,
2025-04-11T03:52:12.6767000Z         kv_group_num: int,
2025-04-11T03:52:12.6767088Z         same_context_len: bool,
2025-04-11T03:52:12.6767163Z         q_len: int,
2025-04-11T03:52:12.6767253Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6767345Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6767420Z     ):
2025-04-11T03:52:12.6767534Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6767725Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6767910Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6768080Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6768244Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6768399Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6768478Z     
2025-04-11T03:52:12.6768564Z         torch.manual_seed(123)
2025-04-11T03:52:12.6768655Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6768750Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6768754Z 
2025-04-11T03:52:12.6769020Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6769137Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6769142Z 
2025-04-11T03:52:12.6769219Z device = None
2025-04-11T03:52:12.6769223Z 
2025-04-11T03:52:12.6769344Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6769493Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6769566Z     
2025-04-11T03:52:12.6769645Z         Args:
2025-04-11T03:52:12.6769813Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6769983Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6770184Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6770262Z         """
2025-04-11T03:52:12.6770342Z         _lazy_init()
2025-04-11T03:52:12.6770448Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6770558Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6770668Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6770960Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6771099Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6771266Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6771270Z 
2025-04-11T03:52:12.6771512Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6771683Z _____________ test_flash_decoding[True-False-5-False-1-16-8-16-16] _____________
2025-04-11T03:52:12.6771693Z 
2025-04-11T03:52:12.6771847Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6772016Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6772112Z use_new_kcache_layout = True
2025-04-11T03:52:12.6772116Z 
2025-04-11T03:52:12.6772316Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6772426Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6772545Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6772693Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6772814Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6772930Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6773076Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6773186Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6773331Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6773485Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6773578Z     def test_flash_decoding(
2025-04-11T03:52:12.6773661Z         bsz: int,
2025-04-11T03:52:12.6773747Z         block_size: int,
2025-04-11T03:52:12.6773846Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6773933Z         num_attn_heads: int,
2025-04-11T03:52:12.6774024Z         kv_group_num: int,
2025-04-11T03:52:12.6774115Z         same_context_len: bool,
2025-04-11T03:52:12.6774196Z         q_len: int,
2025-04-11T03:52:12.6774287Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6774378Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6774460Z     ):
2025-04-11T03:52:12.6774575Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6774767Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6774958Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6775232Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6775402Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6775558Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6775633Z     
2025-04-11T03:52:12.6775720Z         torch.manual_seed(123)
2025-04-11T03:52:12.6775810Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6775907Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6775910Z 
2025-04-11T03:52:12.6776065Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6776182Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6776279Z 
2025-04-11T03:52:12.6776357Z device = None
2025-04-11T03:52:12.6776361Z 
2025-04-11T03:52:12.6776483Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6776633Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6776707Z     
2025-04-11T03:52:12.6776784Z         Args:
2025-04-11T03:52:12.6776947Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6777114Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6777219Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6777297Z         """
2025-04-11T03:52:12.6777376Z         _lazy_init()
2025-04-11T03:52:12.6777471Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6777579Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6777685Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6777970Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6778108Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6778275Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6778279Z 
2025-04-11T03:52:12.6778517Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6778684Z _____________ test_flash_decoding[True-False-5-False-1-16-8-32-7] ______________
2025-04-11T03:52:12.6778691Z 
2025-04-11T03:52:12.6778840Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6779002Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6779096Z use_new_kcache_layout = True
2025-04-11T03:52:12.6779099Z 
2025-04-11T03:52:12.6779294Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6779406Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6779523Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6779666Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6779785Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6779899Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6780039Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6780144Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6780283Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6780434Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6780526Z     def test_flash_decoding(
2025-04-11T03:52:12.6780604Z         bsz: int,
2025-04-11T03:52:12.6780684Z         block_size: int,
2025-04-11T03:52:12.6780778Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6780865Z         num_attn_heads: int,
2025-04-11T03:52:12.6780952Z         kv_group_num: int,
2025-04-11T03:52:12.6781039Z         same_context_len: bool,
2025-04-11T03:52:12.6781116Z         q_len: int,
2025-04-11T03:52:12.6781206Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6781411Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6781492Z     ):
2025-04-11T03:52:12.6781604Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6781795Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6781979Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6782151Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6782317Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6782472Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6782668Z     
2025-04-11T03:52:12.6782754Z         torch.manual_seed(123)
2025-04-11T03:52:12.6782843Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6782944Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6782948Z 
2025-04-11T03:52:12.6783105Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6783224Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6783227Z 
2025-04-11T03:52:12.6783305Z device = None
2025-04-11T03:52:12.6783309Z 
2025-04-11T03:52:12.6783427Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6783575Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6783650Z     
2025-04-11T03:52:12.6783725Z         Args:
2025-04-11T03:52:12.6783889Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6784057Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6784166Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6784242Z         """
2025-04-11T03:52:12.6784323Z         _lazy_init()
2025-04-11T03:52:12.6784424Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6784536Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6784644Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6784928Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6785063Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6785220Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6785224Z 
2025-04-11T03:52:12.6785463Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6785639Z _____________ test_flash_decoding[True-False-5-False-1-16-8-32-16] _____________
2025-04-11T03:52:12.6785642Z 
2025-04-11T03:52:12.6785794Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6785964Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6786061Z use_new_kcache_layout = True
2025-04-11T03:52:12.6786064Z 
2025-04-11T03:52:12.6786263Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6786374Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6786491Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6786633Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6786750Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6786864Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6787009Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6787118Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6787257Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6787408Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6787604Z     def test_flash_decoding(
2025-04-11T03:52:12.6787683Z         bsz: int,
2025-04-11T03:52:12.6787765Z         block_size: int,
2025-04-11T03:52:12.6787860Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6787940Z         num_attn_heads: int,
2025-04-11T03:52:12.6788025Z         kv_group_num: int,
2025-04-11T03:52:12.6788109Z         same_context_len: bool,
2025-04-11T03:52:12.6788183Z         q_len: int,
2025-04-11T03:52:12.6788272Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6788360Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6788478Z     ):
2025-04-11T03:52:12.6788591Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6788784Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6789101Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6789275Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6789441Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6789598Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6789673Z     
2025-04-11T03:52:12.6789760Z         torch.manual_seed(123)
2025-04-11T03:52:12.6789848Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6789946Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6789949Z 
2025-04-11T03:52:12.6790103Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6790219Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6790225Z 
2025-04-11T03:52:12.6790303Z device = None
2025-04-11T03:52:12.6790306Z 
2025-04-11T03:52:12.6790426Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6790574Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6790651Z     
2025-04-11T03:52:12.6790725Z         Args:
2025-04-11T03:52:12.6790891Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6791058Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6791165Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6791242Z         """
2025-04-11T03:52:12.6791320Z         _lazy_init()
2025-04-11T03:52:12.6791415Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6791523Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6791627Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6791914Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6792053Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6792217Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6792220Z 
2025-04-11T03:52:12.6792460Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6792630Z _____________ test_flash_decoding[True-False-5-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.6792634Z 
2025-04-11T03:52:12.6792781Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6792943Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6793035Z use_new_kcache_layout = True
2025-04-11T03:52:12.6793039Z 
2025-04-11T03:52:12.6793236Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6793347Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6793464Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6793606Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6793833Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6793949Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6794092Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6794198Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6794338Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6794485Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6794577Z     def test_flash_decoding(
2025-04-11T03:52:12.6794656Z         bsz: int,
2025-04-11T03:52:12.6794737Z         block_size: int,
2025-04-11T03:52:12.6794829Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6795010Z         num_attn_heads: int,
2025-04-11T03:52:12.6795096Z         kv_group_num: int,
2025-04-11T03:52:12.6795181Z         same_context_len: bool,
2025-04-11T03:52:12.6795258Z         q_len: int,
2025-04-11T03:52:12.6795351Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6795441Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6795518Z     ):
2025-04-11T03:52:12.6795629Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6795827Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6796009Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6796181Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6796349Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6796509Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6796586Z     
2025-04-11T03:52:12.6796672Z         torch.manual_seed(123)
2025-04-11T03:52:12.6796764Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6796856Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6796860Z 
2025-04-11T03:52:12.6797018Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6797135Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6797138Z 
2025-04-11T03:52:12.6797214Z device = None
2025-04-11T03:52:12.6797218Z 
2025-04-11T03:52:12.6797338Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6797487Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6797559Z     
2025-04-11T03:52:12.6797634Z         Args:
2025-04-11T03:52:12.6797800Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6797970Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6798078Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6798155Z         """
2025-04-11T03:52:12.6798233Z         _lazy_init()
2025-04-11T03:52:12.6798333Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6798436Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6798541Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6798829Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6798963Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6799123Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6799127Z 
2025-04-11T03:52:12.6799367Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6799541Z ____________ test_flash_decoding[True-False-5-False-1-16-16-16-16] _____________
2025-04-11T03:52:12.6799548Z 
2025-04-11T03:52:12.6799699Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6799973Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6800069Z use_new_kcache_layout = True
2025-04-11T03:52:12.6800072Z 
2025-04-11T03:52:12.6800271Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6800379Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6800496Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6800638Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6800754Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6800869Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6801005Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6801207Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6801345Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6801496Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6801589Z     def test_flash_decoding(
2025-04-11T03:52:12.6801665Z         bsz: int,
2025-04-11T03:52:12.6801746Z         block_size: int,
2025-04-11T03:52:12.6801840Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6801924Z         num_attn_heads: int,
2025-04-11T03:52:12.6802013Z         kv_group_num: int,
2025-04-11T03:52:12.6802101Z         same_context_len: bool,
2025-04-11T03:52:12.6802176Z         q_len: int,
2025-04-11T03:52:12.6802267Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6802355Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6802434Z     ):
2025-04-11T03:52:12.6802545Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6802738Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6802923Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6803097Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6803266Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6803421Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6803500Z     
2025-04-11T03:52:12.6803585Z         torch.manual_seed(123)
2025-04-11T03:52:12.6803677Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6803766Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6803770Z 
2025-04-11T03:52:12.6803923Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6804040Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6804048Z 
2025-04-11T03:52:12.6804126Z device = None
2025-04-11T03:52:12.6804130Z 
2025-04-11T03:52:12.6804252Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6804402Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6804477Z     
2025-04-11T03:52:12.6804553Z         Args:
2025-04-11T03:52:12.6804718Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6804887Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6804994Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6805072Z         """
2025-04-11T03:52:12.6805151Z         _lazy_init()
2025-04-11T03:52:12.6805254Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6805357Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6805462Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6805752Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6805891Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6806157Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6806162Z 
2025-04-11T03:52:12.6806401Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6806571Z _____________ test_flash_decoding[True-False-5-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.6806575Z 
2025-04-11T03:52:12.6806723Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6806888Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6806977Z use_new_kcache_layout = True
2025-04-11T03:52:12.6806980Z 
2025-04-11T03:52:12.6807179Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6807382Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6807499Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6807641Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6807763Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6807881Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6808017Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6808125Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6808267Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6808420Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6808512Z     def test_flash_decoding(
2025-04-11T03:52:12.6808587Z         bsz: int,
2025-04-11T03:52:12.6808666Z         block_size: int,
2025-04-11T03:52:12.6808759Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6808849Z         num_attn_heads: int,
2025-04-11T03:52:12.6808936Z         kv_group_num: int,
2025-04-11T03:52:12.6809020Z         same_context_len: bool,
2025-04-11T03:52:12.6809097Z         q_len: int,
2025-04-11T03:52:12.6809182Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6809276Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6809353Z     ):
2025-04-11T03:52:12.6809465Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6809658Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6809840Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6810012Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6810180Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6810336Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6810417Z     
2025-04-11T03:52:12.6810502Z         torch.manual_seed(123)
2025-04-11T03:52:12.6810594Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6810685Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6810689Z 
2025-04-11T03:52:12.6810849Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6810965Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6810969Z 
2025-04-11T03:52:12.6811046Z device = None
2025-04-11T03:52:12.6811050Z 
2025-04-11T03:52:12.6811174Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6811324Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6811398Z     
2025-04-11T03:52:12.6811472Z         Args:
2025-04-11T03:52:12.6811641Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6811812Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6811921Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6811998Z         """
2025-04-11T03:52:12.6812075Z         _lazy_init()
2025-04-11T03:52:12.6812276Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6812380Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6812486Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6812775Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6812909Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6813067Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6813071Z 
2025-04-11T03:52:12.6813310Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6813488Z ____________ test_flash_decoding[True-False-5-False-1-16-16-32-16] _____________
2025-04-11T03:52:12.6813601Z 
2025-04-11T03:52:12.6813757Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6813929Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6814018Z use_new_kcache_layout = True
2025-04-11T03:52:12.6814022Z 
2025-04-11T03:52:12.6814219Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6814329Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6814445Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6814589Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6814705Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6814822Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6814958Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6815066Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6815207Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6815357Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6815449Z     def test_flash_decoding(
2025-04-11T03:52:12.6815525Z         bsz: int,
2025-04-11T03:52:12.6815609Z         block_size: int,
2025-04-11T03:52:12.6815698Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6815782Z         num_attn_heads: int,
2025-04-11T03:52:12.6815872Z         kv_group_num: int,
2025-04-11T03:52:12.6815956Z         same_context_len: bool,
2025-04-11T03:52:12.6816035Z         q_len: int,
2025-04-11T03:52:12.6816120Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6816210Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6816291Z     ):
2025-04-11T03:52:12.6816401Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6816594Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6816776Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6816952Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6817115Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6817272Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6817349Z     
2025-04-11T03:52:12.6817436Z         torch.manual_seed(123)
2025-04-11T03:52:12.6817527Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6817618Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6817621Z 
2025-04-11T03:52:12.6817775Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6817893Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6817900Z 
2025-04-11T03:52:12.6817977Z device = None
2025-04-11T03:52:12.6817981Z 
2025-04-11T03:52:12.6818101Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6818253Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6818330Z     
2025-04-11T03:52:12.6818512Z         Args:
2025-04-11T03:52:12.6818689Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6818854Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6818961Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6819038Z         """
2025-04-11T03:52:12.6819116Z         _lazy_init()
2025-04-11T03:52:12.6819218Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6819320Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6819425Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6819714Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6819960Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6820162Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6820168Z 
2025-04-11T03:52:12.6820419Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6820593Z _____________ test_flash_decoding[True-False-5-False-4-16-8-16-7] ______________
2025-04-11T03:52:12.6820597Z 
2025-04-11T03:52:12.6820745Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6820913Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6820999Z use_new_kcache_layout = True
2025-04-11T03:52:12.6821003Z 
2025-04-11T03:52:12.6821200Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6821311Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6821428Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6821574Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6821695Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6821814Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6821951Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6822056Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6822194Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6822344Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6822436Z     def test_flash_decoding(
2025-04-11T03:52:12.6822511Z         bsz: int,
2025-04-11T03:52:12.6822594Z         block_size: int,
2025-04-11T03:52:12.6822682Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6822767Z         num_attn_heads: int,
2025-04-11T03:52:12.6822852Z         kv_group_num: int,
2025-04-11T03:52:12.6822938Z         same_context_len: bool,
2025-04-11T03:52:12.6823020Z         q_len: int,
2025-04-11T03:52:12.6823105Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6823198Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6823275Z     ):
2025-04-11T03:52:12.6823386Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6823579Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6823760Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6823934Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6824099Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6824257Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6824336Z     
2025-04-11T03:52:12.6824424Z         torch.manual_seed(123)
2025-04-11T03:52:12.6824518Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6824611Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6824614Z 
2025-04-11T03:52:12.6824881Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6824996Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6825000Z 
2025-04-11T03:52:12.6825078Z device = None
2025-04-11T03:52:12.6825082Z 
2025-04-11T03:52:12.6825204Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6825353Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6825431Z     
2025-04-11T03:52:12.6825504Z         Args:
2025-04-11T03:52:12.6825674Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6825837Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6826039Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6826119Z         """
2025-04-11T03:52:12.6826197Z         _lazy_init()
2025-04-11T03:52:12.6826298Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6826403Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6826509Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6826792Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6826928Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6827090Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6827094Z 
2025-04-11T03:52:12.6827335Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6827508Z _____________ test_flash_decoding[True-False-5-False-4-16-8-16-16] _____________
2025-04-11T03:52:12.6827515Z 
2025-04-11T03:52:12.6827664Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6827836Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6827924Z use_new_kcache_layout = True
2025-04-11T03:52:12.6827928Z 
2025-04-11T03:52:12.6828132Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6828235Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6828351Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6828549Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6828665Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6828786Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6828923Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6829032Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6829171Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6829323Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6829419Z     def test_flash_decoding(
2025-04-11T03:52:12.6829495Z         bsz: int,
2025-04-11T03:52:12.6829582Z         block_size: int,
2025-04-11T03:52:12.6829670Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6829753Z         num_attn_heads: int,
2025-04-11T03:52:12.6829841Z         kv_group_num: int,
2025-04-11T03:52:12.6829925Z         same_context_len: bool,
2025-04-11T03:52:12.6830004Z         q_len: int,
2025-04-11T03:52:12.6830090Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6830178Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6830255Z     ):
2025-04-11T03:52:12.6830365Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6830561Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6830748Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6831051Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6831218Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6831379Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6831455Z     
2025-04-11T03:52:12.6831545Z         torch.manual_seed(123)
2025-04-11T03:52:12.6831638Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6831731Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6831735Z 
2025-04-11T03:52:12.6831895Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6832006Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6832010Z 
2025-04-11T03:52:12.6832199Z device = None
2025-04-11T03:52:12.6832207Z 
2025-04-11T03:52:12.6832326Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6832480Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6832556Z     
2025-04-11T03:52:12.6832633Z         Args:
2025-04-11T03:52:12.6832807Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6832972Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6833081Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6833157Z         """
2025-04-11T03:52:12.6833236Z         _lazy_init()
2025-04-11T03:52:12.6833334Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6833437Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6833545Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6833830Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6833972Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6834137Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6834142Z 
2025-04-11T03:52:12.6834385Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6834555Z _____________ test_flash_decoding[True-False-5-False-4-16-8-32-7] ______________
2025-04-11T03:52:12.6834559Z 
2025-04-11T03:52:12.6834708Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6834876Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6834964Z use_new_kcache_layout = True
2025-04-11T03:52:12.6834968Z 
2025-04-11T03:52:12.6835174Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6835283Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6835401Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6835545Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6835667Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6835784Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6835921Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6836028Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6836163Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6836315Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6836405Z     def test_flash_decoding(
2025-04-11T03:52:12.6836479Z         bsz: int,
2025-04-11T03:52:12.6836563Z         block_size: int,
2025-04-11T03:52:12.6836651Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6836738Z         num_attn_heads: int,
2025-04-11T03:52:12.6836825Z         kv_group_num: int,
2025-04-11T03:52:12.6836911Z         same_context_len: bool,
2025-04-11T03:52:12.6836989Z         q_len: int,
2025-04-11T03:52:12.6837074Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6837271Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6837351Z     ):
2025-04-11T03:52:12.6837462Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6837658Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6837840Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6838017Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6838181Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6838337Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6838509Z     
2025-04-11T03:52:12.6838596Z         torch.manual_seed(123)
2025-04-11T03:52:12.6838690Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6838780Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6838784Z 
2025-04-11T03:52:12.6838944Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6839057Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6839061Z 
2025-04-11T03:52:12.6839137Z device = None
2025-04-11T03:52:12.6839146Z 
2025-04-11T03:52:12.6839263Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6839413Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6839489Z     
2025-04-11T03:52:12.6839561Z         Args:
2025-04-11T03:52:12.6839731Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6839897Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6840012Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6840090Z         """
2025-04-11T03:52:12.6840167Z         _lazy_init()
2025-04-11T03:52:12.6840267Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6840372Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6840478Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6840758Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6840895Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6841056Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6841061Z 
2025-04-11T03:52:12.6841296Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6841466Z _____________ test_flash_decoding[True-False-5-False-4-16-8-32-16] _____________
2025-04-11T03:52:12.6841473Z 
2025-04-11T03:52:12.6841623Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6841788Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6841879Z use_new_kcache_layout = True
2025-04-11T03:52:12.6841883Z 
2025-04-11T03:52:12.6842083Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6842187Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6842303Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6842447Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6842563Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6842678Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6842817Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6842925Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6843060Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6843212Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6843406Z     def test_flash_decoding(
2025-04-11T03:52:12.6843486Z         bsz: int,
2025-04-11T03:52:12.6843570Z         block_size: int,
2025-04-11T03:52:12.6843661Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6843745Z         num_attn_heads: int,
2025-04-11T03:52:12.6843833Z         kv_group_num: int,
2025-04-11T03:52:12.6843919Z         same_context_len: bool,
2025-04-11T03:52:12.6843997Z         q_len: int,
2025-04-11T03:52:12.6844084Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6844174Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6844247Z     ):
2025-04-11T03:52:12.6844357Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6844556Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6844841Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6845017Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6845184Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6845344Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6845417Z     
2025-04-11T03:52:12.6845503Z         torch.manual_seed(123)
2025-04-11T03:52:12.6845596Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6845686Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6845689Z 
2025-04-11T03:52:12.6845846Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6845958Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6845962Z 
2025-04-11T03:52:12.6846044Z device = None
2025-04-11T03:52:12.6846048Z 
2025-04-11T03:52:12.6846165Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6846314Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6846393Z     
2025-04-11T03:52:12.6846471Z         Args:
2025-04-11T03:52:12.6846642Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6846807Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6846917Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6846990Z         """
2025-04-11T03:52:12.6847066Z         _lazy_init()
2025-04-11T03:52:12.6847168Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6847269Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6847376Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6847663Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6847803Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6847964Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6847970Z 
2025-04-11T03:52:12.6848207Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6848379Z _____________ test_flash_decoding[True-False-5-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.6848383Z 
2025-04-11T03:52:12.6848533Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6848701Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6848788Z use_new_kcache_layout = True
2025-04-11T03:52:12.6848791Z 
2025-04-11T03:52:12.6848994Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6849102Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6849219Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6849361Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6849669Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6849794Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6849931Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6850038Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6850174Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6850324Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6850416Z     def test_flash_decoding(
2025-04-11T03:52:12.6850490Z         bsz: int,
2025-04-11T03:52:12.6850574Z         block_size: int,
2025-04-11T03:52:12.6850666Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6850886Z         num_attn_heads: int,
2025-04-11T03:52:12.6850971Z         kv_group_num: int,
2025-04-11T03:52:12.6851055Z         same_context_len: bool,
2025-04-11T03:52:12.6851137Z         q_len: int,
2025-04-11T03:52:12.6851222Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6851317Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6851389Z     ):
2025-04-11T03:52:12.6851500Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6851693Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6851872Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6852045Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6852206Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6852367Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6852443Z     
2025-04-11T03:52:12.6852529Z         torch.manual_seed(123)
2025-04-11T03:52:12.6852623Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6852714Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6852718Z 
2025-04-11T03:52:12.6852878Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6852990Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6852994Z 
2025-04-11T03:52:12.6853073Z device = None
2025-04-11T03:52:12.6853077Z 
2025-04-11T03:52:12.6853195Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6853345Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6853422Z     
2025-04-11T03:52:12.6853497Z         Args:
2025-04-11T03:52:12.6853668Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6853834Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6853945Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6854018Z         """
2025-04-11T03:52:12.6854096Z         _lazy_init()
2025-04-11T03:52:12.6854195Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6854301Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6854410Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6854690Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6854830Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6854982Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6854986Z 
2025-04-11T03:52:12.6855218Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6855391Z ____________ test_flash_decoding[True-False-5-False-4-16-16-16-16] _____________
2025-04-11T03:52:12.6855398Z 
2025-04-11T03:52:12.6855550Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6855715Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6855912Z use_new_kcache_layout = True
2025-04-11T03:52:12.6855917Z 
2025-04-11T03:52:12.6856125Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6856229Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6856353Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6856493Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6856612Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6856732Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6856871Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6857073Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6857210Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6857360Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6857455Z     def test_flash_decoding(
2025-04-11T03:52:12.6857531Z         bsz: int,
2025-04-11T03:52:12.6857617Z         block_size: int,
2025-04-11T03:52:12.6857708Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6857795Z         num_attn_heads: int,
2025-04-11T03:52:12.6857877Z         kv_group_num: int,
2025-04-11T03:52:12.6857962Z         same_context_len: bool,
2025-04-11T03:52:12.6858045Z         q_len: int,
2025-04-11T03:52:12.6858130Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6858221Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6858295Z     ):
2025-04-11T03:52:12.6858406Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6858602Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6858786Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6858963Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6859130Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6859291Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6859364Z     
2025-04-11T03:52:12.6859451Z         torch.manual_seed(123)
2025-04-11T03:52:12.6859546Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6859637Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6859641Z 
2025-04-11T03:52:12.6859801Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6859914Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6859918Z 
2025-04-11T03:52:12.6860000Z device = None
2025-04-11T03:52:12.6860004Z 
2025-04-11T03:52:12.6860121Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6860271Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6860344Z     
2025-04-11T03:52:12.6860419Z         Args:
2025-04-11T03:52:12.6860590Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6860754Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6860864Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6860938Z         """
2025-04-11T03:52:12.6861015Z         _lazy_init()
2025-04-11T03:52:12.6861115Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6861215Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6861325Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6861604Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6861749Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6861906Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6862012Z 
2025-04-11T03:52:12.6862252Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6862422Z _____________ test_flash_decoding[True-False-5-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.6862426Z 
2025-04-11T03:52:12.6862577Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6862745Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6862836Z use_new_kcache_layout = True
2025-04-11T03:52:12.6862840Z 
2025-04-11T03:52:12.6863041Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6863244Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6863368Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6863506Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6863625Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6863743Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6863881Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6863986Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6864121Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6864278Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6864366Z     def test_flash_decoding(
2025-04-11T03:52:12.6864442Z         bsz: int,
2025-04-11T03:52:12.6864528Z         block_size: int,
2025-04-11T03:52:12.6864618Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6864704Z         num_attn_heads: int,
2025-04-11T03:52:12.6864791Z         kv_group_num: int,
2025-04-11T03:52:12.6864876Z         same_context_len: bool,
2025-04-11T03:52:12.6864955Z         q_len: int,
2025-04-11T03:52:12.6865039Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6865134Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6865205Z     ):
2025-04-11T03:52:12.6865314Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6865512Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6865693Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6865867Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6866032Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6866193Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6866267Z     
2025-04-11T03:52:12.6866353Z         torch.manual_seed(123)
2025-04-11T03:52:12.6866449Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6866539Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6866543Z 
2025-04-11T03:52:12.6866705Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6866817Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6866820Z 
2025-04-11T03:52:12.6866899Z device = None
2025-04-11T03:52:12.6866903Z 
2025-04-11T03:52:12.6867018Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6867172Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6867242Z     
2025-04-11T03:52:12.6867316Z         Args:
2025-04-11T03:52:12.6867484Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6867648Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6867761Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6867834Z         """
2025-04-11T03:52:12.6867910Z         _lazy_init()
2025-04-11T03:52:12.6868010Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6868209Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6868318Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6868652Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6868790Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6868947Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6868951Z 
2025-04-11T03:52:12.6869185Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6869357Z ____________ test_flash_decoding[True-False-5-False-4-16-16-32-16] _____________
2025-04-11T03:52:12.6869467Z 
2025-04-11T03:52:12.6869620Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6869789Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6869880Z use_new_kcache_layout = True
2025-04-11T03:52:12.6869884Z 
2025-04-11T03:52:12.6870086Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6870193Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6870315Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6870452Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6870570Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6870688Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6870823Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6870933Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6871069Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6871224Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6871314Z     def test_flash_decoding(
2025-04-11T03:52:12.6871392Z         bsz: int,
2025-04-11T03:52:12.6871476Z         block_size: int,
2025-04-11T03:52:12.6871565Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6871651Z         num_attn_heads: int,
2025-04-11T03:52:12.6871733Z         kv_group_num: int,
2025-04-11T03:52:12.6871821Z         same_context_len: bool,
2025-04-11T03:52:12.6871902Z         q_len: int,
2025-04-11T03:52:12.6871985Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6872078Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6872151Z     ):
2025-04-11T03:52:12.6872262Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6872456Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6872640Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6872813Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6872977Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6873136Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6873209Z     
2025-04-11T03:52:12.6873296Z         torch.manual_seed(123)
2025-04-11T03:52:12.6873388Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6873478Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6873482Z 
2025-04-11T03:52:12.6873639Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6873751Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6873755Z 
2025-04-11T03:52:12.6873836Z device = None
2025-04-11T03:52:12.6873839Z 
2025-04-11T03:52:12.6873955Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6874106Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6874178Z     
2025-04-11T03:52:12.6874376Z         Args:
2025-04-11T03:52:12.6874550Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6874714Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6874826Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6874899Z         """
2025-04-11T03:52:12.6874978Z         _lazy_init()
2025-04-11T03:52:12.6875076Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6875178Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6875288Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6875572Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6875862Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6876018Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6876026Z 
2025-04-11T03:52:12.6876267Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6876434Z ______________ test_flash_decoding[False-True-1-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.6876438Z 
2025-04-11T03:52:12.6876587Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6876751Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6876840Z use_new_kcache_layout = False
2025-04-11T03:52:12.6876844Z 
2025-04-11T03:52:12.6877043Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6877151Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6877270Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6877408Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6877527Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6877644Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6877781Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6877890Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6878022Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6878176Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6878262Z     def test_flash_decoding(
2025-04-11T03:52:12.6878337Z         bsz: int,
2025-04-11T03:52:12.6878424Z         block_size: int,
2025-04-11T03:52:12.6878514Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6878601Z         num_attn_heads: int,
2025-04-11T03:52:12.6878685Z         kv_group_num: int,
2025-04-11T03:52:12.6878771Z         same_context_len: bool,
2025-04-11T03:52:12.6878851Z         q_len: int,
2025-04-11T03:52:12.6878935Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6879028Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6879101Z     ):
2025-04-11T03:52:12.6879212Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6879401Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6879581Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6879757Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6879921Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6880079Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6880154Z     
2025-04-11T03:52:12.6880245Z         torch.manual_seed(123)
2025-04-11T03:52:12.6880333Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6880425Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6880429Z 
2025-04-11T03:52:12.6880694Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6880810Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6880813Z 
2025-04-11T03:52:12.6880894Z device = None
2025-04-11T03:52:12.6880897Z 
2025-04-11T03:52:12.6881014Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6881167Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6881238Z     
2025-04-11T03:52:12.6881311Z         Args:
2025-04-11T03:52:12.6881481Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6881644Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6881847Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6881921Z         """
2025-04-11T03:52:12.6882001Z         _lazy_init()
2025-04-11T03:52:12.6882098Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6882201Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6882312Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6882594Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6882734Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6882891Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6882895Z 
2025-04-11T03:52:12.6883136Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6883302Z _____________ test_flash_decoding[False-True-1-True-1-16-8-16-16] ______________
2025-04-11T03:52:12.6883309Z 
2025-04-11T03:52:12.6883458Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6883625Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6883717Z use_new_kcache_layout = False
2025-04-11T03:52:12.6883721Z 
2025-04-11T03:52:12.6883923Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6884033Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6884152Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6884294Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6884413Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6884525Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6884660Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6884770Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6884904Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6885058Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6885144Z     def test_flash_decoding(
2025-04-11T03:52:12.6885220Z         bsz: int,
2025-04-11T03:52:12.6885305Z         block_size: int,
2025-04-11T03:52:12.6885394Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6885480Z         num_attn_heads: int,
2025-04-11T03:52:12.6885563Z         kv_group_num: int,
2025-04-11T03:52:12.6885652Z         same_context_len: bool,
2025-04-11T03:52:12.6885727Z         q_len: int,
2025-04-11T03:52:12.6885811Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6885907Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6885978Z     ):
2025-04-11T03:52:12.6886091Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6886280Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6886463Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6886635Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6886901Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6887065Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6887137Z     
2025-04-11T03:52:12.6887225Z         torch.manual_seed(123)
2025-04-11T03:52:12.6887315Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6887406Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6887410Z 
2025-04-11T03:52:12.6887571Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6887686Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6887691Z 
2025-04-11T03:52:12.6887772Z device = None
2025-04-11T03:52:12.6887872Z 
2025-04-11T03:52:12.6887990Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6888142Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6888214Z     
2025-04-11T03:52:12.6888292Z         Args:
2025-04-11T03:52:12.6888464Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6888628Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6888736Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6888808Z         """
2025-04-11T03:52:12.6888887Z         _lazy_init()
2025-04-11T03:52:12.6888983Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6889086Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6889197Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6889478Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6889621Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6889777Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6889783Z 
2025-04-11T03:52:12.6890026Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6890193Z ______________ test_flash_decoding[False-True-1-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.6890197Z 
2025-04-11T03:52:12.6890348Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6890509Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6890599Z use_new_kcache_layout = False
2025-04-11T03:52:12.6890603Z 
2025-04-11T03:52:12.6890808Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6890916Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6891038Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6891173Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6891295Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6891409Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6891544Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6891650Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6891786Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6891940Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6892026Z     def test_flash_decoding(
2025-04-11T03:52:12.6892099Z         bsz: int,
2025-04-11T03:52:12.6892185Z         block_size: int,
2025-04-11T03:52:12.6892273Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6892358Z         num_attn_heads: int,
2025-04-11T03:52:12.6892446Z         kv_group_num: int,
2025-04-11T03:52:12.6892534Z         same_context_len: bool,
2025-04-11T03:52:12.6892612Z         q_len: int,
2025-04-11T03:52:12.6892696Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6892788Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6892967Z     ):
2025-04-11T03:52:12.6893084Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6893275Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6893454Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6893626Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6893789Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6893948Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6894112Z     
2025-04-11T03:52:12.6894205Z         torch.manual_seed(123)
2025-04-11T03:52:12.6894297Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6894388Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6894392Z 
2025-04-11T03:52:12.6894553Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6894666Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6894670Z 
2025-04-11T03:52:12.6894752Z device = None
2025-04-11T03:52:12.6894756Z 
2025-04-11T03:52:12.6894874Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6895029Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6895101Z     
2025-04-11T03:52:12.6895175Z         Args:
2025-04-11T03:52:12.6895344Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6895509Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6895621Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6895693Z         """
2025-04-11T03:52:12.6895775Z         _lazy_init()
2025-04-11T03:52:12.6895872Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6895977Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6896088Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6896368Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6896507Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6896663Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6896667Z 
2025-04-11T03:52:12.6896909Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6897073Z _____________ test_flash_decoding[False-True-1-True-1-16-8-32-16] ______________
2025-04-11T03:52:12.6897079Z 
2025-04-11T03:52:12.6897233Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6897390Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6897481Z use_new_kcache_layout = False
2025-04-11T03:52:12.6897485Z 
2025-04-11T03:52:12.6897690Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6897794Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6897914Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6898054Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6898173Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6898285Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6898422Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6898530Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6898670Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6898822Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6898907Z     def test_flash_decoding(
2025-04-11T03:52:12.6899117Z         bsz: int,
2025-04-11T03:52:12.6899202Z         block_size: int,
2025-04-11T03:52:12.6899292Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6899381Z         num_attn_heads: int,
2025-04-11T03:52:12.6899464Z         kv_group_num: int,
2025-04-11T03:52:12.6899551Z         same_context_len: bool,
2025-04-11T03:52:12.6899627Z         q_len: int,
2025-04-11T03:52:12.6899710Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6899804Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6899876Z     ):
2025-04-11T03:52:12.6899989Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6900178Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6900456Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6900628Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6900793Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6900956Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6901029Z     
2025-04-11T03:52:12.6901124Z         torch.manual_seed(123)
2025-04-11T03:52:12.6901213Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6901306Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6901313Z 
2025-04-11T03:52:12.6901467Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6901581Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6901585Z 
2025-04-11T03:52:12.6901669Z device = None
2025-04-11T03:52:12.6901676Z 
2025-04-11T03:52:12.6901793Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6901944Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6902015Z     
2025-04-11T03:52:12.6902091Z         Args:
2025-04-11T03:52:12.6902260Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6902423Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6902534Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6902608Z         """
2025-04-11T03:52:12.6902689Z         _lazy_init()
2025-04-11T03:52:12.6902788Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6902887Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6902998Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6903278Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6903422Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6903578Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6903585Z 
2025-04-11T03:52:12.6903825Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6903990Z _____________ test_flash_decoding[False-True-1-True-1-16-16-16-7] ______________
2025-04-11T03:52:12.6903994Z 
2025-04-11T03:52:12.6904145Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6904306Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6904395Z use_new_kcache_layout = False
2025-04-11T03:52:12.6904399Z 
2025-04-11T03:52:12.6904604Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6904708Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6904830Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6904968Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6905090Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6905306Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6905446Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6905553Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6905686Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6905840Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6905927Z     def test_flash_decoding(
2025-04-11T03:52:12.6906008Z         bsz: int,
2025-04-11T03:52:12.6906089Z         block_size: int,
2025-04-11T03:52:12.6906177Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6906264Z         num_attn_heads: int,
2025-04-11T03:52:12.6906439Z         kv_group_num: int,
2025-04-11T03:52:12.6906528Z         same_context_len: bool,
2025-04-11T03:52:12.6906603Z         q_len: int,
2025-04-11T03:52:12.6906688Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6906784Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6906858Z     ):
2025-04-11T03:52:12.6906972Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6907163Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6907347Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6907516Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6907680Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6907838Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6907912Z     
2025-04-11T03:52:12.6908001Z         torch.manual_seed(123)
2025-04-11T03:52:12.6908090Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6908181Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6908188Z 
2025-04-11T03:52:12.6908346Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6908503Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6908507Z 
2025-04-11T03:52:12.6908592Z device = None
2025-04-11T03:52:12.6908595Z 
2025-04-11T03:52:12.6908715Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6908867Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6908937Z     
2025-04-11T03:52:12.6909013Z         Args:
2025-04-11T03:52:12.6909178Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6909342Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6909456Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6909529Z         """
2025-04-11T03:52:12.6909612Z         _lazy_init()
2025-04-11T03:52:12.6909710Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6909815Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6909927Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6910212Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6910350Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6910507Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6910511Z 
2025-04-11T03:52:12.6910757Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6910923Z _____________ test_flash_decoding[False-True-1-True-1-16-16-16-16] _____________
2025-04-11T03:52:12.6910930Z 
2025-04-11T03:52:12.6911084Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6911242Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6911465Z use_new_kcache_layout = False
2025-04-11T03:52:12.6911476Z 
2025-04-11T03:52:12.6911672Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6911777Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6911898Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6912036Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6912156Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6912269Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6912403Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6912509Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6912751Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6912906Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6912994Z     def test_flash_decoding(
2025-04-11T03:52:12.6913075Z         bsz: int,
2025-04-11T03:52:12.6913156Z         block_size: int,
2025-04-11T03:52:12.6913245Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6913332Z         num_attn_heads: int,
2025-04-11T03:52:12.6913412Z         kv_group_num: int,
2025-04-11T03:52:12.6913502Z         same_context_len: bool,
2025-04-11T03:52:12.6913578Z         q_len: int,
2025-04-11T03:52:12.6913663Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6913753Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6913824Z     ):
2025-04-11T03:52:12.6913935Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6914124Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6914309Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6914476Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6914639Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6914802Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6914872Z     
2025-04-11T03:52:12.6914962Z         torch.manual_seed(123)
2025-04-11T03:52:12.6915055Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6915148Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6915152Z 
2025-04-11T03:52:12.6915306Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6915418Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6915422Z 
2025-04-11T03:52:12.6915502Z device = None
2025-04-11T03:52:12.6915509Z 
2025-04-11T03:52:12.6915627Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6915779Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6915850Z     
2025-04-11T03:52:12.6915927Z         Args:
2025-04-11T03:52:12.6916096Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6916261Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6916370Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6916442Z         """
2025-04-11T03:52:12.6916523Z         _lazy_init()
2025-04-11T03:52:12.6916621Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6916727Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6916831Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6917113Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6917254Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6917414Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6917418Z 
2025-04-11T03:52:12.6917763Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6917934Z _____________ test_flash_decoding[False-True-1-True-1-16-16-32-7] ______________
2025-04-11T03:52:12.6917938Z 
2025-04-11T03:52:12.6918093Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6918255Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6918350Z use_new_kcache_layout = False
2025-04-11T03:52:12.6918354Z 
2025-04-11T03:52:12.6918555Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6918659Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6918879Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6919019Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6919140Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6919257Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6919401Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6919506Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6919642Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6919798Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6919883Z     def test_flash_decoding(
2025-04-11T03:52:12.6919962Z         bsz: int,
2025-04-11T03:52:12.6920046Z         block_size: int,
2025-04-11T03:52:12.6920135Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6920227Z         num_attn_heads: int,
2025-04-11T03:52:12.6920316Z         kv_group_num: int,
2025-04-11T03:52:12.6920404Z         same_context_len: bool,
2025-04-11T03:52:12.6920482Z         q_len: int,
2025-04-11T03:52:12.6920566Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6920671Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6920771Z     ):
2025-04-11T03:52:12.6920891Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6921083Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6921271Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6921440Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6921604Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6921763Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6921838Z     
2025-04-11T03:52:12.6921931Z         torch.manual_seed(123)
2025-04-11T03:52:12.6922020Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6922113Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6922118Z 
2025-04-11T03:52:12.6922274Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6922386Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6922393Z 
2025-04-11T03:52:12.6922470Z device = None
2025-04-11T03:52:12.6922474Z 
2025-04-11T03:52:12.6922590Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6922741Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6922813Z     
2025-04-11T03:52:12.6922891Z         Args:
2025-04-11T03:52:12.6923055Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6923219Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6923333Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6923407Z         """
2025-04-11T03:52:12.6923487Z         _lazy_init()
2025-04-11T03:52:12.6923584Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6923801Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6923910Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6924191Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6924331Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6924489Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6924493Z 
2025-04-11T03:52:12.6924735Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6924901Z _____________ test_flash_decoding[False-True-1-True-1-16-16-32-16] _____________
2025-04-11T03:52:12.6925003Z 
2025-04-11T03:52:12.6925159Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6925318Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6925413Z use_new_kcache_layout = False
2025-04-11T03:52:12.6925417Z 
2025-04-11T03:52:12.6925616Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6925719Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6925844Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6925981Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6926101Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6926216Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6926356Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6926459Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6926595Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6926748Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6926835Z     def test_flash_decoding(
2025-04-11T03:52:12.6926916Z         bsz: int,
2025-04-11T03:52:12.6926997Z         block_size: int,
2025-04-11T03:52:12.6927087Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6927175Z         num_attn_heads: int,
2025-04-11T03:52:12.6927257Z         kv_group_num: int,
2025-04-11T03:52:12.6927344Z         same_context_len: bool,
2025-04-11T03:52:12.6927419Z         q_len: int,
2025-04-11T03:52:12.6927507Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6927596Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6927668Z     ):
2025-04-11T03:52:12.6927783Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6927973Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6928161Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6928328Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6928496Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6928656Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6928726Z     
2025-04-11T03:52:12.6928817Z         torch.manual_seed(123)
2025-04-11T03:52:12.6928907Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6929006Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6929009Z 
2025-04-11T03:52:12.6929163Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6929274Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6929281Z 
2025-04-11T03:52:12.6929358Z device = None
2025-04-11T03:52:12.6929365Z 
2025-04-11T03:52:12.6929483Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6929635Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6929705Z     
2025-04-11T03:52:12.6929781Z         Args:
2025-04-11T03:52:12.6930049Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6930222Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6930329Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6930402Z         """
2025-04-11T03:52:12.6930485Z         _lazy_init()
2025-04-11T03:52:12.6930582Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6930688Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6930794Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6931078Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6931316Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6931472Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6931476Z 
2025-04-11T03:52:12.6931722Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6931891Z ______________ test_flash_decoding[False-True-1-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.6931894Z 
2025-04-11T03:52:12.6932048Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6932209Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6932304Z use_new_kcache_layout = False
2025-04-11T03:52:12.6932308Z 
2025-04-11T03:52:12.6932507Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6932611Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6932735Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6932873Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6932997Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6933112Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6933250Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6933354Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6933489Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6933647Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6933733Z     def test_flash_decoding(
2025-04-11T03:52:12.6933810Z         bsz: int,
2025-04-11T03:52:12.6933892Z         block_size: int,
2025-04-11T03:52:12.6933985Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6934068Z         num_attn_heads: int,
2025-04-11T03:52:12.6934156Z         kv_group_num: int,
2025-04-11T03:52:12.6934244Z         same_context_len: bool,
2025-04-11T03:52:12.6934321Z         q_len: int,
2025-04-11T03:52:12.6934409Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6934497Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6934573Z     ):
2025-04-11T03:52:12.6934688Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6934883Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6935067Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6935236Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6935400Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6935557Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6935632Z     
2025-04-11T03:52:12.6935723Z         torch.manual_seed(123)
2025-04-11T03:52:12.6935813Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6935907Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6935911Z 
2025-04-11T03:52:12.6936178Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6936300Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6936303Z 
2025-04-11T03:52:12.6936382Z device = None
2025-04-11T03:52:12.6936386Z 
2025-04-11T03:52:12.6936503Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6936657Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6936731Z     
2025-04-11T03:52:12.6936807Z         Args:
2025-04-11T03:52:12.6936973Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6937141Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6937339Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6937411Z         """
2025-04-11T03:52:12.6937492Z         _lazy_init()
2025-04-11T03:52:12.6937590Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6937701Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6937806Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6938096Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6938235Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6938394Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6938398Z 
2025-04-11T03:52:12.6938642Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6938810Z _____________ test_flash_decoding[False-True-1-True-4-16-8-16-16] ______________
2025-04-11T03:52:12.6938816Z 
2025-04-11T03:52:12.6938973Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6939134Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6939230Z use_new_kcache_layout = False
2025-04-11T03:52:12.6939234Z 
2025-04-11T03:52:12.6939434Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6939541Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6939660Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6939799Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6939919Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6940031Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6940170Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6940274Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6940416Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6940573Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6940660Z     def test_flash_decoding(
2025-04-11T03:52:12.6940745Z         bsz: int,
2025-04-11T03:52:12.6940826Z         block_size: int,
2025-04-11T03:52:12.6940919Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6941003Z         num_attn_heads: int,
2025-04-11T03:52:12.6941086Z         kv_group_num: int,
2025-04-11T03:52:12.6941176Z         same_context_len: bool,
2025-04-11T03:52:12.6941253Z         q_len: int,
2025-04-11T03:52:12.6941339Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6941427Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6941498Z     ):
2025-04-11T03:52:12.6941614Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6941805Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6941992Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6942162Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6942471Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6942633Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6942706Z     
2025-04-11T03:52:12.6942797Z         torch.manual_seed(123)
2025-04-11T03:52:12.6942887Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6942982Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6942986Z 
2025-04-11T03:52:12.6943140Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6943256Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6943260Z 
2025-04-11T03:52:12.6943340Z device = None
2025-04-11T03:52:12.6943450Z 
2025-04-11T03:52:12.6943568Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6943721Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6943793Z     
2025-04-11T03:52:12.6943874Z         Args:
2025-04-11T03:52:12.6944045Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6944218Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6944324Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6944398Z         """
2025-04-11T03:52:12.6944479Z         _lazy_init()
2025-04-11T03:52:12.6944574Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6944679Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6944784Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6945073Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6945214Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6945371Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6945375Z 
2025-04-11T03:52:12.6945623Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6945792Z ______________ test_flash_decoding[False-True-1-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.6945796Z 
2025-04-11T03:52:12.6945951Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6946115Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6946207Z use_new_kcache_layout = False
2025-04-11T03:52:12.6946211Z 
2025-04-11T03:52:12.6946410Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6946518Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6946637Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6946778Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6946899Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6947014Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6947153Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6947255Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6947392Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6947544Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6947631Z     def test_flash_decoding(
2025-04-11T03:52:12.6947712Z         bsz: int,
2025-04-11T03:52:12.6947791Z         block_size: int,
2025-04-11T03:52:12.6947881Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6947963Z         num_attn_heads: int,
2025-04-11T03:52:12.6948047Z         kv_group_num: int,
2025-04-11T03:52:12.6948136Z         same_context_len: bool,
2025-04-11T03:52:12.6948213Z         q_len: int,
2025-04-11T03:52:12.6948301Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6948387Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6948510Z     ):
2025-04-11T03:52:12.6948835Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6949029Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6949214Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6949383Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6949549Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6949706Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6949778Z     
2025-04-11T03:52:12.6949977Z         torch.manual_seed(123)
2025-04-11T03:52:12.6950066Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6950162Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6950166Z 
2025-04-11T03:52:12.6950321Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6950440Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6950444Z 
2025-04-11T03:52:12.6950521Z device = None
2025-04-11T03:52:12.6950525Z 
2025-04-11T03:52:12.6950645Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6950794Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6950866Z     
2025-04-11T03:52:12.6950941Z         Args:
2025-04-11T03:52:12.6951106Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6951272Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6951381Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6951454Z         """
2025-04-11T03:52:12.6951535Z         _lazy_init()
2025-04-11T03:52:12.6951630Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6951737Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6951843Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6952128Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6952265Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6952420Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6952429Z 
2025-04-11T03:52:12.6952666Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6952830Z _____________ test_flash_decoding[False-True-1-True-4-16-8-32-16] ______________
2025-04-11T03:52:12.6952837Z 
2025-04-11T03:52:12.6952992Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6953153Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6953245Z use_new_kcache_layout = False
2025-04-11T03:52:12.6953251Z 
2025-04-11T03:52:12.6953450Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6953555Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6953673Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6953809Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6953930Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6954042Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6954181Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6954284Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6954426Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6954574Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6954660Z     def test_flash_decoding(
2025-04-11T03:52:12.6954740Z         bsz: int,
2025-04-11T03:52:12.6954926Z         block_size: int,
2025-04-11T03:52:12.6955025Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6955108Z         num_attn_heads: int,
2025-04-11T03:52:12.6955191Z         kv_group_num: int,
2025-04-11T03:52:12.6955282Z         same_context_len: bool,
2025-04-11T03:52:12.6955358Z         q_len: int,
2025-04-11T03:52:12.6955446Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6955534Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6955609Z     ):
2025-04-11T03:52:12.6955720Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6955910Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6956193Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6956362Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6956530Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6956688Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6956764Z     
2025-04-11T03:52:12.6956849Z         torch.manual_seed(123)
2025-04-11T03:52:12.6956939Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6957034Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6957038Z 
2025-04-11T03:52:12.6957189Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6957305Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6957309Z 
2025-04-11T03:52:12.6957385Z device = None
2025-04-11T03:52:12.6957392Z 
2025-04-11T03:52:12.6957512Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6957661Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6957731Z     
2025-04-11T03:52:12.6957810Z         Args:
2025-04-11T03:52:12.6957979Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6958147Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6958253Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6958326Z         """
2025-04-11T03:52:12.6958403Z         _lazy_init()
2025-04-11T03:52:12.6958499Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6958605Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6958712Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6959003Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6959141Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6959300Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6959307Z 
2025-04-11T03:52:12.6959549Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6959717Z _____________ test_flash_decoding[False-True-1-True-4-16-16-16-7] ______________
2025-04-11T03:52:12.6959721Z 
2025-04-11T03:52:12.6959873Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6960033Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6960124Z use_new_kcache_layout = False
2025-04-11T03:52:12.6960128Z 
2025-04-11T03:52:12.6960327Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6960435Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6960556Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6960693Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6960812Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6961025Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6961169Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6961275Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6961417Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6961569Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6961657Z     def test_flash_decoding(
2025-04-11T03:52:12.6961738Z         bsz: int,
2025-04-11T03:52:12.6961818Z         block_size: int,
2025-04-11T03:52:12.6961909Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6961993Z         num_attn_heads: int,
2025-04-11T03:52:12.6962075Z         kv_group_num: int,
2025-04-11T03:52:12.6962262Z         same_context_len: bool,
2025-04-11T03:52:12.6962337Z         q_len: int,
2025-04-11T03:52:12.6962428Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6962519Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6962595Z     ):
2025-04-11T03:52:12.6962708Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6962901Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6963085Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6963256Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6963419Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6963575Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6963650Z     
2025-04-11T03:52:12.6963739Z         torch.manual_seed(123)
2025-04-11T03:52:12.6963826Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6963921Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6963925Z 
2025-04-11T03:52:12.6964080Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6964198Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6964201Z 
2025-04-11T03:52:12.6964280Z device = None
2025-04-11T03:52:12.6964283Z 
2025-04-11T03:52:12.6964404Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6964551Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6964621Z     
2025-04-11T03:52:12.6964699Z         Args:
2025-04-11T03:52:12.6964863Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6965030Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6965140Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6965214Z         """
2025-04-11T03:52:12.6965291Z         _lazy_init()
2025-04-11T03:52:12.6965386Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6965493Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6965599Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6965886Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6966021Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6966184Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6966188Z 
2025-04-11T03:52:12.6966426Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6966590Z _____________ test_flash_decoding[False-True-1-True-4-16-16-16-16] _____________
2025-04-11T03:52:12.6966602Z 
2025-04-11T03:52:12.6966755Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6966914Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6967006Z use_new_kcache_layout = False
2025-04-11T03:52:12.6967116Z 
2025-04-11T03:52:12.6967317Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6967423Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6967541Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6967682Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6967802Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6967916Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6968056Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6968157Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6968407Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6968559Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6968645Z     def test_flash_decoding(
2025-04-11T03:52:12.6968727Z         bsz: int,
2025-04-11T03:52:12.6968814Z         block_size: int,
2025-04-11T03:52:12.6968905Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6968989Z         num_attn_heads: int,
2025-04-11T03:52:12.6969075Z         kv_group_num: int,
2025-04-11T03:52:12.6969159Z         same_context_len: bool,
2025-04-11T03:52:12.6969234Z         q_len: int,
2025-04-11T03:52:12.6969322Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6969410Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6969485Z     ):
2025-04-11T03:52:12.6969594Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6969785Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6969978Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6970150Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6970318Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6970476Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6970550Z     
2025-04-11T03:52:12.6970638Z         torch.manual_seed(123)
2025-04-11T03:52:12.6970727Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6970823Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6970827Z 
2025-04-11T03:52:12.6970980Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6971095Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6971099Z 
2025-04-11T03:52:12.6971175Z device = None
2025-04-11T03:52:12.6971179Z 
2025-04-11T03:52:12.6971305Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6971453Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6971523Z     
2025-04-11T03:52:12.6971603Z         Args:
2025-04-11T03:52:12.6971773Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6971942Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6972047Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6972123Z         """
2025-04-11T03:52:12.6972200Z         _lazy_init()
2025-04-11T03:52:12.6972297Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6972404Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6972512Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6972801Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6972940Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6973100Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6973104Z 
2025-04-11T03:52:12.6973465Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6973635Z _____________ test_flash_decoding[False-True-1-True-4-16-16-32-7] ______________
2025-04-11T03:52:12.6973642Z 
2025-04-11T03:52:12.6973791Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6973951Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6974041Z use_new_kcache_layout = False
2025-04-11T03:52:12.6974045Z 
2025-04-11T03:52:12.6974243Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6974351Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6974557Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6974697Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6974813Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6974927Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6975068Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6975171Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6975309Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6975461Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6975552Z     def test_flash_decoding(
2025-04-11T03:52:12.6975630Z         bsz: int,
2025-04-11T03:52:12.6975711Z         block_size: int,
2025-04-11T03:52:12.6975808Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6975891Z         num_attn_heads: int,
2025-04-11T03:52:12.6975979Z         kv_group_num: int,
2025-04-11T03:52:12.6976071Z         same_context_len: bool,
2025-04-11T03:52:12.6976147Z         q_len: int,
2025-04-11T03:52:12.6976239Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6976326Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6976401Z     ):
2025-04-11T03:52:12.6976518Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6976709Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6976892Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6977062Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6977232Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6977390Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6977465Z     
2025-04-11T03:52:12.6977555Z         torch.manual_seed(123)
2025-04-11T03:52:12.6977645Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6977741Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6977745Z 
2025-04-11T03:52:12.6977900Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6978024Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6978028Z 
2025-04-11T03:52:12.6978104Z device = None
2025-04-11T03:52:12.6978107Z 
2025-04-11T03:52:12.6978226Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6978374Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6978448Z     
2025-04-11T03:52:12.6978521Z         Args:
2025-04-11T03:52:12.6978689Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6978855Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6978965Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6979041Z         """
2025-04-11T03:52:12.6979118Z         _lazy_init()
2025-04-11T03:52:12.6979212Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6979315Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6979527Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6979821Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6979961Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6980121Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6980125Z 
2025-04-11T03:52:12.6980365Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6980537Z _____________ test_flash_decoding[False-True-1-True-4-16-16-32-16] _____________
2025-04-11T03:52:12.6980631Z 
2025-04-11T03:52:12.6980789Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6980952Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6981044Z use_new_kcache_layout = False
2025-04-11T03:52:12.6981051Z 
2025-04-11T03:52:12.6981249Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6981357Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6981475Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6981616Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6981731Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6981842Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6981983Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6982086Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6982230Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6982379Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6982468Z     def test_flash_decoding(
2025-04-11T03:52:12.6982544Z         bsz: int,
2025-04-11T03:52:12.6982628Z         block_size: int,
2025-04-11T03:52:12.6982721Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6982803Z         num_attn_heads: int,
2025-04-11T03:52:12.6982889Z         kv_group_num: int,
2025-04-11T03:52:12.6982972Z         same_context_len: bool,
2025-04-11T03:52:12.6983046Z         q_len: int,
2025-04-11T03:52:12.6983134Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6983220Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6983293Z     ):
2025-04-11T03:52:12.6983405Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6983600Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6983785Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6983954Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6984123Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6984278Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6984354Z     
2025-04-11T03:52:12.6984441Z         torch.manual_seed(123)
2025-04-11T03:52:12.6984530Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6984625Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6984629Z 
2025-04-11T03:52:12.6984780Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6984898Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6984901Z 
2025-04-11T03:52:12.6984977Z device = None
2025-04-11T03:52:12.6984981Z 
2025-04-11T03:52:12.6985105Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6985253Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6985328Z     
2025-04-11T03:52:12.6985402Z         Args:
2025-04-11T03:52:12.6985673Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6985847Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6985954Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6986031Z         """
2025-04-11T03:52:12.6986109Z         _lazy_init()
2025-04-11T03:52:12.6986204Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6986310Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6986417Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6986706Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6986944Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6987106Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6987110Z 
2025-04-11T03:52:12.6987350Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6987519Z _____________ test_flash_decoding[False-True-1-False-1-16-8-16-7] ______________
2025-04-11T03:52:12.6987523Z 
2025-04-11T03:52:12.6987672Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6987833Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6987926Z use_new_kcache_layout = False
2025-04-11T03:52:12.6987930Z 
2025-04-11T03:52:12.6988130Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6988239Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6988360Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6988545Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6988662Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6988778Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6988923Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6989026Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6989165Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6989317Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6989409Z     def test_flash_decoding(
2025-04-11T03:52:12.6989484Z         bsz: int,
2025-04-11T03:52:12.6989564Z         block_size: int,
2025-04-11T03:52:12.6989660Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6989742Z         num_attn_heads: int,
2025-04-11T03:52:12.6989831Z         kv_group_num: int,
2025-04-11T03:52:12.6989919Z         same_context_len: bool,
2025-04-11T03:52:12.6989993Z         q_len: int,
2025-04-11T03:52:12.6990084Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6990174Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6990251Z     ):
2025-04-11T03:52:12.6990364Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6990557Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6990737Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6990908Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6991073Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6991229Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6991304Z     
2025-04-11T03:52:12.6991393Z         torch.manual_seed(123)
2025-04-11T03:52:12.6991484Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6991574Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6991578Z 
2025-04-11T03:52:12.6991731Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6991980Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6991985Z 
2025-04-11T03:52:12.6992068Z device = None
2025-04-11T03:52:12.6992072Z 
2025-04-11T03:52:12.6992196Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6992345Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6992419Z     
2025-04-11T03:52:12.6992491Z         Args:
2025-04-11T03:52:12.6992659Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6992827Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6993041Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6993119Z         """
2025-04-11T03:52:12.6993198Z         _lazy_init()
2025-04-11T03:52:12.6993300Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6993408Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6993516Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6993805Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6993942Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6994101Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6994105Z 
2025-04-11T03:52:12.6994347Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6994514Z _____________ test_flash_decoding[False-True-1-False-1-16-8-16-16] _____________
2025-04-11T03:52:12.6994520Z 
2025-04-11T03:52:12.6994671Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6994839Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6994927Z use_new_kcache_layout = False
2025-04-11T03:52:12.6994934Z 
2025-04-11T03:52:12.6995135Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6995243Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6995359Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6995503Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6995619Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6995737Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6995873Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6995976Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6996119Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6996268Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6996357Z     def test_flash_decoding(
2025-04-11T03:52:12.6996434Z         bsz: int,
2025-04-11T03:52:12.6996518Z         block_size: int,
2025-04-11T03:52:12.6996610Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6996692Z         num_attn_heads: int,
2025-04-11T03:52:12.6996777Z         kv_group_num: int,
2025-04-11T03:52:12.6996862Z         same_context_len: bool,
2025-04-11T03:52:12.6996940Z         q_len: int,
2025-04-11T03:52:12.6997026Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6997114Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6997190Z     ):
2025-04-11T03:52:12.6997300Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6997495Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6997679Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6997848Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6998109Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6998269Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6998346Z     
2025-04-11T03:52:12.6998433Z         torch.manual_seed(123)
2025-04-11T03:52:12.6998525Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6998614Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6998618Z 
2025-04-11T03:52:12.6998771Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6998888Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6998892Z 
2025-04-11T03:52:12.6998968Z device = None
2025-04-11T03:52:12.6998972Z 
2025-04-11T03:52:12.6999189Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6999338Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6999412Z     
2025-04-11T03:52:12.6999487Z         Args:
2025-04-11T03:52:12.6999655Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6999824Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6999931Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7000009Z         """
2025-04-11T03:52:12.7000089Z         _lazy_init()
2025-04-11T03:52:12.7000189Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7000291Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7000396Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7000681Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7000821Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7000978Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7000982Z 
2025-04-11T03:52:12.7001224Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7001393Z _____________ test_flash_decoding[False-True-1-False-1-16-8-32-7] ______________
2025-04-11T03:52:12.7001396Z 
2025-04-11T03:52:12.7001546Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7001716Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7001807Z use_new_kcache_layout = False
2025-04-11T03:52:12.7001811Z 
2025-04-11T03:52:12.7002007Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7002116Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7002239Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7002380Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7002497Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7002619Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7002754Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7002858Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7002998Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7003150Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7003240Z     def test_flash_decoding(
2025-04-11T03:52:12.7003315Z         bsz: int,
2025-04-11T03:52:12.7003396Z         block_size: int,
2025-04-11T03:52:12.7003489Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7003572Z         num_attn_heads: int,
2025-04-11T03:52:12.7003657Z         kv_group_num: int,
2025-04-11T03:52:12.7003744Z         same_context_len: bool,
2025-04-11T03:52:12.7003823Z         q_len: int,
2025-04-11T03:52:12.7003908Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7003994Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7004070Z     ):
2025-04-11T03:52:12.7004283Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7004479Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7004664Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7004838Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7005006Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7005166Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7005246Z     
2025-04-11T03:52:12.7005336Z         torch.manual_seed(123)
2025-04-11T03:52:12.7005524Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7005617Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7005620Z 
2025-04-11T03:52:12.7005775Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7005896Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7005900Z 
2025-04-11T03:52:12.7005976Z device = None
2025-04-11T03:52:12.7005980Z 
2025-04-11T03:52:12.7006100Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7006250Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7006325Z     
2025-04-11T03:52:12.7006397Z         Args:
2025-04-11T03:52:12.7006562Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7006732Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7006837Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7006920Z         """
2025-04-11T03:52:12.7006997Z         _lazy_init()
2025-04-11T03:52:12.7007096Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7007199Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7007306Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7007595Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7007730Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7007893Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7007897Z 
2025-04-11T03:52:12.7008139Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7008311Z _____________ test_flash_decoding[False-True-1-False-1-16-8-32-16] _____________
2025-04-11T03:52:12.7008318Z 
2025-04-11T03:52:12.7008469Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7008636Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7008723Z use_new_kcache_layout = False
2025-04-11T03:52:12.7008726Z 
2025-04-11T03:52:12.7008929Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7009037Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7009153Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7009295Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7009414Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7009533Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7009668Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7009771Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7009917Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7010070Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7010157Z     def test_flash_decoding(
2025-04-11T03:52:12.7010234Z         bsz: int,
2025-04-11T03:52:12.7010425Z         block_size: int,
2025-04-11T03:52:12.7010519Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7010604Z         num_attn_heads: int,
2025-04-11T03:52:12.7010693Z         kv_group_num: int,
2025-04-11T03:52:12.7010778Z         same_context_len: bool,
2025-04-11T03:52:12.7010857Z         q_len: int,
2025-04-11T03:52:12.7010944Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7011032Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7011109Z     ):
2025-04-11T03:52:12.7011220Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7011417Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7011692Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7011864Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7012029Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7012185Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7012261Z     
2025-04-11T03:52:12.7012348Z         torch.manual_seed(123)
2025-04-11T03:52:12.7012442Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7012532Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7012536Z 
2025-04-11T03:52:12.7012695Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7012806Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7012810Z 
2025-04-11T03:52:12.7012886Z device = None
2025-04-11T03:52:12.7012890Z 
2025-04-11T03:52:12.7013018Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7013165Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7013239Z     
2025-04-11T03:52:12.7013314Z         Args:
2025-04-11T03:52:12.7013489Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7013655Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7013761Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7013840Z         """
2025-04-11T03:52:12.7013918Z         _lazy_init()
2025-04-11T03:52:12.7014017Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7014118Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7014222Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7014507Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7014646Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7014808Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7014813Z 
2025-04-11T03:52:12.7015052Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7015222Z _____________ test_flash_decoding[False-True-1-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.7015226Z 
2025-04-11T03:52:12.7015373Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7015538Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7015627Z use_new_kcache_layout = False
2025-04-11T03:52:12.7015630Z 
2025-04-11T03:52:12.7015831Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7015935Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7016055Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7016197Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7016312Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7016551Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7016691Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7016797Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7016936Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7017089Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7017178Z     def test_flash_decoding(
2025-04-11T03:52:12.7017255Z         bsz: int,
2025-04-11T03:52:12.7017341Z         block_size: int,
2025-04-11T03:52:12.7017431Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7017514Z         num_attn_heads: int,
2025-04-11T03:52:12.7017603Z         kv_group_num: int,
2025-04-11T03:52:12.7017786Z         same_context_len: bool,
2025-04-11T03:52:12.7017864Z         q_len: int,
2025-04-11T03:52:12.7017952Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7018041Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7018117Z     ):
2025-04-11T03:52:12.7018231Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7018427Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7018609Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7018783Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7018947Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7019104Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7019180Z     
2025-04-11T03:52:12.7019267Z         torch.manual_seed(123)
2025-04-11T03:52:12.7019366Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7019456Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7019460Z 
2025-04-11T03:52:12.7019617Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7019731Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7019735Z 
2025-04-11T03:52:12.7019810Z device = None
2025-04-11T03:52:12.7019814Z 
2025-04-11T03:52:12.7019937Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7020087Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7020162Z     
2025-04-11T03:52:12.7020235Z         Args:
2025-04-11T03:52:12.7020407Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7020570Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7020675Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7020756Z         """
2025-04-11T03:52:12.7020832Z         _lazy_init()
2025-04-11T03:52:12.7020934Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7021036Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7021150Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7021495Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7021633Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7021795Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7021799Z 
2025-04-11T03:52:12.7022038Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7022211Z ____________ test_flash_decoding[False-True-1-False-1-16-16-16-16] _____________
2025-04-11T03:52:12.7022215Z 
2025-04-11T03:52:12.7022373Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7022538Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7022626Z use_new_kcache_layout = False
2025-04-11T03:52:12.7022630Z 
2025-04-11T03:52:12.7022944Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7023049Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7023169Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7023313Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7023429Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7023547Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7023683Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7023791Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7024024Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7024173Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7024265Z     def test_flash_decoding(
2025-04-11T03:52:12.7024341Z         bsz: int,
2025-04-11T03:52:12.7024436Z         block_size: int,
2025-04-11T03:52:12.7024525Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7024607Z         num_attn_heads: int,
2025-04-11T03:52:12.7024700Z         kv_group_num: int,
2025-04-11T03:52:12.7024786Z         same_context_len: bool,
2025-04-11T03:52:12.7024865Z         q_len: int,
2025-04-11T03:52:12.7024951Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7025039Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7025117Z     ):
2025-04-11T03:52:12.7025228Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7025423Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7025606Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7025785Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7025951Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7026108Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7026183Z     
2025-04-11T03:52:12.7026270Z         torch.manual_seed(123)
2025-04-11T03:52:12.7026363Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7026454Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7026457Z 
2025-04-11T03:52:12.7026613Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7026725Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7026729Z 
2025-04-11T03:52:12.7026804Z device = None
2025-04-11T03:52:12.7026811Z 
2025-04-11T03:52:12.7026930Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7027080Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7027157Z     
2025-04-11T03:52:12.7027231Z         Args:
2025-04-11T03:52:12.7027406Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7027571Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7027676Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7027753Z         """
2025-04-11T03:52:12.7027831Z         _lazy_init()
2025-04-11T03:52:12.7027931Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7028033Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7028138Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7028468Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7028610Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7028770Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7028774Z 
2025-04-11T03:52:12.7029129Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7029304Z _____________ test_flash_decoding[False-True-1-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.7029308Z 
2025-04-11T03:52:12.7029457Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7029622Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7029712Z use_new_kcache_layout = False
2025-04-11T03:52:12.7029716Z 
2025-04-11T03:52:12.7029918Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7030023Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7030262Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7030408Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7030524Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7030643Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7030781Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7030889Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7031024Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7031174Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7031266Z     def test_flash_decoding(
2025-04-11T03:52:12.7031343Z         bsz: int,
2025-04-11T03:52:12.7031429Z         block_size: int,
2025-04-11T03:52:12.7031520Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7031602Z         num_attn_heads: int,
2025-04-11T03:52:12.7031688Z         kv_group_num: int,
2025-04-11T03:52:12.7031776Z         same_context_len: bool,
2025-04-11T03:52:12.7031855Z         q_len: int,
2025-04-11T03:52:12.7031942Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7032035Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7032106Z     ):
2025-04-11T03:52:12.7032219Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7032415Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7032596Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7032770Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7032932Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7033091Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7033165Z     
2025-04-11T03:52:12.7033254Z         torch.manual_seed(123)
2025-04-11T03:52:12.7033351Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7033446Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7033450Z 
2025-04-11T03:52:12.7033610Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7033724Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7033727Z 
2025-04-11T03:52:12.7033809Z device = None
2025-04-11T03:52:12.7033812Z 
2025-04-11T03:52:12.7033931Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7034080Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7034155Z     
2025-04-11T03:52:12.7034228Z         Args:
2025-04-11T03:52:12.7034398Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7034564Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7034672Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7034748Z         """
2025-04-11T03:52:12.7034825Z         _lazy_init()
2025-04-11T03:52:12.7034927Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7035027Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7035237Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7035523Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7035659Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7035822Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7035826Z 
2025-04-11T03:52:12.7036065Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7036241Z ____________ test_flash_decoding[False-True-1-False-1-16-16-32-16] _____________
2025-04-11T03:52:12.7036244Z 
2025-04-11T03:52:12.7036499Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7036666Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7036754Z use_new_kcache_layout = False
2025-04-11T03:52:12.7036758Z 
2025-04-11T03:52:12.7036963Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7037067Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7037185Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7037331Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7037449Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7037568Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7037706Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7037813Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7037950Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7038100Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7038195Z     def test_flash_decoding(
2025-04-11T03:52:12.7038271Z         bsz: int,
2025-04-11T03:52:12.7038359Z         block_size: int,
2025-04-11T03:52:12.7038449Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7038529Z         num_attn_heads: int,
2025-04-11T03:52:12.7038617Z         kv_group_num: int,
2025-04-11T03:52:12.7038701Z         same_context_len: bool,
2025-04-11T03:52:12.7038781Z         q_len: int,
2025-04-11T03:52:12.7038866Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7038957Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7039030Z     ):
2025-04-11T03:52:12.7039141Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7039335Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7039513Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7039689Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7039852Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7040013Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7040083Z     
2025-04-11T03:52:12.7040168Z         torch.manual_seed(123)
2025-04-11T03:52:12.7040263Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7040353Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7040357Z 
2025-04-11T03:52:12.7040514Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7040624Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7040627Z 
2025-04-11T03:52:12.7040707Z device = None
2025-04-11T03:52:12.7040711Z 
2025-04-11T03:52:12.7040828Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7040980Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7041056Z     
2025-04-11T03:52:12.7041128Z         Args:
2025-04-11T03:52:12.7041396Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7041564Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7041673Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7041747Z         """
2025-04-11T03:52:12.7041826Z         _lazy_init()
2025-04-11T03:52:12.7041929Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7042029Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7042140Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7042421Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7042738Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7042898Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7042902Z 
2025-04-11T03:52:12.7043146Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7043324Z _____________ test_flash_decoding[False-True-1-False-4-16-8-16-7] ______________
2025-04-11T03:52:12.7043328Z 
2025-04-11T03:52:12.7043481Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7043653Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7043748Z use_new_kcache_layout = False
2025-04-11T03:52:12.7043751Z 
2025-04-11T03:52:12.7043959Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7044066Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7044189Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7044336Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7044458Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7044582Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7044723Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7044836Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7044973Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7045127Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7045222Z     def test_flash_decoding(
2025-04-11T03:52:12.7045303Z         bsz: int,
2025-04-11T03:52:12.7045391Z         block_size: int,
2025-04-11T03:52:12.7045485Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7045574Z         num_attn_heads: int,
2025-04-11T03:52:12.7045661Z         kv_group_num: int,
2025-04-11T03:52:12.7045752Z         same_context_len: bool,
2025-04-11T03:52:12.7045833Z         q_len: int,
2025-04-11T03:52:12.7045922Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7046014Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7046092Z     ):
2025-04-11T03:52:12.7046209Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7046411Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7046595Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7046771Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7046935Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7047099Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7047173Z     
2025-04-11T03:52:12.7047262Z         torch.manual_seed(123)
2025-04-11T03:52:12.7047367Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7047461Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7047465Z 
2025-04-11T03:52:12.7047628Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7047845Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7047850Z 
2025-04-11T03:52:12.7047934Z device = None
2025-04-11T03:52:12.7047938Z 
2025-04-11T03:52:12.7048057Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7048206Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7048279Z     
2025-04-11T03:52:12.7048352Z         Args:
2025-04-11T03:52:12.7048522Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7048685Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7048795Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7048959Z         """
2025-04-11T03:52:12.7049038Z         _lazy_init()
2025-04-11T03:52:12.7049139Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7049241Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7049350Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7049633Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7049775Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7049933Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7049937Z 
2025-04-11T03:52:12.7050172Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7050346Z _____________ test_flash_decoding[False-True-1-False-4-16-8-16-16] _____________
2025-04-11T03:52:12.7050350Z 
2025-04-11T03:52:12.7050501Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7050668Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7050757Z use_new_kcache_layout = False
2025-04-11T03:52:12.7050761Z 
2025-04-11T03:52:12.7050966Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7051072Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7051194Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7051332Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7051449Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7051569Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7051704Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7051812Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7051950Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7052108Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7052199Z     def test_flash_decoding(
2025-04-11T03:52:12.7052275Z         bsz: int,
2025-04-11T03:52:12.7052363Z         block_size: int,
2025-04-11T03:52:12.7052454Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7052540Z         num_attn_heads: int,
2025-04-11T03:52:12.7052622Z         kv_group_num: int,
2025-04-11T03:52:12.7052708Z         same_context_len: bool,
2025-04-11T03:52:12.7052788Z         q_len: int,
2025-04-11T03:52:12.7052873Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7052968Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7053041Z     ):
2025-04-11T03:52:12.7053152Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7053347Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7053527Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7053703Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7053970Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7054137Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7054207Z     
2025-04-11T03:52:12.7054298Z         torch.manual_seed(123)
2025-04-11T03:52:12.7054394Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7054484Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7054488Z 
2025-04-11T03:52:12.7054648Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7054762Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7054766Z 
2025-04-11T03:52:12.7054846Z device = None
2025-04-11T03:52:12.7054850Z 
2025-04-11T03:52:12.7054968Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7055216Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7055287Z     
2025-04-11T03:52:12.7055361Z         Args:
2025-04-11T03:52:12.7055535Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7055698Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7055808Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7055880Z         """
2025-04-11T03:52:12.7055957Z         _lazy_init()
2025-04-11T03:52:12.7056057Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7056161Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7056273Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7056558Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7056703Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7056860Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7056864Z 
2025-04-11T03:52:12.7057111Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7057283Z _____________ test_flash_decoding[False-True-1-False-4-16-8-32-7] ______________
2025-04-11T03:52:12.7057286Z 
2025-04-11T03:52:12.7057435Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7057603Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7057692Z use_new_kcache_layout = False
2025-04-11T03:52:12.7057695Z 
2025-04-11T03:52:12.7057899Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7058002Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7058123Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7058265Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7058380Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7058497Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7058635Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7058741Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7058877Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7059028Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7059116Z     def test_flash_decoding(
2025-04-11T03:52:12.7059191Z         bsz: int,
2025-04-11T03:52:12.7059277Z         block_size: int,
2025-04-11T03:52:12.7059366Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7059452Z         num_attn_heads: int,
2025-04-11T03:52:12.7059535Z         kv_group_num: int,
2025-04-11T03:52:12.7059626Z         same_context_len: bool,
2025-04-11T03:52:12.7059708Z         q_len: int,
2025-04-11T03:52:12.7059791Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7059886Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7059956Z     ):
2025-04-11T03:52:12.7060168Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7060365Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7060545Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7060719Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7060879Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7061038Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7061110Z     
2025-04-11T03:52:12.7061198Z         torch.manual_seed(123)
2025-04-11T03:52:12.7061401Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7061491Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7061496Z 
2025-04-11T03:52:12.7061657Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7061777Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7061781Z 
2025-04-11T03:52:12.7061861Z device = None
2025-04-11T03:52:12.7061865Z 
2025-04-11T03:52:12.7061981Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7062137Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7062208Z     
2025-04-11T03:52:12.7062282Z         Args:
2025-04-11T03:52:12.7062452Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7062616Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7062727Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7062804Z         """
2025-04-11T03:52:12.7062882Z         _lazy_init()
2025-04-11T03:52:12.7062983Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7063082Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7063193Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7063474Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7063614Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7063772Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7063776Z 
2025-04-11T03:52:12.7064019Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7064184Z _____________ test_flash_decoding[False-True-1-False-4-16-8-32-16] _____________
2025-04-11T03:52:12.7064188Z 
2025-04-11T03:52:12.7064342Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7064508Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7064597Z use_new_kcache_layout = False
2025-04-11T03:52:12.7064601Z 
2025-04-11T03:52:12.7064805Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7064910Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7065031Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7065167Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7065281Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7065397Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7065533Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7065641Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7065776Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7065932Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7066018Z     def test_flash_decoding(
2025-04-11T03:52:12.7066094Z         bsz: int,
2025-04-11T03:52:12.7066285Z         block_size: int,
2025-04-11T03:52:12.7066381Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7066468Z         num_attn_heads: int,
2025-04-11T03:52:12.7066550Z         kv_group_num: int,
2025-04-11T03:52:12.7066634Z         same_context_len: bool,
2025-04-11T03:52:12.7066715Z         q_len: int,
2025-04-11T03:52:12.7066798Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7066889Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7066961Z     ):
2025-04-11T03:52:12.7067073Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7067264Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7067446Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7067728Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7067892Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7068055Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7068126Z     
2025-04-11T03:52:12.7068218Z         torch.manual_seed(123)
2025-04-11T03:52:12.7068308Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7068397Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7068401Z 
2025-04-11T03:52:12.7068594Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7068708Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7068712Z 
2025-04-11T03:52:12.7068792Z device = None
2025-04-11T03:52:12.7068796Z 
2025-04-11T03:52:12.7068912Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7069068Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7069139Z     
2025-04-11T03:52:12.7069211Z         Args:
2025-04-11T03:52:12.7069385Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7069550Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7069661Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7069734Z         """
2025-04-11T03:52:12.7069816Z         _lazy_init()
2025-04-11T03:52:12.7069912Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7070012Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7070124Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7070409Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7070554Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7070711Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7070715Z 
2025-04-11T03:52:12.7070959Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7071122Z _____________ test_flash_decoding[False-True-1-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.7071126Z 
2025-04-11T03:52:12.7071277Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7071441Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7071529Z use_new_kcache_layout = False
2025-04-11T03:52:12.7071533Z 
2025-04-11T03:52:12.7071732Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7071835Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7071954Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7072096Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7072210Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7072326Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7072573Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7072685Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7072825Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7072983Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7073074Z     def test_flash_decoding(
2025-04-11T03:52:12.7073153Z         bsz: int,
2025-04-11T03:52:12.7073245Z         block_size: int,
2025-04-11T03:52:12.7073337Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7073427Z         num_attn_heads: int,
2025-04-11T03:52:12.7073513Z         kv_group_num: int,
2025-04-11T03:52:12.7073706Z         same_context_len: bool,
2025-04-11T03:52:12.7073786Z         q_len: int,
2025-04-11T03:52:12.7073873Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7073965Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7074037Z     ):
2025-04-11T03:52:12.7074155Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7074347Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7074531Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7074705Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7074869Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7075030Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7075101Z     
2025-04-11T03:52:12.7075191Z         torch.manual_seed(123)
2025-04-11T03:52:12.7075284Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7075372Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7075376Z 
2025-04-11T03:52:12.7075534Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7075650Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7075653Z 
2025-04-11T03:52:12.7075737Z device = None
2025-04-11T03:52:12.7075741Z 
2025-04-11T03:52:12.7075857Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7076014Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7076085Z     
2025-04-11T03:52:12.7076158Z         Args:
2025-04-11T03:52:12.7076329Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7076494Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7076605Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7076681Z         """
2025-04-11T03:52:12.7076761Z         _lazy_init()
2025-04-11T03:52:12.7076858Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7076963Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7077074Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7077357Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7077497Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7077654Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7077658Z 
2025-04-11T03:52:12.7077898Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7078067Z ____________ test_flash_decoding[False-True-1-False-4-16-16-16-16] _____________
2025-04-11T03:52:12.7078071Z 
2025-04-11T03:52:12.7078228Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7078390Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7078480Z use_new_kcache_layout = False
2025-04-11T03:52:12.7078484Z 
2025-04-11T03:52:12.7078798Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7078906Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7079027Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7079166Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7079288Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7079401Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7079538Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7079645Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7079780Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7080037Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7080126Z     def test_flash_decoding(
2025-04-11T03:52:12.7080203Z         bsz: int,
2025-04-11T03:52:12.7080290Z         block_size: int,
2025-04-11T03:52:12.7080381Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7080469Z         num_attn_heads: int,
2025-04-11T03:52:12.7080553Z         kv_group_num: int,
2025-04-11T03:52:12.7080644Z         same_context_len: bool,
2025-04-11T03:52:12.7080721Z         q_len: int,
2025-04-11T03:52:12.7080807Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7080900Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7080973Z     ):
2025-04-11T03:52:12.7081087Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7081279Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7081460Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7081636Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7081797Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7081962Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7082034Z     
2025-04-11T03:52:12.7082121Z         torch.manual_seed(123)
2025-04-11T03:52:12.7082211Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7082303Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7082306Z 
2025-04-11T03:52:12.7082465Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7082576Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7082580Z 
2025-04-11T03:52:12.7082662Z device = None
2025-04-11T03:52:12.7082666Z 
2025-04-11T03:52:12.7082783Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7082937Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7083008Z     
2025-04-11T03:52:12.7083082Z         Args:
2025-04-11T03:52:12.7083256Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7083419Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7083529Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7083604Z         """
2025-04-11T03:52:12.7083685Z         _lazy_init()
2025-04-11T03:52:12.7083781Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7083881Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7083989Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7084270Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7084413Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7084569Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7084573Z 
2025-04-11T03:52:12.7084941Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7085111Z _____________ test_flash_decoding[False-True-1-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.7085114Z 
2025-04-11T03:52:12.7085267Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7085429Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7085516Z use_new_kcache_layout = False
2025-04-11T03:52:12.7085520Z 
2025-04-11T03:52:12.7085723Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7085825Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7085946Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7086186Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7086304Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7086418Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7086560Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7086669Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7086803Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7086961Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7087049Z     def test_flash_decoding(
2025-04-11T03:52:12.7087128Z         bsz: int,
2025-04-11T03:52:12.7087209Z         block_size: int,
2025-04-11T03:52:12.7087299Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7087383Z         num_attn_heads: int,
2025-04-11T03:52:12.7087468Z         kv_group_num: int,
2025-04-11T03:52:12.7087557Z         same_context_len: bool,
2025-04-11T03:52:12.7087637Z         q_len: int,
2025-04-11T03:52:12.7087724Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7087815Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7087886Z     ):
2025-04-11T03:52:12.7088006Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7088196Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7088378Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7088553Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7088718Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7088879Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7088952Z     
2025-04-11T03:52:12.7089041Z         torch.manual_seed(123)
2025-04-11T03:52:12.7089132Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7089221Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7089231Z 
2025-04-11T03:52:12.7089389Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7089502Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7089506Z 
2025-04-11T03:52:12.7089588Z device = None
2025-04-11T03:52:12.7089592Z 
2025-04-11T03:52:12.7089710Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7089862Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7089933Z     
2025-04-11T03:52:12.7090013Z         Args:
2025-04-11T03:52:12.7090182Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7090348Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7090457Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7090533Z         """
2025-04-11T03:52:12.7090614Z         _lazy_init()
2025-04-11T03:52:12.7090713Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7090816Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7091032Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7091315Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7091458Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7091616Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7091620Z 
2025-04-11T03:52:12.7091864Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7092032Z ____________ test_flash_decoding[False-True-1-False-4-16-16-32-16] _____________
2025-04-11T03:52:12.7092036Z 
2025-04-11T03:52:12.7092188Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7092476Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7092565Z use_new_kcache_layout = False
2025-04-11T03:52:12.7092570Z 
2025-04-11T03:52:12.7092774Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7092879Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7092999Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7093136Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7093257Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7093370Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7093508Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7093617Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7093752Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7093909Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7093996Z     def test_flash_decoding(
2025-04-11T03:52:12.7094078Z         bsz: int,
2025-04-11T03:52:12.7094161Z         block_size: int,
2025-04-11T03:52:12.7094257Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7094349Z         num_attn_heads: int,
2025-04-11T03:52:12.7094432Z         kv_group_num: int,
2025-04-11T03:52:12.7094519Z         same_context_len: bool,
2025-04-11T03:52:12.7094596Z         q_len: int,
2025-04-11T03:52:12.7094681Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7094775Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7094848Z     ):
2025-04-11T03:52:12.7094963Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7095152Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7095338Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7095510Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7095670Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7095833Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7095904Z     
2025-04-11T03:52:12.7095993Z         torch.manual_seed(123)
2025-04-11T03:52:12.7096081Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7096171Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7096178Z 
2025-04-11T03:52:12.7096330Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7096442Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7096446Z 
2025-04-11T03:52:12.7096527Z device = None
2025-04-11T03:52:12.7096531Z 
2025-04-11T03:52:12.7096646Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7096801Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7096872Z     
2025-04-11T03:52:12.7096950Z         Args:
2025-04-11T03:52:12.7097215Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7097383Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7097494Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7097567Z         """
2025-04-11T03:52:12.7097649Z         _lazy_init()
2025-04-11T03:52:12.7097745Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7097848Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7097958Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7098240Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7098473Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7098635Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7098639Z 
2025-04-11T03:52:12.7098886Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7099055Z ______________ test_flash_decoding[False-True-5-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.7099059Z 
2025-04-11T03:52:12.7099216Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7099380Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7099470Z use_new_kcache_layout = False
2025-04-11T03:52:12.7099480Z 
2025-04-11T03:52:12.7099682Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7099786Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7099908Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7100050Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7100169Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7100283Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7100422Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7100530Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7100667Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7100821Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7100908Z     def test_flash_decoding(
2025-04-11T03:52:12.7100988Z         bsz: int,
2025-04-11T03:52:12.7101070Z         block_size: int,
2025-04-11T03:52:12.7101161Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7101250Z         num_attn_heads: int,
2025-04-11T03:52:12.7101334Z         kv_group_num: int,
2025-04-11T03:52:12.7101420Z         same_context_len: bool,
2025-04-11T03:52:12.7101501Z         q_len: int,
2025-04-11T03:52:12.7101586Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7101678Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7101749Z     ):
2025-04-11T03:52:12.7101870Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7102061Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7102245Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7102414Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7102573Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7102734Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7102807Z     
2025-04-11T03:52:12.7102898Z         torch.manual_seed(123)
2025-04-11T03:52:12.7102991Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7103084Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7103088Z 
2025-04-11T03:52:12.7103243Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7103461Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7103465Z 
2025-04-11T03:52:12.7103551Z device = None
2025-04-11T03:52:12.7103554Z 
2025-04-11T03:52:12.7103671Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7103824Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7103896Z     
2025-04-11T03:52:12.7103973Z         Args:
2025-04-11T03:52:12.7104139Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7104301Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7104410Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7104580Z         """
2025-04-11T03:52:12.7104662Z         _lazy_init()
2025-04-11T03:52:12.7104760Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7104867Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7104975Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7105261Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7105400Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7105559Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7105562Z 
2025-04-11T03:52:12.7105809Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7105978Z _____________ test_flash_decoding[False-True-5-True-1-16-8-16-16] ______________
2025-04-11T03:52:12.7105982Z 
2025-04-11T03:52:12.7106136Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7106299Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7106391Z use_new_kcache_layout = False
2025-04-11T03:52:12.7106394Z 
2025-04-11T03:52:12.7106595Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7106698Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7106823Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7106964Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7107084Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7107197Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7107338Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7107447Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7107583Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7107743Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7107831Z     def test_flash_decoding(
2025-04-11T03:52:12.7107910Z         bsz: int,
2025-04-11T03:52:12.7107990Z         block_size: int,
2025-04-11T03:52:12.7108082Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7108170Z         num_attn_heads: int,
2025-04-11T03:52:12.7108253Z         kv_group_num: int,
2025-04-11T03:52:12.7108344Z         same_context_len: bool,
2025-04-11T03:52:12.7108460Z         q_len: int,
2025-04-11T03:52:12.7108547Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7108641Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7108712Z     ):
2025-04-11T03:52:12.7108826Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7109018Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7109203Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7109380Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7109543Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7109836Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7109911Z     
2025-04-11T03:52:12.7110003Z         torch.manual_seed(123)
2025-04-11T03:52:12.7110091Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7110186Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7110190Z 
2025-04-11T03:52:12.7110346Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7110459Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7110467Z 
2025-04-11T03:52:12.7110543Z device = None
2025-04-11T03:52:12.7110548Z 
2025-04-11T03:52:12.7110664Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7110925Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7110995Z     
2025-04-11T03:52:12.7111073Z         Args:
2025-04-11T03:52:12.7111241Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7111403Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7111513Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7111587Z         """
2025-04-11T03:52:12.7111676Z         _lazy_init()
2025-04-11T03:52:12.7111772Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7111878Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7111981Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7112265Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7112413Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7112571Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7112576Z 
2025-04-11T03:52:12.7112822Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7112985Z ______________ test_flash_decoding[False-True-5-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.7112989Z 
2025-04-11T03:52:12.7113144Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7113303Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7113396Z use_new_kcache_layout = False
2025-04-11T03:52:12.7113400Z 
2025-04-11T03:52:12.7113600Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7113705Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7113827Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7113967Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7114089Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7114201Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7114344Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7114448Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7114584Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7114740Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7114826Z     def test_flash_decoding(
2025-04-11T03:52:12.7114906Z         bsz: int,
2025-04-11T03:52:12.7114989Z         block_size: int,
2025-04-11T03:52:12.7115077Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7115166Z         num_attn_heads: int,
2025-04-11T03:52:12.7115247Z         kv_group_num: int,
2025-04-11T03:52:12.7115334Z         same_context_len: bool,
2025-04-11T03:52:12.7115413Z         q_len: int,
2025-04-11T03:52:12.7115504Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7115591Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7115665Z     ):
2025-04-11T03:52:12.7115882Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7116079Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7116267Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7116438Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7116606Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7116766Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7116840Z     
2025-04-11T03:52:12.7116931Z         torch.manual_seed(123)
2025-04-11T03:52:12.7117116Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7117210Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7117213Z 
2025-04-11T03:52:12.7117367Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7117483Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7117490Z 
2025-04-11T03:52:12.7117567Z device = None
2025-04-11T03:52:12.7117571Z 
2025-04-11T03:52:12.7117687Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7117838Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7117909Z     
2025-04-11T03:52:12.7117985Z         Args:
2025-04-11T03:52:12.7118149Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7118317Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7118424Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7118501Z         """
2025-04-11T03:52:12.7118583Z         _lazy_init()
2025-04-11T03:52:12.7118680Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7118783Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7118890Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7119174Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7119312Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7119472Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7119475Z 
2025-04-11T03:52:12.7119717Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7119881Z _____________ test_flash_decoding[False-True-5-True-1-16-8-32-16] ______________
2025-04-11T03:52:12.7119885Z 
2025-04-11T03:52:12.7120036Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7120199Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7120294Z use_new_kcache_layout = False
2025-04-11T03:52:12.7120298Z 
2025-04-11T03:52:12.7120498Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7120601Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7120721Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7120859Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7120978Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7121089Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7121226Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7121328Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7121461Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7121617Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7121704Z     def test_flash_decoding(
2025-04-11T03:52:12.7121787Z         bsz: int,
2025-04-11T03:52:12.7121882Z         block_size: int,
2025-04-11T03:52:12.7122109Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7122196Z         num_attn_heads: int,
2025-04-11T03:52:12.7122282Z         kv_group_num: int,
2025-04-11T03:52:12.7122372Z         same_context_len: bool,
2025-04-11T03:52:12.7122448Z         q_len: int,
2025-04-11T03:52:12.7122534Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7122624Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7122697Z     ):
2025-04-11T03:52:12.7122811Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7123003Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7123187Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7123457Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7123620Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7123780Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7123852Z     
2025-04-11T03:52:12.7123944Z         torch.manual_seed(123)
2025-04-11T03:52:12.7124032Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7124126Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7124130Z 
2025-04-11T03:52:12.7124283Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7124401Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7124405Z 
2025-04-11T03:52:12.7124483Z device = None
2025-04-11T03:52:12.7124486Z 
2025-04-11T03:52:12.7124604Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7124761Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7124832Z     
2025-04-11T03:52:12.7124907Z         Args:
2025-04-11T03:52:12.7125075Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7125245Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7125350Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7125423Z         """
2025-04-11T03:52:12.7125508Z         _lazy_init()
2025-04-11T03:52:12.7125606Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7125711Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7125818Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7126103Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7126241Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7126402Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7126406Z 
2025-04-11T03:52:12.7126654Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7126821Z _____________ test_flash_decoding[False-True-5-True-1-16-16-16-7] ______________
2025-04-11T03:52:12.7126824Z 
2025-04-11T03:52:12.7126979Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7127138Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7127229Z use_new_kcache_layout = False
2025-04-11T03:52:12.7127233Z 
2025-04-11T03:52:12.7127433Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7127542Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7127661Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7127806Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7127928Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7128041Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7128297Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7128402Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7128539Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7128693Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7128780Z     def test_flash_decoding(
2025-04-11T03:52:12.7128862Z         bsz: int,
2025-04-11T03:52:12.7128943Z         block_size: int,
2025-04-11T03:52:12.7129036Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7129118Z         num_attn_heads: int,
2025-04-11T03:52:12.7129200Z         kv_group_num: int,
2025-04-11T03:52:12.7129290Z         same_context_len: bool,
2025-04-11T03:52:12.7129471Z         q_len: int,
2025-04-11T03:52:12.7129564Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7129653Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7129726Z     ):
2025-04-11T03:52:12.7129841Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7130037Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7130219Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7130391Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7130558Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7130715Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7130789Z     
2025-04-11T03:52:12.7130881Z         torch.manual_seed(123)
2025-04-11T03:52:12.7130969Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7131069Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7131072Z 
2025-04-11T03:52:12.7131224Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7131345Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7131348Z 
2025-04-11T03:52:12.7131426Z device = None
2025-04-11T03:52:12.7131429Z 
2025-04-11T03:52:12.7131547Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7131705Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7131777Z     
2025-04-11T03:52:12.7131854Z         Args:
2025-04-11T03:52:12.7132019Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7132188Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7132295Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7132370Z         """
2025-04-11T03:52:12.7132452Z         _lazy_init()
2025-04-11T03:52:12.7132547Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7132653Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7132759Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7133043Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7133180Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7133335Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7133339Z 
2025-04-11T03:52:12.7133583Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7133750Z _____________ test_flash_decoding[False-True-5-True-1-16-16-16-16] _____________
2025-04-11T03:52:12.7133754Z 
2025-04-11T03:52:12.7133908Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7134075Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7134165Z use_new_kcache_layout = False
2025-04-11T03:52:12.7134168Z 
2025-04-11T03:52:12.7134477Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7134589Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7134709Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7134846Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7134968Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7135084Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7135224Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7135333Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7135474Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7135723Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7135810Z     def test_flash_decoding(
2025-04-11T03:52:12.7135891Z         bsz: int,
2025-04-11T03:52:12.7135972Z         block_size: int,
2025-04-11T03:52:12.7136070Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7136154Z         num_attn_heads: int,
2025-04-11T03:52:12.7136237Z         kv_group_num: int,
2025-04-11T03:52:12.7136331Z         same_context_len: bool,
2025-04-11T03:52:12.7136407Z         q_len: int,
2025-04-11T03:52:12.7136496Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7136584Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7136655Z     ):
2025-04-11T03:52:12.7136772Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7136963Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7137150Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7137323Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7137488Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7137647Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7137719Z     
2025-04-11T03:52:12.7137812Z         torch.manual_seed(123)
2025-04-11T03:52:12.7137902Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7137997Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7138001Z 
2025-04-11T03:52:12.7138155Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7138271Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7138275Z 
2025-04-11T03:52:12.7138353Z device = None
2025-04-11T03:52:12.7138356Z 
2025-04-11T03:52:12.7138480Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7138632Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7138705Z     
2025-04-11T03:52:12.7138784Z         Args:
2025-04-11T03:52:12.7138953Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7139121Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7139228Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7139303Z         """
2025-04-11T03:52:12.7139386Z         _lazy_init()
2025-04-11T03:52:12.7139481Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7139588Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7139695Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7139981Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7140116Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7140275Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7140283Z 
2025-04-11T03:52:12.7140631Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7140808Z _____________ test_flash_decoding[False-True-5-True-1-16-16-32-7] ______________
2025-04-11T03:52:12.7140812Z 
2025-04-11T03:52:12.7140965Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7141126Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7141219Z use_new_kcache_layout = False
2025-04-11T03:52:12.7141223Z 
2025-04-11T03:52:12.7141421Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7141526Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7141644Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7141948Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7142072Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7142186Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7142329Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7142434Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7142575Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7142724Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7142812Z     def test_flash_decoding(
2025-04-11T03:52:12.7142894Z         bsz: int,
2025-04-11T03:52:12.7142974Z         block_size: int,
2025-04-11T03:52:12.7143070Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7143152Z         num_attn_heads: int,
2025-04-11T03:52:12.7143234Z         kv_group_num: int,
2025-04-11T03:52:12.7143323Z         same_context_len: bool,
2025-04-11T03:52:12.7143403Z         q_len: int,
2025-04-11T03:52:12.7143493Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7143581Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7143653Z     ):
2025-04-11T03:52:12.7143772Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7143966Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7144148Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7144315Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7144478Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7144636Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7144709Z     
2025-04-11T03:52:12.7144796Z         torch.manual_seed(123)
2025-04-11T03:52:12.7144884Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7144981Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7144985Z 
2025-04-11T03:52:12.7145137Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7145258Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7145261Z 
2025-04-11T03:52:12.7145338Z device = None
2025-04-11T03:52:12.7145342Z 
2025-04-11T03:52:12.7145465Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7145613Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7145683Z     
2025-04-11T03:52:12.7145759Z         Args:
2025-04-11T03:52:12.7145923Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7146089Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7146195Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7146275Z         """
2025-04-11T03:52:12.7146352Z         _lazy_init()
2025-04-11T03:52:12.7146447Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7146555Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7146662Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7147051Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7147190Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7147352Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7147360Z 
2025-04-11T03:52:12.7147599Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7147768Z _____________ test_flash_decoding[False-True-5-True-1-16-16-32-16] _____________
2025-04-11T03:52:12.7147772Z 
2025-04-11T03:52:12.7147929Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7148179Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7148272Z use_new_kcache_layout = False
2025-04-11T03:52:12.7148275Z 
2025-04-11T03:52:12.7148519Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7148632Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7148750Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7148889Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7149011Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7149128Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7149268Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7149371Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7149511Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7149667Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7149752Z     def test_flash_decoding(
2025-04-11T03:52:12.7149836Z         bsz: int,
2025-04-11T03:52:12.7149918Z         block_size: int,
2025-04-11T03:52:12.7150014Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7150099Z         num_attn_heads: int,
2025-04-11T03:52:12.7150181Z         kv_group_num: int,
2025-04-11T03:52:12.7150271Z         same_context_len: bool,
2025-04-11T03:52:12.7150347Z         q_len: int,
2025-04-11T03:52:12.7150435Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7150524Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7150598Z     ):
2025-04-11T03:52:12.7150706Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7150898Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7151085Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7151258Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7151424Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7151587Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7151663Z     
2025-04-11T03:52:12.7151751Z         torch.manual_seed(123)
2025-04-11T03:52:12.7151838Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7151932Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7151935Z 
2025-04-11T03:52:12.7152090Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7152204Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7152208Z 
2025-04-11T03:52:12.7152287Z device = None
2025-04-11T03:52:12.7152290Z 
2025-04-11T03:52:12.7152414Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7152568Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7152640Z     
2025-04-11T03:52:12.7152720Z         Args:
2025-04-11T03:52:12.7152884Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7153188Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7153299Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7153375Z         """
2025-04-11T03:52:12.7153454Z         _lazy_init()
2025-04-11T03:52:12.7153550Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7153658Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7153762Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7154053Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7154190Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7154476Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7154480Z 
2025-04-11T03:52:12.7154718Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7154889Z ______________ test_flash_decoding[False-True-5-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.7154893Z 
2025-04-11T03:52:12.7155048Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7155208Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7155301Z use_new_kcache_layout = False
2025-04-11T03:52:12.7155305Z 
2025-04-11T03:52:12.7155502Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7155608Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7155725Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7155870Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7155985Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7156098Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7156242Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7156346Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7156484Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7156636Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7156723Z     def test_flash_decoding(
2025-04-11T03:52:12.7156803Z         bsz: int,
2025-04-11T03:52:12.7156884Z         block_size: int,
2025-04-11T03:52:12.7156976Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7157059Z         num_attn_heads: int,
2025-04-11T03:52:12.7157144Z         kv_group_num: int,
2025-04-11T03:52:12.7157229Z         same_context_len: bool,
2025-04-11T03:52:12.7157306Z         q_len: int,
2025-04-11T03:52:12.7157397Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7157485Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7157562Z     ):
2025-04-11T03:52:12.7157670Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7157866Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7158047Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7158214Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7158383Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7158538Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7158613Z     
2025-04-11T03:52:12.7158698Z         torch.manual_seed(123)
2025-04-11T03:52:12.7158788Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7158887Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7158891Z 
2025-04-11T03:52:12.7159044Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7159266Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7159271Z 
2025-04-11T03:52:12.7159354Z device = None
2025-04-11T03:52:12.7159358Z 
2025-04-11T03:52:12.7159481Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7159630Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7159701Z     
2025-04-11T03:52:12.7159777Z         Args:
2025-04-11T03:52:12.7159945Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7160114Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7160220Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7160394Z         """
2025-04-11T03:52:12.7160473Z         _lazy_init()
2025-04-11T03:52:12.7160571Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7160677Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7160785Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7161080Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7161217Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7161382Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7161386Z 
2025-04-11T03:52:12.7161626Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7161798Z _____________ test_flash_decoding[False-True-5-True-4-16-8-16-16] ______________
2025-04-11T03:52:12.7161807Z 
2025-04-11T03:52:12.7161960Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7162129Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7162224Z use_new_kcache_layout = False
2025-04-11T03:52:12.7162228Z 
2025-04-11T03:52:12.7162430Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7162542Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7162659Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7162802Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7162926Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7163044Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7163187Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7163293Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7163437Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7163597Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7163691Z     def test_flash_decoding(
2025-04-11T03:52:12.7163766Z         bsz: int,
2025-04-11T03:52:12.7163848Z         block_size: int,
2025-04-11T03:52:12.7163945Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7164029Z         num_attn_heads: int,
2025-04-11T03:52:12.7164116Z         kv_group_num: int,
2025-04-11T03:52:12.7164202Z         same_context_len: bool,
2025-04-11T03:52:12.7164278Z         q_len: int,
2025-04-11T03:52:12.7164366Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7164455Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7164533Z     ):
2025-04-11T03:52:12.7164645Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7164839Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7165023Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7165201Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7165373Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7165643Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7165726Z     
2025-04-11T03:52:12.7165817Z         torch.manual_seed(123)
2025-04-11T03:52:12.7165907Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7166007Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7166010Z 
2025-04-11T03:52:12.7166166Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7166283Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7166286Z 
2025-04-11T03:52:12.7166363Z device = None
2025-04-11T03:52:12.7166366Z 
2025-04-11T03:52:12.7166489Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7166742Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7166822Z     
2025-04-11T03:52:12.7166898Z         Args:
2025-04-11T03:52:12.7167064Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7167239Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7167346Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7167425Z         """
2025-04-11T03:52:12.7167504Z         _lazy_init()
2025-04-11T03:52:12.7167601Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7167711Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7167815Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7168103Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7168238Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7168401Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7168405Z 
2025-04-11T03:52:12.7168644Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7168818Z ______________ test_flash_decoding[False-True-5-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.7168822Z 
2025-04-11T03:52:12.7168971Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7169131Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7169225Z use_new_kcache_layout = False
2025-04-11T03:52:12.7169228Z 
2025-04-11T03:52:12.7169426Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7169535Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7169652Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7169799Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7169917Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7170029Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7170172Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7170279Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7170420Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7170571Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7170663Z     def test_flash_decoding(
2025-04-11T03:52:12.7170741Z         bsz: int,
2025-04-11T03:52:12.7170822Z         block_size: int,
2025-04-11T03:52:12.7170916Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7170998Z         num_attn_heads: int,
2025-04-11T03:52:12.7171086Z         kv_group_num: int,
2025-04-11T03:52:12.7171172Z         same_context_len: bool,
2025-04-11T03:52:12.7171250Z         q_len: int,
2025-04-11T03:52:12.7171339Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7171427Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7171503Z     ):
2025-04-11T03:52:12.7171613Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7171906Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7172093Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7172268Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7172440Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7172598Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7172673Z     
2025-04-11T03:52:12.7172761Z         torch.manual_seed(123)
2025-04-11T03:52:12.7172849Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7173040Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7173044Z 
2025-04-11T03:52:12.7173201Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7173317Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7173323Z 
2025-04-11T03:52:12.7173402Z device = None
2025-04-11T03:52:12.7173406Z 
2025-04-11T03:52:12.7173526Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7173673Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7173749Z     
2025-04-11T03:52:12.7173824Z         Args:
2025-04-11T03:52:12.7173990Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7174159Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7174266Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7174346Z         """
2025-04-11T03:52:12.7174428Z         _lazy_init()
2025-04-11T03:52:12.7174524Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7174630Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7174734Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7175027Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7175162Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7175324Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7175328Z 
2025-04-11T03:52:12.7175567Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7175737Z _____________ test_flash_decoding[False-True-5-True-4-16-8-32-16] ______________
2025-04-11T03:52:12.7175741Z 
2025-04-11T03:52:12.7175890Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7176054Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7176146Z use_new_kcache_layout = False
2025-04-11T03:52:12.7176150Z 
2025-04-11T03:52:12.7176353Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7176461Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7176578Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7176724Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7176842Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7176956Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7177097Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7177200Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7177345Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7177498Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7177592Z     def test_flash_decoding(
2025-04-11T03:52:12.7177668Z         bsz: int,
2025-04-11T03:52:12.7177750Z         block_size: int,
2025-04-11T03:52:12.7177971Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7178058Z         num_attn_heads: int,
2025-04-11T03:52:12.7178144Z         kv_group_num: int,
2025-04-11T03:52:12.7178229Z         same_context_len: bool,
2025-04-11T03:52:12.7178306Z         q_len: int,
2025-04-11T03:52:12.7178396Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7178484Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7178558Z     ):
2025-04-11T03:52:12.7178671Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7178868Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7179052Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7179331Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7179500Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7179663Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7179739Z     
2025-04-11T03:52:12.7179826Z         torch.manual_seed(123)
2025-04-11T03:52:12.7179919Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7180014Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7180018Z 
2025-04-11T03:52:12.7180173Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7180295Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7180298Z 
2025-04-11T03:52:12.7180375Z device = None
2025-04-11T03:52:12.7180381Z 
2025-04-11T03:52:12.7180503Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7180657Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7180735Z     
2025-04-11T03:52:12.7180808Z         Args:
2025-04-11T03:52:12.7180974Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7181148Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7181255Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7181333Z         """
2025-04-11T03:52:12.7181412Z         _lazy_init()
2025-04-11T03:52:12.7181514Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7181615Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7181720Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7182009Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7182146Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7182311Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7182315Z 
2025-04-11T03:52:12.7182553Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7182727Z _____________ test_flash_decoding[False-True-5-True-4-16-16-16-7] ______________
2025-04-11T03:52:12.7182731Z 
2025-04-11T03:52:12.7182884Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7183048Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7183138Z use_new_kcache_layout = False
2025-04-11T03:52:12.7183142Z 
2025-04-11T03:52:12.7183342Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7183449Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7183563Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7183711Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7183826Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7183946Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7184187Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7184295Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7184434Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7184585Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7184675Z     def test_flash_decoding(
2025-04-11T03:52:12.7184753Z         bsz: int,
2025-04-11T03:52:12.7184835Z         block_size: int,
2025-04-11T03:52:12.7184931Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7185014Z         num_attn_heads: int,
2025-04-11T03:52:12.7185102Z         kv_group_num: int,
2025-04-11T03:52:12.7185186Z         same_context_len: bool,
2025-04-11T03:52:12.7185352Z         q_len: int,
2025-04-11T03:52:12.7185445Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7185534Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7185611Z     ):
2025-04-11T03:52:12.7185721Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7185916Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7186099Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7186267Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7186434Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7186591Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7186668Z     
2025-04-11T03:52:12.7186754Z         torch.manual_seed(123)
2025-04-11T03:52:12.7186848Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7186943Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7186947Z 
2025-04-11T03:52:12.7187103Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7187221Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7187228Z 
2025-04-11T03:52:12.7187306Z device = None
2025-04-11T03:52:12.7187310Z 
2025-04-11T03:52:12.7187430Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7187578Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7187651Z     
2025-04-11T03:52:12.7187723Z         Args:
2025-04-11T03:52:12.7187887Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7188053Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7188159Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7188236Z         """
2025-04-11T03:52:12.7188317Z         _lazy_init()
2025-04-11T03:52:12.7188454Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7188555Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7188662Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7188954Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7189089Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7189251Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7189255Z 
2025-04-11T03:52:12.7189494Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7189664Z _____________ test_flash_decoding[False-True-5-True-4-16-16-16-16] _____________
2025-04-11T03:52:12.7189667Z 
2025-04-11T03:52:12.7189820Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7189987Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7190073Z use_new_kcache_layout = False
2025-04-11T03:52:12.7190077Z 
2025-04-11T03:52:12.7190400Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7190514Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7190635Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7190778Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7190894Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7191015Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7191152Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7191259Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7191399Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7191660Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7191754Z     def test_flash_decoding(
2025-04-11T03:52:12.7191829Z         bsz: int,
2025-04-11T03:52:12.7191910Z         block_size: int,
2025-04-11T03:52:12.7192002Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7192087Z         num_attn_heads: int,
2025-04-11T03:52:12.7192175Z         kv_group_num: int,
2025-04-11T03:52:12.7192260Z         same_context_len: bool,
2025-04-11T03:52:12.7192337Z         q_len: int,
2025-04-11T03:52:12.7192422Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7192509Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7192588Z     ):
2025-04-11T03:52:12.7192698Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7192891Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7193069Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7193241Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7193406Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7193570Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7193646Z     
2025-04-11T03:52:12.7193733Z         torch.manual_seed(123)
2025-04-11T03:52:12.7193823Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7193913Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7193916Z 
2025-04-11T03:52:12.7194069Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7194183Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7194186Z 
2025-04-11T03:52:12.7194263Z device = None
2025-04-11T03:52:12.7194267Z 
2025-04-11T03:52:12.7194390Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7194540Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7194618Z     
2025-04-11T03:52:12.7194691Z         Args:
2025-04-11T03:52:12.7194858Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7195030Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7195135Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7195212Z         """
2025-04-11T03:52:12.7195291Z         _lazy_init()
2025-04-11T03:52:12.7195391Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7195492Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7195596Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7195880Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7196016Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7196183Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7196187Z 
2025-04-11T03:52:12.7196426Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7196697Z _____________ test_flash_decoding[False-True-5-True-4-16-16-32-7] ______________
2025-04-11T03:52:12.7196703Z 
2025-04-11T03:52:12.7196853Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7197017Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7197106Z use_new_kcache_layout = False
2025-04-11T03:52:12.7197110Z 
2025-04-11T03:52:12.7197307Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7197414Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7197531Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7197776Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7197895Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7198012Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7198149Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7198256Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7198398Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7198549Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7198644Z     def test_flash_decoding(
2025-04-11T03:52:12.7198720Z         bsz: int,
2025-04-11T03:52:12.7198806Z         block_size: int,
2025-04-11T03:52:12.7198896Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7198980Z         num_attn_heads: int,
2025-04-11T03:52:12.7199068Z         kv_group_num: int,
2025-04-11T03:52:12.7199154Z         same_context_len: bool,
2025-04-11T03:52:12.7199233Z         q_len: int,
2025-04-11T03:52:12.7199323Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7199410Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7199488Z     ):
2025-04-11T03:52:12.7199597Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7199792Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7199973Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7200148Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7200312Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7200468Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7200545Z     
2025-04-11T03:52:12.7200631Z         torch.manual_seed(123)
2025-04-11T03:52:12.7200723Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7200817Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7200820Z 
2025-04-11T03:52:12.7200978Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7201088Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7201094Z 
2025-04-11T03:52:12.7201171Z device = None
2025-04-11T03:52:12.7201174Z 
2025-04-11T03:52:12.7201297Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7201447Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7201521Z     
2025-04-11T03:52:12.7201592Z         Args:
2025-04-11T03:52:12.7201760Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7201925Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7202030Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7202109Z         """
2025-04-11T03:52:12.7202192Z         _lazy_init()
2025-04-11T03:52:12.7202301Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7202401Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7202505Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7202912Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7203053Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7203216Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7203220Z 
2025-04-11T03:52:12.7203463Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7203636Z _____________ test_flash_decoding[False-True-5-True-4-16-16-32-16] _____________
2025-04-11T03:52:12.7203640Z 
2025-04-11T03:52:12.7203794Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7204057Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7204147Z use_new_kcache_layout = False
2025-04-11T03:52:12.7204151Z 
2025-04-11T03:52:12.7204348Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7204458Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7204577Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7204721Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7204838Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7204959Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7205096Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7205200Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7205340Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7205493Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7205587Z     def test_flash_decoding(
2025-04-11T03:52:12.7205662Z         bsz: int,
2025-04-11T03:52:12.7205750Z         block_size: int,
2025-04-11T03:52:12.7205839Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7205927Z         num_attn_heads: int,
2025-04-11T03:52:12.7206014Z         kv_group_num: int,
2025-04-11T03:52:12.7206099Z         same_context_len: bool,
2025-04-11T03:52:12.7206176Z         q_len: int,
2025-04-11T03:52:12.7206261Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7206351Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7206430Z     ):
2025-04-11T03:52:12.7206543Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7206737Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7206921Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7207098Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7207259Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7207418Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7207494Z     
2025-04-11T03:52:12.7207580Z         torch.manual_seed(123)
2025-04-11T03:52:12.7207675Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7207765Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7207768Z 
2025-04-11T03:52:12.7207924Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7208039Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7208042Z 
2025-04-11T03:52:12.7208121Z device = None
2025-04-11T03:52:12.7208124Z 
2025-04-11T03:52:12.7208245Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7208393Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7208474Z     
2025-04-11T03:52:12.7208547Z         Args:
2025-04-11T03:52:12.7208721Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7208991Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7209099Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7209177Z         """
2025-04-11T03:52:12.7209253Z         _lazy_init()
2025-04-11T03:52:12.7209352Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7209454Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7209559Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7209843Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7209980Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7210240Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7210244Z 
2025-04-11T03:52:12.7210482Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7210657Z _____________ test_flash_decoding[False-True-5-False-1-16-8-16-7] ______________
2025-04-11T03:52:12.7210660Z 
2025-04-11T03:52:12.7210813Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7210978Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7211066Z use_new_kcache_layout = False
2025-04-11T03:52:12.7211069Z 
2025-04-11T03:52:12.7211271Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7211377Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7211496Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7211641Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7211758Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7211876Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7212015Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7212125Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7212262Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7212413Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7212506Z     def test_flash_decoding(
2025-04-11T03:52:12.7212583Z         bsz: int,
2025-04-11T03:52:12.7212666Z         block_size: int,
2025-04-11T03:52:12.7212755Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7212837Z         num_attn_heads: int,
2025-04-11T03:52:12.7212928Z         kv_group_num: int,
2025-04-11T03:52:12.7213012Z         same_context_len: bool,
2025-04-11T03:52:12.7213093Z         q_len: int,
2025-04-11T03:52:12.7213181Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7213268Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7213345Z     ):
2025-04-11T03:52:12.7213456Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7213653Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7213833Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7214007Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7214169Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7214327Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7214404Z     
2025-04-11T03:52:12.7214491Z         torch.manual_seed(123)
2025-04-11T03:52:12.7214584Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7214681Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7214684Z 
2025-04-11T03:52:12.7214843Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7214955Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7215077Z 
2025-04-11T03:52:12.7215160Z device = None
2025-04-11T03:52:12.7215167Z 
2025-04-11T03:52:12.7215285Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7215435Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7215511Z     
2025-04-11T03:52:12.7215584Z         Args:
2025-04-11T03:52:12.7215751Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7215917Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7216024Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7216101Z         """
2025-04-11T03:52:12.7216270Z         _lazy_init()
2025-04-11T03:52:12.7216373Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7216477Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7216586Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7216876Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7217013Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7217176Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7217180Z 
2025-04-11T03:52:12.7217419Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7217592Z _____________ test_flash_decoding[False-True-5-False-1-16-8-16-16] _____________
2025-04-11T03:52:12.7217596Z 
2025-04-11T03:52:12.7217747Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7217918Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7218008Z use_new_kcache_layout = False
2025-04-11T03:52:12.7218011Z 
2025-04-11T03:52:12.7218218Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7218324Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7218442Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7218588Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7218705Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7218822Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7218960Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7219069Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7219206Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7219365Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7219458Z     def test_flash_decoding(
2025-04-11T03:52:12.7219533Z         bsz: int,
2025-04-11T03:52:12.7219619Z         block_size: int,
2025-04-11T03:52:12.7219710Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7219798Z         num_attn_heads: int,
2025-04-11T03:52:12.7219887Z         kv_group_num: int,
2025-04-11T03:52:12.7219971Z         same_context_len: bool,
2025-04-11T03:52:12.7220049Z         q_len: int,
2025-04-11T03:52:12.7220134Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7220223Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7220300Z     ):
2025-04-11T03:52:12.7220412Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7220611Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7220792Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7220971Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7221136Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7221400Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7221476Z     
2025-04-11T03:52:12.7221564Z         torch.manual_seed(123)
2025-04-11T03:52:12.7221660Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7221751Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7221755Z 
2025-04-11T03:52:12.7221912Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7222022Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7222026Z 
2025-04-11T03:52:12.7222107Z device = None
2025-04-11T03:52:12.7222110Z 
2025-04-11T03:52:12.7222229Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7222376Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7222591Z     
2025-04-11T03:52:12.7222668Z         Args:
2025-04-11T03:52:12.7222841Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7223011Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7223122Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7223195Z         """
2025-04-11T03:52:12.7223275Z         _lazy_init()
2025-04-11T03:52:12.7223379Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7223479Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7223588Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7223874Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7224011Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7224179Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7224183Z 
2025-04-11T03:52:12.7224424Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7224600Z _____________ test_flash_decoding[False-True-5-False-1-16-8-32-7] ______________
2025-04-11T03:52:12.7224604Z 
2025-04-11T03:52:12.7224754Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7224921Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7225009Z use_new_kcache_layout = False
2025-04-11T03:52:12.7225013Z 
2025-04-11T03:52:12.7225215Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7225318Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7225435Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7225580Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7225698Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7225818Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7225960Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7226069Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7226208Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7229731Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7229824Z     def test_flash_decoding(
2025-04-11T03:52:12.7229902Z         bsz: int,
2025-04-11T03:52:12.7229989Z         block_size: int,
2025-04-11T03:52:12.7230078Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7230162Z         num_attn_heads: int,
2025-04-11T03:52:12.7230247Z         kv_group_num: int,
2025-04-11T03:52:12.7230331Z         same_context_len: bool,
2025-04-11T03:52:12.7230412Z         q_len: int,
2025-04-11T03:52:12.7230502Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7230594Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7230667Z     ):
2025-04-11T03:52:12.7230777Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7231105Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7231290Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7231465Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7231627Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7231787Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7231861Z     
2025-04-11T03:52:12.7231949Z         torch.manual_seed(123)
2025-04-11T03:52:12.7232047Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7232240Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7232244Z 
2025-04-11T03:52:12.7232408Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7232520Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7232524Z 
2025-04-11T03:52:12.7232611Z device = None
2025-04-11T03:52:12.7232615Z 
2025-04-11T03:52:12.7232733Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7232882Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7232956Z     
2025-04-11T03:52:12.7233031Z         Args:
2025-04-11T03:52:12.7233200Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7233367Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7233479Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7233551Z         """
2025-04-11T03:52:12.7233632Z         _lazy_init()
2025-04-11T03:52:12.7233734Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7233838Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7233947Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7234237Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7234383Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7234541Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7234545Z 
2025-04-11T03:52:12.7234790Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7234967Z _____________ test_flash_decoding[False-True-5-False-1-16-8-32-16] _____________
2025-04-11T03:52:12.7234970Z 
2025-04-11T03:52:12.7235120Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7235291Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7235381Z use_new_kcache_layout = False
2025-04-11T03:52:12.7235385Z 
2025-04-11T03:52:12.7235595Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7235704Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7235825Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7235969Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7236086Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7236207Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7236344Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7236455Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7236589Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7236745Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7236837Z     def test_flash_decoding(
2025-04-11T03:52:12.7236911Z         bsz: int,
2025-04-11T03:52:12.7236998Z         block_size: int,
2025-04-11T03:52:12.7237087Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7237279Z         num_attn_heads: int,
2025-04-11T03:52:12.7237364Z         kv_group_num: int,
2025-04-11T03:52:12.7237450Z         same_context_len: bool,
2025-04-11T03:52:12.7237533Z         q_len: int,
2025-04-11T03:52:12.7237620Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7237717Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7237789Z     ):
2025-04-11T03:52:12.7237900Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7238100Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7238281Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7238550Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7238711Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7238878Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7238952Z     
2025-04-11T03:52:12.7239040Z         torch.manual_seed(123)
2025-04-11T03:52:12.7239139Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7239231Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7239235Z 
2025-04-11T03:52:12.7239395Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7239507Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7239511Z 
2025-04-11T03:52:12.7239593Z device = None
2025-04-11T03:52:12.7239597Z 
2025-04-11T03:52:12.7239717Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7239872Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7239948Z     
2025-04-11T03:52:12.7240022Z         Args:
2025-04-11T03:52:12.7240194Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7240363Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7240472Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7240547Z         """
2025-04-11T03:52:12.7240623Z         _lazy_init()
2025-04-11T03:52:12.7240723Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7240824Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7240934Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7241217Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7241359Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7241521Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7241524Z 
2025-04-11T03:52:12.7241761Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7241936Z _____________ test_flash_decoding[False-True-5-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.7241940Z 
2025-04-11T03:52:12.7242092Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7242261Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7242349Z use_new_kcache_layout = False
2025-04-11T03:52:12.7242352Z 
2025-04-11T03:52:12.7242554Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7242658Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7242779Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7242922Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7243040Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7243159Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7243296Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7243576Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7243716Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7243873Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7243960Z     def test_flash_decoding(
2025-04-11T03:52:12.7244038Z         bsz: int,
2025-04-11T03:52:12.7244124Z         block_size: int,
2025-04-11T03:52:12.7244213Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7244303Z         num_attn_heads: int,
2025-04-11T03:52:12.7244390Z         kv_group_num: int,
2025-04-11T03:52:12.7244478Z         same_context_len: bool,
2025-04-11T03:52:12.7244559Z         q_len: int,
2025-04-11T03:52:12.7244749Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7244843Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7244917Z     ):
2025-04-11T03:52:12.7245033Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7245235Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7245418Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7245595Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7245759Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7245921Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7245993Z     
2025-04-11T03:52:12.7246081Z         torch.manual_seed(123)
2025-04-11T03:52:12.7246176Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7246269Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7246273Z 
2025-04-11T03:52:12.7246431Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7246543Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7246547Z 
2025-04-11T03:52:12.7246633Z device = None
2025-04-11T03:52:12.7246636Z 
2025-04-11T03:52:12.7246755Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7246913Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7246985Z     
2025-04-11T03:52:12.7247061Z         Args:
2025-04-11T03:52:12.7247233Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7247399Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7247512Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7247586Z         """
2025-04-11T03:52:12.7247667Z         _lazy_init()
2025-04-11T03:52:12.7247768Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7247870Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7247979Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7248266Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7248407Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7248565Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7248569Z 
2025-04-11T03:52:12.7248818Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7248990Z ____________ test_flash_decoding[False-True-5-False-1-16-16-16-16] _____________
2025-04-11T03:52:12.7248994Z 
2025-04-11T03:52:12.7249146Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7249318Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7249408Z use_new_kcache_layout = False
2025-04-11T03:52:12.7249411Z 
2025-04-11T03:52:12.7249718Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7249826Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7249953Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7250094Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7250212Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7250332Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7250469Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7250579Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7250714Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7250874Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7251087Z     def test_flash_decoding(
2025-04-11T03:52:12.7251165Z         bsz: int,
2025-04-11T03:52:12.7251253Z         block_size: int,
2025-04-11T03:52:12.7251346Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7251442Z         num_attn_heads: int,
2025-04-11T03:52:12.7251526Z         kv_group_num: int,
2025-04-11T03:52:12.7251614Z         same_context_len: bool,
2025-04-11T03:52:12.7251698Z         q_len: int,
2025-04-11T03:52:12.7251783Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7251875Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7251947Z     ):
2025-04-11T03:52:12.7252066Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7252256Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7252438Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7252615Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7252776Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7252940Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7253013Z     
2025-04-11T03:52:12.7253107Z         torch.manual_seed(123)
2025-04-11T03:52:12.7253199Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7253290Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7253294Z 
2025-04-11T03:52:12.7253453Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7253564Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7253568Z 
2025-04-11T03:52:12.7253650Z device = None
2025-04-11T03:52:12.7253654Z 
2025-04-11T03:52:12.7253771Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7253928Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7254004Z     
2025-04-11T03:52:12.7254077Z         Args:
2025-04-11T03:52:12.7254248Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7254415Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7254525Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7254599Z         """
2025-04-11T03:52:12.7254684Z         _lazy_init()
2025-04-11T03:52:12.7254780Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7254881Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7254993Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7255273Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7255413Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7255572Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7255576Z 
2025-04-11T03:52:12.7255819Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7256094Z _____________ test_flash_decoding[False-True-5-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.7256098Z 
2025-04-11T03:52:12.7256253Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7256420Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7256509Z use_new_kcache_layout = False
2025-04-11T03:52:12.7256513Z 
2025-04-11T03:52:12.7256716Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7256821Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7256942Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7257081Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7257289Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7257408Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7257555Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7257670Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7257806Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7257964Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7258055Z     def test_flash_decoding(
2025-04-11T03:52:12.7258133Z         bsz: int,
2025-04-11T03:52:12.7258227Z         block_size: int,
2025-04-11T03:52:12.7258320Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7258410Z         num_attn_heads: int,
2025-04-11T03:52:12.7258493Z         kv_group_num: int,
2025-04-11T03:52:12.7258579Z         same_context_len: bool,
2025-04-11T03:52:12.7258663Z         q_len: int,
2025-04-11T03:52:12.7258756Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7258851Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7258924Z     ):
2025-04-11T03:52:12.7259041Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7259235Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7259418Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7259598Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7259759Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7259921Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7259995Z     
2025-04-11T03:52:12.7260087Z         torch.manual_seed(123)
2025-04-11T03:52:12.7260178Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7260270Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7260279Z 
2025-04-11T03:52:12.7260441Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7260555Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7260559Z 
2025-04-11T03:52:12.7260647Z device = None
2025-04-11T03:52:12.7260651Z 
2025-04-11T03:52:12.7260769Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7260922Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7260995Z     
2025-04-11T03:52:12.7261070Z         Args:
2025-04-11T03:52:12.7261244Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7261409Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7261519Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7261593Z         """
2025-04-11T03:52:12.7261678Z         _lazy_init()
2025-04-11T03:52:12.7261778Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7261882Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7261997Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7262388Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7262534Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7262693Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7262697Z 
2025-04-11T03:52:12.7262942Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7263111Z ____________ test_flash_decoding[False-True-5-False-1-16-16-32-16] _____________
2025-04-11T03:52:12.7263115Z 
2025-04-11T03:52:12.7263269Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7263537Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7263630Z use_new_kcache_layout = False
2025-04-11T03:52:12.7263634Z 
2025-04-11T03:52:12.7263843Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7263952Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7264076Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7264215Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7264342Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7264457Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7264596Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7264710Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7264849Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7265005Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7265098Z     def test_flash_decoding(
2025-04-11T03:52:12.7265178Z         bsz: int,
2025-04-11T03:52:12.7265268Z         block_size: int,
2025-04-11T03:52:12.7265358Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7265457Z         num_attn_heads: int,
2025-04-11T03:52:12.7265545Z         kv_group_num: int,
2025-04-11T03:52:12.7265639Z         same_context_len: bool,
2025-04-11T03:52:12.7265717Z         q_len: int,
2025-04-11T03:52:12.7265804Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7265902Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7265976Z     ):
2025-04-11T03:52:12.7266095Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7266288Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7266471Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7266654Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7266817Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7266984Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7267057Z     
2025-04-11T03:52:12.7267148Z         torch.manual_seed(123)
2025-04-11T03:52:12.7267239Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7267331Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7267334Z 
2025-04-11T03:52:12.7267497Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7267612Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7267615Z 
2025-04-11T03:52:12.7267701Z device = None
2025-04-11T03:52:12.7267704Z 
2025-04-11T03:52:12.7267822Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7267982Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7268058Z     
2025-04-11T03:52:12.7268133Z         Args:
2025-04-11T03:52:12.7268305Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7268620Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7268740Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7268817Z         """
2025-04-11T03:52:12.7268907Z         _lazy_init()
2025-04-11T03:52:12.7269006Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7269113Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7269232Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7269515Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7269664Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7269936Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7269940Z 
2025-04-11T03:52:12.7270188Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7270361Z _____________ test_flash_decoding[False-True-5-False-4-16-8-16-7] ______________
2025-04-11T03:52:12.7270366Z 
2025-04-11T03:52:12.7270528Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7270694Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7270785Z use_new_kcache_layout = False
2025-04-11T03:52:12.7270789Z 
2025-04-11T03:52:12.7270996Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7271101Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7271230Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7271370Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7271495Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7271609Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7271746Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7271860Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7271997Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7272154Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7272245Z     def test_flash_decoding(
2025-04-11T03:52:12.7272322Z         bsz: int,
2025-04-11T03:52:12.7272412Z         block_size: int,
2025-04-11T03:52:12.7272503Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7272593Z         num_attn_heads: int,
2025-04-11T03:52:12.7272678Z         kv_group_num: int,
2025-04-11T03:52:12.7272767Z         same_context_len: bool,
2025-04-11T03:52:12.7323207Z         q_len: int,
2025-04-11T03:52:12.7323400Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7323504Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7323600Z     ):
2025-04-11T03:52:12.7323736Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7323993Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7324197Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7324394Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7324587Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7324768Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7324859Z     
2025-04-11T03:52:12.7324962Z         torch.manual_seed(123)
2025-04-11T03:52:12.7325071Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7325174Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7325187Z 
2025-04-11T03:52:12.7325364Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7325500Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7325505Z 
2025-04-11T03:52:12.7325805Z device = None
2025-04-11T03:52:12.7325811Z 
2025-04-11T03:52:12.7325968Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7326148Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7326237Z     
2025-04-11T03:52:12.7326320Z         Args:
2025-04-11T03:52:12.7326517Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7326711Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7326827Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7326921Z         """
2025-04-11T03:52:12.7327011Z         _lazy_init()
2025-04-11T03:52:12.7327278Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7327396Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7327513Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7327833Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7327978Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7328156Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7328161Z 
2025-04-11T03:52:12.7328418Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7328608Z _____________ test_flash_decoding[False-True-5-False-4-16-8-16-16] _____________
2025-04-11T03:52:12.7328612Z 
2025-04-11T03:52:12.7328774Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7328949Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7329058Z use_new_kcache_layout = False
2025-04-11T03:52:12.7329062Z 
2025-04-11T03:52:12.7329270Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7329400Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7329528Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7329686Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7329811Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7329933Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7330085Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7330197Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7330347Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7330504Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7330613Z     def test_flash_decoding(
2025-04-11T03:52:12.7330695Z         bsz: int,
2025-04-11T03:52:12.7330786Z         block_size: int,
2025-04-11T03:52:12.7330895Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7330991Z         num_attn_heads: int,
2025-04-11T03:52:12.7331089Z         kv_group_num: int,
2025-04-11T03:52:12.7331183Z         same_context_len: bool,
2025-04-11T03:52:12.7331268Z         q_len: int,
2025-04-11T03:52:12.7331370Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7331467Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7331552Z     ):
2025-04-11T03:52:12.7331676Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7331893Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7332087Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7332268Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7332450Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7332731Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7332819Z     
2025-04-11T03:52:12.7332914Z         torch.manual_seed(123)
2025-04-11T03:52:12.7333025Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7333127Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7333131Z 
2025-04-11T03:52:12.7333297Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7333428Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7333431Z 
2025-04-11T03:52:12.7333518Z device = None
2025-04-11T03:52:12.7333522Z 
2025-04-11T03:52:12.7333660Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7333817Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7334009Z     
2025-04-11T03:52:12.7334093Z         Args:
2025-04-11T03:52:12.7334274Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7334467Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7334586Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7334679Z         """
2025-04-11T03:52:12.7334767Z         _lazy_init()
2025-04-11T03:52:12.7334872Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7334995Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7335111Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7335419Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7335564Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7335741Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7335745Z 
2025-04-11T03:52:12.7335993Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7336180Z _____________ test_flash_decoding[False-True-5-False-4-16-8-32-7] ______________
2025-04-11T03:52:12.7336184Z 
2025-04-11T03:52:12.7336341Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7336512Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7336617Z use_new_kcache_layout = False
2025-04-11T03:52:12.7336621Z 
2025-04-11T03:52:12.7336826Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7336944Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7337068Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7337223Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7337349Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7337469Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7337621Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7337738Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7337891Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7338048Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7338151Z     def test_flash_decoding(
2025-04-11T03:52:12.7338235Z         bsz: int,
2025-04-11T03:52:12.7338323Z         block_size: int,
2025-04-11T03:52:12.7338431Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7338520Z         num_attn_heads: int,
2025-04-11T03:52:12.7338618Z         kv_group_num: int,
2025-04-11T03:52:12.7338710Z         same_context_len: bool,
2025-04-11T03:52:12.7338792Z         q_len: int,
2025-04-11T03:52:12.7338895Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7338988Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7339070Z     ):
2025-04-11T03:52:12.7339187Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7339502Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7339694Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7339869Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7340046Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7340209Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7340294Z     
2025-04-11T03:52:12.7340385Z         torch.manual_seed(123)
2025-04-11T03:52:12.7340488Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7340585Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7340753Z 
2025-04-11T03:52:12.7340918Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7341045Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7341049Z 
2025-04-11T03:52:12.7341137Z device = None
2025-04-11T03:52:12.7341141Z 
2025-04-11T03:52:12.7341278Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7341435Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7341522Z     
2025-04-11T03:52:12.7341605Z         Args:
2025-04-11T03:52:12.7341777Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7341958Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7342072Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7342160Z         """
2025-04-11T03:52:12.7342243Z         _lazy_init()
2025-04-11T03:52:12.7342359Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7342468Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7342584Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7342891Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7343038Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7343208Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7343212Z 
2025-04-11T03:52:12.7343460Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7343646Z _____________ test_flash_decoding[False-True-5-False-4-16-8-32-16] _____________
2025-04-11T03:52:12.7343650Z 
2025-04-11T03:52:12.7343809Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7343993Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7344087Z use_new_kcache_layout = False
2025-04-11T03:52:12.7344091Z 
2025-04-11T03:52:12.7344296Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7344422Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7344550Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7344707Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7344830Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7344955Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7345098Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7345211Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7345363Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7345520Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7345624Z     def test_flash_decoding(
2025-04-11T03:52:12.7345704Z         bsz: int,
2025-04-11T03:52:12.7345790Z         block_size: int,
2025-04-11T03:52:12.7345896Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7346084Z         num_attn_heads: int,
2025-04-11T03:52:12.7346186Z         kv_group_num: int,
2025-04-11T03:52:12.7346275Z         same_context_len: bool,
2025-04-11T03:52:12.7346369Z         q_len: int,
2025-04-11T03:52:12.7346459Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7346554Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7346639Z     ):
2025-04-11T03:52:12.7346755Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7346968Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7347159Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7347337Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7347645Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7347806Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7347899Z     
2025-04-11T03:52:12.7347991Z         torch.manual_seed(123)
2025-04-11T03:52:12.7348095Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7348189Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7348193Z 
2025-04-11T03:52:12.7348355Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7348529Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7348533Z 
2025-04-11T03:52:12.7348618Z device = None
2025-04-11T03:52:12.7348622Z 
2025-04-11T03:52:12.7348755Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7348914Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7349002Z     
2025-04-11T03:52:12.7349083Z         Args:
2025-04-11T03:52:12.7349254Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7349435Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7349545Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7349632Z         """
2025-04-11T03:52:12.7349715Z         _lazy_init()
2025-04-11T03:52:12.7349824Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7349932Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7350042Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7350340Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7350482Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7350657Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7350661Z 
2025-04-11T03:52:12.7350907Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7351097Z _____________ test_flash_decoding[False-True-5-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.7351101Z 
2025-04-11T03:52:12.7351257Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7351432Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7351527Z use_new_kcache_layout = False
2025-04-11T03:52:12.7351530Z 
2025-04-11T03:52:12.7351739Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7351856Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7351979Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7352126Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7352249Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7352375Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7352514Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7352744Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7352898Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7353053Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7353153Z     def test_flash_decoding(
2025-04-11T03:52:12.7353235Z         bsz: int,
2025-04-11T03:52:12.7353322Z         block_size: int,
2025-04-11T03:52:12.7353426Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7353514Z         num_attn_heads: int,
2025-04-11T03:52:12.7353611Z         kv_group_num: int,
2025-04-11T03:52:12.7353702Z         same_context_len: bool,
2025-04-11T03:52:12.7353793Z         q_len: int,
2025-04-11T03:52:12.7353991Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7354089Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7354174Z     ):
2025-04-11T03:52:12.7354290Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7354498Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7354686Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7354869Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7355038Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7355204Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7355291Z     
2025-04-11T03:52:12.7355386Z         torch.manual_seed(123)
2025-04-11T03:52:12.7355492Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7355588Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7355595Z 
2025-04-11T03:52:12.7355756Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7355886Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7355890Z 
2025-04-11T03:52:12.7355976Z device = None
2025-04-11T03:52:12.7355980Z 
2025-04-11T03:52:12.7356113Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7356269Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7356356Z     
2025-04-11T03:52:12.7356438Z         Args:
2025-04-11T03:52:12.7356619Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7356789Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7356900Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7356992Z         """
2025-04-11T03:52:12.7357076Z         _lazy_init()
2025-04-11T03:52:12.7357194Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7357304Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7357415Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7357722Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7357863Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7358034Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7358038Z 
2025-04-11T03:52:12.7358278Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7358460Z ____________ test_flash_decoding[False-True-5-False-4-16-16-16-16] _____________
2025-04-11T03:52:12.7358464Z 
2025-04-11T03:52:12.7358622Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7358795Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7358895Z use_new_kcache_layout = False
2025-04-11T03:52:12.7358899Z 
2025-04-11T03:52:12.7359103Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7359326Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7359453Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7359606Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7359725Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7359852Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7359996Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7360106Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7360255Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7360409Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7360605Z     def test_flash_decoding(
2025-04-11T03:52:12.7360689Z         bsz: int,
2025-04-11T03:52:12.7360785Z         block_size: int,
2025-04-11T03:52:12.7360884Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7360976Z         num_attn_heads: int,
2025-04-11T03:52:12.7361077Z         kv_group_num: int,
2025-04-11T03:52:12.7361170Z         same_context_len: bool,
2025-04-11T03:52:12.7361260Z         q_len: int,
2025-04-11T03:52:12.7361349Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7361443Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7361531Z     ):
2025-04-11T03:52:12.7361646Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7361854Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7362039Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7362225Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7362396Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7362558Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7362646Z     
2025-04-11T03:52:12.7362738Z         torch.manual_seed(123)
2025-04-11T03:52:12.7362843Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7362941Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7362944Z 
2025-04-11T03:52:12.7363110Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7363228Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7363232Z 
2025-04-11T03:52:12.7363313Z device = None
2025-04-11T03:52:12.7363318Z 
2025-04-11T03:52:12.7363446Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7363599Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7363687Z     
2025-04-11T03:52:12.7363766Z         Args:
2025-04-11T03:52:12.7363942Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7364110Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7364218Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7364306Z         """
2025-04-11T03:52:12.7364390Z         _lazy_init()
2025-04-11T03:52:12.7364498Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7364606Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7364716Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7365006Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7365145Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7365314Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7365321Z 
2025-04-11T03:52:12.7365564Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7365841Z _____________ test_flash_decoding[False-True-5-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.7365846Z 
2025-04-11T03:52:12.7366001Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7366175Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7366274Z use_new_kcache_layout = False
2025-04-11T03:52:12.7366278Z 
2025-04-11T03:52:12.7366493Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7366606Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7366730Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7366885Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7367101Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7367228Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7367371Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7367485Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7367636Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7367793Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7367896Z     def test_flash_decoding(
2025-04-11T03:52:12.7367978Z         bsz: int,
2025-04-11T03:52:12.7368075Z         block_size: int,
2025-04-11T03:52:12.7368170Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7368257Z         num_attn_heads: int,
2025-04-11T03:52:12.7368353Z         kv_group_num: int,
2025-04-11T03:52:12.7368443Z         same_context_len: bool,
2025-04-11T03:52:12.7368533Z         q_len: int,
2025-04-11T03:52:12.7368623Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7368718Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7368805Z     ):
2025-04-11T03:52:12.7368920Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7369132Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7369322Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7369508Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7369677Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7369841Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7369928Z     
2025-04-11T03:52:12.7370019Z         torch.manual_seed(123)
2025-04-11T03:52:12.7370123Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7370220Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7370228Z 
2025-04-11T03:52:12.7370395Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7370512Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7370516Z 
2025-04-11T03:52:12.7370604Z device = None
2025-04-11T03:52:12.7370616Z 
2025-04-11T03:52:12.7370739Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7370894Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7370978Z     
2025-04-11T03:52:12.7371056Z         Args:
2025-04-11T03:52:12.7371237Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7371406Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7371517Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7371602Z         """
2025-04-11T03:52:12.7371685Z         _lazy_init()
2025-04-11T03:52:12.7371797Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7371903Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7372021Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7372437Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7372583Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7372753Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7372757Z 
2025-04-11T03:52:12.7373003Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7373179Z ____________ test_flash_decoding[False-True-5-False-4-16-16-32-16] _____________
2025-04-11T03:52:12.7373183Z 
2025-04-11T03:52:12.7373336Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7373503Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7373705Z use_new_kcache_layout = False
2025-04-11T03:52:12.7373709Z 
2025-04-11T03:52:12.7373912Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7374023Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7374143Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7374285Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7374401Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7374522Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7374657Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7374764Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7374901Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7375051Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7375145Z     def test_flash_decoding(
2025-04-11T03:52:12.7375222Z         bsz: int,
2025-04-11T03:52:12.7375312Z         block_size: int,
2025-04-11T03:52:12.7375403Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7375497Z         num_attn_heads: int,
2025-04-11T03:52:12.7375582Z         kv_group_num: int,
2025-04-11T03:52:12.7375666Z         same_context_len: bool,
2025-04-11T03:52:12.7375747Z         q_len: int,
2025-04-11T03:52:12.7375831Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7375921Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7375993Z     ):
2025-04-11T03:52:12.7376103Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7376300Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7376478Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7376650Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7376818Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7376980Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7377056Z     
2025-04-11T03:52:12.7377141Z         torch.manual_seed(123)
2025-04-11T03:52:12.7377236Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7377325Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7377329Z 
2025-04-11T03:52:12.7377488Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7377604Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7377608Z 
2025-04-11T03:52:12.7377689Z device = None
2025-04-11T03:52:12.7377693Z 
2025-04-11T03:52:12.7377809Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7377961Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7378038Z     
2025-04-11T03:52:12.7378114Z         Args:
2025-04-11T03:52:12.7378286Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7378559Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7378672Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7378747Z         """
2025-04-11T03:52:12.7378826Z         _lazy_init()
2025-04-11T03:52:12.7378929Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7379032Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7379142Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7379428Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7379569Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7379723Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7379837Z 
2025-04-11T03:52:12.7380075Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7380250Z _____________ test_flash_decoding[False-False-1-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.7380254Z 
2025-04-11T03:52:12.7380406Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7380573Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7380661Z use_new_kcache_layout = False
2025-04-11T03:52:12.7380665Z 
2025-04-11T03:52:12.7380866Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7380970Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7381092Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7381230Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7381349Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7381467Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7381601Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7381713Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7381849Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7381999Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7382092Z     def test_flash_decoding(
2025-04-11T03:52:12.7382168Z         bsz: int,
2025-04-11T03:52:12.7382257Z         block_size: int,
2025-04-11T03:52:12.7382347Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7382434Z         num_attn_heads: int,
2025-04-11T03:52:12.7382518Z         kv_group_num: int,
2025-04-11T03:52:12.7382602Z         same_context_len: bool,
2025-04-11T03:52:12.7382684Z         q_len: int,
2025-04-11T03:52:12.7382767Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7382864Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7382936Z     ):
2025-04-11T03:52:12.7383048Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7383245Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7383423Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7383596Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7383757Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7383915Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7383985Z     
2025-04-11T03:52:12.7384072Z         torch.manual_seed(123)
2025-04-11T03:52:12.7384167Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7384257Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7384264Z 
2025-04-11T03:52:12.7384422Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7384534Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7384538Z 
2025-04-11T03:52:12.7384619Z device = None
2025-04-11T03:52:12.7384736Z 
2025-04-11T03:52:12.7384856Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7385012Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7385082Z     
2025-04-11T03:52:12.7385158Z         Args:
2025-04-11T03:52:12.7385329Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7385494Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7385608Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7385681Z         """
2025-04-11T03:52:12.7385758Z         _lazy_init()
2025-04-11T03:52:12.7385956Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7386063Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7386173Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7386461Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7386603Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7386759Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7386763Z 
2025-04-11T03:52:12.7387002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7387175Z _____________ test_flash_decoding[False-False-1-True-1-16-8-16-16] _____________
2025-04-11T03:52:12.7387179Z 
2025-04-11T03:52:12.7387331Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7387497Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7387590Z use_new_kcache_layout = False
2025-04-11T03:52:12.7387594Z 
2025-04-11T03:52:12.7387796Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7387903Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7388025Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7388163Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7388278Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7388396Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7388576Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7388688Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7388824Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7388977Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7389070Z     def test_flash_decoding(
2025-04-11T03:52:12.7389146Z         bsz: int,
2025-04-11T03:52:12.7389231Z         block_size: int,
2025-04-11T03:52:12.7389322Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7389408Z         num_attn_heads: int,
2025-04-11T03:52:12.7389495Z         kv_group_num: int,
2025-04-11T03:52:12.7389581Z         same_context_len: bool,
2025-04-11T03:52:12.7389662Z         q_len: int,
2025-04-11T03:52:12.7389748Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7389842Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7389913Z     ):
2025-04-11T03:52:12.7390025Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7390219Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7390401Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7390575Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7390741Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7390901Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7391090Z     
2025-04-11T03:52:12.7391183Z         torch.manual_seed(123)
2025-04-11T03:52:12.7391277Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7391370Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7391374Z 
2025-04-11T03:52:12.7391534Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7391647Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7391651Z 
2025-04-11T03:52:12.7391733Z device = None
2025-04-11T03:52:12.7391737Z 
2025-04-11T03:52:12.7391856Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7392011Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7392207Z     
2025-04-11T03:52:12.7392284Z         Args:
2025-04-11T03:52:12.7392457Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7392628Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7392741Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7392815Z         """
2025-04-11T03:52:12.7392894Z         _lazy_init()
2025-04-11T03:52:12.7392996Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7393099Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7393211Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7393496Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7393638Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7393797Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7393804Z 
2025-04-11T03:52:12.7394052Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7394221Z _____________ test_flash_decoding[False-False-1-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.7394226Z 
2025-04-11T03:52:12.7394376Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7394546Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7394635Z use_new_kcache_layout = False
2025-04-11T03:52:12.7394639Z 
2025-04-11T03:52:12.7394844Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7394948Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7395071Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7395209Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7395331Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7395450Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7395584Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7395697Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7395834Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7395989Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7396077Z     def test_flash_decoding(
2025-04-11T03:52:12.7396153Z         bsz: int,
2025-04-11T03:52:12.7396241Z         block_size: int,
2025-04-11T03:52:12.7396332Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7396419Z         num_attn_heads: int,
2025-04-11T03:52:12.7396504Z         kv_group_num: int,
2025-04-11T03:52:12.7396589Z         same_context_len: bool,
2025-04-11T03:52:12.7396670Z         q_len: int,
2025-04-11T03:52:12.7396755Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7396848Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7396919Z     ):
2025-04-11T03:52:12.7397031Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7397351Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7397539Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7397720Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7397882Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7398045Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7398118Z     
2025-04-11T03:52:12.7398210Z         torch.manual_seed(123)
2025-04-11T03:52:12.7398302Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7398395Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7398490Z 
2025-04-11T03:52:12.7398650Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7398762Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7398766Z 
2025-04-11T03:52:12.7398849Z device = None
2025-04-11T03:52:12.7398856Z 
2025-04-11T03:52:12.7398973Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7399127Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7399199Z     
2025-04-11T03:52:12.7399273Z         Args:
2025-04-11T03:52:12.7399444Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7399610Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7399721Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7399796Z         """
2025-04-11T03:52:12.7399874Z         _lazy_init()
2025-04-11T03:52:12.7399977Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7400078Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7400189Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7400475Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7400615Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7400772Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7400776Z 
2025-04-11T03:52:12.7401020Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7401189Z _____________ test_flash_decoding[False-False-1-True-1-16-8-32-16] _____________
2025-04-11T03:52:12.7401193Z 
2025-04-11T03:52:12.7401343Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7401508Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7401600Z use_new_kcache_layout = False
2025-04-11T03:52:12.7401604Z 
2025-04-11T03:52:12.7401809Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7401917Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7402042Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7402182Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7402298Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7402416Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7402554Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7402664Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7402801Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7402958Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7403051Z     def test_flash_decoding(
2025-04-11T03:52:12.7403127Z         bsz: int,
2025-04-11T03:52:12.7403213Z         block_size: int,
2025-04-11T03:52:12.7403304Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7403392Z         num_attn_heads: int,
2025-04-11T03:52:12.7403585Z         kv_group_num: int,
2025-04-11T03:52:12.7403677Z         same_context_len: bool,
2025-04-11T03:52:12.7403759Z         q_len: int,
2025-04-11T03:52:12.7403845Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7403942Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7404013Z     ):
2025-04-11T03:52:12.7404128Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7404321Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7404504Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7404680Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7404950Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7405113Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7405188Z     
2025-04-11T03:52:12.7405280Z         torch.manual_seed(123)
2025-04-11T03:52:12.7405371Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7405461Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7405465Z 
2025-04-11T03:52:12.7405626Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7405740Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7405743Z 
2025-04-11T03:52:12.7405826Z device = None
2025-04-11T03:52:12.7405829Z 
2025-04-11T03:52:12.7405944Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7406099Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7406172Z     
2025-04-11T03:52:12.7406247Z         Args:
2025-04-11T03:52:12.7406417Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7406586Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7406695Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7406768Z         """
2025-04-11T03:52:12.7406854Z         _lazy_init()
2025-04-11T03:52:12.7406950Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7407053Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7407164Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7407446Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7407586Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7407741Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7407748Z 
2025-04-11T03:52:12.7407990Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7408160Z _____________ test_flash_decoding[False-False-1-True-1-16-16-16-7] _____________
2025-04-11T03:52:12.7408164Z 
2025-04-11T03:52:12.7408314Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7408481Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7408569Z use_new_kcache_layout = False
2025-04-11T03:52:12.7408573Z 
2025-04-11T03:52:12.7408778Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7408882Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7409003Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7409142Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7409267Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7409382Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7409518Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7409733Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7409875Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7410032Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7410121Z     def test_flash_decoding(
2025-04-11T03:52:12.7410200Z         bsz: int,
2025-04-11T03:52:12.7410288Z         block_size: int,
2025-04-11T03:52:12.7410379Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7410464Z         num_attn_heads: int,
2025-04-11T03:52:12.7410548Z         kv_group_num: int,
2025-04-11T03:52:12.7410639Z         same_context_len: bool,
2025-04-11T03:52:12.7410716Z         q_len: int,
2025-04-11T03:52:12.7410804Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7410994Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7411066Z     ):
2025-04-11T03:52:12.7411183Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7411377Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7411559Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7411735Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7411901Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7412063Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7412135Z     
2025-04-11T03:52:12.7412228Z         torch.manual_seed(123)
2025-04-11T03:52:12.7412320Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7412411Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7412420Z 
2025-04-11T03:52:12.7412582Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7412695Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7412699Z 
2025-04-11T03:52:12.7412781Z device = None
2025-04-11T03:52:12.7412790Z 
2025-04-11T03:52:12.7412909Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7413062Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7413133Z     
2025-04-11T03:52:12.7413207Z         Args:
2025-04-11T03:52:12.7413382Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7413548Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7413659Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7413732Z         """
2025-04-11T03:52:12.7413814Z         _lazy_init()
2025-04-11T03:52:12.7413913Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7414017Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7414128Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7414415Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7414558Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7414711Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7414715Z 
2025-04-11T03:52:12.7414956Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7415128Z ____________ test_flash_decoding[False-False-1-True-1-16-16-16-16] _____________
2025-04-11T03:52:12.7415131Z 
2025-04-11T03:52:12.7415291Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7415454Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7415546Z use_new_kcache_layout = False
2025-04-11T03:52:12.7415550Z 
2025-04-11T03:52:12.7415756Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7415960Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7416085Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7416224Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7416347Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7416462Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7416598Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7416709Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7416844Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7416998Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7417192Z     def test_flash_decoding(
2025-04-11T03:52:12.7417271Z         bsz: int,
2025-04-11T03:52:12.7417355Z         block_size: int,
2025-04-11T03:52:12.7417446Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7417536Z         num_attn_heads: int,
2025-04-11T03:52:12.7417625Z         kv_group_num: int,
2025-04-11T03:52:12.7417716Z         same_context_len: bool,
2025-04-11T03:52:12.7417793Z         q_len: int,
2025-04-11T03:52:12.7417879Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7417973Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7418044Z     ):
2025-04-11T03:52:12.7418161Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7418353Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7418534Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7418709Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7418876Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7419039Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7419115Z     
2025-04-11T03:52:12.7419214Z         torch.manual_seed(123)
2025-04-11T03:52:12.7419305Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7419398Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7419402Z 
2025-04-11T03:52:12.7419566Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7419680Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7419684Z 
2025-04-11T03:52:12.7419769Z device = None
2025-04-11T03:52:12.7419773Z 
2025-04-11T03:52:12.7419891Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7420043Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7420118Z     
2025-04-11T03:52:12.7420196Z         Args:
2025-04-11T03:52:12.7420361Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7420527Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7420638Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7420711Z         """
2025-04-11T03:52:12.7420794Z         _lazy_init()
2025-04-11T03:52:12.7420889Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7420996Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7421106Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7421387Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7421527Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7421683Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7421689Z 
2025-04-11T03:52:12.7421932Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7422204Z _____________ test_flash_decoding[False-False-1-True-1-16-16-32-7] _____________
2025-04-11T03:52:12.7422209Z 
2025-04-11T03:52:12.7422366Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7422529Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7422622Z use_new_kcache_layout = False
2025-04-11T03:52:12.7422626Z 
2025-04-11T03:52:12.7422834Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7422939Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7423061Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7423202Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7423413Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7423532Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7423686Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7423840Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7423980Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7424137Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7424228Z     def test_flash_decoding(
2025-04-11T03:52:12.7424309Z         bsz: int,
2025-04-11T03:52:12.7424391Z         block_size: int,
2025-04-11T03:52:12.7424482Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7424571Z         num_attn_heads: int,
2025-04-11T03:52:12.7424654Z         kv_group_num: int,
2025-04-11T03:52:12.7424745Z         same_context_len: bool,
2025-04-11T03:52:12.7424821Z         q_len: int,
2025-04-11T03:52:12.7424905Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7425002Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7425073Z     ):
2025-04-11T03:52:12.7425191Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7425388Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7425573Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7425744Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7425908Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7426070Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7426143Z     
2025-04-11T03:52:12.7426235Z         torch.manual_seed(123)
2025-04-11T03:52:12.7426326Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7426416Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7426426Z 
2025-04-11T03:52:12.7426582Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7426698Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7426701Z 
2025-04-11T03:52:12.7426784Z device = None
2025-04-11T03:52:12.7426791Z 
2025-04-11T03:52:12.7426909Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7427065Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7427138Z     
2025-04-11T03:52:12.7427215Z         Args:
2025-04-11T03:52:12.7427384Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7427550Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7427662Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7427734Z         """
2025-04-11T03:52:12.7427815Z         _lazy_init()
2025-04-11T03:52:12.7427914Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7428018Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7428128Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7428563Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7428709Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7428871Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7428875Z 
2025-04-11T03:52:12.7429121Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7429289Z ____________ test_flash_decoding[False-False-1-True-1-16-16-32-16] _____________
2025-04-11T03:52:12.7429293Z 
2025-04-11T03:52:12.7429448Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7429610Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7429802Z use_new_kcache_layout = False
2025-04-11T03:52:12.7429809Z 
2025-04-11T03:52:12.7430011Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7430117Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7430241Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7430379Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7430500Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7430614Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7430751Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7430861Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7430995Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7431149Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7431242Z     def test_flash_decoding(
2025-04-11T03:52:12.7431322Z         bsz: int,
2025-04-11T03:52:12.7431405Z         block_size: int,
2025-04-11T03:52:12.7431494Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7431582Z         num_attn_heads: int,
2025-04-11T03:52:12.7431668Z         kv_group_num: int,
2025-04-11T03:52:12.7431757Z         same_context_len: bool,
2025-04-11T03:52:12.7431832Z         q_len: int,
2025-04-11T03:52:12.7431918Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7432011Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7432081Z     ):
2025-04-11T03:52:12.7432197Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7432389Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7432573Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7432743Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7432911Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7433079Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7433150Z     
2025-04-11T03:52:12.7433246Z         torch.manual_seed(123)
2025-04-11T03:52:12.7433337Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7433431Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7433435Z 
2025-04-11T03:52:12.7433590Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7433706Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7433710Z 
2025-04-11T03:52:12.7433793Z device = None
2025-04-11T03:52:12.7433796Z 
2025-04-11T03:52:12.7433913Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7434069Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7434143Z     
2025-04-11T03:52:12.7434221Z         Args:
2025-04-11T03:52:12.7434388Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7434552Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7434781Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7434859Z         """
2025-04-11T03:52:12.7434941Z         _lazy_init()
2025-04-11T03:52:12.7435038Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7435144Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7435248Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7435530Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7435670Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7435829Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7435930Z 
2025-04-11T03:52:12.7436176Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7436346Z _____________ test_flash_decoding[False-False-1-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.7436350Z 
2025-04-11T03:52:12.7436504Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7436669Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7436762Z use_new_kcache_layout = False
2025-04-11T03:52:12.7436770Z 
2025-04-11T03:52:12.7436973Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7437082Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7437209Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7437351Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7437479Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7437598Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7437742Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7437853Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7437990Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7438147Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7438238Z     def test_flash_decoding(
2025-04-11T03:52:12.7438324Z         bsz: int,
2025-04-11T03:52:12.7438411Z         block_size: int,
2025-04-11T03:52:12.7438504Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7438599Z         num_attn_heads: int,
2025-04-11T03:52:12.7438685Z         kv_group_num: int,
2025-04-11T03:52:12.7438776Z         same_context_len: bool,
2025-04-11T03:52:12.7438858Z         q_len: int,
2025-04-11T03:52:12.7438947Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7439047Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7439122Z     ):
2025-04-11T03:52:12.7439240Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7439438Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7439626Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7439797Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7439965Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7440128Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7440203Z     
2025-04-11T03:52:12.7440299Z         torch.manual_seed(123)
2025-04-11T03:52:12.7440395Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7440493Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7440500Z 
2025-04-11T03:52:12.7440658Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7440774Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7440781Z 
2025-04-11T03:52:12.7440863Z device = None
2025-04-11T03:52:12.7440975Z 
2025-04-11T03:52:12.7441095Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7441253Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7441326Z     
2025-04-11T03:52:12.7441406Z         Args:
2025-04-11T03:52:12.7441572Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7441741Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7441853Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7441927Z         """
2025-04-11T03:52:12.7442010Z         _lazy_init()
2025-04-11T03:52:12.7442105Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7442408Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7442517Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7442802Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7442946Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7443104Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7443108Z 
2025-04-11T03:52:12.7443357Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7443524Z _____________ test_flash_decoding[False-False-1-True-4-16-8-16-16] _____________
2025-04-11T03:52:12.7443528Z 
2025-04-11T03:52:12.7443681Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7443842Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7443940Z use_new_kcache_layout = False
2025-04-11T03:52:12.7443944Z 
2025-04-11T03:52:12.7444144Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7444254Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7444377Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7444517Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7444637Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7444751Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7444888Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7444993Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7445127Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7445280Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7445371Z     def test_flash_decoding(
2025-04-11T03:52:12.7445451Z         bsz: int,
2025-04-11T03:52:12.7445534Z         block_size: int,
2025-04-11T03:52:12.7445625Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7445713Z         num_attn_heads: int,
2025-04-11T03:52:12.7445799Z         kv_group_num: int,
2025-04-11T03:52:12.7445891Z         same_context_len: bool,
2025-04-11T03:52:12.7445968Z         q_len: int,
2025-04-11T03:52:12.7446058Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7446146Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7446215Z     ):
2025-04-11T03:52:12.7446328Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7446519Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7446703Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7446873Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7447041Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7447197Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7447268Z     
2025-04-11T03:52:12.7447466Z         torch.manual_seed(123)
2025-04-11T03:52:12.7447559Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7447654Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7447658Z 
2025-04-11T03:52:12.7447815Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7447932Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7447939Z 
2025-04-11T03:52:12.7448017Z device = None
2025-04-11T03:52:12.7448021Z 
2025-04-11T03:52:12.7448143Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7448303Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7448471Z     
2025-04-11T03:52:12.7448547Z         Args:
2025-04-11T03:52:12.7448715Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7448883Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7448995Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7449066Z         """
2025-04-11T03:52:12.7449147Z         _lazy_init()
2025-04-11T03:52:12.7449244Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7449351Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7449458Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7449740Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7449881Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7450036Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7450043Z 
2025-04-11T03:52:12.7450288Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7450458Z _____________ test_flash_decoding[False-False-1-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.7450465Z 
2025-04-11T03:52:12.7450619Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7450782Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7450875Z use_new_kcache_layout = False
2025-04-11T03:52:12.7450879Z 
2025-04-11T03:52:12.7451081Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7451184Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7451305Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7451444Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7451569Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7451685Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7451827Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7451937Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7452075Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7452232Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7452320Z     def test_flash_decoding(
2025-04-11T03:52:12.7452399Z         bsz: int,
2025-04-11T03:52:12.7452482Z         block_size: int,
2025-04-11T03:52:12.7452576Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7452659Z         num_attn_heads: int,
2025-04-11T03:52:12.7452744Z         kv_group_num: int,
2025-04-11T03:52:12.7452832Z         same_context_len: bool,
2025-04-11T03:52:12.7452909Z         q_len: int,
2025-04-11T03:52:12.7453002Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7453095Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7453166Z     ):
2025-04-11T03:52:12.7453283Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7453476Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7453790Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7453970Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7454137Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7454294Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7454368Z     
2025-04-11T03:52:12.7454463Z         torch.manual_seed(123)
2025-04-11T03:52:12.7454553Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7454647Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7454650Z 
2025-04-11T03:52:12.7454900Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7455018Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7455022Z 
2025-04-11T03:52:12.7455101Z device = None
2025-04-11T03:52:12.7455105Z 
2025-04-11T03:52:12.7455226Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7455382Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7455454Z     
2025-04-11T03:52:12.7455533Z         Args:
2025-04-11T03:52:12.7455705Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7455873Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7455981Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7456055Z         """
2025-04-11T03:52:12.7456138Z         _lazy_init()
2025-04-11T03:52:12.7456233Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7456344Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7456449Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7456731Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7456871Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7457028Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7457032Z 
2025-04-11T03:52:12.7457279Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7457444Z _____________ test_flash_decoding[False-False-1-True-4-16-8-32-16] _____________
2025-04-11T03:52:12.7457448Z 
2025-04-11T03:52:12.7457600Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7457761Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7457856Z use_new_kcache_layout = False
2025-04-11T03:52:12.7457859Z 
2025-04-11T03:52:12.7458059Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7458168Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7458285Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7458420Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7458543Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7458657Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7458796Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7458900Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7459035Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7459191Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7459281Z     def test_flash_decoding(
2025-04-11T03:52:12.7459362Z         bsz: int,
2025-04-11T03:52:12.7459445Z         block_size: int,
2025-04-11T03:52:12.7459542Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7459625Z         num_attn_heads: int,
2025-04-11T03:52:12.7459816Z         kv_group_num: int,
2025-04-11T03:52:12.7459911Z         same_context_len: bool,
2025-04-11T03:52:12.7459990Z         q_len: int,
2025-04-11T03:52:12.7460079Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7460167Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7460239Z     ):
2025-04-11T03:52:12.7460356Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7460547Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7460732Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7460902Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7461176Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7461333Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7461403Z     
2025-04-11T03:52:12.7461498Z         torch.manual_seed(123)
2025-04-11T03:52:12.7461589Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7461687Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7461691Z 
2025-04-11T03:52:12.7461847Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7461964Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7461968Z 
2025-04-11T03:52:12.7462045Z device = None
2025-04-11T03:52:12.7462049Z 
2025-04-11T03:52:12.7462164Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7462320Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7462394Z     
2025-04-11T03:52:12.7462472Z         Args:
2025-04-11T03:52:12.7462637Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7462805Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7462913Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7462986Z         """
2025-04-11T03:52:12.7463067Z         _lazy_init()
2025-04-11T03:52:12.7463164Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7463272Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7463379Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7463662Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7463797Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7463954Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7463960Z 
2025-04-11T03:52:12.7464205Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7464368Z _____________ test_flash_decoding[False-False-1-True-4-16-16-16-7] _____________
2025-04-11T03:52:12.7464374Z 
2025-04-11T03:52:12.7464531Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7464694Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7464788Z use_new_kcache_layout = False
2025-04-11T03:52:12.7464792Z 
2025-04-11T03:52:12.7464989Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7465095Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7465215Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7465353Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7465479Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7465592Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7465732Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7465836Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7466112Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7466267Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7466357Z     def test_flash_decoding(
2025-04-11T03:52:12.7466439Z         bsz: int,
2025-04-11T03:52:12.7466523Z         block_size: int,
2025-04-11T03:52:12.7466616Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7466698Z         num_attn_heads: int,
2025-04-11T03:52:12.7466782Z         kv_group_num: int,
2025-04-11T03:52:12.7466872Z         same_context_len: bool,
2025-04-11T03:52:12.7466949Z         q_len: int,
2025-04-11T03:52:12.7467039Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7467225Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7467297Z     ):
2025-04-11T03:52:12.7467412Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7467607Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7467800Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7467974Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7468143Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7468302Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7468373Z     
2025-04-11T03:52:12.7468515Z         torch.manual_seed(123)
2025-04-11T03:52:12.7468607Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7468700Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7468703Z 
2025-04-11T03:52:12.7468863Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7468981Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7468985Z 
2025-04-11T03:52:12.7469061Z device = None
2025-04-11T03:52:12.7469065Z 
2025-04-11T03:52:12.7469188Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7469339Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7469411Z     
2025-04-11T03:52:12.7469490Z         Args:
2025-04-11T03:52:12.7469653Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7469820Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7469927Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7470000Z         """
2025-04-11T03:52:12.7470082Z         _lazy_init()
2025-04-11T03:52:12.7470177Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7470288Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7470393Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7470681Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7470819Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7470976Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7470980Z 
2025-04-11T03:52:12.7471220Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7471388Z ____________ test_flash_decoding[False-False-1-True-4-16-16-16-16] _____________
2025-04-11T03:52:12.7471392Z 
2025-04-11T03:52:12.7471548Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7471713Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7471811Z use_new_kcache_layout = False
2025-04-11T03:52:12.7471815Z 
2025-04-11T03:52:12.7472015Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7472233Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7472355Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7472493Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7472616Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7472729Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7472867Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7472973Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7473112Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7473263Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7473460Z     def test_flash_decoding(
2025-04-11T03:52:12.7473541Z         bsz: int,
2025-04-11T03:52:12.7473623Z         block_size: int,
2025-04-11T03:52:12.7473717Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7473800Z         num_attn_heads: int,
2025-04-11T03:52:12.7473885Z         kv_group_num: int,
2025-04-11T03:52:12.7473976Z         same_context_len: bool,
2025-04-11T03:52:12.7474052Z         q_len: int,
2025-04-11T03:52:12.7474140Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7474231Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7474302Z     ):
2025-04-11T03:52:12.7474420Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7474611Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7474794Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7474964Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7475133Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7475288Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7475363Z     
2025-04-11T03:52:12.7475453Z         torch.manual_seed(123)
2025-04-11T03:52:12.7475545Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7475637Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7475641Z 
2025-04-11T03:52:12.7475793Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7475907Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7475911Z 
2025-04-11T03:52:12.7475987Z device = None
2025-04-11T03:52:12.7475990Z 
2025-04-11T03:52:12.7476110Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7476259Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7476332Z     
2025-04-11T03:52:12.7476411Z         Args:
2025-04-11T03:52:12.7476577Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7476741Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7476850Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7476925Z         """
2025-04-11T03:52:12.7477006Z         _lazy_init()
2025-04-11T03:52:12.7477104Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7477209Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7477314Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7477598Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7477736Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7477890Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7477901Z 
2025-04-11T03:52:12.7478138Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7478303Z _____________ test_flash_decoding[False-False-1-True-4-16-16-32-7] _____________
2025-04-11T03:52:12.7478422Z 
2025-04-11T03:52:12.7478578Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7478741Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7478832Z use_new_kcache_layout = False
2025-04-11T03:52:12.7478836Z 
2025-04-11T03:52:12.7479039Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7479146Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7479263Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7479404Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7479628Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7479746Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7479888Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7479995Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7480140Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7480291Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7480380Z     def test_flash_decoding(
2025-04-11T03:52:12.7480459Z         bsz: int,
2025-04-11T03:52:12.7480540Z         block_size: int,
2025-04-11T03:52:12.7480636Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7480717Z         num_attn_heads: int,
2025-04-11T03:52:12.7480798Z         kv_group_num: int,
2025-04-11T03:52:12.7480887Z         same_context_len: bool,
2025-04-11T03:52:12.7480962Z         q_len: int,
2025-04-11T03:52:12.7481054Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7481145Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7481220Z     ):
2025-04-11T03:52:12.7481331Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7481524Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7481715Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7481885Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7482049Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7482207Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7482280Z     
2025-04-11T03:52:12.7482367Z         torch.manual_seed(123)
2025-04-11T03:52:12.7482455Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7482553Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7482556Z 
2025-04-11T03:52:12.7482717Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7482833Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7482837Z 
2025-04-11T03:52:12.7482915Z device = None
2025-04-11T03:52:12.7482919Z 
2025-04-11T03:52:12.7483041Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7483194Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7483263Z     
2025-04-11T03:52:12.7483341Z         Args:
2025-04-11T03:52:12.7483508Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7483677Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7483781Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7483858Z         """
2025-04-11T03:52:12.7483936Z         _lazy_init()
2025-04-11T03:52:12.7484032Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7484146Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7484253Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7484644Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7484785Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7484943Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7484950Z 
2025-04-11T03:52:12.7485188Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7485358Z ____________ test_flash_decoding[False-False-1-True-4-16-16-32-16] _____________
2025-04-11T03:52:12.7485362Z 
2025-04-11T03:52:12.7485516Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7485678Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7485869Z use_new_kcache_layout = False
2025-04-11T03:52:12.7485873Z 
2025-04-11T03:52:12.7486073Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7486181Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7486302Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7486440Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7486561Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7486675Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7486815Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7486923Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7487062Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7487213Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7487304Z     def test_flash_decoding(
2025-04-11T03:52:12.7487387Z         bsz: int,
2025-04-11T03:52:12.7487468Z         block_size: int,
2025-04-11T03:52:12.7487560Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7487643Z         num_attn_heads: int,
2025-04-11T03:52:12.7487730Z         kv_group_num: int,
2025-04-11T03:52:12.7487815Z         same_context_len: bool,
2025-04-11T03:52:12.7487891Z         q_len: int,
2025-04-11T03:52:12.7487983Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7488072Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7488145Z     ):
2025-04-11T03:52:12.7488254Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7488446Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7488630Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7488798Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7488968Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7489123Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7489199Z     
2025-04-11T03:52:12.7489289Z         torch.manual_seed(123)
2025-04-11T03:52:12.7489379Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7489476Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7489480Z 
2025-04-11T03:52:12.7489634Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7489749Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7489752Z 
2025-04-11T03:52:12.7489830Z device = None
2025-04-11T03:52:12.7489835Z 
2025-04-11T03:52:12.7489956Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7490106Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7490179Z     
2025-04-11T03:52:12.7490257Z         Args:
2025-04-11T03:52:12.7490421Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7490585Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7490805Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7490886Z         """
2025-04-11T03:52:12.7490964Z         _lazy_init()
2025-04-11T03:52:12.7491061Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7491171Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7491277Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7491566Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7491703Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7491863Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7491961Z 
2025-04-11T03:52:12.7492202Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7492375Z _____________ test_flash_decoding[False-False-1-False-1-16-8-16-7] _____________
2025-04-11T03:52:12.7492387Z 
2025-04-11T03:52:12.7492541Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7492705Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7492800Z use_new_kcache_layout = False
2025-04-11T03:52:12.7492804Z 
2025-04-11T03:52:12.7493002Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7493110Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7493228Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7493368Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7493485Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7493603Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7493741Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7493845Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7493988Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7494138Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7494227Z     def test_flash_decoding(
2025-04-11T03:52:12.7494306Z         bsz: int,
2025-04-11T03:52:12.7494388Z         block_size: int,
2025-04-11T03:52:12.7494480Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7494563Z         num_attn_heads: int,
2025-04-11T03:52:12.7494650Z         kv_group_num: int,
2025-04-11T03:52:12.7494734Z         same_context_len: bool,
2025-04-11T03:52:12.7494811Z         q_len: int,
2025-04-11T03:52:12.7494901Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7494993Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7495067Z     ):
2025-04-11T03:52:12.7495178Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7495369Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7495558Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7495728Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7495894Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7496051Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7496127Z     
2025-04-11T03:52:12.7496214Z         torch.manual_seed(123)
2025-04-11T03:52:12.7496302Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7496397Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7496401Z 
2025-04-11T03:52:12.7496565Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7496683Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7496687Z 
2025-04-11T03:52:12.7496763Z device = None
2025-04-11T03:52:12.7496767Z 
2025-04-11T03:52:12.7497000Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7497154Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7497225Z     
2025-04-11T03:52:12.7497305Z         Args:
2025-04-11T03:52:12.7497476Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7497646Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7497750Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7497826Z         """
2025-04-11T03:52:12.7497905Z         _lazy_init()
2025-04-11T03:52:12.7498002Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7498203Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7498312Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7498602Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7498738Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7498900Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7498905Z 
2025-04-11T03:52:12.7499149Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7499320Z ____________ test_flash_decoding[False-False-1-False-1-16-8-16-16] _____________
2025-04-11T03:52:12.7499328Z 
2025-04-11T03:52:12.7499478Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7499640Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7499736Z use_new_kcache_layout = False
2025-04-11T03:52:12.7499740Z 
2025-04-11T03:52:12.7499941Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7500048Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7500170Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7500312Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7500429Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7500544Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7500686Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7500791Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7500933Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7501082Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7501177Z     def test_flash_decoding(
2025-04-11T03:52:12.7501255Z         bsz: int,
2025-04-11T03:52:12.7501336Z         block_size: int,
2025-04-11T03:52:12.7501431Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7501515Z         num_attn_heads: int,
2025-04-11T03:52:12.7501605Z         kv_group_num: int,
2025-04-11T03:52:12.7501692Z         same_context_len: bool,
2025-04-11T03:52:12.7501768Z         q_len: int,
2025-04-11T03:52:12.7501862Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7501950Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7502026Z     ):
2025-04-11T03:52:12.7502139Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7502330Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7502516Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7502687Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7502856Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7503013Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7503089Z     
2025-04-11T03:52:12.7503283Z         torch.manual_seed(123)
2025-04-11T03:52:12.7503377Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7503473Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7503477Z 
2025-04-11T03:52:12.7503634Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7503755Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7503759Z 
2025-04-11T03:52:12.7503838Z device = None
2025-04-11T03:52:12.7503842Z 
2025-04-11T03:52:12.7503965Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7504115Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7504191Z     
2025-04-11T03:52:12.7504364Z         Args:
2025-04-11T03:52:12.7504531Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7504702Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7504812Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7504889Z         """
2025-04-11T03:52:12.7504967Z         _lazy_init()
2025-04-11T03:52:12.7505065Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7505174Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7505282Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7505571Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7505707Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7505868Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7505875Z 
2025-04-11T03:52:12.7506117Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7506289Z _____________ test_flash_decoding[False-False-1-False-1-16-8-32-7] _____________
2025-04-11T03:52:12.7506295Z 
2025-04-11T03:52:12.7506447Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7506610Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7506704Z use_new_kcache_layout = False
2025-04-11T03:52:12.7506707Z 
2025-04-11T03:52:12.7506907Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7507019Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7507138Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7507282Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7507398Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7507518Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7507659Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7507764Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7507910Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7508060Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7508152Z     def test_flash_decoding(
2025-04-11T03:52:12.7508228Z         bsz: int,
2025-04-11T03:52:12.7508311Z         block_size: int,
2025-04-11T03:52:12.7508405Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7508530Z         num_attn_heads: int,
2025-04-11T03:52:12.7508617Z         kv_group_num: int,
2025-04-11T03:52:12.7508706Z         same_context_len: bool,
2025-04-11T03:52:12.7508784Z         q_len: int,
2025-04-11T03:52:12.7508876Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7508963Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7509042Z     ):
2025-04-11T03:52:12.7509152Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7509349Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7509643Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7509821Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7509989Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7510146Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7510221Z     
2025-04-11T03:52:12.7510312Z         torch.manual_seed(123)
2025-04-11T03:52:12.7510407Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7510501Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7510504Z 
2025-04-11T03:52:12.7510774Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7510894Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7510898Z 
2025-04-11T03:52:12.7510979Z device = None
2025-04-11T03:52:12.7510982Z 
2025-04-11T03:52:12.7511112Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7511264Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7511339Z     
2025-04-11T03:52:12.7511413Z         Args:
2025-04-11T03:52:12.7511580Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7511746Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7511851Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7511934Z         """
2025-04-11T03:52:12.7512011Z         _lazy_init()
2025-04-11T03:52:12.7512107Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7512216Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7512324Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7512615Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7512753Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7512913Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7512917Z 
2025-04-11T03:52:12.7513158Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7513329Z ____________ test_flash_decoding[False-False-1-False-1-16-8-32-16] _____________
2025-04-11T03:52:12.7513332Z 
2025-04-11T03:52:12.7513483Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7513647Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7513747Z use_new_kcache_layout = False
2025-04-11T03:52:12.7513750Z 
2025-04-11T03:52:12.7513949Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7514059Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7514178Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7514320Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7514434Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7514548Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7514690Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7514795Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7514936Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7515086Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7515178Z     def test_flash_decoding(
2025-04-11T03:52:12.7515257Z         bsz: int,
2025-04-11T03:52:12.7515340Z         block_size: int,
2025-04-11T03:52:12.7515434Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7515517Z         num_attn_heads: int,
2025-04-11T03:52:12.7515722Z         kv_group_num: int,
2025-04-11T03:52:12.7515813Z         same_context_len: bool,
2025-04-11T03:52:12.7515891Z         q_len: int,
2025-04-11T03:52:12.7515981Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7516073Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7516147Z     ):
2025-04-11T03:52:12.7516258Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7516452Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7516632Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7516802Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7517067Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7517223Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7517299Z     
2025-04-11T03:52:12.7517390Z         torch.manual_seed(123)
2025-04-11T03:52:12.7517486Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7517579Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7517583Z 
2025-04-11T03:52:12.7517739Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7517856Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7517860Z 
2025-04-11T03:52:12.7517936Z device = None
2025-04-11T03:52:12.7517940Z 
2025-04-11T03:52:12.7518061Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7518213Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7518286Z     
2025-04-11T03:52:12.7518366Z         Args:
2025-04-11T03:52:12.7518531Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7518699Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7518806Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7518881Z         """
2025-04-11T03:52:12.7518959Z         _lazy_init()
2025-04-11T03:52:12.7519058Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7519160Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7519266Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7519552Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7519687Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7519848Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7519854Z 
2025-04-11T03:52:12.7520093Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7520269Z ____________ test_flash_decoding[False-False-1-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.7520276Z 
2025-04-11T03:52:12.7520430Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7520595Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7520685Z use_new_kcache_layout = False
2025-04-11T03:52:12.7520688Z 
2025-04-11T03:52:12.7520889Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7520998Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7521116Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7521256Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7521375Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7521495Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7521630Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7521737Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7521983Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7522136Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7522229Z     def test_flash_decoding(
2025-04-11T03:52:12.7522306Z         bsz: int,
2025-04-11T03:52:12.7522389Z         block_size: int,
2025-04-11T03:52:12.7522485Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7522569Z         num_attn_heads: int,
2025-04-11T03:52:12.7522656Z         kv_group_num: int,
2025-04-11T03:52:12.7522742Z         same_context_len: bool,
2025-04-11T03:52:12.7522823Z         q_len: int,
2025-04-11T03:52:12.7522909Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7522998Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7523168Z     ):
2025-04-11T03:52:12.7523282Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7523484Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7523671Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7523845Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7524014Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7524172Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7524256Z     
2025-04-11T03:52:12.7524377Z         torch.manual_seed(123)
2025-04-11T03:52:12.7524476Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7524569Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7524573Z 
2025-04-11T03:52:12.7524735Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7524854Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7524858Z 
2025-04-11T03:52:12.7524935Z device = None
2025-04-11T03:52:12.7524938Z 
2025-04-11T03:52:12.7525066Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7525217Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7525291Z     
2025-04-11T03:52:12.7525365Z         Args:
2025-04-11T03:52:12.7525531Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7525698Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7525802Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7525879Z         """
2025-04-11T03:52:12.7525956Z         _lazy_init()
2025-04-11T03:52:12.7526055Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7526161Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7526266Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7526559Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7526693Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7526851Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7526855Z 
2025-04-11T03:52:12.7527095Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7527269Z ____________ test_flash_decoding[False-False-1-False-1-16-16-16-16] ____________
2025-04-11T03:52:12.7527272Z 
2025-04-11T03:52:12.7527424Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7527595Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7527688Z use_new_kcache_layout = False
2025-04-11T03:52:12.7527692Z 
2025-04-11T03:52:12.7527893Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7528000Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7528227Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7528371Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7528487Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7528607Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7528745Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7528851Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7528994Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7529143Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7529234Z     def test_flash_decoding(
2025-04-11T03:52:12.7529404Z         bsz: int,
2025-04-11T03:52:12.7529488Z         block_size: int,
2025-04-11T03:52:12.7529584Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7529669Z         num_attn_heads: int,
2025-04-11T03:52:12.7529755Z         kv_group_num: int,
2025-04-11T03:52:12.7529844Z         same_context_len: bool,
2025-04-11T03:52:12.7529925Z         q_len: int,
2025-04-11T03:52:12.7530010Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7530098Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7530176Z     ):
2025-04-11T03:52:12.7530285Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7530479Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7530664Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7530839Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7531004Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7531162Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7531238Z     
2025-04-11T03:52:12.7531328Z         torch.manual_seed(123)
2025-04-11T03:52:12.7531422Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7531514Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7531518Z 
2025-04-11T03:52:12.7531674Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7531790Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7531794Z 
2025-04-11T03:52:12.7531872Z device = None
2025-04-11T03:52:12.7531876Z 
2025-04-11T03:52:12.7531998Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7532147Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7532220Z     
2025-04-11T03:52:12.7532296Z         Args:
2025-04-11T03:52:12.7532464Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7532628Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7532736Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7532815Z         """
2025-04-11T03:52:12.7532892Z         _lazy_init()
2025-04-11T03:52:12.7532994Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7533099Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7533208Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7533494Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7533628Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7533790Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7533796Z 
2025-04-11T03:52:12.7534038Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7534213Z ____________ test_flash_decoding[False-False-1-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.7534217Z 
2025-04-11T03:52:12.7534477Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7534647Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7534737Z use_new_kcache_layout = False
2025-04-11T03:52:12.7534740Z 
2025-04-11T03:52:12.7534938Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7535047Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7535164Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7535304Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7535420Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7535656Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7535793Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7535902Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7536050Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7536198Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7536291Z     def test_flash_decoding(
2025-04-11T03:52:12.7536367Z         bsz: int,
2025-04-11T03:52:12.7536452Z         block_size: int,
2025-04-11T03:52:12.7536542Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7536624Z         num_attn_heads: int,
2025-04-11T03:52:12.7536712Z         kv_group_num: int,
2025-04-11T03:52:12.7536799Z         same_context_len: bool,
2025-04-11T03:52:12.7536878Z         q_len: int,
2025-04-11T03:52:12.7536965Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7537056Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7537135Z     ):
2025-04-11T03:52:12.7537244Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7537438Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7537620Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7537794Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7537956Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7538111Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7538185Z     
2025-04-11T03:52:12.7538273Z         torch.manual_seed(123)
2025-04-11T03:52:12.7538363Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7538453Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7538457Z 
2025-04-11T03:52:12.7538614Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7538730Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7538734Z 
2025-04-11T03:52:12.7538813Z device = None
2025-04-11T03:52:12.7538817Z 
2025-04-11T03:52:12.7538940Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7539090Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7539165Z     
2025-04-11T03:52:12.7539237Z         Args:
2025-04-11T03:52:12.7539405Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7539568Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7539673Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7539751Z         """
2025-04-11T03:52:12.7539828Z         _lazy_init()
2025-04-11T03:52:12.7539927Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7540034Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7540139Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7540597Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7540738Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7540900Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7540904Z 
2025-04-11T03:52:12.7541145Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7541322Z ____________ test_flash_decoding[False-False-1-False-1-16-16-32-16] ____________
2025-04-11T03:52:12.7541325Z 
2025-04-11T03:52:12.7541477Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7541645Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7541835Z use_new_kcache_layout = False
2025-04-11T03:52:12.7541839Z 
2025-04-11T03:52:12.7542043Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7542148Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7542268Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7542410Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7542526Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7542641Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7542775Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7542879Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7543017Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7543166Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7543259Z     def test_flash_decoding(
2025-04-11T03:52:12.7543339Z         bsz: int,
2025-04-11T03:52:12.7543423Z         block_size: int,
2025-04-11T03:52:12.7543514Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7543597Z         num_attn_heads: int,
2025-04-11T03:52:12.7543683Z         kv_group_num: int,
2025-04-11T03:52:12.7543771Z         same_context_len: bool,
2025-04-11T03:52:12.7543849Z         q_len: int,
2025-04-11T03:52:12.7543934Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7544021Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7544096Z     ):
2025-04-11T03:52:12.7544206Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7544405Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7544586Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7544761Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7544929Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7545086Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7545160Z     
2025-04-11T03:52:12.7545249Z         torch.manual_seed(123)
2025-04-11T03:52:12.7545344Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7545436Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7545440Z 
2025-04-11T03:52:12.7545597Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7545711Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7545714Z 
2025-04-11T03:52:12.7545791Z device = None
2025-04-11T03:52:12.7545795Z 
2025-04-11T03:52:12.7545918Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7546068Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7546141Z     
2025-04-11T03:52:12.7546217Z         Args:
2025-04-11T03:52:12.7546383Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7546547Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7546760Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7546841Z         """
2025-04-11T03:52:12.7546921Z         _lazy_init()
2025-04-11T03:52:12.7547018Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7547121Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7547232Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7547516Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7547653Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7547813Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7547909Z 
2025-04-11T03:52:12.7548154Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7548326Z _____________ test_flash_decoding[False-False-1-False-4-16-8-16-7] _____________
2025-04-11T03:52:12.7548329Z 
2025-04-11T03:52:12.7548521Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7548687Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7548780Z use_new_kcache_layout = False
2025-04-11T03:52:12.7548784Z 
2025-04-11T03:52:12.7548988Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7549092Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7549212Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7549357Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7549475Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7549598Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7549735Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7549846Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7549983Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7550133Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7550228Z     def test_flash_decoding(
2025-04-11T03:52:12.7550304Z         bsz: int,
2025-04-11T03:52:12.7550394Z         block_size: int,
2025-04-11T03:52:12.7550483Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7550565Z         num_attn_heads: int,
2025-04-11T03:52:12.7550652Z         kv_group_num: int,
2025-04-11T03:52:12.7550736Z         same_context_len: bool,
2025-04-11T03:52:12.7550814Z         q_len: int,
2025-04-11T03:52:12.7550900Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7550988Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7551065Z     ):
2025-04-11T03:52:12.7551177Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7551371Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7551552Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7551724Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7551886Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7552041Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7552117Z     
2025-04-11T03:52:12.7552202Z         torch.manual_seed(123)
2025-04-11T03:52:12.7552295Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7552385Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7552388Z 
2025-04-11T03:52:12.7552547Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7552662Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7552666Z 
2025-04-11T03:52:12.7552743Z device = None
2025-04-11T03:52:12.7552750Z 
2025-04-11T03:52:12.7552984Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7553138Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7553215Z     
2025-04-11T03:52:12.7553290Z         Args:
2025-04-11T03:52:12.7553464Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7553632Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7553736Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7553814Z         """
2025-04-11T03:52:12.7553891Z         _lazy_init()
2025-04-11T03:52:12.7553995Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7554204Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7554313Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7554601Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7554736Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7554896Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7554900Z 
2025-04-11T03:52:12.7555139Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7555314Z ____________ test_flash_decoding[False-False-1-False-4-16-8-16-16] _____________
2025-04-11T03:52:12.7555318Z 
2025-04-11T03:52:12.7555467Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7555633Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7555728Z use_new_kcache_layout = False
2025-04-11T03:52:12.7555732Z 
2025-04-11T03:52:12.7555935Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7556042Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7556163Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7556307Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7556427Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7556544Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7556681Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7556792Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7556928Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7557077Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7557173Z     def test_flash_decoding(
2025-04-11T03:52:12.7557252Z         bsz: int,
2025-04-11T03:52:12.7557336Z         block_size: int,
2025-04-11T03:52:12.7557428Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7557513Z         num_attn_heads: int,
2025-04-11T03:52:12.7557598Z         kv_group_num: int,
2025-04-11T03:52:12.7557687Z         same_context_len: bool,
2025-04-11T03:52:12.7557767Z         q_len: int,
2025-04-11T03:52:12.7557852Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7557947Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7558019Z     ):
2025-04-11T03:52:12.7558133Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7558330Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7558509Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7558685Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7558852Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7559013Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7559085Z     
2025-04-11T03:52:12.7559311Z         torch.manual_seed(123)
2025-04-11T03:52:12.7559409Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7559499Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7559503Z 
2025-04-11T03:52:12.7559663Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7559776Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7559780Z 
2025-04-11T03:52:12.7559865Z device = None
2025-04-11T03:52:12.7559869Z 
2025-04-11T03:52:12.7559989Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7560140Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7560216Z     
2025-04-11T03:52:12.7560389Z         Args:
2025-04-11T03:52:12.7560558Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7560723Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7560836Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7560910Z         """
2025-04-11T03:52:12.7560987Z         _lazy_init()
2025-04-11T03:52:12.7561088Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7561191Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7561301Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7561586Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7561722Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7561885Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7561892Z 
2025-04-11T03:52:12.7562136Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7562308Z _____________ test_flash_decoding[False-False-1-False-4-16-8-32-7] _____________
2025-04-11T03:52:12.7562312Z 
2025-04-11T03:52:12.7562467Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7562636Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7562726Z use_new_kcache_layout = False
2025-04-11T03:52:12.7562730Z 
2025-04-11T03:52:12.7562937Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7563043Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7563162Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7563305Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7563423Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7563545Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7563682Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7563790Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7563927Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7564079Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7564173Z     def test_flash_decoding(
2025-04-11T03:52:12.7564250Z         bsz: int,
2025-04-11T03:52:12.7564335Z         block_size: int,
2025-04-11T03:52:12.7564425Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7564511Z         num_attn_heads: int,
2025-04-11T03:52:12.7564595Z         kv_group_num: int,
2025-04-11T03:52:12.7564683Z         same_context_len: bool,
2025-04-11T03:52:12.7564762Z         q_len: int,
2025-04-11T03:52:12.7564847Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7564939Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7565014Z     ):
2025-04-11T03:52:12.7565125Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7565325Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7565617Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7565796Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7565957Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7566118Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7566192Z     
2025-04-11T03:52:12.7566278Z         torch.manual_seed(123)
2025-04-11T03:52:12.7566374Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7566466Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7566470Z 
2025-04-11T03:52:12.7566631Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7566843Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7566847Z 
2025-04-11T03:52:12.7566928Z device = None
2025-04-11T03:52:12.7566932Z 
2025-04-11T03:52:12.7567053Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7567205Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7567279Z     
2025-04-11T03:52:12.7567355Z         Args:
2025-04-11T03:52:12.7567524Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7567689Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7567798Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7567871Z         """
2025-04-11T03:52:12.7567948Z         _lazy_init()
2025-04-11T03:52:12.7568049Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7568157Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7568268Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7568554Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7568696Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7568851Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7568855Z 
2025-04-11T03:52:12.7569094Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7569267Z ____________ test_flash_decoding[False-False-1-False-4-16-8-32-16] _____________
2025-04-11T03:52:12.7569271Z 
2025-04-11T03:52:12.7569421Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7569589Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7569681Z use_new_kcache_layout = False
2025-04-11T03:52:12.7569685Z 
2025-04-11T03:52:12.7569888Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7569993Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7570118Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7570257Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7570373Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7570491Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7570627Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7570737Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7570873Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7571024Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7571115Z     def test_flash_decoding(
2025-04-11T03:52:12.7571195Z         bsz: int,
2025-04-11T03:52:12.7571282Z         block_size: int,
2025-04-11T03:52:12.7571371Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7571459Z         num_attn_heads: int,
2025-04-11T03:52:12.7571542Z         kv_group_num: int,
2025-04-11T03:52:12.7571733Z         same_context_len: bool,
2025-04-11T03:52:12.7571817Z         q_len: int,
2025-04-11T03:52:12.7571904Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7571995Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7572067Z     ):
2025-04-11T03:52:12.7572179Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7572380Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7572565Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7572741Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7573026Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7573188Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7573261Z     
2025-04-11T03:52:12.7573351Z         torch.manual_seed(123)
2025-04-11T03:52:12.7573446Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7573539Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7573543Z 
2025-04-11T03:52:12.7573699Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7573810Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7573815Z 
2025-04-11T03:52:12.7573897Z device = None
2025-04-11T03:52:12.7573901Z 
2025-04-11T03:52:12.7574019Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7574168Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7574243Z     
2025-04-11T03:52:12.7574321Z         Args:
2025-04-11T03:52:12.7574493Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7574658Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7574773Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7574846Z         """
2025-04-11T03:52:12.7574924Z         _lazy_init()
2025-04-11T03:52:12.7575026Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7575130Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7575241Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7575523Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7575662Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7575819Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7575826Z 
2025-04-11T03:52:12.7576069Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7576244Z ____________ test_flash_decoding[False-False-1-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.7576248Z 
2025-04-11T03:52:12.7576403Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7576572Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7576662Z use_new_kcache_layout = False
2025-04-11T03:52:12.7576666Z 
2025-04-11T03:52:12.7576870Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7576974Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7577094Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7577234Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7577352Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7577475Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7577613Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7577723Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7577961Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7578120Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7578209Z     def test_flash_decoding(
2025-04-11T03:52:12.7578284Z         bsz: int,
2025-04-11T03:52:12.7578371Z         block_size: int,
2025-04-11T03:52:12.7578463Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7578548Z         num_attn_heads: int,
2025-04-11T03:52:12.7578631Z         kv_group_num: int,
2025-04-11T03:52:12.7578717Z         same_context_len: bool,
2025-04-11T03:52:12.7578797Z         q_len: int,
2025-04-11T03:52:12.7578883Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7578976Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7579146Z     ):
2025-04-11T03:52:12.7579260Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7579460Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7579647Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7579828Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7579993Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7580152Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7580223Z     
2025-04-11T03:52:12.7580309Z         torch.manual_seed(123)
2025-04-11T03:52:12.7580406Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7580499Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7580503Z 
2025-04-11T03:52:12.7580665Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7580781Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7580785Z 
2025-04-11T03:52:12.7580864Z device = None
2025-04-11T03:52:12.7580868Z 
2025-04-11T03:52:12.7580991Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7581146Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7581215Z     
2025-04-11T03:52:12.7581291Z         Args:
2025-04-11T03:52:12.7581466Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7581633Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7581741Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7581814Z         """
2025-04-11T03:52:12.7581891Z         _lazy_init()
2025-04-11T03:52:12.7581992Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7582098Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7582206Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7582488Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7582632Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7582789Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7582793Z 
2025-04-11T03:52:12.7583032Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7583212Z ____________ test_flash_decoding[False-False-1-False-4-16-16-16-16] ____________
2025-04-11T03:52:12.7583215Z 
2025-04-11T03:52:12.7583368Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7583537Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7583632Z use_new_kcache_layout = False
2025-04-11T03:52:12.7583636Z 
2025-04-11T03:52:12.7583839Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7583945Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7584192Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7584335Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7584451Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7584570Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7584705Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7584814Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7584947Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7585103Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7585191Z     def test_flash_decoding(
2025-04-11T03:52:12.7585365Z         bsz: int,
2025-04-11T03:52:12.7585454Z         block_size: int,
2025-04-11T03:52:12.7585544Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7585632Z         num_attn_heads: int,
2025-04-11T03:52:12.7585716Z         kv_group_num: int,
2025-04-11T03:52:12.7585805Z         same_context_len: bool,
2025-04-11T03:52:12.7585890Z         q_len: int,
2025-04-11T03:52:12.7585976Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7586071Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7586143Z     ):
2025-04-11T03:52:12.7586251Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7586447Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7586630Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7586804Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7586972Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7587132Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7587203Z     
2025-04-11T03:52:12.7587295Z         torch.manual_seed(123)
2025-04-11T03:52:12.7587390Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7587482Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7587486Z 
2025-04-11T03:52:12.7587645Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7587758Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7587762Z 
2025-04-11T03:52:12.7587843Z device = None
2025-04-11T03:52:12.7587846Z 
2025-04-11T03:52:12.7587963Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7588117Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7588187Z     
2025-04-11T03:52:12.7588262Z         Args:
2025-04-11T03:52:12.7588474Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7588642Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7588755Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7588829Z         """
2025-04-11T03:52:12.7588907Z         _lazy_init()
2025-04-11T03:52:12.7589007Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7589112Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7589220Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7589505Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7589644Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7589799Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7589806Z 
2025-04-11T03:52:12.7590049Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7590218Z ____________ test_flash_decoding[False-False-1-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.7590222Z 
2025-04-11T03:52:12.7590482Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7590652Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7590741Z use_new_kcache_layout = False
2025-04-11T03:52:12.7590746Z 
2025-04-11T03:52:12.7590949Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7591054Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7591174Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7591311Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7591428Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7591651Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7591787Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7591896Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7592034Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7592189Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7592278Z     def test_flash_decoding(
2025-04-11T03:52:12.7592354Z         bsz: int,
2025-04-11T03:52:12.7592442Z         block_size: int,
2025-04-11T03:52:12.7592531Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7592617Z         num_attn_heads: int,
2025-04-11T03:52:12.7592699Z         kv_group_num: int,
2025-04-11T03:52:12.7592787Z         same_context_len: bool,
2025-04-11T03:52:12.7592867Z         q_len: int,
2025-04-11T03:52:12.7592952Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7593044Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7593118Z     ):
2025-04-11T03:52:12.7593231Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7593424Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7593606Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7593783Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7593945Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7594105Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7594176Z     
2025-04-11T03:52:12.7594267Z         torch.manual_seed(123)
2025-04-11T03:52:12.7594355Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7594445Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7594449Z 
2025-04-11T03:52:12.7594609Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7594724Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7594728Z 
2025-04-11T03:52:12.7594808Z device = None
2025-04-11T03:52:12.7594812Z 
2025-04-11T03:52:12.7594930Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7595083Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7595153Z     
2025-04-11T03:52:12.7595227Z         Args:
2025-04-11T03:52:12.7595397Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7595560Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7595669Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7595744Z         """
2025-04-11T03:52:12.7595826Z         _lazy_init()
2025-04-11T03:52:12.7595922Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7596028Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7596141Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7596423Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7596672Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7596833Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7596837Z 
2025-04-11T03:52:12.7597081Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7597254Z ____________ test_flash_decoding[False-False-1-False-4-16-16-32-16] ____________
2025-04-11T03:52:12.7597258Z 
2025-04-11T03:52:12.7597410Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7597578Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7597759Z use_new_kcache_layout = False
2025-04-11T03:52:12.7597763Z 
2025-04-11T03:52:12.7597972Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7598076Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7598201Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7598343Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7598465Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7598581Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7598719Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7598828Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7598965Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7599119Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7599209Z     def test_flash_decoding(
2025-04-11T03:52:12.7599287Z         bsz: int,
2025-04-11T03:52:12.7599372Z         block_size: int,
2025-04-11T03:52:12.7599461Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7599546Z         num_attn_heads: int,
2025-04-11T03:52:12.7599629Z         kv_group_num: int,
2025-04-11T03:52:12.7599723Z         same_context_len: bool,
2025-04-11T03:52:12.7599801Z         q_len: int,
2025-04-11T03:52:12.7599885Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7599976Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7600047Z     ):
2025-04-11T03:52:12.7600161Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7600355Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7600535Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7600711Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7600877Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7601038Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7601108Z     
2025-04-11T03:52:12.7601199Z         torch.manual_seed(123)
2025-04-11T03:52:12.7601288Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7601378Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7601382Z 
2025-04-11T03:52:12.7601541Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7601653Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7601657Z 
2025-04-11T03:52:12.7601738Z device = None
2025-04-11T03:52:12.7601741Z 
2025-04-11T03:52:12.7601856Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7602011Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7602082Z     
2025-04-11T03:52:12.7602160Z         Args:
2025-04-11T03:52:12.7602331Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7602493Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7602699Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7602773Z         """
2025-04-11T03:52:12.7602856Z         _lazy_init()
2025-04-11T03:52:12.7602951Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7603055Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7603163Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7603442Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7603581Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7603737Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7603850Z 
2025-04-11T03:52:12.7604099Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7604268Z _____________ test_flash_decoding[False-False-5-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.7604271Z 
2025-04-11T03:52:12.7604431Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7604596Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7604685Z use_new_kcache_layout = False
2025-04-11T03:52:12.7604689Z 
2025-04-11T03:52:12.7604899Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7605003Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7605123Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7605265Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7605386Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7605502Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7605638Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7605747Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7605885Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7606040Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7606127Z     def test_flash_decoding(
2025-04-11T03:52:12.7606202Z         bsz: int,
2025-04-11T03:52:12.7606287Z         block_size: int,
2025-04-11T03:52:12.7606377Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7606464Z         num_attn_heads: int,
2025-04-11T03:52:12.7606548Z         kv_group_num: int,
2025-04-11T03:52:12.7606637Z         same_context_len: bool,
2025-04-11T03:52:12.7606714Z         q_len: int,
2025-04-11T03:52:12.7606799Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7606892Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7606967Z     ):
2025-04-11T03:52:12.7607081Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7607274Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7607457Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7607632Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7607794Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7607954Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7608023Z     
2025-04-11T03:52:12.7608113Z         torch.manual_seed(123)
2025-04-11T03:52:12.7608202Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7608295Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7608299Z 
2025-04-11T03:52:12.7608457Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7608571Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7608575Z 
2025-04-11T03:52:12.7608655Z device = None
2025-04-11T03:52:12.7608659Z 
2025-04-11T03:52:12.7608888Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7609043Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7609115Z     
2025-04-11T03:52:12.7609190Z         Args:
2025-04-11T03:52:12.7609359Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7609522Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7609631Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7609704Z         """
2025-04-11T03:52:12.7609784Z         _lazy_init()
2025-04-11T03:52:12.7609882Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7610078Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7610189Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7610473Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7610615Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7610770Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7610775Z 
2025-04-11T03:52:12.7611020Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7611187Z _____________ test_flash_decoding[False-False-5-True-1-16-8-16-16] _____________
2025-04-11T03:52:12.7611191Z 
2025-04-11T03:52:12.7611347Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7611510Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7611601Z use_new_kcache_layout = False
2025-04-11T03:52:12.7611605Z 
2025-04-11T03:52:12.7611807Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7611908Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7612031Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7612169Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7612289Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7612401Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7612535Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7612645Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7612779Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7612930Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7613018Z     def test_flash_decoding(
2025-04-11T03:52:12.7613100Z         bsz: int,
2025-04-11T03:52:12.7613182Z         block_size: int,
2025-04-11T03:52:12.7613270Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7613357Z         num_attn_heads: int,
2025-04-11T03:52:12.7613439Z         kv_group_num: int,
2025-04-11T03:52:12.7613531Z         same_context_len: bool,
2025-04-11T03:52:12.7613607Z         q_len: int,
2025-04-11T03:52:12.7613692Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7613784Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7613856Z     ):
2025-04-11T03:52:12.7613970Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7614163Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7614347Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7614518Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7614686Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7614848Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7614920Z     
2025-04-11T03:52:12.7615011Z         torch.manual_seed(123)
2025-04-11T03:52:12.7615196Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7615294Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7615302Z 
2025-04-11T03:52:12.7615458Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7615569Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7615573Z 
2025-04-11T03:52:12.7615655Z device = None
2025-04-11T03:52:12.7615660Z 
2025-04-11T03:52:12.7615777Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7615930Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7616000Z     
2025-04-11T03:52:12.7616081Z         Args:
2025-04-11T03:52:12.7616340Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7616504Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7616615Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7616691Z         """
2025-04-11T03:52:12.7616773Z         _lazy_init()
2025-04-11T03:52:12.7616869Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7616971Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7617081Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7617363Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7617503Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7617657Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7617664Z 
2025-04-11T03:52:12.7617908Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7618075Z _____________ test_flash_decoding[False-False-5-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.7618079Z 
2025-04-11T03:52:12.7618235Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7618398Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7618489Z use_new_kcache_layout = False
2025-04-11T03:52:12.7618493Z 
2025-04-11T03:52:12.7618696Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7618804Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7618928Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7619068Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7619189Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7619307Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7619442Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7619548Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7619687Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7619840Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7619928Z     def test_flash_decoding(
2025-04-11T03:52:12.7620009Z         bsz: int,
2025-04-11T03:52:12.7620094Z         block_size: int,
2025-04-11T03:52:12.7620183Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7620272Z         num_attn_heads: int,
2025-04-11T03:52:12.7620355Z         kv_group_num: int,
2025-04-11T03:52:12.7620442Z         same_context_len: bool,
2025-04-11T03:52:12.7620520Z         q_len: int,
2025-04-11T03:52:12.7620608Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7620700Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7620777Z     ):
2025-04-11T03:52:12.7620893Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7621085Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7621377Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7621554Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7621716Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7621880Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7621953Z     
2025-04-11T03:52:12.7622046Z         torch.manual_seed(123)
2025-04-11T03:52:12.7622135Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7622229Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7622233Z 
2025-04-11T03:52:12.7622391Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7622605Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7622609Z 
2025-04-11T03:52:12.7622692Z device = None
2025-04-11T03:52:12.7622696Z 
2025-04-11T03:52:12.7622816Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7622972Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7623044Z     
2025-04-11T03:52:12.7623126Z         Args:
2025-04-11T03:52:12.7623299Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7623468Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7623580Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7623657Z         """
2025-04-11T03:52:12.7623741Z         _lazy_init()
2025-04-11T03:52:12.7623840Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7623950Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7624061Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7624347Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7624494Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7624653Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7624658Z 
2025-04-11T03:52:12.7624921Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7625115Z _____________ test_flash_decoding[False-False-5-True-1-16-8-32-16] _____________
2025-04-11T03:52:12.7625120Z 
2025-04-11T03:52:12.7625272Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7625435Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7625529Z use_new_kcache_layout = False
2025-04-11T03:52:12.7625537Z 
2025-04-11T03:52:12.7625737Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7625840Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7625966Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7626108Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7626228Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7626344Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7626479Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7626587Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7626722Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7626878Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7626967Z     def test_flash_decoding(
2025-04-11T03:52:12.7627049Z         bsz: int,
2025-04-11T03:52:12.7627132Z         block_size: int,
2025-04-11T03:52:12.7627220Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7627308Z         num_attn_heads: int,
2025-04-11T03:52:12.7627390Z         kv_group_num: int,
2025-04-11T03:52:12.7627591Z         same_context_len: bool,
2025-04-11T03:52:12.7627675Z         q_len: int,
2025-04-11T03:52:12.7627761Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7627857Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7627929Z     ):
2025-04-11T03:52:12.7628049Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7628241Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7628474Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7628647Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7628932Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7629094Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7629167Z     
2025-04-11T03:52:12.7629258Z         torch.manual_seed(123)
2025-04-11T03:52:12.7629351Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7629446Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7629450Z 
2025-04-11T03:52:12.7629606Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7629718Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7629721Z 
2025-04-11T03:52:12.7629802Z device = None
2025-04-11T03:52:12.7629806Z 
2025-04-11T03:52:12.7629924Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7630079Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7630150Z     
2025-04-11T03:52:12.7630225Z         Args:
2025-04-11T03:52:12.7630392Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7630555Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7630668Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7630742Z         """
2025-04-11T03:52:12.7630823Z         _lazy_init()
2025-04-11T03:52:12.7630916Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7631021Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7631124Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7631406Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7631543Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7631699Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7631706Z 
2025-04-11T03:52:12.7631947Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7632113Z _____________ test_flash_decoding[False-False-5-True-1-16-16-16-7] _____________
2025-04-11T03:52:12.7632116Z 
2025-04-11T03:52:12.7632271Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7632436Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7632532Z use_new_kcache_layout = False
2025-04-11T03:52:12.7632536Z 
2025-04-11T03:52:12.7632736Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7632840Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7632960Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7633097Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7633217Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7633335Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7633474Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7633579Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7633839Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7634001Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7634089Z     def test_flash_decoding(
2025-04-11T03:52:12.7634168Z         bsz: int,
2025-04-11T03:52:12.7634253Z         block_size: int,
2025-04-11T03:52:12.7634342Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7634429Z         num_attn_heads: int,
2025-04-11T03:52:12.7634512Z         kv_group_num: int,
2025-04-11T03:52:12.7634603Z         same_context_len: bool,
2025-04-11T03:52:12.7634680Z         q_len: int,
2025-04-11T03:52:12.7634769Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7634858Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7635039Z     ):
2025-04-11T03:52:12.7635154Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7635345Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7635533Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7635705Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7635868Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7636029Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7636098Z     
2025-04-11T03:52:12.7636192Z         torch.manual_seed(123)
2025-04-11T03:52:12.7636279Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7636376Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7636379Z 
2025-04-11T03:52:12.7636533Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7636649Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7636656Z 
2025-04-11T03:52:12.7636734Z device = None
2025-04-11T03:52:12.7636737Z 
2025-04-11T03:52:12.7636857Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7637015Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7637085Z     
2025-04-11T03:52:12.7637165Z         Args:
2025-04-11T03:52:12.7637331Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7637495Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7637602Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7637675Z         """
2025-04-11T03:52:12.7637758Z         _lazy_init()
2025-04-11T03:52:12.7637852Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7637960Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7638067Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7638348Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7638492Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7638650Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7638654Z 
2025-04-11T03:52:12.7638899Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7639070Z ____________ test_flash_decoding[False-False-5-True-1-16-16-16-16] _____________
2025-04-11T03:52:12.7639074Z 
2025-04-11T03:52:12.7639230Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7639391Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7639487Z use_new_kcache_layout = False
2025-04-11T03:52:12.7639491Z 
2025-04-11T03:52:12.7639687Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7639791Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7640107Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7640251Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7640373Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7640487Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7640632Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7640738Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7640874Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7641029Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7641115Z     def test_flash_decoding(
2025-04-11T03:52:12.7641296Z         bsz: int,
2025-04-11T03:52:12.7641381Z         block_size: int,
2025-04-11T03:52:12.7641471Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7641558Z         num_attn_heads: int,
2025-04-11T03:52:12.7641640Z         kv_group_num: int,
2025-04-11T03:52:12.7641734Z         same_context_len: bool,
2025-04-11T03:52:12.7641810Z         q_len: int,
2025-04-11T03:52:12.7641899Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7641989Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7642060Z     ):
2025-04-11T03:52:12.7642175Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7642369Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7642554Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7642728Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7642896Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7643054Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7643124Z     
2025-04-11T03:52:12.7643217Z         torch.manual_seed(123)
2025-04-11T03:52:12.7643308Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7643404Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7643408Z 
2025-04-11T03:52:12.7643566Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7643681Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7643685Z 
2025-04-11T03:52:12.7643762Z device = None
2025-04-11T03:52:12.7643765Z 
2025-04-11T03:52:12.7643880Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7644032Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7644101Z     
2025-04-11T03:52:12.7644178Z         Args:
2025-04-11T03:52:12.7644348Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7644516Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7644623Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7644696Z         """
2025-04-11T03:52:12.7644778Z         _lazy_init()
2025-04-11T03:52:12.7644872Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7644979Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7645085Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7645372Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7645513Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7645668Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7645675Z 
2025-04-11T03:52:12.7645918Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7646089Z _____________ test_flash_decoding[False-False-5-True-1-16-16-32-7] _____________
2025-04-11T03:52:12.7646092Z 
2025-04-11T03:52:12.7646354Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7646524Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7646618Z use_new_kcache_layout = False
2025-04-11T03:52:12.7646622Z 
2025-04-11T03:52:12.7646822Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7646927Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7647051Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7647190Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7647310Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7647525Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7647667Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7647772Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7647911Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7648067Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7648154Z     def test_flash_decoding(
2025-04-11T03:52:12.7648234Z         bsz: int,
2025-04-11T03:52:12.7648317Z         block_size: int,
2025-04-11T03:52:12.7648412Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7648495Z         num_attn_heads: int,
2025-04-11T03:52:12.7648578Z         kv_group_num: int,
2025-04-11T03:52:12.7648667Z         same_context_len: bool,
2025-04-11T03:52:12.7648743Z         q_len: int,
2025-04-11T03:52:12.7648831Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7648918Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7648992Z     ):
2025-04-11T03:52:12.7649106Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7649297Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7649484Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7649658Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7649826Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7649984Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7650054Z     
2025-04-11T03:52:12.7650146Z         torch.manual_seed(123)
2025-04-11T03:52:12.7650236Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7650331Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7650335Z 
2025-04-11T03:52:12.7650490Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7650609Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7650613Z 
2025-04-11T03:52:12.7650691Z device = None
2025-04-11T03:52:12.7650695Z 
2025-04-11T03:52:12.7650820Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7650974Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7651044Z     
2025-04-11T03:52:12.7651122Z         Args:
2025-04-11T03:52:12.7651285Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7651454Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7651557Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7651630Z         """
2025-04-11T03:52:12.7651712Z         _lazy_init()
2025-04-11T03:52:12.7651804Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7651912Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7652020Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7652303Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7652560Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7652718Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7652722Z 
2025-04-11T03:52:12.7652964Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7653132Z ____________ test_flash_decoding[False-False-5-True-1-16-16-32-16] _____________
2025-04-11T03:52:12.7653135Z 
2025-04-11T03:52:12.7653291Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7653452Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7653545Z use_new_kcache_layout = False
2025-04-11T03:52:12.7653637Z 
2025-04-11T03:52:12.7653839Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7653948Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7654067Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7654208Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7654331Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7654445Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7654583Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7654688Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7654827Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7654981Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7655070Z     def test_flash_decoding(
2025-04-11T03:52:12.7655154Z         bsz: int,
2025-04-11T03:52:12.7655234Z         block_size: int,
2025-04-11T03:52:12.7655327Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7655412Z         num_attn_heads: int,
2025-04-11T03:52:12.7655495Z         kv_group_num: int,
2025-04-11T03:52:12.7655591Z         same_context_len: bool,
2025-04-11T03:52:12.7655670Z         q_len: int,
2025-04-11T03:52:12.7655759Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7655848Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7655918Z     ):
2025-04-11T03:52:12.7656036Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7656233Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7656416Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7656588Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7656754Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7656916Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7656986Z     
2025-04-11T03:52:12.7657078Z         torch.manual_seed(123)
2025-04-11T03:52:12.7657172Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7657269Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7657273Z 
2025-04-11T03:52:12.7657428Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7657546Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7657550Z 
2025-04-11T03:52:12.7657627Z device = None
2025-04-11T03:52:12.7657631Z 
2025-04-11T03:52:12.7657749Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7657907Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7657976Z     
2025-04-11T03:52:12.7658056Z         Args:
2025-04-11T03:52:12.7658230Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7658400Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7658609Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7658685Z         """
2025-04-11T03:52:12.7658767Z         _lazy_init()
2025-04-11T03:52:12.7658864Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7658973Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7659078Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7659369Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7659505Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7659663Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7659666Z 
2025-04-11T03:52:12.7660012Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7660180Z _____________ test_flash_decoding[False-False-5-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.7660184Z 
2025-04-11T03:52:12.7660344Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7660508Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7660601Z use_new_kcache_layout = False
2025-04-11T03:52:12.7660605Z 
2025-04-11T03:52:12.7660803Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7660912Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7661032Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7661171Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7661293Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7661410Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7661549Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7661655Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7661799Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7661949Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7662038Z     def test_flash_decoding(
2025-04-11T03:52:12.7662119Z         bsz: int,
2025-04-11T03:52:12.7662201Z         block_size: int,
2025-04-11T03:52:12.7662296Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7662380Z         num_attn_heads: int,
2025-04-11T03:52:12.7662464Z         kv_group_num: int,
2025-04-11T03:52:12.7662554Z         same_context_len: bool,
2025-04-11T03:52:12.7662629Z         q_len: int,
2025-04-11T03:52:12.7662715Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7662804Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7662877Z     ):
2025-04-11T03:52:12.7662990Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7663184Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7663372Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7663542Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7663710Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7663871Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7663940Z     
2025-04-11T03:52:12.7664033Z         torch.manual_seed(123)
2025-04-11T03:52:12.7664121Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7664218Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7664222Z 
2025-04-11T03:52:12.7664378Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7664496Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7664500Z 
2025-04-11T03:52:12.7664577Z device = None
2025-04-11T03:52:12.7664581Z 
2025-04-11T03:52:12.7664797Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7664950Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7665020Z     
2025-04-11T03:52:12.7665100Z         Args:
2025-04-11T03:52:12.7665266Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7665435Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7665541Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7665614Z         """
2025-04-11T03:52:12.7665696Z         _lazy_init()
2025-04-11T03:52:12.7665791Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7665898Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7666107Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7666395Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7666532Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7666690Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7668802Z 
2025-04-11T03:52:12.7669062Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7669230Z _____________ test_flash_decoding[False-False-5-True-4-16-8-16-16] _____________
2025-04-11T03:52:12.7669234Z 
2025-04-11T03:52:12.7669391Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7669555Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7669647Z use_new_kcache_layout = False
2025-04-11T03:52:12.7669653Z 
2025-04-11T03:52:12.7669854Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7669958Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7670085Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7670226Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7670345Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7670502Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7670641Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7670752Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7670888Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7671042Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7671129Z     def test_flash_decoding(
2025-04-11T03:52:12.7671210Z         bsz: int,
2025-04-11T03:52:12.7671293Z         block_size: int,
2025-04-11T03:52:12.7671382Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7671469Z         num_attn_heads: int,
2025-04-11T03:52:12.7671553Z         kv_group_num: int,
2025-04-11T03:52:12.7671645Z         same_context_len: bool,
2025-04-11T03:52:12.7671721Z         q_len: int,
2025-04-11T03:52:12.7671809Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7671900Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7671973Z     ):
2025-04-11T03:52:12.7672087Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7672280Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7672464Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7672636Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7672797Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7672959Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7673030Z     
2025-04-11T03:52:12.7673124Z         torch.manual_seed(123)
2025-04-11T03:52:12.7673327Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7673425Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7673433Z 
2025-04-11T03:52:12.7673590Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7673707Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7673710Z 
2025-04-11T03:52:12.7673794Z device = None
2025-04-11T03:52:12.7673798Z 
2025-04-11T03:52:12.7673916Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7674068Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7674140Z     
2025-04-11T03:52:12.7674219Z         Args:
2025-04-11T03:52:12.7674444Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7674611Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7674723Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7674800Z         """
2025-04-11T03:52:12.7674882Z         _lazy_init()
2025-04-11T03:52:12.7674975Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7675076Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7675313Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7675601Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7675743Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7675902Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7675906Z 
2025-04-11T03:52:12.7676158Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7676325Z _____________ test_flash_decoding[False-False-5-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.7676329Z 
2025-04-11T03:52:12.7676487Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7676648Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7676740Z use_new_kcache_layout = False
2025-04-11T03:52:12.7676754Z 
2025-04-11T03:52:12.7676954Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7677062Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7677184Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7677324Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7677444Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7677560Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7677697Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7677807Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7677946Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7678101Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7678189Z     def test_flash_decoding(
2025-04-11T03:52:12.7678271Z         bsz: int,
2025-04-11T03:52:12.7678352Z         block_size: int,
2025-04-11T03:52:12.7678441Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7678531Z         num_attn_heads: int,
2025-04-11T03:52:12.7678614Z         kv_group_num: int,
2025-04-11T03:52:12.7678703Z         same_context_len: bool,
2025-04-11T03:52:12.7678779Z         q_len: int,
2025-04-11T03:52:12.7678865Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7678955Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7679027Z     ):
2025-04-11T03:52:12.7679138Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7679332Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7679622Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7679797Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7679961Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7680124Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7680195Z     
2025-04-11T03:52:12.7680285Z         torch.manual_seed(123)
2025-04-11T03:52:12.7680373Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7680467Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7680471Z 
2025-04-11T03:52:12.7680628Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7680794Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7680798Z 
2025-04-11T03:52:12.7680879Z device = None
2025-04-11T03:52:12.7680883Z 
2025-04-11T03:52:12.7681005Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7681159Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7681229Z     
2025-04-11T03:52:12.7681307Z         Args:
2025-04-11T03:52:12.7681525Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7681694Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7681804Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7681878Z         """
2025-04-11T03:52:12.7681960Z         _lazy_init()
2025-04-11T03:52:12.7682057Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7682162Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7682269Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7682551Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7682697Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7682852Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7682856Z 
2025-04-11T03:52:12.7683103Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7683268Z _____________ test_flash_decoding[False-False-5-True-4-16-8-32-16] _____________
2025-04-11T03:52:12.7683272Z 
2025-04-11T03:52:12.7683427Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7683590Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7683683Z use_new_kcache_layout = False
2025-04-11T03:52:12.7683688Z 
2025-04-11T03:52:12.7683886Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7683992Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7684119Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7684260Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7684379Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7684494Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7684637Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7684741Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7684875Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7685031Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7685117Z     def test_flash_decoding(
2025-04-11T03:52:12.7685196Z         bsz: int,
2025-04-11T03:52:12.7685281Z         block_size: int,
2025-04-11T03:52:12.7685370Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7685458Z         num_attn_heads: int,
2025-04-11T03:52:12.7685540Z         kv_group_num: int,
2025-04-11T03:52:12.7685730Z         same_context_len: bool,
2025-04-11T03:52:12.7685811Z         q_len: int,
2025-04-11T03:52:12.7685899Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7685993Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7686066Z     ):
2025-04-11T03:52:12.7686182Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7686374Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7686559Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7686729Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7686892Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7687114Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7687186Z     
2025-04-11T03:52:12.7687278Z         torch.manual_seed(123)
2025-04-11T03:52:12.7687370Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7687468Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7687472Z 
2025-04-11T03:52:12.7687627Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7687797Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7687805Z 
2025-04-11T03:52:12.7687883Z device = None
2025-04-11T03:52:12.7687886Z 
2025-04-11T03:52:12.7688003Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7688156Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7688225Z     
2025-04-11T03:52:12.7688301Z         Args:
2025-04-11T03:52:12.7688470Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7688635Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7688742Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7688818Z         """
2025-04-11T03:52:12.7688902Z         _lazy_init()
2025-04-11T03:52:12.7688998Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7689107Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7689213Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7689496Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7689637Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7689793Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7689797Z 
2025-04-11T03:52:12.7690042Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7690207Z _____________ test_flash_decoding[False-False-5-True-4-16-16-16-7] _____________
2025-04-11T03:52:12.7690211Z 
2025-04-11T03:52:12.7690367Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7690528Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7690622Z use_new_kcache_layout = False
2025-04-11T03:52:12.7690627Z 
2025-04-11T03:52:12.7690829Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7690932Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7691052Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7691188Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7691309Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7691426Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7691565Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7691671Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7691910Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7692071Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7692158Z     def test_flash_decoding(
2025-04-11T03:52:12.7692236Z         bsz: int,
2025-04-11T03:52:12.7692323Z         block_size: int,
2025-04-11T03:52:12.7692410Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7692496Z         num_attn_heads: int,
2025-04-11T03:52:12.7692581Z         kv_group_num: int,
2025-04-11T03:52:12.7692670Z         same_context_len: bool,
2025-04-11T03:52:12.7692745Z         q_len: int,
2025-04-11T03:52:12.7692833Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7692922Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7693050Z     ):
2025-04-11T03:52:12.7693164Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7693358Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7693548Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7693720Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7693890Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7694104Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7694176Z     
2025-04-11T03:52:12.7694265Z         torch.manual_seed(123)
2025-04-11T03:52:12.7694357Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7694453Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7694457Z 
2025-04-11T03:52:12.7694611Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7694723Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7694730Z 
2025-04-11T03:52:12.7694806Z device = None
2025-04-11T03:52:12.7694810Z 
2025-04-11T03:52:12.7694926Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7695085Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7695154Z     
2025-04-11T03:52:12.7695232Z         Args:
2025-04-11T03:52:12.7695398Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7695566Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7695671Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7695744Z         """
2025-04-11T03:52:12.7695827Z         _lazy_init()
2025-04-11T03:52:12.7695922Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7696030Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7696139Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7696416Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7696560Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7696719Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7696723Z 
2025-04-11T03:52:12.7696966Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7697133Z ____________ test_flash_decoding[False-False-5-True-4-16-16-16-16] _____________
2025-04-11T03:52:12.7697137Z 
2025-04-11T03:52:12.7697292Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7697455Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7697549Z use_new_kcache_layout = False
2025-04-11T03:52:12.7697554Z 
2025-04-11T03:52:12.7697753Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7697857Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7698103Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7698247Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7698369Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7698485Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7698623Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7698729Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7698867Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7699024Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7699113Z     def test_flash_decoding(
2025-04-11T03:52:12.7699197Z         bsz: int,
2025-04-11T03:52:12.7699349Z         block_size: int,
2025-04-11T03:52:12.7699444Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7699528Z         num_attn_heads: int,
2025-04-11T03:52:12.7699610Z         kv_group_num: int,
2025-04-11T03:52:12.7699702Z         same_context_len: bool,
2025-04-11T03:52:12.7699785Z         q_len: int,
2025-04-11T03:52:12.7699873Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7699961Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7700090Z     ):
2025-04-11T03:52:12.7700207Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7700404Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7700592Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7700767Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7700936Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7701100Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7701171Z     
2025-04-11T03:52:12.7701263Z         torch.manual_seed(123)
2025-04-11T03:52:12.7701357Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7701452Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7701456Z 
2025-04-11T03:52:12.7701613Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7701730Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7701735Z 
2025-04-11T03:52:12.7701812Z device = None
2025-04-11T03:52:12.7701815Z 
2025-04-11T03:52:12.7701935Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7702090Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7702163Z     
2025-04-11T03:52:12.7702242Z         Args:
2025-04-11T03:52:12.7702414Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7702584Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7702687Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7702764Z         """
2025-04-11T03:52:12.7702847Z         _lazy_init()
2025-04-11T03:52:12.7702942Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7703047Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7703156Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7703440Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7703578Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7703735Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7703739Z 
2025-04-11T03:52:12.7703989Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7704156Z _____________ test_flash_decoding[False-False-5-True-4-16-16-32-7] _____________
2025-04-11T03:52:12.7704160Z 
2025-04-11T03:52:12.7704416Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7704581Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7704673Z use_new_kcache_layout = False
2025-04-11T03:52:12.7704679Z 
2025-04-11T03:52:12.7704878Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7704987Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7705103Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7705241Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7705360Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7705523Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7705663Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7705767Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7705908Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7706064Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7706150Z     def test_flash_decoding(
2025-04-11T03:52:12.7706232Z         bsz: int,
2025-04-11T03:52:12.7706374Z         block_size: int,
2025-04-11T03:52:12.7706467Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7706550Z         num_attn_heads: int,
2025-04-11T03:52:12.7706632Z         kv_group_num: int,
2025-04-11T03:52:12.7706723Z         same_context_len: bool,
2025-04-11T03:52:12.7706800Z         q_len: int,
2025-04-11T03:52:12.7706889Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7706978Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7707050Z     ):
2025-04-11T03:52:12.7707166Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7707357Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7707544Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7707714Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7707879Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7708039Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7708109Z     
2025-04-11T03:52:12.7708203Z         torch.manual_seed(123)
2025-04-11T03:52:12.7708292Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7708390Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7708394Z 
2025-04-11T03:52:12.7708592Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7708712Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7708716Z 
2025-04-11T03:52:12.7708793Z device = None
2025-04-11T03:52:12.7708797Z 
2025-04-11T03:52:12.7708916Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7709074Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7709144Z     
2025-04-11T03:52:12.7709221Z         Args:
2025-04-11T03:52:12.7709389Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7709561Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7709665Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7709740Z         """
2025-04-11T03:52:12.7709825Z         _lazy_init()
2025-04-11T03:52:12.7709921Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7710029Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7710137Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7710421Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7710677Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7710838Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7710842Z 
2025-04-11T03:52:12.7711087Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7711258Z ____________ test_flash_decoding[False-False-5-True-4-16-16-32-16] _____________
2025-04-11T03:52:12.7711261Z 
2025-04-11T03:52:12.7711418Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7711581Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7711677Z use_new_kcache_layout = False
2025-04-11T03:52:12.7711738Z 
2025-04-11T03:52:12.7711937Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7712044Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7712160Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7712300Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7712419Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7712590Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7712732Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7712837Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7712977Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7713128Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7713217Z     def test_flash_decoding(
2025-04-11T03:52:12.7713295Z         bsz: int,
2025-04-11T03:52:12.7713379Z         block_size: int,
2025-04-11T03:52:12.7713470Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7713552Z         num_attn_heads: int,
2025-04-11T03:52:12.7713634Z         kv_group_num: int,
2025-04-11T03:52:12.7713725Z         same_context_len: bool,
2025-04-11T03:52:12.7713803Z         q_len: int,
2025-04-11T03:52:12.7713891Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7713979Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7714052Z     ):
2025-04-11T03:52:12.7714168Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7714359Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7714544Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7714715Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7714883Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7715041Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7715114Z     
2025-04-11T03:52:12.7715206Z         torch.manual_seed(123)
2025-04-11T03:52:12.7715296Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7715391Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7715395Z 
2025-04-11T03:52:12.7715550Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7715669Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7715673Z 
2025-04-11T03:52:12.7715755Z device = None
2025-04-11T03:52:12.7715758Z 
2025-04-11T03:52:12.7715878Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7716028Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7716098Z     
2025-04-11T03:52:12.7716177Z         Args:
2025-04-11T03:52:12.7716339Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7716509Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7716613Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7716795Z         """
2025-04-11T03:52:12.7716880Z         _lazy_init()
2025-04-11T03:52:12.7716974Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7717080Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7717188Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7717472Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7717609Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7717762Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7717769Z 
2025-04-11T03:52:12.7718055Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7718227Z _____________ test_flash_decoding[False-False-5-False-1-16-8-16-7] _____________
2025-04-11T03:52:12.7718231Z 
2025-04-11T03:52:12.7718389Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7718554Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7718647Z use_new_kcache_layout = False
2025-04-11T03:52:12.7718705Z 
2025-04-11T03:52:12.7718904Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7719012Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7719130Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7719272Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7719394Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7719507Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7719647Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7719750Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7719896Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7720045Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7720135Z     def test_flash_decoding(
2025-04-11T03:52:12.7720218Z         bsz: int,
2025-04-11T03:52:12.7720302Z         block_size: int,
2025-04-11T03:52:12.7720394Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7720477Z         num_attn_heads: int,
2025-04-11T03:52:12.7720560Z         kv_group_num: int,
2025-04-11T03:52:12.7720649Z         same_context_len: bool,
2025-04-11T03:52:12.7720725Z         q_len: int,
2025-04-11T03:52:12.7720814Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7720901Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7720971Z     ):
2025-04-11T03:52:12.7721087Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7721278Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7721464Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7721635Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7721801Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7721957Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7722032Z     
2025-04-11T03:52:12.7722119Z         torch.manual_seed(123)
2025-04-11T03:52:12.7722206Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7722300Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7722304Z 
2025-04-11T03:52:12.7722457Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7722571Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7722574Z 
2025-04-11T03:52:12.7722651Z device = None
2025-04-11T03:52:12.7722654Z 
2025-04-11T03:52:12.7722776Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7723060Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7723135Z     
2025-04-11T03:52:12.7723216Z         Args:
2025-04-11T03:52:12.7723383Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7723554Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7723658Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7723737Z         """
2025-04-11T03:52:12.7723815Z         _lazy_init()
2025-04-11T03:52:12.7723911Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7724018Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7724177Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7724464Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7724602Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7724759Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7724766Z 
2025-04-11T03:52:12.7725050Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7725218Z ____________ test_flash_decoding[False-False-5-False-1-16-8-16-16] _____________
2025-04-11T03:52:12.7725222Z 
2025-04-11T03:52:12.7725375Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7725595Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7725694Z use_new_kcache_layout = False
2025-04-11T03:52:12.7725701Z 
2025-04-11T03:52:12.7725900Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7726007Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7726124Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7726263Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7726382Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7726497Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7726637Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7726739Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7726877Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7727027Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7727114Z     def test_flash_decoding(
2025-04-11T03:52:12.7727193Z         bsz: int,
2025-04-11T03:52:12.7727275Z         block_size: int,
2025-04-11T03:52:12.7727368Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7727450Z         num_attn_heads: int,
2025-04-11T03:52:12.7727531Z         kv_group_num: int,
2025-04-11T03:52:12.7727622Z         same_context_len: bool,
2025-04-11T03:52:12.7727700Z         q_len: int,
2025-04-11T03:52:12.7727789Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7727876Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7727954Z     ):
2025-04-11T03:52:12.7728065Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7728256Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7728442Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7728611Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7728775Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7728933Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7729010Z     
2025-04-11T03:52:12.7729098Z         torch.manual_seed(123)
2025-04-11T03:52:12.7729297Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7729399Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7729403Z 
2025-04-11T03:52:12.7729559Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7729677Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7729680Z 
2025-04-11T03:52:12.7729759Z device = None
2025-04-11T03:52:12.7729763Z 
2025-04-11T03:52:12.7729887Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7730040Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7730110Z     
2025-04-11T03:52:12.7730189Z         Args:
2025-04-11T03:52:12.7730356Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7730575Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7730681Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7730761Z         """
2025-04-11T03:52:12.7730840Z         _lazy_init()
2025-04-11T03:52:12.7730938Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7731045Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7731203Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7731487Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7731625Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7731784Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7731788Z 
2025-04-11T03:52:12.7732029Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7732195Z _____________ test_flash_decoding[False-False-5-False-1-16-8-32-7] _____________
2025-04-11T03:52:12.7732199Z 
2025-04-11T03:52:12.7732353Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7732518Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7732611Z use_new_kcache_layout = False
2025-04-11T03:52:12.7732617Z 
2025-04-11T03:52:12.7732814Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7732922Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7733038Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7733180Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7733296Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7733408Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7733552Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7733659Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7733796Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7733952Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7734042Z     def test_flash_decoding(
2025-04-11T03:52:12.7734122Z         bsz: int,
2025-04-11T03:52:12.7734205Z         block_size: int,
2025-04-11T03:52:12.7734297Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7734383Z         num_attn_heads: int,
2025-04-11T03:52:12.7734472Z         kv_group_num: int,
2025-04-11T03:52:12.7734559Z         same_context_len: bool,
2025-04-11T03:52:12.7734636Z         q_len: int,
2025-04-11T03:52:12.7734725Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7734813Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7734888Z     ):
2025-04-11T03:52:12.7734998Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7735188Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7735481Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7735656Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7735822Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7735981Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7736054Z     
2025-04-11T03:52:12.7736143Z         torch.manual_seed(123)
2025-04-11T03:52:12.7736232Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7736329Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7736333Z 
2025-04-11T03:52:12.7736487Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7736728Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7736732Z 
2025-04-11T03:52:12.7736813Z device = None
2025-04-11T03:52:12.7736817Z 
2025-04-11T03:52:12.7736940Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7737092Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7737162Z     
2025-04-11T03:52:12.7737242Z         Args:
2025-04-11T03:52:12.7737408Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7737637Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7737744Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7737819Z         """
2025-04-11T03:52:12.7737898Z         _lazy_init()
2025-04-11T03:52:12.7737995Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7738103Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7738209Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7738495Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7738636Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7738797Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7738801Z 
2025-04-11T03:52:12.7739039Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7739206Z ____________ test_flash_decoding[False-False-5-False-1-16-8-32-16] _____________
2025-04-11T03:52:12.7739213Z 
2025-04-11T03:52:12.7739361Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7739524Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7739620Z use_new_kcache_layout = False
2025-04-11T03:52:12.7739625Z 
2025-04-11T03:52:12.7739823Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7739930Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7740049Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7740190Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7740305Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7740419Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7740563Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7740668Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7740808Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7740958Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7741049Z     def test_flash_decoding(
2025-04-11T03:52:12.7741125Z         bsz: int,
2025-04-11T03:52:12.7741214Z         block_size: int,
2025-04-11T03:52:12.7741308Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7741390Z         num_attn_heads: int,
2025-04-11T03:52:12.7741479Z         kv_group_num: int,
2025-04-11T03:52:12.7741565Z         same_context_len: bool,
2025-04-11T03:52:12.7741744Z         q_len: int,
2025-04-11T03:52:12.7741841Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7741929Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7742005Z     ):
2025-04-11T03:52:12.7742121Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7742316Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7742506Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7742680Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7742848Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7743056Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7743130Z     
2025-04-11T03:52:12.7743217Z         torch.manual_seed(123)
2025-04-11T03:52:12.7743310Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7743408Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7743412Z 
2025-04-11T03:52:12.7743571Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7743745Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7743749Z 
2025-04-11T03:52:12.7743829Z device = None
2025-04-11T03:52:12.7743834Z 
2025-04-11T03:52:12.7743956Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7744109Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7744186Z     
2025-04-11T03:52:12.7744263Z         Args:
2025-04-11T03:52:12.7744430Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7744600Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7744705Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7744785Z         """
2025-04-11T03:52:12.7744867Z         _lazy_init()
2025-04-11T03:52:12.7744963Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7745071Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7745183Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7745468Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7745602Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7745766Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7745770Z 
2025-04-11T03:52:12.7746007Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7746181Z ____________ test_flash_decoding[False-False-5-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.7746185Z 
2025-04-11T03:52:12.7746338Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7746505Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7746600Z use_new_kcache_layout = False
2025-04-11T03:52:12.7746605Z 
2025-04-11T03:52:12.7746803Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7746912Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7747030Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7747174Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7747290Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7747405Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7747548Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7747654Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7747790Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7748058Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7748154Z     def test_flash_decoding(
2025-04-11T03:52:12.7748232Z         bsz: int,
2025-04-11T03:52:12.7748316Z         block_size: int,
2025-04-11T03:52:12.7748451Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7748536Z         num_attn_heads: int,
2025-04-11T03:52:12.7748624Z         kv_group_num: int,
2025-04-11T03:52:12.7748711Z         same_context_len: bool,
2025-04-11T03:52:12.7748786Z         q_len: int,
2025-04-11T03:52:12.7748877Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7748966Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7749041Z     ):
2025-04-11T03:52:12.7749210Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7749403Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7749590Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7749761Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7749927Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7750145Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7750221Z     
2025-04-11T03:52:12.7750306Z         torch.manual_seed(123)
2025-04-11T03:52:12.7750396Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7750495Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7750498Z 
2025-04-11T03:52:12.7750653Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7750771Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7750774Z 
2025-04-11T03:52:12.7750852Z device = None
2025-04-11T03:52:12.7750856Z 
2025-04-11T03:52:12.7750974Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7751126Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7751198Z     
2025-04-11T03:52:12.7751272Z         Args:
2025-04-11T03:52:12.7751437Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7751610Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7751714Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7751792Z         """
2025-04-11T03:52:12.7751871Z         _lazy_init()
2025-04-11T03:52:12.7751965Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7752072Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7752180Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7752465Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7752604Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7752767Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7752771Z 
2025-04-11T03:52:12.7753009Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7753187Z ____________ test_flash_decoding[False-False-5-False-1-16-16-16-16] ____________
2025-04-11T03:52:12.7753190Z 
2025-04-11T03:52:12.7753343Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7753508Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7753601Z use_new_kcache_layout = False
2025-04-11T03:52:12.7753606Z 
2025-04-11T03:52:12.7753804Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7753912Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7754029Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7754285Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7754405Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7754520Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7754662Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7754766Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7754908Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7755057Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7755148Z     def test_flash_decoding(
2025-04-11T03:52:12.7755225Z         bsz: int,
2025-04-11T03:52:12.7755361Z         block_size: int,
2025-04-11T03:52:12.7755455Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7755539Z         num_attn_heads: int,
2025-04-11T03:52:12.7755625Z         kv_group_num: int,
2025-04-11T03:52:12.7755712Z         same_context_len: bool,
2025-04-11T03:52:12.7755791Z         q_len: int,
2025-04-11T03:52:12.7755884Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7755972Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7756048Z     ):
2025-04-11T03:52:12.7756230Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7756430Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7756609Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7756779Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7756944Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7757103Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7757177Z     
2025-04-11T03:52:12.7757263Z         torch.manual_seed(123)
2025-04-11T03:52:12.7757361Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7757452Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7757456Z 
2025-04-11T03:52:12.7757614Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7757737Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7757741Z 
2025-04-11T03:52:12.7757821Z device = None
2025-04-11T03:52:12.7757824Z 
2025-04-11T03:52:12.7757944Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7758095Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7758167Z     
2025-04-11T03:52:12.7758241Z         Args:
2025-04-11T03:52:12.7758406Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7758575Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7758679Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7758760Z         """
2025-04-11T03:52:12.7758838Z         _lazy_init()
2025-04-11T03:52:12.7758939Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7759047Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7759154Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7759440Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7759577Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7759740Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7759744Z 
2025-04-11T03:52:12.7759981Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7760155Z ____________ test_flash_decoding[False-False-5-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.7760159Z 
2025-04-11T03:52:12.7760419Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7760586Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7760679Z use_new_kcache_layout = False
2025-04-11T03:52:12.7760685Z 
2025-04-11T03:52:12.7760884Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7760993Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7761111Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7761251Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7761367Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7761484Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7761674Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7761780Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7761924Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7762084Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7762180Z     def test_flash_decoding(
2025-04-11T03:52:12.7762260Z         bsz: int,
2025-04-11T03:52:12.7762405Z         block_size: int,
2025-04-11T03:52:12.7762501Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7762584Z         num_attn_heads: int,
2025-04-11T03:52:12.7762672Z         kv_group_num: int,
2025-04-11T03:52:12.7762760Z         same_context_len: bool,
2025-04-11T03:52:12.7762838Z         q_len: int,
2025-04-11T03:52:12.7762929Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7763017Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7763093Z     ):
2025-04-11T03:52:12.7763203Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7763398Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7763580Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7763750Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7763919Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7764076Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7764152Z     
2025-04-11T03:52:12.7764238Z         torch.manual_seed(123)
2025-04-11T03:52:12.7764334Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7764423Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7764427Z 
2025-04-11T03:52:12.7764581Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7764700Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7764704Z 
2025-04-11T03:52:12.7764780Z device = None
2025-04-11T03:52:12.7764784Z 
2025-04-11T03:52:12.7764907Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7765057Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7765136Z     
2025-04-11T03:52:12.7765209Z         Args:
2025-04-11T03:52:12.7765374Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7765542Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7765648Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7765725Z         """
2025-04-11T03:52:12.7765804Z         _lazy_init()
2025-04-11T03:52:12.7765902Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7766006Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7766113Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7766397Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7766640Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7766804Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7766808Z 
2025-04-11T03:52:12.7767044Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7767219Z ____________ test_flash_decoding[False-False-5-False-1-16-16-32-16] ____________
2025-04-11T03:52:12.7767222Z 
2025-04-11T03:52:12.7767374Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7767544Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7767634Z use_new_kcache_layout = False
2025-04-11T03:52:12.7767689Z 
2025-04-11T03:52:12.7767891Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7768002Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7768120Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7768265Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7768382Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7768505Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7768708Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7768814Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7768955Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7769103Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7769193Z     def test_flash_decoding(
2025-04-11T03:52:12.7769269Z         bsz: int,
2025-04-11T03:52:12.7769354Z         block_size: int,
2025-04-11T03:52:12.7769446Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7769530Z         num_attn_heads: int,
2025-04-11T03:52:12.7769617Z         kv_group_num: int,
2025-04-11T03:52:12.7769703Z         same_context_len: bool,
2025-04-11T03:52:12.7769786Z         q_len: int,
2025-04-11T03:52:12.7769873Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7769959Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7770036Z     ):
2025-04-11T03:52:12.7770146Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7770340Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7770521Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7770693Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7770861Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7771020Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7771097Z     
2025-04-11T03:52:12.7771187Z         torch.manual_seed(123)
2025-04-11T03:52:12.7771282Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7771375Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7771379Z 
2025-04-11T03:52:12.7771533Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7771654Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7771658Z 
2025-04-11T03:52:12.7771736Z device = None
2025-04-11T03:52:12.7771739Z 
2025-04-11T03:52:12.7771861Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7772009Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7772087Z     
2025-04-11T03:52:12.7772161Z         Args:
2025-04-11T03:52:12.7772324Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7772491Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7772596Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7772783Z         """
2025-04-11T03:52:12.7772865Z         _lazy_init()
2025-04-11T03:52:12.7772964Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7773068Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7773176Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7773464Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7773598Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7773760Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7773764Z 
2025-04-11T03:52:12.7774002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7774235Z _____________ test_flash_decoding[False-False-5-False-4-16-8-16-7] _____________
2025-04-11T03:52:12.7774239Z 
2025-04-11T03:52:12.7774391Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7774558Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7774651Z use_new_kcache_layout = False
2025-04-11T03:52:12.7774700Z 
2025-04-11T03:52:12.7774900Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7775007Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7775125Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7775272Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7775389Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7775508Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7775645Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7775750Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7775891Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7776043Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7776137Z     def test_flash_decoding(
2025-04-11T03:52:12.7776212Z         bsz: int,
2025-04-11T03:52:12.7776302Z         block_size: int,
2025-04-11T03:52:12.7776393Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7776479Z         num_attn_heads: int,
2025-04-11T03:52:12.7776567Z         kv_group_num: int,
2025-04-11T03:52:12.7776652Z         same_context_len: bool,
2025-04-11T03:52:12.7776731Z         q_len: int,
2025-04-11T03:52:12.7776820Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7776909Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7776984Z     ):
2025-04-11T03:52:12.7777098Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7777294Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7777483Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7777658Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7777822Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7777980Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7778057Z     
2025-04-11T03:52:12.7778144Z         torch.manual_seed(123)
2025-04-11T03:52:12.7778236Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7778326Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7778330Z 
2025-04-11T03:52:12.7778488Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7778603Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7778608Z 
2025-04-11T03:52:12.7778684Z device = None
2025-04-11T03:52:12.7778688Z 
2025-04-11T03:52:12.7778812Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7779067Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7779144Z     
2025-04-11T03:52:12.7779219Z         Args:
2025-04-11T03:52:12.7779391Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7779562Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7779667Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7779747Z         """
2025-04-11T03:52:12.7779825Z         _lazy_init()
2025-04-11T03:52:12.7779925Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7780028Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7780183Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7780475Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7780613Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7780772Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7780777Z 
2025-04-11T03:52:12.7781015Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7781244Z ____________ test_flash_decoding[False-False-5-False-4-16-8-16-16] _____________
2025-04-11T03:52:12.7781247Z 
2025-04-11T03:52:12.7781401Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7781574Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7781665Z use_new_kcache_layout = False
2025-04-11T03:52:12.7781671Z 
2025-04-11T03:52:12.7781873Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7781984Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7782104Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7782254Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7782375Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7782495Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7782638Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7782745Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7782891Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7783044Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7783140Z     def test_flash_decoding(
2025-04-11T03:52:12.7783221Z         bsz: int,
2025-04-11T03:52:12.7783311Z         block_size: int,
2025-04-11T03:52:12.7783405Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7783490Z         num_attn_heads: int,
2025-04-11T03:52:12.7783582Z         kv_group_num: int,
2025-04-11T03:52:12.7783673Z         same_context_len: bool,
2025-04-11T03:52:12.7783760Z         q_len: int,
2025-04-11T03:52:12.7783850Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7783941Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7784021Z     ):
2025-04-11T03:52:12.7784138Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7784339Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7784522Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7784699Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7784866Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7785028Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7785107Z     
2025-04-11T03:52:12.7785197Z         torch.manual_seed(123)
2025-04-11T03:52:12.7785292Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7785500Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7785504Z 
2025-04-11T03:52:12.7785667Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7785783Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7785787Z 
2025-04-11T03:52:12.7785867Z device = None
2025-04-11T03:52:12.7785871Z 
2025-04-11T03:52:12.7785992Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7786143Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7786217Z     
2025-04-11T03:52:12.7786293Z         Args:
2025-04-11T03:52:12.7786462Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7786677Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7786784Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7786863Z         """
2025-04-11T03:52:12.7786946Z         _lazy_init()
2025-04-11T03:52:12.7787045Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7787148Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7787302Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7787590Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7787724Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7787886Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7787890Z 
2025-04-11T03:52:12.7788126Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7788300Z _____________ test_flash_decoding[False-False-5-False-4-16-8-32-7] _____________
2025-04-11T03:52:12.7788304Z 
2025-04-11T03:52:12.7788494Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7788664Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7788752Z use_new_kcache_layout = False
2025-04-11T03:52:12.7788757Z 
2025-04-11T03:52:12.7788957Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7789066Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7789185Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7789329Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7789445Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7789558Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7789698Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7789806Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7789944Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7790095Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7790188Z     def test_flash_decoding(
2025-04-11T03:52:12.7790263Z         bsz: int,
2025-04-11T03:52:12.7790349Z         block_size: int,
2025-04-11T03:52:12.7790438Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7790522Z         num_attn_heads: int,
2025-04-11T03:52:12.7790610Z         kv_group_num: int,
2025-04-11T03:52:12.7790697Z         same_context_len: bool,
2025-04-11T03:52:12.7790776Z         q_len: int,
2025-04-11T03:52:12.7790864Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7790950Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7791026Z     ):
2025-04-11T03:52:12.7791137Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7791330Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7791622Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7791808Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7791971Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7792132Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7792209Z     
2025-04-11T03:52:12.7792297Z         torch.manual_seed(123)
2025-04-11T03:52:12.7792392Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7792486Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7792489Z 
2025-04-11T03:52:12.7792647Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7792832Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7792836Z 
2025-04-11T03:52:12.7792915Z device = None
2025-04-11T03:52:12.7792922Z 
2025-04-11T03:52:12.7793040Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7793198Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7793282Z     
2025-04-11T03:52:12.7793359Z         Args:
2025-04-11T03:52:12.7793533Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7793763Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7793871Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7793950Z         """
2025-04-11T03:52:12.7794029Z         _lazy_init()
2025-04-11T03:52:12.7794128Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7794230Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7794338Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7794622Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7794758Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7794923Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7794927Z 
2025-04-11T03:52:12.7795164Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7795339Z ____________ test_flash_decoding[False-False-5-False-4-16-8-32-16] _____________
2025-04-11T03:52:12.7795343Z 
2025-04-11T03:52:12.7795492Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7795660Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7795748Z use_new_kcache_layout = False
2025-04-11T03:52:12.7795754Z 
2025-04-11T03:52:12.7795954Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7796058Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7796175Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7796321Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7796438Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7796557Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7796695Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7796803Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7796937Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7797085Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7797179Z     def test_flash_decoding(
2025-04-11T03:52:12.7797253Z         bsz: int,
2025-04-11T03:52:12.7797340Z         block_size: int,
2025-04-11T03:52:12.7797429Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7797511Z         num_attn_heads: int,
2025-04-11T03:52:12.7797598Z         kv_group_num: int,
2025-04-11T03:52:12.7797685Z         same_context_len: bool,
2025-04-11T03:52:12.7797870Z         q_len: int,
2025-04-11T03:52:12.7797960Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7798049Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7798124Z     ):
2025-04-11T03:52:12.7798238Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7798436Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7798617Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7798790Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7798952Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7799160Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7799232Z     
2025-04-11T03:52:12.7799318Z         torch.manual_seed(123)
2025-04-11T03:52:12.7799412Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7799508Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7799512Z 
2025-04-11T03:52:12.7799671Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7799841Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7799845Z 
2025-04-11T03:52:12.7799930Z device = None
2025-04-11T03:52:12.7799933Z 
2025-04-11T03:52:12.7800051Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7800205Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7800280Z     
2025-04-11T03:52:12.7800355Z         Args:
2025-04-11T03:52:12.7800526Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7800695Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7800804Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7800879Z         """
2025-04-11T03:52:12.7800960Z         _lazy_init()
2025-04-11T03:52:12.7801061Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7801163Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7801274Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7801555Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7801690Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7801850Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7801854Z 
2025-04-11T03:52:12.7802090Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7802266Z ____________ test_flash_decoding[False-False-5-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.7802270Z 
2025-04-11T03:52:12.7802422Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7802589Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7802679Z use_new_kcache_layout = False
2025-04-11T03:52:12.7802685Z 
2025-04-11T03:52:12.7802888Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7802991Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7803107Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7803255Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7803372Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7803488Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7803628Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7803739Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7803874Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7804135Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7804232Z     def test_flash_decoding(
2025-04-11T03:52:12.7804310Z         bsz: int,
2025-04-11T03:52:12.7804397Z         block_size: int,
2025-04-11T03:52:12.7804486Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7804569Z         num_attn_heads: int,
2025-04-11T03:52:12.7804655Z         kv_group_num: int,
2025-04-11T03:52:12.7804739Z         same_context_len: bool,
2025-04-11T03:52:12.7804817Z         q_len: int,
2025-04-11T03:52:12.7804902Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7804996Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7805068Z     ):
2025-04-11T03:52:12.7805231Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7805433Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7805615Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7805791Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7805956Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7806174Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7806247Z     
2025-04-11T03:52:12.7806334Z         torch.manual_seed(123)
2025-04-11T03:52:12.7806428Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7806519Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7806523Z 
2025-04-11T03:52:12.7806680Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7806794Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7806797Z 
2025-04-11T03:52:12.7806879Z device = None
2025-04-11T03:52:12.7806883Z 
2025-04-11T03:52:12.7807001Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7807155Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7807229Z     
2025-04-11T03:52:12.7807304Z         Args:
2025-04-11T03:52:12.7807473Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7807639Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7807748Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7807820Z         """
2025-04-11T03:52:12.7807900Z         _lazy_init()
2025-04-11T03:52:12.7808001Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7808105Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7808214Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7808496Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7808634Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7808795Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7808799Z 
2025-04-11T03:52:12.7809042Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7809216Z ____________ test_flash_decoding[False-False-5-False-4-16-16-16-16] ____________
2025-04-11T03:52:12.7809220Z 
2025-04-11T03:52:12.7809373Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7809542Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7809633Z use_new_kcache_layout = False
2025-04-11T03:52:12.7809638Z 
2025-04-11T03:52:12.7809840Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7809943Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7810061Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7810316Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7810434Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7810555Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7810694Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7810804Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7810941Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7811092Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7811183Z     def test_flash_decoding(
2025-04-11T03:52:12.7811258Z         bsz: int,
2025-04-11T03:52:12.7811396Z         block_size: int,
2025-04-11T03:52:12.7811485Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7811573Z         num_attn_heads: int,
2025-04-11T03:52:12.7811657Z         kv_group_num: int,
2025-04-11T03:52:12.7811742Z         same_context_len: bool,
2025-04-11T03:52:12.7811828Z         q_len: int,
2025-04-11T03:52:12.7811913Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7812004Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7812076Z     ):
2025-04-11T03:52:12.7812251Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7812451Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7812635Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7812810Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7812976Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7813138Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7813210Z     
2025-04-11T03:52:12.7813296Z         torch.manual_seed(123)
2025-04-11T03:52:12.7813390Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7813484Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7813488Z 
2025-04-11T03:52:12.7813647Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7813762Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7813766Z 
2025-04-11T03:52:12.7813848Z device = None
2025-04-11T03:52:12.7813851Z 
2025-04-11T03:52:12.7813970Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7814119Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7814196Z     
2025-04-11T03:52:12.7814272Z         Args:
2025-04-11T03:52:12.7814443Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7814609Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7814718Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7814791Z         """
2025-04-11T03:52:12.7814873Z         _lazy_init()
2025-04-11T03:52:12.7814974Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7815075Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7815186Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7815468Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7815607Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7815762Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7815766Z 
2025-04-11T03:52:12.7816006Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7816183Z ____________ test_flash_decoding[False-False-5-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.7816187Z 
2025-04-11T03:52:12.7816455Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7816628Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7816720Z use_new_kcache_layout = False
2025-04-11T03:52:12.7816725Z 
2025-04-11T03:52:12.7816929Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7817034Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7817158Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7817296Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7817411Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7817529Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7817734Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7817844Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7817982Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7818134Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7818229Z     def test_flash_decoding(
2025-04-11T03:52:12.7818305Z         bsz: int,
2025-04-11T03:52:12.7818451Z         block_size: int,
2025-04-11T03:52:12.7818544Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7818633Z         num_attn_heads: int,
2025-04-11T03:52:12.7818716Z         kv_group_num: int,
2025-04-11T03:52:12.7818801Z         same_context_len: bool,
2025-04-11T03:52:12.7818881Z         q_len: int,
2025-04-11T03:52:12.7818967Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7819058Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7819129Z     ):
2025-04-11T03:52:12.7819241Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7819444Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7819626Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7819803Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7819964Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7820123Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7820195Z     
2025-04-11T03:52:12.7820281Z         torch.manual_seed(123)
2025-04-11T03:52:12.7820376Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7820470Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7820474Z 
2025-04-11T03:52:12.7820632Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7820742Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7820748Z 
2025-04-11T03:52:12.7820828Z device = None
2025-04-11T03:52:12.7820831Z 
2025-04-11T03:52:12.7820952Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7821110Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7821182Z     
2025-04-11T03:52:12.7821258Z         Args:
2025-04-11T03:52:12.7821428Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7821595Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7821706Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7821778Z         """
2025-04-11T03:52:12.7821857Z         _lazy_init()
2025-04-11T03:52:12.7821959Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7822062Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7822171Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7822451Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7822703Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7822863Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7822867Z 
2025-04-11T03:52:12.7823106Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7823281Z ____________ test_flash_decoding[False-False-5-False-4-16-16-32-16] ____________
2025-04-11T03:52:12.7823285Z 
2025-04-11T03:52:12.7823439Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7823607Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7823694Z use_new_kcache_layout = False
2025-04-11T03:52:12.7823698Z 
2025-04-11T03:52:12.7823964Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7824079Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7824208Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7824352Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7824473Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7824599Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7824788Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7824898Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7825036Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7825191Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7825282Z     def test_flash_decoding(
2025-04-11T03:52:12.7825362Z         bsz: int,
2025-04-11T03:52:12.7825453Z         block_size: int,
2025-04-11T03:52:12.7825548Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7825638Z         num_attn_heads: int,
2025-04-11T03:52:12.7825723Z         kv_group_num: int,
2025-04-11T03:52:12.7825810Z         same_context_len: bool,
2025-04-11T03:52:12.7825898Z         q_len: int,
2025-04-11T03:52:12.7825985Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7826087Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7826183Z     ):
2025-04-11T03:52:12.7826305Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7826507Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7826689Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7826870Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7827031Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7827192Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7827264Z     
2025-04-11T03:52:12.7827352Z         torch.manual_seed(123)
2025-04-11T03:52:12.7827449Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7827548Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7827552Z 
2025-04-11T03:52:12.7827713Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7827832Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7827836Z 
2025-04-11T03:52:12.7827916Z device = None
2025-04-11T03:52:12.7827920Z 
2025-04-11T03:52:12.7828040Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7828195Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7828266Z     
2025-04-11T03:52:12.7828340Z         Args:
2025-04-11T03:52:12.7828556Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7828725Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7828835Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7828909Z         """
2025-04-11T03:52:12.7829118Z         _lazy_init()
2025-04-11T03:52:12.7829225Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7829333Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7829446Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7829730Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7829869Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7830026Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7830030Z 
2025-04-11T03:52:12.7830274Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7830506Z ________________ test_copy_kv_to_caches[True-1-True-16-16-16-7] ________________
2025-04-11T03:52:12.7830510Z 
2025-04-11T03:52:12.7830661Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7830821Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7830825Z 
2025-04-11T03:52:12.7831027Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7831199Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7831325Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7831463Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7831579Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7831716Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7831829Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7831986Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7832079Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7832157Z         bsz: int,
2025-04-11T03:52:12.7832244Z         block_size: int,
2025-04-11T03:52:12.7832336Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7832417Z         num_kv_heads: int,
2025-04-11T03:52:12.7832506Z         same_context_len: bool,
2025-04-11T03:52:12.7832585Z         n_tokens: int,
2025-04-11T03:52:12.7832680Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7832754Z     ):
2025-04-11T03:52:12.7832840Z         torch.manual_seed(123)
2025-04-11T03:52:12.7832933Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7833024Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7833027Z 
2025-04-11T03:52:12.7833187Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7833303Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7833309Z 
2025-04-11T03:52:12.7833389Z device = None
2025-04-11T03:52:12.7833393Z 
2025-04-11T03:52:12.7833511Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7833666Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7833740Z     
2025-04-11T03:52:12.7833813Z         Args:
2025-04-11T03:52:12.7833981Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7834148Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7834258Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7834331Z         """
2025-04-11T03:52:12.7834410Z         _lazy_init()
2025-04-11T03:52:12.7834510Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7834615Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7834725Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7835013Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7835152Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7835490Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7835495Z 
2025-04-11T03:52:12.7835738Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7835900Z _______________ test_copy_kv_to_caches[True-1-True-16-16-16-32] ________________
2025-04-11T03:52:12.7835905Z 
2025-04-11T03:52:12.7836054Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7836211Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7836214Z 
2025-04-11T03:52:12.7836412Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7836579Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7836709Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7836854Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7836972Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7837108Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7837222Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7837431Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7837526Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7837602Z         bsz: int,
2025-04-11T03:52:12.7837685Z         block_size: int,
2025-04-11T03:52:12.7837782Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7837866Z         num_kv_heads: int,
2025-04-11T03:52:12.7837955Z         same_context_len: bool,
2025-04-11T03:52:12.7838033Z         n_tokens: int,
2025-04-11T03:52:12.7838124Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7838199Z     ):
2025-04-11T03:52:12.7838289Z         torch.manual_seed(123)
2025-04-11T03:52:12.7838386Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7838477Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7838481Z 
2025-04-11T03:52:12.7838639Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7838750Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7838756Z 
2025-04-11T03:52:12.7838835Z device = None
2025-04-11T03:52:12.7838839Z 
2025-04-11T03:52:12.7838960Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7839111Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7839188Z     
2025-04-11T03:52:12.7839262Z         Args:
2025-04-11T03:52:12.7839432Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7839597Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7839713Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7839788Z         """
2025-04-11T03:52:12.7839865Z         _lazy_init()
2025-04-11T03:52:12.7839966Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7840069Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7840180Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7840460Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7840596Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7840754Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7840758Z 
2025-04-11T03:52:12.7840996Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7841153Z ________________ test_copy_kv_to_caches[True-1-True-16-16-32-7] ________________
2025-04-11T03:52:12.7841158Z 
2025-04-11T03:52:12.7841307Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7841460Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7841588Z 
2025-04-11T03:52:12.7841792Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7841903Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7842030Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7842166Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7842284Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7842422Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7842534Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7842682Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7842830Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7842906Z         bsz: int,
2025-04-11T03:52:12.7842989Z         block_size: int,
2025-04-11T03:52:12.7843085Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7843172Z         num_kv_heads: int,
2025-04-11T03:52:12.7843262Z         same_context_len: bool,
2025-04-11T03:52:12.7843340Z         n_tokens: int,
2025-04-11T03:52:12.7843428Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7843562Z     ):
2025-04-11T03:52:12.7843651Z         torch.manual_seed(123)
2025-04-11T03:52:12.7843742Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7843831Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7843835Z 
2025-04-11T03:52:12.7843990Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7844104Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7844107Z 
2025-04-11T03:52:12.7844182Z device = None
2025-04-11T03:52:12.7844188Z 
2025-04-11T03:52:12.7844313Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7844462Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7844538Z     
2025-04-11T03:52:12.7844613Z         Args:
2025-04-11T03:52:12.7844786Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7844954Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7845063Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7845141Z         """
2025-04-11T03:52:12.7845218Z         _lazy_init()
2025-04-11T03:52:12.7845318Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7845419Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7845532Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7845811Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7845949Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7846107Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7846111Z 
2025-04-11T03:52:12.7846351Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7846509Z _______________ test_copy_kv_to_caches[True-1-True-16-16-32-32] ________________
2025-04-11T03:52:12.7846515Z 
2025-04-11T03:52:12.7846665Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7846821Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7846824Z 
2025-04-11T03:52:12.7847020Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7847128Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7847250Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7847385Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7847503Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7847640Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7847871Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7848025Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7848122Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7848198Z         bsz: int,
2025-04-11T03:52:12.7848278Z         block_size: int,
2025-04-11T03:52:12.7848376Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7848458Z         num_kv_heads: int,
2025-04-11T03:52:12.7848548Z         same_context_len: bool,
2025-04-11T03:52:12.7848628Z         n_tokens: int,
2025-04-11T03:52:12.7848716Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7848795Z     ):
2025-04-11T03:52:12.7848879Z         torch.manual_seed(123)
2025-04-11T03:52:12.7849024Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7849116Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7849120Z 
2025-04-11T03:52:12.7849270Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7849391Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7849394Z 
2025-04-11T03:52:12.7849472Z device = None
2025-04-11T03:52:12.7849475Z 
2025-04-11T03:52:12.7849597Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7849795Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7849871Z     
2025-04-11T03:52:12.7849946Z         Args:
2025-04-11T03:52:12.7850112Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7850282Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7850389Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7850469Z         """
2025-04-11T03:52:12.7850546Z         _lazy_init()
2025-04-11T03:52:12.7850646Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7850750Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7850860Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7851147Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7851285Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7851448Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7851451Z 
2025-04-11T03:52:12.7851687Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7851845Z ________________ test_copy_kv_to_caches[True-1-True-16-16-64-7] ________________
2025-04-11T03:52:12.7851849Z 
2025-04-11T03:52:12.7851999Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7852156Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7852160Z 
2025-04-11T03:52:12.7852360Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7852465Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7852592Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7852726Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7852845Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7852982Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7853094Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7853245Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7853335Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7853418Z         bsz: int,
2025-04-11T03:52:12.7853501Z         block_size: int,
2025-04-11T03:52:12.7853598Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7853681Z         num_kv_heads: int,
2025-04-11T03:52:12.7853765Z         same_context_len: bool,
2025-04-11T03:52:12.7853952Z         n_tokens: int,
2025-04-11T03:52:12.7854045Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7854125Z     ):
2025-04-11T03:52:12.7854210Z         torch.manual_seed(123)
2025-04-11T03:52:12.7854303Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7854396Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7854400Z 
2025-04-11T03:52:12.7854551Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7854667Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7854671Z 
2025-04-11T03:52:12.7854747Z device = None
2025-04-11T03:52:12.7854750Z 
2025-04-11T03:52:12.7854874Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7855076Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7855154Z     
2025-04-11T03:52:12.7855230Z         Args:
2025-04-11T03:52:12.7855401Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7855578Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7855689Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7855810Z         """
2025-04-11T03:52:12.7855890Z         _lazy_init()
2025-04-11T03:52:12.7855989Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7856093Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7856202Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7856492Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7856627Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7856792Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7856796Z 
2025-04-11T03:52:12.7857040Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7857199Z _______________ test_copy_kv_to_caches[True-1-True-16-16-64-32] ________________
2025-04-11T03:52:12.7857203Z 
2025-04-11T03:52:12.7857355Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7857513Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7857517Z 
2025-04-11T03:52:12.7857716Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7857819Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7857948Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7858082Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7858204Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7858343Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7858456Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7858612Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7858704Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7858787Z         bsz: int,
2025-04-11T03:52:12.7858872Z         block_size: int,
2025-04-11T03:52:12.7858966Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7859052Z         num_kv_heads: int,
2025-04-11T03:52:12.7859140Z         same_context_len: bool,
2025-04-11T03:52:12.7859225Z         n_tokens: int,
2025-04-11T03:52:12.7859317Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7859393Z     ):
2025-04-11T03:52:12.7859480Z         torch.manual_seed(123)
2025-04-11T03:52:12.7859570Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7859670Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7859674Z 
2025-04-11T03:52:12.7859826Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7859945Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7859950Z 
2025-04-11T03:52:12.7860130Z device = None
2025-04-11T03:52:12.7860135Z 
2025-04-11T03:52:12.7860261Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7860413Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7860488Z     
2025-04-11T03:52:12.7860563Z         Args:
2025-04-11T03:52:12.7860736Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7860907Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7861015Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7861094Z         """
2025-04-11T03:52:12.7861232Z         _lazy_init()
2025-04-11T03:52:12.7861325Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7861432Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7861536Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7861824Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7861959Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7862191Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7862195Z 
2025-04-11T03:52:12.7862432Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7862592Z _______________ test_copy_kv_to_caches[True-1-False-16-16-16-7] ________________
2025-04-11T03:52:12.7862596Z 
2025-04-11T03:52:12.7862744Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7862900Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7862904Z 
2025-04-11T03:52:12.7863108Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7863217Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7863345Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7863477Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7863595Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7863734Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7863842Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7863997Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7864087Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7864170Z         bsz: int,
2025-04-11T03:52:12.7864252Z         block_size: int,
2025-04-11T03:52:12.7864351Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7864436Z         num_kv_heads: int,
2025-04-11T03:52:12.7864521Z         same_context_len: bool,
2025-04-11T03:52:12.7864602Z         n_tokens: int,
2025-04-11T03:52:12.7864692Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7864775Z     ):
2025-04-11T03:52:12.7864862Z         torch.manual_seed(123)
2025-04-11T03:52:12.7864950Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7865045Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7865052Z 
2025-04-11T03:52:12.7865203Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7865318Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7865322Z 
2025-04-11T03:52:12.7865399Z device = None
2025-04-11T03:52:12.7865403Z 
2025-04-11T03:52:12.7865526Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7865676Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7865750Z     
2025-04-11T03:52:12.7865829Z         Args:
2025-04-11T03:52:12.7865996Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7866165Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7866378Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7866461Z         """
2025-04-11T03:52:12.7866539Z         _lazy_init()
2025-04-11T03:52:12.7866638Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7866746Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7866853Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7867137Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7867271Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7867436Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7867491Z 
2025-04-11T03:52:12.7867731Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7867886Z _______________ test_copy_kv_to_caches[True-1-False-16-16-16-32] _______________
2025-04-11T03:52:12.7867895Z 
2025-04-11T03:52:12.7868044Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7868199Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7868259Z 
2025-04-11T03:52:12.7868499Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7868603Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7868729Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7868862Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7868980Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7869118Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7869226Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7869381Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7869476Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7869557Z         bsz: int,
2025-04-11T03:52:12.7869639Z         block_size: int,
2025-04-11T03:52:12.7869729Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7869820Z         num_kv_heads: int,
2025-04-11T03:52:12.7869905Z         same_context_len: bool,
2025-04-11T03:52:12.7869989Z         n_tokens: int,
2025-04-11T03:52:12.7870079Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7870156Z     ):
2025-04-11T03:52:12.7870245Z         torch.manual_seed(123)
2025-04-11T03:52:12.7870333Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7870429Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7870432Z 
2025-04-11T03:52:12.7870581Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7870699Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7870703Z 
2025-04-11T03:52:12.7870779Z device = None
2025-04-11T03:52:12.7870783Z 
2025-04-11T03:52:12.7870908Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7871057Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7871128Z     
2025-04-11T03:52:12.7871208Z         Args:
2025-04-11T03:52:12.7871373Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7871541Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7871646Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7871721Z         """
2025-04-11T03:52:12.7871804Z         _lazy_init()
2025-04-11T03:52:12.7871899Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7872007Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7872111Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7872396Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7872647Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7872811Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7872820Z 
2025-04-11T03:52:12.7873061Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7873213Z _______________ test_copy_kv_to_caches[True-1-False-16-16-32-7] ________________
2025-04-11T03:52:12.7873217Z 
2025-04-11T03:52:12.7873368Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7873520Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7873578Z 
2025-04-11T03:52:12.7873780Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7873886Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7874014Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7874154Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7874269Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7874411Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7874581Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7874735Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7874828Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7874909Z         bsz: int,
2025-04-11T03:52:12.7874991Z         block_size: int,
2025-04-11T03:52:12.7875080Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7875170Z         num_kv_heads: int,
2025-04-11T03:52:12.7875255Z         same_context_len: bool,
2025-04-11T03:52:12.7875336Z         n_tokens: int,
2025-04-11T03:52:12.7875424Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7875494Z     ):
2025-04-11T03:52:12.7875588Z         torch.manual_seed(123)
2025-04-11T03:52:12.7875682Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7875778Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7875782Z 
2025-04-11T03:52:12.7875935Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7876053Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7876057Z 
2025-04-11T03:52:12.7876133Z device = None
2025-04-11T03:52:12.7876137Z 
2025-04-11T03:52:12.7876255Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7876410Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7876481Z     
2025-04-11T03:52:12.7876564Z         Args:
2025-04-11T03:52:12.7876731Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7876901Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7877007Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7877082Z         """
2025-04-11T03:52:12.7877167Z         _lazy_init()
2025-04-11T03:52:12.7877261Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7877367Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7877473Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7877759Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7877894Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7878054Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7878058Z 
2025-04-11T03:52:12.7878299Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7878454Z _______________ test_copy_kv_to_caches[True-1-False-16-16-32-32] _______________
2025-04-11T03:52:12.7878458Z 
2025-04-11T03:52:12.7878743Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7878896Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7878900Z 
2025-04-11T03:52:12.7879101Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7879208Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7879335Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7879467Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7879583Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7879727Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7879884Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7880037Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7880126Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7880202Z         bsz: int,
2025-04-11T03:52:12.7880294Z         block_size: int,
2025-04-11T03:52:12.7880385Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7880475Z         num_kv_heads: int,
2025-04-11T03:52:12.7880559Z         same_context_len: bool,
2025-04-11T03:52:12.7880699Z         n_tokens: int,
2025-04-11T03:52:12.7880788Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7880860Z     ):
2025-04-11T03:52:12.7880951Z         torch.manual_seed(123)
2025-04-11T03:52:12.7881039Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7881133Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7881136Z 
2025-04-11T03:52:12.7881288Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7881401Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7881410Z 
2025-04-11T03:52:12.7881486Z device = None
2025-04-11T03:52:12.7881490Z 
2025-04-11T03:52:12.7881609Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7881767Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7881838Z     
2025-04-11T03:52:12.7881915Z         Args:
2025-04-11T03:52:12.7882083Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7882258Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7882363Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7882436Z         """
2025-04-11T03:52:12.7882521Z         _lazy_init()
2025-04-11T03:52:12.7882617Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7882722Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7882826Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7883112Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7883251Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7883410Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7883414Z 
2025-04-11T03:52:12.7883659Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7883815Z _______________ test_copy_kv_to_caches[True-1-False-16-16-64-7] ________________
2025-04-11T03:52:12.7883819Z 
2025-04-11T03:52:12.7883970Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7884120Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7884124Z 
2025-04-11T03:52:12.7884322Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7884429Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7884552Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7884687Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7884909Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7885054Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7885161Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7885316Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7885408Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7885483Z         bsz: int,
2025-04-11T03:52:12.7885573Z         block_size: int,
2025-04-11T03:52:12.7885663Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7885751Z         num_kv_heads: int,
2025-04-11T03:52:12.7885839Z         same_context_len: bool,
2025-04-11T03:52:12.7885918Z         n_tokens: int,
2025-04-11T03:52:12.7886074Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7886146Z     ):
2025-04-11T03:52:12.7886237Z         torch.manual_seed(123)
2025-04-11T03:52:12.7886326Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7886420Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7886427Z 
2025-04-11T03:52:12.7886577Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7886688Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7886748Z 
2025-04-11T03:52:12.7886831Z device = None
2025-04-11T03:52:12.7886835Z 
2025-04-11T03:52:12.7886954Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7887107Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7887179Z     
2025-04-11T03:52:12.7887256Z         Args:
2025-04-11T03:52:12.7887423Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7887588Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7887698Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7887772Z         """
2025-04-11T03:52:12.7887856Z         _lazy_init()
2025-04-11T03:52:12.7887955Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7888061Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7888165Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7888449Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7888586Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7888742Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7888746Z 
2025-04-11T03:52:12.7888986Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7889142Z _______________ test_copy_kv_to_caches[True-1-False-16-16-64-32] _______________
2025-04-11T03:52:12.7889145Z 
2025-04-11T03:52:12.7889296Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7889452Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7889456Z 
2025-04-11T03:52:12.7889654Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7889763Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7889885Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7890021Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7890138Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7890280Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7890387Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7890542Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7890633Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7890710Z         bsz: int,
2025-04-11T03:52:12.7890798Z         block_size: int,
2025-04-11T03:52:12.7890889Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7891081Z         num_kv_heads: int,
2025-04-11T03:52:12.7891169Z         same_context_len: bool,
2025-04-11T03:52:12.7891247Z         n_tokens: int,
2025-04-11T03:52:12.7891342Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7891417Z     ):
2025-04-11T03:52:12.7891506Z         torch.manual_seed(123)
2025-04-11T03:52:12.7891594Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7891682Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7891690Z 
2025-04-11T03:52:12.7891841Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7891954Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7891958Z 
2025-04-11T03:52:12.7892086Z device = None
2025-04-11T03:52:12.7892090Z 
2025-04-11T03:52:12.7892208Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7892362Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7892435Z     
2025-04-11T03:52:12.7892516Z         Args:
2025-04-11T03:52:12.7892685Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7892853Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7893021Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7893095Z         """
2025-04-11T03:52:12.7893178Z         _lazy_init()
2025-04-11T03:52:12.7893271Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7893373Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7893484Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7893765Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7893907Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7894065Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7894072Z 
2025-04-11T03:52:12.7894311Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7894463Z ________________ test_copy_kv_to_caches[True-5-True-16-16-16-7] ________________
2025-04-11T03:52:12.7894469Z 
2025-04-11T03:52:12.7894622Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7894774Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7894778Z 
2025-04-11T03:52:12.7894976Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7895085Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7895209Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7895348Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7895463Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7895612Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7895721Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7895869Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7895967Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7896041Z         bsz: int,
2025-04-11T03:52:12.7896128Z         block_size: int,
2025-04-11T03:52:12.7896216Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7896300Z         num_kv_heads: int,
2025-04-11T03:52:12.7896386Z         same_context_len: bool,
2025-04-11T03:52:12.7896464Z         n_tokens: int,
2025-04-11T03:52:12.7896559Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7896630Z     ):
2025-04-11T03:52:12.7896721Z         torch.manual_seed(123)
2025-04-11T03:52:12.7896809Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7896897Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7896900Z 
2025-04-11T03:52:12.7897058Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7897272Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7897277Z 
2025-04-11T03:52:12.7897361Z device = None
2025-04-11T03:52:12.7897367Z 
2025-04-11T03:52:12.7897488Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7897645Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7897716Z     
2025-04-11T03:52:12.7897791Z         Args:
2025-04-11T03:52:12.7897963Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7898131Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7898290Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7898367Z         """
2025-04-11T03:52:12.7898450Z         _lazy_init()
2025-04-11T03:52:12.7898545Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7898649Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7898760Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7899039Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7899236Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7899391Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7899396Z 
2025-04-11T03:52:12.7899638Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7899789Z _______________ test_copy_kv_to_caches[True-5-True-16-16-16-32] ________________
2025-04-11T03:52:12.7899795Z 
2025-04-11T03:52:12.7899951Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7900104Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7900108Z 
2025-04-11T03:52:12.7900308Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7900418Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7900544Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7900685Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7900803Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7900946Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7901058Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7901207Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7901304Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7901384Z         bsz: int,
2025-04-11T03:52:12.7901473Z         block_size: int,
2025-04-11T03:52:12.7901564Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7901647Z         num_kv_heads: int,
2025-04-11T03:52:12.7901738Z         same_context_len: bool,
2025-04-11T03:52:12.7901820Z         n_tokens: int,
2025-04-11T03:52:12.7901915Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7901987Z     ):
2025-04-11T03:52:12.7902073Z         torch.manual_seed(123)
2025-04-11T03:52:12.7902169Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7902258Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7902262Z 
2025-04-11T03:52:12.7902419Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7902530Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7902534Z 
2025-04-11T03:52:12.7902617Z device = None
2025-04-11T03:52:12.7902620Z 
2025-04-11T03:52:12.7902740Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7902897Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7902968Z     
2025-04-11T03:52:12.7903041Z         Args:
2025-04-11T03:52:12.7903329Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7903500Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7903611Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7903687Z         """
2025-04-11T03:52:12.7903767Z         _lazy_init()
2025-04-11T03:52:12.7903867Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7903971Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7904079Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7904358Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7904549Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7904706Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7904710Z 
2025-04-11T03:52:12.7904951Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7905104Z ________________ test_copy_kv_to_caches[True-5-True-16-16-32-7] ________________
2025-04-11T03:52:12.7905108Z 
2025-04-11T03:52:12.7905313Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7905469Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7905473Z 
2025-04-11T03:52:12.7905670Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7905780Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7905905Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7906049Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7906164Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7906304Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7906417Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7906566Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7906663Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7906738Z         bsz: int,
2025-04-11T03:52:12.7906829Z         block_size: int,
2025-04-11T03:52:12.7906920Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7907005Z         num_kv_heads: int,
2025-04-11T03:52:12.7907094Z         same_context_len: bool,
2025-04-11T03:52:12.7907174Z         n_tokens: int,
2025-04-11T03:52:12.7907266Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7907336Z     ):
2025-04-11T03:52:12.7907420Z         torch.manual_seed(123)
2025-04-11T03:52:12.7907513Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7907606Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7907609Z 
2025-04-11T03:52:12.7907765Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7907880Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7907887Z 
2025-04-11T03:52:12.7907966Z device = None
2025-04-11T03:52:12.7907970Z 
2025-04-11T03:52:12.7908090Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7908241Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7908317Z     
2025-04-11T03:52:12.7908391Z         Args:
2025-04-11T03:52:12.7908632Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7908797Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7908907Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7908982Z         """
2025-04-11T03:52:12.7909061Z         _lazy_init()
2025-04-11T03:52:12.7909160Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7909263Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7909373Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7909779Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7909921Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7910080Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7910084Z 
2025-04-11T03:52:12.7910319Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7910477Z _______________ test_copy_kv_to_caches[True-5-True-16-16-32-32] ________________
2025-04-11T03:52:12.7910480Z 
2025-04-11T03:52:12.7910630Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7910846Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7910850Z 
2025-04-11T03:52:12.7911048Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7911160Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7911286Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7911428Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7911594Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7911729Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7911841Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7911990Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7912085Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7912161Z         bsz: int,
2025-04-11T03:52:12.7912242Z         block_size: int,
2025-04-11T03:52:12.7912338Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7912422Z         num_kv_heads: int,
2025-04-11T03:52:12.7912514Z         same_context_len: bool,
2025-04-11T03:52:12.7912592Z         n_tokens: int,
2025-04-11T03:52:12.7912690Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7912760Z     ):
2025-04-11T03:52:12.7912846Z         torch.manual_seed(123)
2025-04-11T03:52:12.7912939Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7913031Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7913037Z 
2025-04-11T03:52:12.7913192Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7913303Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7913307Z 
2025-04-11T03:52:12.7913386Z device = None
2025-04-11T03:52:12.7913390Z 
2025-04-11T03:52:12.7913511Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7913661Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7913737Z     
2025-04-11T03:52:12.7913811Z         Args:
2025-04-11T03:52:12.7913981Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7914146Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7914255Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7914329Z         """
2025-04-11T03:52:12.7914407Z         _lazy_init()
2025-04-11T03:52:12.7914507Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7914609Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7914715Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7914994Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7915127Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7915290Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7915294Z 
2025-04-11T03:52:12.7915527Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7915795Z ________________ test_copy_kv_to_caches[True-5-True-16-16-64-7] ________________
2025-04-11T03:52:12.7915799Z 
2025-04-11T03:52:12.7915948Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7916107Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7916110Z 
2025-04-11T03:52:12.7916306Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7916416Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7916543Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7916678Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7916797Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7916995Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7917109Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7917259Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7917353Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7917432Z         bsz: int,
2025-04-11T03:52:12.7917513Z         block_size: int,
2025-04-11T03:52:12.7917608Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7917750Z         num_kv_heads: int,
2025-04-11T03:52:12.7917839Z         same_context_len: bool,
2025-04-11T03:52:12.7917919Z         n_tokens: int,
2025-04-11T03:52:12.7918008Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7918083Z     ):
2025-04-11T03:52:12.7918170Z         torch.manual_seed(123)
2025-04-11T03:52:12.7918264Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7918359Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7918362Z 
2025-04-11T03:52:12.7918522Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7918633Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7918638Z 
2025-04-11T03:52:12.7918715Z device = None
2025-04-11T03:52:12.7918718Z 
2025-04-11T03:52:12.7918845Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7918995Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7919070Z     
2025-04-11T03:52:12.7919145Z         Args:
2025-04-11T03:52:12.7919315Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7919481Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7919588Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7919669Z         """
2025-04-11T03:52:12.7919750Z         _lazy_init()
2025-04-11T03:52:12.7919849Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7919958Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7920071Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7920356Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7920492Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7920657Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7920662Z 
2025-04-11T03:52:12.7920900Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7921059Z _______________ test_copy_kv_to_caches[True-5-True-16-16-64-32] ________________
2025-04-11T03:52:12.7921063Z 
2025-04-11T03:52:12.7921212Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7921368Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7921374Z 
2025-04-11T03:52:12.7921573Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7921684Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7921912Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7922049Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7922173Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7922314Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7922428Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7922578Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7922672Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7922748Z         bsz: int,
2025-04-11T03:52:12.7922829Z         block_size: int,
2025-04-11T03:52:12.7922924Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7923005Z         num_kv_heads: int,
2025-04-11T03:52:12.7923147Z         same_context_len: bool,
2025-04-11T03:52:12.7923227Z         n_tokens: int,
2025-04-11T03:52:12.7923316Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7923396Z     ):
2025-04-11T03:52:12.7923483Z         torch.manual_seed(123)
2025-04-11T03:52:12.7923582Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7923673Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7923677Z 
2025-04-11T03:52:12.7923827Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7924003Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7924007Z 
2025-04-11T03:52:12.7924083Z device = None
2025-04-11T03:52:12.7924087Z 
2025-04-11T03:52:12.7924210Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7924359Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7924431Z     
2025-04-11T03:52:12.7924507Z         Args:
2025-04-11T03:52:12.7924679Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7924843Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7924952Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7925028Z         """
2025-04-11T03:52:12.7925107Z         _lazy_init()
2025-04-11T03:52:12.7925210Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7925314Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7925419Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7925703Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7925839Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7925999Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7926004Z 
2025-04-11T03:52:12.7926237Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7926390Z _______________ test_copy_kv_to_caches[True-5-False-16-16-16-7] ________________
2025-04-11T03:52:12.7926394Z 
2025-04-11T03:52:12.7926542Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7926713Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7926720Z 
2025-04-11T03:52:12.7926949Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7927056Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7927181Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7927315Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7927437Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7927575Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7927694Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7927843Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7927931Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7928013Z         bsz: int,
2025-04-11T03:52:12.7928208Z         block_size: int,
2025-04-11T03:52:12.7928306Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7928388Z         num_kv_heads: int,
2025-04-11T03:52:12.7928477Z         same_context_len: bool,
2025-04-11T03:52:12.7928562Z         n_tokens: int,
2025-04-11T03:52:12.7928651Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7928730Z     ):
2025-04-11T03:52:12.7928818Z         torch.manual_seed(123)
2025-04-11T03:52:12.7928919Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7929009Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7929012Z 
2025-04-11T03:52:12.7929162Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7929337Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7929341Z 
2025-04-11T03:52:12.7929419Z device = None
2025-04-11T03:52:12.7929422Z 
2025-04-11T03:52:12.7929544Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7929698Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7929773Z     
2025-04-11T03:52:12.7929845Z         Args:
2025-04-11T03:52:12.7930012Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7930243Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7930347Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7930425Z         """
2025-04-11T03:52:12.7930504Z         _lazy_init()
2025-04-11T03:52:12.7930602Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7930703Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7930810Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7931094Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7931232Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7931392Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7931395Z 
2025-04-11T03:52:12.7931629Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7931788Z _______________ test_copy_kv_to_caches[True-5-False-16-16-16-32] _______________
2025-04-11T03:52:12.7931792Z 
2025-04-11T03:52:12.7931939Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7932095Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7932099Z 
2025-04-11T03:52:12.7932294Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7932400Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7932526Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7932661Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7932779Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7932918Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7933030Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7933182Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7933270Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7933350Z         bsz: int,
2025-04-11T03:52:12.7933430Z         block_size: int,
2025-04-11T03:52:12.7933522Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7933603Z         num_kv_heads: int,
2025-04-11T03:52:12.7933687Z         same_context_len: bool,
2025-04-11T03:52:12.7933770Z         n_tokens: int,
2025-04-11T03:52:12.7933857Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7933930Z     ):
2025-04-11T03:52:12.7934016Z         torch.manual_seed(123)
2025-04-11T03:52:12.7934103Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7934392Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7934397Z 
2025-04-11T03:52:12.7934550Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7942684Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7942701Z 
2025-04-11T03:52:12.7942827Z device = None
2025-04-11T03:52:12.7942832Z 
2025-04-11T03:52:12.7942983Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7943150Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7943230Z     
2025-04-11T03:52:12.7943312Z         Args:
2025-04-11T03:52:12.7943493Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7943796Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7943910Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7943991Z         """
2025-04-11T03:52:12.7944075Z         _lazy_init()
2025-04-11T03:52:12.7944196Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7944306Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7944420Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7944791Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7944938Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7945106Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7945111Z 
2025-04-11T03:52:12.7945361Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7945532Z _______________ test_copy_kv_to_caches[True-5-False-16-16-32-7] ________________
2025-04-11T03:52:12.7945537Z 
2025-04-11T03:52:12.7945693Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7945859Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7945863Z 
2025-04-11T03:52:12.7946070Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7946189Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7946325Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7946463Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7946584Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7946727Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7946843Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7947000Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7947097Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7947177Z         bsz: int,
2025-04-11T03:52:12.7947265Z         block_size: int,
2025-04-11T03:52:12.7947371Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7947455Z         num_kv_heads: int,
2025-04-11T03:52:12.7947550Z         same_context_len: bool,
2025-04-11T03:52:12.7947629Z         n_tokens: int,
2025-04-11T03:52:12.7947731Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7947807Z     ):
2025-04-11T03:52:12.7947896Z         torch.manual_seed(123)
2025-04-11T03:52:12.7947994Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7948089Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7948093Z 
2025-04-11T03:52:12.7948252Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7948370Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7948376Z 
2025-04-11T03:52:12.7948507Z device = None
2025-04-11T03:52:12.7948512Z 
2025-04-11T03:52:12.7948638Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7948793Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7949014Z     
2025-04-11T03:52:12.7949092Z         Args:
2025-04-11T03:52:12.7949273Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7949442Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7949560Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7949637Z         """
2025-04-11T03:52:12.7949716Z         _lazy_init()
2025-04-11T03:52:12.7949823Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7949929Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7950040Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7950326Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7950569Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7950740Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7950744Z 
2025-04-11T03:52:12.7950983Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7951194Z _______________ test_copy_kv_to_caches[True-5-False-16-16-32-32] _______________
2025-04-11T03:52:12.7951198Z 
2025-04-11T03:52:12.7951351Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7951512Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7951516Z 
2025-04-11T03:52:12.7951718Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7951829Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7951956Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7952093Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7952216Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7952362Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7952480Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7952628Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7952729Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7952807Z         bsz: int,
2025-04-11T03:52:12.7952892Z         block_size: int,
2025-04-11T03:52:12.7952989Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7953076Z         num_kv_heads: int,
2025-04-11T03:52:12.7953165Z         same_context_len: bool,
2025-04-11T03:52:12.7953245Z         n_tokens: int,
2025-04-11T03:52:12.7953334Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7953415Z     ):
2025-04-11T03:52:12.7953505Z         torch.manual_seed(123)
2025-04-11T03:52:12.7953602Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7953698Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7953702Z 
2025-04-11T03:52:12.7953867Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7953981Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7953985Z 
2025-04-11T03:52:12.7954062Z device = None
2025-04-11T03:52:12.7954068Z 
2025-04-11T03:52:12.7954191Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7954342Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7954417Z     
2025-04-11T03:52:12.7954491Z         Args:
2025-04-11T03:52:12.7954664Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7954832Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7954942Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7955020Z         """
2025-04-11T03:52:12.7955102Z         _lazy_init()
2025-04-11T03:52:12.7955202Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7955414Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7955525Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7955816Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7955956Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7956120Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7956124Z 
2025-04-11T03:52:12.7956365Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7956526Z _______________ test_copy_kv_to_caches[True-5-False-16-16-64-7] ________________
2025-04-11T03:52:12.7956581Z 
2025-04-11T03:52:12.7956735Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7956895Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7956898Z 
2025-04-11T03:52:12.7957103Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7957216Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7957406Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7957541Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7957663Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7957806Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7957918Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7958069Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7958167Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7958245Z         bsz: int,
2025-04-11T03:52:12.7958330Z         block_size: int,
2025-04-11T03:52:12.7958426Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7958509Z         num_kv_heads: int,
2025-04-11T03:52:12.7958604Z         same_context_len: bool,
2025-04-11T03:52:12.7958684Z         n_tokens: int,
2025-04-11T03:52:12.7958774Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7958851Z     ):
2025-04-11T03:52:12.7958940Z         torch.manual_seed(123)
2025-04-11T03:52:12.7959036Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7959128Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7959132Z 
2025-04-11T03:52:12.7959284Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7959403Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7959407Z 
2025-04-11T03:52:12.7959485Z device = None
2025-04-11T03:52:12.7959489Z 
2025-04-11T03:52:12.7959614Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7959763Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7959838Z     
2025-04-11T03:52:12.7959913Z         Args:
2025-04-11T03:52:12.7960084Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7960258Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7960366Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7960447Z         """
2025-04-11T03:52:12.7960528Z         _lazy_init()
2025-04-11T03:52:12.7960631Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7960735Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7960843Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7961132Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7961271Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7961439Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7961443Z 
2025-04-11T03:52:12.7961782Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7961945Z _______________ test_copy_kv_to_caches[True-5-False-16-16-64-32] _______________
2025-04-11T03:52:12.7961951Z 
2025-04-11T03:52:12.7962103Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7962262Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7962266Z 
2025-04-11T03:52:12.7962470Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7962576Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7962703Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7962888Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7963011Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7963150Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7963269Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7963420Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7963508Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7963652Z         bsz: int,
2025-04-11T03:52:12.7963736Z         block_size: int,
2025-04-11T03:52:12.7963832Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7963917Z         num_kv_heads: int,
2025-04-11T03:52:12.7964004Z         same_context_len: bool,
2025-04-11T03:52:12.7964089Z         n_tokens: int,
2025-04-11T03:52:12.7964178Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7964254Z     ):
2025-04-11T03:52:12.7964342Z         torch.manual_seed(123)
2025-04-11T03:52:12.7964440Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7964534Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7964537Z 
2025-04-11T03:52:12.7964688Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7964813Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7964816Z 
2025-04-11T03:52:12.7964893Z device = None
2025-04-11T03:52:12.7964897Z 
2025-04-11T03:52:12.7965021Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7965173Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7965250Z     
2025-04-11T03:52:12.7965326Z         Args:
2025-04-11T03:52:12.7965495Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7965668Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7965776Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7965857Z         """
2025-04-11T03:52:12.7965937Z         _lazy_init()
2025-04-11T03:52:12.7966039Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7966142Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7966251Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7966539Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7966674Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7966838Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7966842Z 
2025-04-11T03:52:12.7967078Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7967233Z _______________ test_copy_kv_to_caches[False-1-True-16-16-16-7] ________________
2025-04-11T03:52:12.7967237Z 
2025-04-11T03:52:12.7967384Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7967539Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7967547Z 
2025-04-11T03:52:12.7967850Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7967957Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7968085Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7968219Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7968341Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7968479Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7968597Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7968746Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7968835Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7968921Z         bsz: int,
2025-04-11T03:52:12.7969057Z         block_size: int,
2025-04-11T03:52:12.7969153Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7969236Z         num_kv_heads: int,
2025-04-11T03:52:12.7969321Z         same_context_len: bool,
2025-04-11T03:52:12.7969408Z         n_tokens: int,
2025-04-11T03:52:12.7969500Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7969576Z     ):
2025-04-11T03:52:12.7969663Z         torch.manual_seed(123)
2025-04-11T03:52:12.7969755Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7969911Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7969914Z 
2025-04-11T03:52:12.7970067Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7970186Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7970191Z 
2025-04-11T03:52:12.7970267Z device = None
2025-04-11T03:52:12.7970271Z 
2025-04-11T03:52:12.7970394Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7970546Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7970624Z     
2025-04-11T03:52:12.7970699Z         Args:
2025-04-11T03:52:12.7970870Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7971044Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7971152Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7971231Z         """
2025-04-11T03:52:12.7971313Z         _lazy_init()
2025-04-11T03:52:12.7971411Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7971524Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7971630Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7971920Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7972056Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7972224Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7972228Z 
2025-04-11T03:52:12.7972467Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7972629Z _______________ test_copy_kv_to_caches[False-1-True-16-16-16-32] _______________
2025-04-11T03:52:12.7972633Z 
2025-04-11T03:52:12.7972786Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7972945Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7972948Z 
2025-04-11T03:52:12.7973153Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7973259Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7973390Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7973527Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7973648Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7973790Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7973901Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7974159Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7974254Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7974338Z         bsz: int,
2025-04-11T03:52:12.7974422Z         block_size: int,
2025-04-11T03:52:12.7974527Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7974611Z         num_kv_heads: int,
2025-04-11T03:52:12.7974699Z         same_context_len: bool,
2025-04-11T03:52:12.7974785Z         n_tokens: int,
2025-04-11T03:52:12.7974875Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7974953Z     ):
2025-04-11T03:52:12.7975038Z         torch.manual_seed(123)
2025-04-11T03:52:12.7975128Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7975227Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7975280Z 
2025-04-11T03:52:12.7975437Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7975557Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7975561Z 
2025-04-11T03:52:12.7975644Z device = None
2025-04-11T03:52:12.7975648Z 
2025-04-11T03:52:12.7975776Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7975930Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7976062Z     
2025-04-11T03:52:12.7976142Z         Args:
2025-04-11T03:52:12.7976309Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7976482Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7976589Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7976668Z         """
2025-04-11T03:52:12.7976750Z         _lazy_init()
2025-04-11T03:52:12.7976849Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7976963Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7977071Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7977361Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7977496Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7977661Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7977667Z 
2025-04-11T03:52:12.7977906Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7978062Z _______________ test_copy_kv_to_caches[False-1-True-16-16-32-7] ________________
2025-04-11T03:52:12.7978072Z 
2025-04-11T03:52:12.7978217Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7978367Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7978373Z 
2025-04-11T03:52:12.7978576Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7978682Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7978817Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7978950Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7979074Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7979213Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7979323Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7979478Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7979569Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7979653Z         bsz: int,
2025-04-11T03:52:12.7979739Z         block_size: int,
2025-04-11T03:52:12.7979829Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7979924Z         num_kv_heads: int,
2025-04-11T03:52:12.7980010Z         same_context_len: bool,
2025-04-11T03:52:12.7980096Z         n_tokens: int,
2025-04-11T03:52:12.7980187Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7980259Z     ):
2025-04-11T03:52:12.7980458Z         torch.manual_seed(123)
2025-04-11T03:52:12.7980554Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7980654Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7980658Z 
2025-04-11T03:52:12.7980811Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7980935Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7980939Z 
2025-04-11T03:52:12.7981019Z device = None
2025-04-11T03:52:12.7981022Z 
2025-04-11T03:52:12.7981148Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7981299Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7981372Z     
2025-04-11T03:52:12.7981504Z         Args:
2025-04-11T03:52:12.7981671Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7981846Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7981957Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7982034Z         """
2025-04-11T03:52:12.7982121Z         _lazy_init()
2025-04-11T03:52:12.7982217Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7982382Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7982489Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7982779Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7982916Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7983074Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7983086Z 
2025-04-11T03:52:12.7983328Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7983481Z _______________ test_copy_kv_to_caches[False-1-True-16-16-32-32] _______________
2025-04-11T03:52:12.7983488Z 
2025-04-11T03:52:12.7983648Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7983799Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7983805Z 
2025-04-11T03:52:12.7984013Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7984119Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7984249Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7984382Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7984499Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7984652Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7984760Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7984918Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7985007Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7985098Z         bsz: int,
2025-04-11T03:52:12.7985182Z         block_size: int,
2025-04-11T03:52:12.7985273Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7985368Z         num_kv_heads: int,
2025-04-11T03:52:12.7985459Z         same_context_len: bool,
2025-04-11T03:52:12.7985545Z         n_tokens: int,
2025-04-11T03:52:12.7985638Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7985713Z     ):
2025-04-11T03:52:12.7985810Z         torch.manual_seed(123)
2025-04-11T03:52:12.7985900Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7985997Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7986001Z 
2025-04-11T03:52:12.7986153Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7986274Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7986278Z 
2025-04-11T03:52:12.7986356Z device = None
2025-04-11T03:52:12.7986360Z 
2025-04-11T03:52:12.7986584Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7986745Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7986817Z     
2025-04-11T03:52:12.7986899Z         Args:
2025-04-11T03:52:12.7987072Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7987242Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7987349Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7987423Z         """
2025-04-11T03:52:12.7987510Z         _lazy_init()
2025-04-11T03:52:12.7987607Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7987714Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7987887Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7988177Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7988315Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7988538Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7988608Z 
2025-04-11T03:52:12.7988859Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7989016Z _______________ test_copy_kv_to_caches[False-1-True-16-16-64-7] ________________
2025-04-11T03:52:12.7989019Z 
2025-04-11T03:52:12.7989173Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7989326Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7989330Z 
2025-04-11T03:52:12.7989534Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7989637Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7989762Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7989894Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7990009Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7990150Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7990259Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7990413Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7990502Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7990580Z         bsz: int,
2025-04-11T03:52:12.7990660Z         block_size: int,
2025-04-11T03:52:12.7990747Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7990833Z         num_kv_heads: int,
2025-04-11T03:52:12.7990918Z         same_context_len: bool,
2025-04-11T03:52:12.7991000Z         n_tokens: int,
2025-04-11T03:52:12.7991088Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7991159Z     ):
2025-04-11T03:52:12.7991250Z         torch.manual_seed(123)
2025-04-11T03:52:12.7991339Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7991436Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7991440Z 
2025-04-11T03:52:12.7991590Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7991708Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7991712Z 
2025-04-11T03:52:12.7991788Z device = None
2025-04-11T03:52:12.7991792Z 
2025-04-11T03:52:12.7991908Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7992060Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7992128Z     
2025-04-11T03:52:12.7992205Z         Args:
2025-04-11T03:52:12.7992370Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7992541Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7992646Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7992717Z         """
2025-04-11T03:52:12.7992924Z         _lazy_init()
2025-04-11T03:52:12.7993024Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7993128Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7993236Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7993521Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7993654Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7993812Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7993816Z 
2025-04-11T03:52:12.7994057Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7994265Z _______________ test_copy_kv_to_caches[False-1-True-16-16-64-32] _______________
2025-04-11T03:52:12.7994269Z 
2025-04-11T03:52:12.7994424Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7994573Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7994576Z 
2025-04-11T03:52:12.7994778Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7994938Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7995068Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7995203Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7995321Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7995465Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7995576Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7995730Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7995822Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7995906Z         bsz: int,
2025-04-11T03:52:12.7995993Z         block_size: int,
2025-04-11T03:52:12.7996088Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7996180Z         num_kv_heads: int,
2025-04-11T03:52:12.7996268Z         same_context_len: bool,
2025-04-11T03:52:12.7996355Z         n_tokens: int,
2025-04-11T03:52:12.7996446Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7996521Z     ):
2025-04-11T03:52:12.7996613Z         torch.manual_seed(123)
2025-04-11T03:52:12.7996705Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7996802Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7996805Z 
2025-04-11T03:52:12.7996957Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7997073Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7997082Z 
2025-04-11T03:52:12.7997162Z device = None
2025-04-11T03:52:12.7997165Z 
2025-04-11T03:52:12.7997286Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7997449Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7997523Z     
2025-04-11T03:52:12.7997606Z         Args:
2025-04-11T03:52:12.7997778Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7997952Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7998060Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7998136Z         """
2025-04-11T03:52:12.7998222Z         _lazy_init()
2025-04-11T03:52:12.7998320Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7998429Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7998537Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7998825Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7998969Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7999237Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7999241Z 
2025-04-11T03:52:12.7999484Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7999639Z _______________ test_copy_kv_to_caches[False-1-False-16-16-16-7] _______________
2025-04-11T03:52:12.7999643Z 
2025-04-11T03:52:12.7999792Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7999949Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7999953Z 
2025-04-11T03:52:12.8000158Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8000315Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8000438Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8000573Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8000692Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8000833Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8000941Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8001142Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8001231Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8001308Z         bsz: int,
2025-04-11T03:52:12.8001394Z         block_size: int,
2025-04-11T03:52:12.8001483Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8001570Z         num_kv_heads: int,
2025-04-11T03:52:12.8001657Z         same_context_len: bool,
2025-04-11T03:52:12.8001737Z         n_tokens: int,
2025-04-11T03:52:12.8001830Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8001906Z     ):
2025-04-11T03:52:12.8001997Z         torch.manual_seed(123)
2025-04-11T03:52:12.8002085Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8002176Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8002180Z 
2025-04-11T03:52:12.8002333Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8002442Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8002448Z 
2025-04-11T03:52:12.8002528Z device = None
2025-04-11T03:52:12.8002531Z 
2025-04-11T03:52:12.8002646Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8002799Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8002869Z     
2025-04-11T03:52:12.8002947Z         Args:
2025-04-11T03:52:12.8003114Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8003278Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8003389Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8003462Z         """
2025-04-11T03:52:12.8003545Z         _lazy_init()
2025-04-11T03:52:12.8003642Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8003745Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8003849Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8004127Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8004266Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8004424Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8004427Z 
2025-04-11T03:52:12.8004661Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8004817Z ______________ test_copy_kv_to_caches[False-1-False-16-16-16-32] _______________
2025-04-11T03:52:12.8004822Z 
2025-04-11T03:52:12.8004972Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8005125Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.8005237Z 
2025-04-11T03:52:12.8005440Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8005545Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8005668Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8005805Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8005919Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8006062Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8006171Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8006320Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8006461Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8006539Z         bsz: int,
2025-04-11T03:52:12.8006626Z         block_size: int,
2025-04-11T03:52:12.8006715Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8006804Z         num_kv_heads: int,
2025-04-11T03:52:12.8006889Z         same_context_len: bool,
2025-04-11T03:52:12.8006967Z         n_tokens: int,
2025-04-11T03:52:12.8007061Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8007208Z     ):
2025-04-11T03:52:12.8007297Z         torch.manual_seed(123)
2025-04-11T03:52:12.8007387Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8007478Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8007487Z 
2025-04-11T03:52:12.8007635Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8007747Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8007752Z 
2025-04-11T03:52:12.8007834Z device = None
2025-04-11T03:52:12.8007840Z 
2025-04-11T03:52:12.8007958Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8008113Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8008183Z     
2025-04-11T03:52:12.8008261Z         Args:
2025-04-11T03:52:12.8008432Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8008598Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8008710Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8008782Z         """
2025-04-11T03:52:12.8008863Z         _lazy_init()
2025-04-11T03:52:12.8008960Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8009061Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8009172Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8009455Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8009596Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8009754Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8009757Z 
2025-04-11T03:52:12.8010002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8010155Z _______________ test_copy_kv_to_caches[False-1-False-16-16-32-7] _______________
2025-04-11T03:52:12.8010160Z 
2025-04-11T03:52:12.8010312Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8010467Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.8010471Z 
2025-04-11T03:52:12.8010666Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8010775Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8010900Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8011046Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8011162Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8011452Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8011566Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8011716Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8011819Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8011900Z         bsz: int,
2025-04-11T03:52:12.8011996Z         block_size: int,
2025-04-11T03:52:12.8012089Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8012181Z         num_kv_heads: int,
2025-04-11T03:52:12.8012272Z         same_context_len: bool,
2025-04-11T03:52:12.8012353Z         n_tokens: int,
2025-04-11T03:52:12.8012454Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8012526Z     ):
2025-04-11T03:52:12.8012618Z         torch.manual_seed(123)
2025-04-11T03:52:12.8012773Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8012869Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8012872Z 
2025-04-11T03:52:12.8013032Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8013147Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8013151Z 
2025-04-11T03:52:12.8013239Z device = None
2025-04-11T03:52:12.8013243Z 
2025-04-11T03:52:12.8013411Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8013570Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8013641Z     
2025-04-11T03:52:12.8013719Z         Args:
2025-04-11T03:52:12.8013895Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8014065Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8014176Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8014251Z         """
2025-04-11T03:52:12.8014335Z         _lazy_init()
2025-04-11T03:52:12.8014432Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8014534Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8014651Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8014935Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8015077Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8015237Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8015241Z 
2025-04-11T03:52:12.8015483Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8015637Z ______________ test_copy_kv_to_caches[False-1-False-16-16-32-32] _______________
2025-04-11T03:52:12.8015643Z 
2025-04-11T03:52:12.8015799Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8015956Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.8015959Z 
2025-04-11T03:52:12.8016163Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8016274Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8016399Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8016538Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8016657Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8016803Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8016913Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8017065Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8017159Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8017238Z         bsz: int,
2025-04-11T03:52:12.8017324Z         block_size: int,
2025-04-11T03:52:12.8017413Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8017494Z         num_kv_heads: int,
2025-04-11T03:52:12.8017584Z         same_context_len: bool,
2025-04-11T03:52:12.8017775Z         n_tokens: int,
2025-04-11T03:52:12.8017872Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8017946Z     ):
2025-04-11T03:52:12.8018037Z         torch.manual_seed(123)
2025-04-11T03:52:12.8018131Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8018220Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8018224Z 
2025-04-11T03:52:12.8018381Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8018493Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8018497Z 
2025-04-11T03:52:12.8018582Z device = None
2025-04-11T03:52:12.8018585Z 
2025-04-11T03:52:12.8018704Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8018913Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8018988Z     
2025-04-11T03:52:12.8019063Z         Args:
2025-04-11T03:52:12.8019239Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8019410Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8019522Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8019641Z         """
2025-04-11T03:52:12.8019720Z         _lazy_init()
2025-04-11T03:52:12.8019820Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8019922Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8020037Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8020319Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8020457Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8020620Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8020624Z 
2025-04-11T03:52:12.8020872Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8021027Z _______________ test_copy_kv_to_caches[False-1-False-16-16-64-7] _______________
2025-04-11T03:52:12.8021030Z 
2025-04-11T03:52:12.8021177Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8021339Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.8021343Z 
2025-04-11T03:52:12.8021542Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8021651Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8021775Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8021914Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8022033Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8022173Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8022288Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8022441Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8022534Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8022611Z         bsz: int,
2025-04-11T03:52:12.8022700Z         block_size: int,
2025-04-11T03:52:12.8022792Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8022876Z         num_kv_heads: int,
2025-04-11T03:52:12.8022970Z         same_context_len: bool,
2025-04-11T03:52:12.8023048Z         n_tokens: int,
2025-04-11T03:52:12.8023148Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8023220Z     ):
2025-04-11T03:52:12.8023308Z         torch.manual_seed(123)
2025-04-11T03:52:12.8023404Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8023500Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8023504Z 
2025-04-11T03:52:12.8023658Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8023769Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8023877Z 
2025-04-11T03:52:12.8023966Z device = None
2025-04-11T03:52:12.8023970Z 
2025-04-11T03:52:12.8024090Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8024241Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8024320Z     
2025-04-11T03:52:12.8024395Z         Args:
2025-04-11T03:52:12.8024569Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8024735Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8024843Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8024918Z         """
2025-04-11T03:52:12.8025051Z         _lazy_init()
2025-04-11T03:52:12.8025156Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8025256Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8025364Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8025644Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8025779Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8025997Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8026001Z 
2025-04-11T03:52:12.8026238Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8026397Z ______________ test_copy_kv_to_caches[False-1-False-16-16-64-32] _______________
2025-04-11T03:52:12.8026400Z 
2025-04-11T03:52:12.8026549Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8026710Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.8026713Z 
2025-04-11T03:52:12.8026909Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8027021Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8027146Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8027302Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8027464Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8027607Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8027721Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8027869Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8027965Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8028042Z         bsz: int,
2025-04-11T03:52:12.8028126Z         block_size: int,
2025-04-11T03:52:12.8028224Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8028307Z         num_kv_heads: int,
2025-04-11T03:52:12.8028401Z         same_context_len: bool,
2025-04-11T03:52:12.8028518Z         n_tokens: int,
2025-04-11T03:52:12.8028616Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8028695Z     ):
2025-04-11T03:52:12.8028782Z         torch.manual_seed(123)
2025-04-11T03:52:12.8028881Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8028973Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8028980Z 
2025-04-11T03:52:12.8029135Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8029248Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8029252Z 
2025-04-11T03:52:12.8029334Z device = None
2025-04-11T03:52:12.8029338Z 
2025-04-11T03:52:12.8029457Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8029609Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8029689Z     
2025-04-11T03:52:12.8029763Z         Args:
2025-04-11T03:52:12.8029943Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8030228Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8030342Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8030420Z         """
2025-04-11T03:52:12.8030499Z         _lazy_init()
2025-04-11T03:52:12.8030605Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8030709Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8030820Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8031101Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8031237Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8031399Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8031474Z 
2025-04-11T03:52:12.8031715Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8031881Z _______________ test_copy_kv_to_caches[False-5-True-16-16-16-7] ________________
2025-04-11T03:52:12.8031885Z 
2025-04-11T03:52:12.8032033Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8032194Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8032258Z 
2025-04-11T03:52:12.8032460Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8032572Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8032698Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8032832Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8032952Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8033092Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8033208Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8033357Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8033456Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8033536Z         bsz: int,
2025-04-11T03:52:12.8033619Z         block_size: int,
2025-04-11T03:52:12.8033713Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8033796Z         num_kv_heads: int,
2025-04-11T03:52:12.8033887Z         same_context_len: bool,
2025-04-11T03:52:12.8033969Z         n_tokens: int,
2025-04-11T03:52:12.8034059Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8034136Z     ):
2025-04-11T03:52:12.8034223Z         torch.manual_seed(123)
2025-04-11T03:52:12.8034319Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8034408Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8034411Z 
2025-04-11T03:52:12.8034575Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8034688Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8034692Z 
2025-04-11T03:52:12.8034768Z device = None
2025-04-11T03:52:12.8034776Z 
2025-04-11T03:52:12.8034896Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8035047Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8035121Z     
2025-04-11T03:52:12.8035197Z         Args:
2025-04-11T03:52:12.8035370Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8035534Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8035638Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8035714Z         """
2025-04-11T03:52:12.8035791Z         _lazy_init()
2025-04-11T03:52:12.8035893Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8035994Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8036104Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8036500Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8036639Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8036803Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8036809Z 
2025-04-11T03:52:12.8037048Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8037205Z _______________ test_copy_kv_to_caches[False-5-True-16-16-16-32] _______________
2025-04-11T03:52:12.8037209Z 
2025-04-11T03:52:12.8037356Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8037510Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8037574Z 
2025-04-11T03:52:12.8037773Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8037880Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8038000Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8038134Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8038251Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8038391Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8038645Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8038795Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8038889Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8038965Z         bsz: int,
2025-04-11T03:52:12.8039046Z         block_size: int,
2025-04-11T03:52:12.8039142Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8039227Z         num_kv_heads: int,
2025-04-11T03:52:12.8039323Z         same_context_len: bool,
2025-04-11T03:52:12.8039403Z         n_tokens: int,
2025-04-11T03:52:12.8039493Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8039569Z     ):
2025-04-11T03:52:12.8039655Z         torch.manual_seed(123)
2025-04-11T03:52:12.8039750Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8039843Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8039847Z 
2025-04-11T03:52:12.8039998Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8040112Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8040116Z 
2025-04-11T03:52:12.8040192Z device = None
2025-04-11T03:52:12.8040196Z 
2025-04-11T03:52:12.8040319Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8040466Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8040538Z     
2025-04-11T03:52:12.8040611Z         Args:
2025-04-11T03:52:12.8040783Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8040950Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8041055Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8041136Z         """
2025-04-11T03:52:12.8041215Z         _lazy_init()
2025-04-11T03:52:12.8041315Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8041417Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8041526Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8041811Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8041946Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8042105Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8042109Z 
2025-04-11T03:52:12.8042344Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8042502Z _______________ test_copy_kv_to_caches[False-5-True-16-16-32-7] ________________
2025-04-11T03:52:12.8042506Z 
2025-04-11T03:52:12.8042749Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8042905Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8042909Z 
2025-04-11T03:52:12.8043110Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8043219Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8043341Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8043473Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8043589Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8043724Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8043891Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8044040Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8044131Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8044214Z         bsz: int,
2025-04-11T03:52:12.8044299Z         block_size: int,
2025-04-11T03:52:12.8044395Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8044477Z         num_kv_heads: int,
2025-04-11T03:52:12.8044567Z         same_context_len: bool,
2025-04-11T03:52:12.8044704Z         n_tokens: int,
2025-04-11T03:52:12.8044792Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8044867Z     ):
2025-04-11T03:52:12.8044954Z         torch.manual_seed(123)
2025-04-11T03:52:12.8045045Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8045137Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8045140Z 
2025-04-11T03:52:12.8045290Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8045404Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8045410Z 
2025-04-11T03:52:12.8045487Z device = None
2025-04-11T03:52:12.8045490Z 
2025-04-11T03:52:12.8045614Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8045763Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8045837Z     
2025-04-11T03:52:12.8045911Z         Args:
2025-04-11T03:52:12.8046077Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8046249Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8046355Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8046437Z         """
2025-04-11T03:52:12.8046514Z         _lazy_init()
2025-04-11T03:52:12.8046613Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8046714Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8046821Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8047109Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8047248Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8047410Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8047414Z 
2025-04-11T03:52:12.8047652Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8047809Z _______________ test_copy_kv_to_caches[False-5-True-16-16-32-32] _______________
2025-04-11T03:52:12.8047813Z 
2025-04-11T03:52:12.8047962Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8048115Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8048119Z 
2025-04-11T03:52:12.8048316Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8048425Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8048551Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8048684Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8048910Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8049054Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8049168Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8049319Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8049410Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8049492Z         bsz: int,
2025-04-11T03:52:12.8049573Z         block_size: int,
2025-04-11T03:52:12.8049667Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8049750Z         num_kv_heads: int,
2025-04-11T03:52:12.8049837Z         same_context_len: bool,
2025-04-11T03:52:12.8049919Z         n_tokens: int,
2025-04-11T03:52:12.8050060Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8050138Z     ):
2025-04-11T03:52:12.8050224Z         torch.manual_seed(123)
2025-04-11T03:52:12.8050312Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8050406Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8050413Z 
2025-04-11T03:52:12.8050560Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8050680Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8050738Z 
2025-04-11T03:52:12.8050818Z device = None
2025-04-11T03:52:12.8050822Z 
2025-04-11T03:52:12.8050943Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8051091Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8051166Z     
2025-04-11T03:52:12.8051242Z         Args:
2025-04-11T03:52:12.8051414Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8051586Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8051692Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8051771Z         """
2025-04-11T03:52:12.8051850Z         _lazy_init()
2025-04-11T03:52:12.8051947Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8052055Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8052161Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8053841Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8053985Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8054141Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8054150Z 
2025-04-11T03:52:12.8054390Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8054547Z _______________ test_copy_kv_to_caches[False-5-True-16-16-64-7] ________________
2025-04-11T03:52:12.8054551Z 
2025-04-11T03:52:12.8054701Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8054855Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8054859Z 
2025-04-11T03:52:12.8055061Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8055170Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8055296Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8055447Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8055563Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8055704Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8055810Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8055968Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8056058Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8056137Z         bsz: int,
2025-04-11T03:52:12.8056223Z         block_size: int,
2025-04-11T03:52:12.8056313Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8056458Z         num_kv_heads: int,
2025-04-11T03:52:12.8056548Z         same_context_len: bool,
2025-04-11T03:52:12.8056627Z         n_tokens: int,
2025-04-11T03:52:12.8056723Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8056796Z     ):
2025-04-11T03:52:12.8056887Z         torch.manual_seed(123)
2025-04-11T03:52:12.8056979Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8057071Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8057080Z 
2025-04-11T03:52:12.8057234Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8057345Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8057349Z 
2025-04-11T03:52:12.8057490Z device = None
2025-04-11T03:52:12.8057494Z 
2025-04-11T03:52:12.8057612Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8057768Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8057839Z     
2025-04-11T03:52:12.8057917Z         Args:
2025-04-11T03:52:12.8058083Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8058249Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8058432Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8058511Z         """
2025-04-11T03:52:12.8058593Z         _lazy_init()
2025-04-11T03:52:12.8058689Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8058791Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8058901Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8059185Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8059327Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8059483Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8059488Z 
2025-04-11T03:52:12.8059732Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8059883Z _______________ test_copy_kv_to_caches[False-5-True-16-16-64-32] _______________
2025-04-11T03:52:12.8059888Z 
2025-04-11T03:52:12.8060123Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8060278Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8060282Z 
2025-04-11T03:52:12.8060484Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8060593Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8060718Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8060858Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8060972Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8061116Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8061225Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8061382Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8061472Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8061549Z         bsz: int,
2025-04-11T03:52:12.8061637Z         block_size: int,
2025-04-11T03:52:12.8061726Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8061812Z         num_kv_heads: int,
2025-04-11T03:52:12.8061898Z         same_context_len: bool,
2025-04-11T03:52:12.8061978Z         n_tokens: int,
2025-04-11T03:52:12.8062073Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8062145Z     ):
2025-04-11T03:52:12.8062234Z         torch.manual_seed(123)
2025-04-11T03:52:12.8062321Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8062411Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8062414Z 
2025-04-11T03:52:12.8062626Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8062740Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8062744Z 
2025-04-11T03:52:12.8062827Z device = None
2025-04-11T03:52:12.8062833Z 
2025-04-11T03:52:12.8062950Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8063107Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8063178Z     
2025-04-11T03:52:12.8063252Z         Args:
2025-04-11T03:52:12.8063426Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8063595Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8063768Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8063843Z         """
2025-04-11T03:52:12.8063928Z         _lazy_init()
2025-04-11T03:52:12.8064023Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8064125Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8064239Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8064522Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8064718Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8064881Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8064885Z 
2025-04-11T03:52:12.8065129Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8065286Z _______________ test_copy_kv_to_caches[False-5-False-16-16-16-7] _______________
2025-04-11T03:52:12.8065291Z 
2025-04-11T03:52:12.8065445Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8065606Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8065610Z 
2025-04-11T03:52:12.8065811Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8065923Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8066048Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8066194Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8066372Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8066518Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8066625Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8066775Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8066873Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8066948Z         bsz: int,
2025-04-11T03:52:12.8067033Z         block_size: int,
2025-04-11T03:52:12.8067124Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8067207Z         num_kv_heads: int,
2025-04-11T03:52:12.8067297Z         same_context_len: bool,
2025-04-11T03:52:12.8067376Z         n_tokens: int,
2025-04-11T03:52:12.8067470Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8067542Z     ):
2025-04-11T03:52:12.8067632Z         torch.manual_seed(123)
2025-04-11T03:52:12.8067724Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8067815Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8067822Z 
2025-04-11T03:52:12.8067976Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8068087Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8068091Z 
2025-04-11T03:52:12.8068171Z device = None
2025-04-11T03:52:12.8068175Z 
2025-04-11T03:52:12.8068293Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8068513Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8068586Z     
2025-04-11T03:52:12.8068659Z         Args:
2025-04-11T03:52:12.8068897Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8069070Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8069181Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8069256Z         """
2025-04-11T03:52:12.8069340Z         _lazy_init()
2025-04-11T03:52:12.8069436Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8069537Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8069646Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8069926Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8070150Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8070307Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8070311Z 
2025-04-11T03:52:12.8070554Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8070709Z ______________ test_copy_kv_to_caches[False-5-False-16-16-16-32] _______________
2025-04-11T03:52:12.8070713Z 
2025-04-11T03:52:12.8070926Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8071088Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8071092Z 
2025-04-11T03:52:12.8071292Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8071404Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8071530Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8071671Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8071785Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8071928Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8072041Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8072191Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8072286Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8072367Z         bsz: int,
2025-04-11T03:52:12.8072454Z         block_size: int,
2025-04-11T03:52:12.8072547Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8072690Z         num_kv_heads: int,
2025-04-11T03:52:12.8072783Z         same_context_len: bool,
2025-04-11T03:52:12.8072862Z         n_tokens: int,
2025-04-11T03:52:12.8072957Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8073032Z     ):
2025-04-11T03:52:12.8073118Z         torch.manual_seed(123)
2025-04-11T03:52:12.8073211Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8073303Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8073306Z 
2025-04-11T03:52:12.8073460Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8073573Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8073577Z 
2025-04-11T03:52:12.8073658Z device = None
2025-04-11T03:52:12.8073662Z 
2025-04-11T03:52:12.8073780Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8073936Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8074006Z     
2025-04-11T03:52:12.8074081Z         Args:
2025-04-11T03:52:12.8074251Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8074418Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8074525Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8074602Z         """
2025-04-11T03:52:12.8074679Z         _lazy_init()
2025-04-11T03:52:12.8074776Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8074877Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8074984Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8075321Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8075460Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8075621Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8075626Z 
2025-04-11T03:52:12.8075867Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8076019Z _______________ test_copy_kv_to_caches[False-5-False-16-16-32-7] _______________
2025-04-11T03:52:12.8076023Z 
2025-04-11T03:52:12.8076172Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8076397Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8076401Z 
2025-04-11T03:52:12.8076600Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8076709Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8076831Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8076968Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8077143Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8077282Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8077396Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8077545Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8077638Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8077717Z         bsz: int,
2025-04-11T03:52:12.8077804Z         block_size: int,
2025-04-11T03:52:12.8077894Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8077976Z         num_kv_heads: int,
2025-04-11T03:52:12.8078069Z         same_context_len: bool,
2025-04-11T03:52:12.8078146Z         n_tokens: int,
2025-04-11T03:52:12.8078243Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8078316Z     ):
2025-04-11T03:52:12.8078404Z         torch.manual_seed(123)
2025-04-11T03:52:12.8078497Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8078588Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8078592Z 
2025-04-11T03:52:12.8078745Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8078910Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8078914Z 
2025-04-11T03:52:12.8078995Z device = None
2025-04-11T03:52:12.8078999Z 
2025-04-11T03:52:12.8079118Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8079266Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8079343Z     
2025-04-11T03:52:12.8079417Z         Args:
2025-04-11T03:52:12.8079585Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8079753Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8079863Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8079935Z         """
2025-04-11T03:52:12.8080014Z         _lazy_init()
2025-04-11T03:52:12.8080113Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8080214Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8080322Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8080602Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8080739Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8080898Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8080902Z 
2025-04-11T03:52:12.8081139Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8081359Z ______________ test_copy_kv_to_caches[False-5-False-16-16-32-32] _______________
2025-04-11T03:52:12.8081363Z 
2025-04-11T03:52:12.8081518Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8081683Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8081687Z 
2025-04-11T03:52:12.8081886Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8081994Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8082116Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8082249Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8082440Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8082581Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8082692Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8082842Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8082937Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8083013Z         bsz: int,
2025-04-11T03:52:12.8083099Z         block_size: int,
2025-04-11T03:52:12.8083256Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8083339Z         num_kv_heads: int,
2025-04-11T03:52:12.8083429Z         same_context_len: bool,
2025-04-11T03:52:12.8083510Z         n_tokens: int,
2025-04-11T03:52:12.8083603Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8083674Z     ):
2025-04-11T03:52:12.8083759Z         torch.manual_seed(123)
2025-04-11T03:52:12.8083851Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8083943Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8083948Z 
2025-04-11T03:52:12.8084099Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8084211Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8084215Z 
2025-04-11T03:52:12.8084293Z device = None
2025-04-11T03:52:12.8084299Z 
2025-04-11T03:52:12.8084416Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8084567Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8084643Z     
2025-04-11T03:52:12.8084717Z         Args:
2025-04-11T03:52:12.8084942Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8085111Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8085219Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8085291Z         """
2025-04-11T03:52:12.8085370Z         _lazy_init()
2025-04-11T03:52:12.8085471Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8085573Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8085684Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8085964Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8086102Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8086259Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8086265Z 
2025-04-11T03:52:12.8086503Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8086659Z _______________ test_copy_kv_to_caches[False-5-False-16-16-64-7] _______________
2025-04-11T03:52:12.8086663Z 
2025-04-11T03:52:12.8086810Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8086967Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8086973Z 
2025-04-11T03:52:12.8087169Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8087278Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8087457Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8087598Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8087716Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8087859Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8087977Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8088131Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8088228Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8088308Z         bsz: int,
2025-04-11T03:52:12.8088393Z         block_size: int,
2025-04-11T03:52:12.8088490Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8088630Z         num_kv_heads: int,
2025-04-11T03:52:12.8088719Z         same_context_len: bool,
2025-04-11T03:52:12.8088799Z         n_tokens: int,
2025-04-11T03:52:12.8088895Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8088967Z     ):
2025-04-11T03:52:12.8089056Z         torch.manual_seed(123)
2025-04-11T03:52:12.8089147Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8089236Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8089240Z 
2025-04-11T03:52:12.8089393Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8089560Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8089565Z 
2025-04-11T03:52:12.8089646Z device = None
2025-04-11T03:52:12.8089650Z 
2025-04-11T03:52:12.8089766Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8089916Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8089992Z     
2025-04-11T03:52:12.8090066Z         Args:
2025-04-11T03:52:12.8090236Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8090401Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8090513Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8090586Z         """
2025-04-11T03:52:12.8090664Z         _lazy_init()
2025-04-11T03:52:12.8090764Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8090866Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8090975Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8091312Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8091448Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8091612Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8091618Z 
2025-04-11T03:52:12.8091857Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8092018Z ______________ test_copy_kv_to_caches[False-5-False-16-16-64-32] _______________
2025-04-11T03:52:12.8092022Z 
2025-04-11T03:52:12.8092170Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8092327Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8092333Z 
2025-04-11T03:52:12.8092531Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8092642Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8092765Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8092898Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8093013Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8093150Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8093263Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8093412Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8093502Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8093631Z         bsz: int,
2025-04-11T03:52:12.8093719Z         block_size: int,
2025-04-11T03:52:12.8093815Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8093896Z         num_kv_heads: int,
2025-04-11T03:52:12.8093985Z         same_context_len: bool,
2025-04-11T03:52:12.8094062Z         n_tokens: int,
2025-04-11T03:52:12.8094154Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8094232Z     ):
2025-04-11T03:52:12.8094320Z         torch.manual_seed(123)
2025-04-11T03:52:12.8094412Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8094503Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8094507Z 
2025-04-11T03:52:12.8094663Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8094833Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8094837Z 
2025-04-11T03:52:12.8094914Z device = None
2025-04-11T03:52:12.8094917Z 
2025-04-11T03:52:12.8095039Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8095188Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8095262Z     
2025-04-11T03:52:12.8095335Z         Args:
2025-04-11T03:52:12.8095550Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8095717Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8095823Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8095902Z         """
2025-04-11T03:52:12.8095981Z         _lazy_init()
2025-04-11T03:52:12.8096081Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8096183Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8096289Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8096572Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8096706Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8096869Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8096873Z 
2025-04-11T03:52:12.8097119Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8097315Z _______________________________ test_layer_norm ________________________________
2025-04-11T03:52:12.8097319Z 
2025-04-11T03:52:12.8097416Z kwargs = {}, val = 2, arg_map = {'M': 2}
2025-04-11T03:52:12.8097754Z partial_func = functools.partial(<function parameterize.<locals>._wrapper.<locals>._execute_function_by_param at 0x7f68f05e1750>, M=2)
2025-04-11T03:52:12.8097758Z 
2025-04-11T03:52:12.8097865Z     def _execute_function_by_param(**kwargs):
2025-04-11T03:52:12.8097953Z         for val in values:
2025-04-11T03:52:12.8098045Z             arg_map = {argument: val}
2025-04-11T03:52:12.8098153Z             partial_func = partial(func, **arg_map)
2025-04-11T03:52:12.8098250Z >           partial_func(**kwargs)
2025-04-11T03:52:12.8098254Z 
2025-04-11T03:52:12.8098351Z colossalai/testing/utils.py:64: 
2025-04-11T03:52:12.8098465Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8098621Z colossalai/testing/utils.py:64: in _execute_function_by_param
2025-04-11T03:52:12.8098717Z     partial_func(**kwargs)
2025-04-11T03:52:12.8098825Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8098829Z 
2025-04-11T03:52:12.8098904Z M = 2, N = 64
2025-04-11T03:52:12.8098908Z 
2025-04-11T03:52:12.8099001Z     @pytest.mark.skipif(
2025-04-11T03:52:12.8099244Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T03:52:12.8099322Z     )
2025-04-11T03:52:12.8099415Z     @parameterize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.8099509Z     @parameterize("N", [64, 128])
2025-04-11T03:52:12.8099601Z     def test_layer_norm(M, N):
2025-04-11T03:52:12.8099743Z         dtype = torch.float16
2025-04-11T03:52:12.8099831Z         eps = 1e-5
2025-04-11T03:52:12.8099912Z         x_shape = (M, N)
2025-04-11T03:52:12.8100004Z         w_shape = (x_shape[-1],)
2025-04-11T03:52:12.8100149Z >       weight = torch.ones(w_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.8100256Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8100544Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8100678Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8100841Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8100906Z 
2025-04-11T03:52:12.8101087Z tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py:30: RuntimeError
2025-04-11T03:52:12.8101248Z ___________________ test_rotary_emb[True-dtype0-64-32-64-4] ____________________
2025-04-11T03:52:12.8101252Z 
2025-04-11T03:52:12.8101395Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, D = 64, dtype = torch.float32
2025-04-11T03:52:12.8101492Z use_new_kcache_layout = True
2025-04-11T03:52:12.8101496Z 
2025-04-11T03:52:12.8101630Z     @pytest.mark.skipif(
2025-04-11T03:52:12.8101873Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T03:52:12.8101951Z     )
2025-04-11T03:52:12.8102062Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.8102172Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T03:52:12.8102269Z     @pytest.mark.parametrize("H", [32])
2025-04-11T03:52:12.8102369Z     @pytest.mark.parametrize("D", [64])
2025-04-11T03:52:12.8102499Z     @pytest.mark.parametrize("dtype", [torch.float32])
2025-04-11T03:52:12.8102656Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8102835Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, D, dtype, use_new_kcache_layout):
2025-04-11T03:52:12.8102936Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T03:52:12.8103042Z         # our crafted op equals to Transformers
2025-04-11T03:52:12.8103172Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.8103302Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.8103462Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T03:52:12.8103634Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T03:52:12.8103737Z         cos, sin = emb(x0, position_ids)
2025-04-11T03:52:12.8103861Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T03:52:12.8103967Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.8104068Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.8104156Z         cos_2 = cos[:, :32]
2025-04-11T03:52:12.8104246Z         sin_2 = sin[:, :32]
2025-04-11T03:52:12.8104367Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.8104503Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T03:52:12.8104714Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T03:52:12.8104848Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T03:52:12.8104920Z     
2025-04-11T03:52:12.8105000Z         # create data
2025-04-11T03:52:12.8105091Z         block_size = 32
2025-04-11T03:52:12.8105186Z         max_num_blocks_per_seq = 4
2025-04-11T03:52:12.8105284Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.8105431Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.8105536Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8105830Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8105965Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8106197Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8106201Z 
2025-04-11T03:52:12.8106404Z tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py:65: RuntimeError
2025-04-11T03:52:12.8106566Z ___________________ test_rotary_emb[False-dtype0-64-32-64-4] ___________________
2025-04-11T03:52:12.8106569Z 
2025-04-11T03:52:12.8106711Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, D = 64, dtype = torch.float32
2025-04-11T03:52:12.8106808Z use_new_kcache_layout = False
2025-04-11T03:52:12.8106812Z 
2025-04-11T03:52:12.8106902Z     @pytest.mark.skipif(
2025-04-11T03:52:12.8107140Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T03:52:12.8107303Z     )
2025-04-11T03:52:12.8107414Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.8107524Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T03:52:12.8107621Z     @pytest.mark.parametrize("H", [32])
2025-04-11T03:52:12.8107723Z     @pytest.mark.parametrize("D", [64])
2025-04-11T03:52:12.8107858Z     @pytest.mark.parametrize("dtype", [torch.float32])
2025-04-11T03:52:12.8108011Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8108251Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, D, dtype, use_new_kcache_layout):
2025-04-11T03:52:12.8108349Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T03:52:12.8108499Z         # our crafted op equals to Transformers
2025-04-11T03:52:12.8108627Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.8108753Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.8108851Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T03:52:12.8109022Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T03:52:12.8109121Z         cos, sin = emb(x0, position_ids)
2025-04-11T03:52:12.8109242Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T03:52:12.8109346Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.8109441Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.8109525Z         cos_2 = cos[:, :32]
2025-04-11T03:52:12.8109616Z         sin_2 = sin[:, :32]
2025-04-11T03:52:12.8109735Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.8109925Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T03:52:12.8110135Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T03:52:12.8110265Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T03:52:12.8110337Z     
2025-04-11T03:52:12.8110417Z         # create data
2025-04-11T03:52:12.8110507Z         block_size = 32
2025-04-11T03:52:12.8110602Z         max_num_blocks_per_seq = 4
2025-04-11T03:52:12.8110699Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.8110840Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.8110948Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8111237Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8111370Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8111533Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8111537Z 
2025-04-11T03:52:12.8111736Z tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py:65: RuntimeError
2025-04-11T03:52:12.8111885Z _____________________ test_get_xine_cache[dtype0-64-64-4] ______________________
2025-04-11T03:52:12.8111889Z 
2025-04-11T03:52:12.8112036Z BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float32
2025-04-11T03:52:12.8112040Z 
2025-04-11T03:52:12.8112137Z     @pytest.mark.skipif(
2025-04-11T03:52:12.8112370Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T03:52:12.8112506Z     )
2025-04-11T03:52:12.8112620Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.8112736Z     @pytest.mark.parametrize("MAX_SEQ_LEN", [64])
2025-04-11T03:52:12.8112850Z     @pytest.mark.parametrize("HEAD_DIM", [64])
2025-04-11T03:52:12.8112977Z     @pytest.mark.parametrize("dtype", [torch.float32])
2025-04-11T03:52:12.8113128Z     def test_get_xine_cache(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
2025-04-11T03:52:12.8113235Z         MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
2025-04-11T03:52:12.8113417Z >       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
2025-04-11T03:52:12.8113524Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8113883Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8114021Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8114183Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8114187Z 
2025-04-11T03:52:12.8114363Z tests/test_infer/test_kernels/triton/test_xine_copy.py:50: RuntimeError
2025-04-11T03:52:12.8114579Z _____________________ test_models_lazy_init[cuda-subset0] ______________________
2025-04-11T03:52:12.8114583Z 
2025-04-11T03:52:12.8114999Z subset = ['custom_hanging_param_model', 'custom_nested_model', 'custom_repeated_computed_layers', 'custom_simple_net', 'diffusers_clip_text_model', 'diffusers_auto_encoder_kl', ...]
2025-04-11T03:52:12.8115088Z default_device = 'cuda'
2025-04-11T03:52:12.8115092Z 
2025-04-11T03:52:12.8115264Z     @pytest.mark.skipif(not SUPPORT_LAZY, reason="requires torch >= 1.12.0")
2025-04-11T03:52:12.8115365Z     @pytest.mark.parametrize(
2025-04-11T03:52:12.8115441Z         "subset",
2025-04-11T03:52:12.8115520Z         (
2025-04-11T03:52:12.8115603Z             [COMMON_MODELS]
2025-04-11T03:52:12.8115687Z             if IS_FAST_TEST
2025-04-11T03:52:12.8115890Z             else ["torchvision", "diffusers", "timm", "transformers", "torchaudio", "deepfm", "dlrm"]
2025-04-11T03:52:12.8115966Z         ),
2025-04-11T03:52:12.8116042Z     )
2025-04-11T03:52:12.8116186Z     @pytest.mark.parametrize("default_device", ["cpu", "cuda"])
2025-04-11T03:52:12.8116310Z     def test_models_lazy_init(subset, default_device):
2025-04-11T03:52:12.8116524Z         sub_model_zoo = model_zoo.get_sub_registry(subset, allow_empty=True)
2025-04-11T03:52:12.8116636Z         for name, entry in sub_model_zoo.items():
2025-04-11T03:52:12.8116806Z             # TODO(ver217): lazy init does not support weight norm, skip these models
2025-04-11T03:52:12.8116886Z             if name in (
2025-04-11T03:52:12.8116988Z                 "torchaudio_wav2vec2_base",
2025-04-11T03:52:12.8117081Z                 "torchaudio_hubert_base",
2025-04-11T03:52:12.8117169Z                 "timm_beit",
2025-04-11T03:52:12.8117264Z                 "timm_vision_transformer",
2025-04-11T03:52:12.8117345Z                 "timm_deit",
2025-04-11T03:52:12.8117434Z                 "timm_beitv2",
2025-04-11T03:52:12.8117516Z                 "timm_deit3",
2025-04-11T03:52:12.8117600Z                 "timm_convit",
2025-04-11T03:52:12.8117694Z                 "timm_tnt_b_patch16_224",
2025-04-11T03:52:12.8117912Z             ) or name.startswith(("transformers_vit", "transformers_blip2", "transformers_whisper")):
2025-04-11T03:52:12.8117994Z                 continue
2025-04-11T03:52:12.8118153Z >           check_lazy_init(entry, verbose=True, default_device=default_device)
2025-04-11T03:52:12.8118157Z 
2025-04-11T03:52:12.8118259Z tests/test_lazy/test_models.py:33: 
2025-04-11T03:52:12.8118370Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8118513Z tests/test_lazy/lazy_init_utils.py:77: in check_lazy_init
2025-04-11T03:52:12.8118598Z     model = model_fn()
2025-04-11T03:52:12.8118767Z tests/kit/model_zoo/custom/hanging_param_model.py:17: in __init__
2025-04-11T03:52:12.8118918Z     self.proj1 = nn.Linear(4, 8)
2025-04-11T03:52:12.8119178Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/linear.py:98: in __init__
2025-04-11T03:52:12.8119382Z     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
2025-04-11T03:52:12.8119493Z colossalai/lazy/lazy_init.py:506: in wrapper
2025-04-11T03:52:12.8119612Z     return self.tensor_cls(target, *args, **kwargs)
2025-04-11T03:52:12.8119720Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8119724Z 
2025-04-11T03:52:12.8119849Z cls = <class 'colossalai.lazy.lazy_init._MyTensor'>
2025-04-11T03:52:12.8120000Z func = <built-in method empty of type object at 0x7f6bf0606840>
2025-04-11T03:52:12.8120161Z concrete_data = None, args = ((8, 4),)
2025-04-11T03:52:12.8120269Z kwargs = {'device': 'cuda', 'dtype': None}
2025-04-11T03:52:12.8120273Z 
2025-04-11T03:52:12.8120439Z     def __new__(cls, func, *args, concrete_data=None, **kwargs) -> "_MyTensor":
2025-04-11T03:52:12.8120529Z         cls._pre_op_fn()
2025-04-11T03:52:12.8120621Z         if concrete_data is not None:
2025-04-11T03:52:12.8120716Z             # uniform api as LazyTensor
2025-04-11T03:52:12.8120857Z             data = concrete_data
2025-04-11T03:52:12.8120934Z         else:
2025-04-11T03:52:12.8121043Z             kwargs["device"] = cls.default_device
2025-04-11T03:52:12.8121137Z >           data = func(*args, **kwargs)
2025-04-11T03:52:12.8121248Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8121536Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8121679Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8121841Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8121845Z 
2025-04-11T03:52:12.8121954Z colossalai/lazy/lazy_init.py:93: RuntimeError
2025-04-11T03:52:12.8122092Z ________________________________ test_lazy_ops _________________________________
2025-04-11T03:52:12.8122097Z 
2025-04-11T03:52:12.8122264Z     @pytest.mark.skipif(not SUPPORT_LAZY, reason="requires torch >= 1.12.0")
2025-04-11T03:52:12.8122357Z     def test_lazy_ops():
2025-04-11T03:52:12.8122447Z         with LazyInitContext():
2025-04-11T03:52:12.8122588Z             x = torch.rand(2, 3)
2025-04-11T03:52:12.8122686Z             assert tuple(x.shape) == (2, 3)
2025-04-11T03:52:12.8122781Z             assert x.device.type == "cpu"
2025-04-11T03:52:12.8122876Z             x.requires_grad is False
2025-04-11T03:52:12.8122956Z             y = x.cuda()
2025-04-11T03:52:12.8123053Z             assert tuple(y.shape) == (2, 3)
2025-04-11T03:52:12.8123149Z             assert y.device.type == "cuda"
2025-04-11T03:52:12.8123241Z             assert y.requires_grad is False
2025-04-11T03:52:12.8123333Z             assert x.cpu() is x
2025-04-11T03:52:12.8123434Z             p = Parameter(torch.empty(2, 3))
2025-04-11T03:52:12.8123532Z             assert tuple(p.shape) == (2, 3)
2025-04-11T03:52:12.8123626Z             assert p.device.type == "cpu"
2025-04-11T03:52:12.8123723Z             assert p.requires_grad is True
2025-04-11T03:52:12.8123821Z             assert isinstance(p, Parameter)
2025-04-11T03:52:12.8123904Z         x.materialize()
2025-04-11T03:52:12.8124001Z         assert tuple(x.shape) == (2, 3)
2025-04-11T03:52:12.8124095Z         assert x.device.type == "cpu"
2025-04-11T03:52:12.8124193Z         assert x.requires_grad is False
2025-04-11T03:52:12.8124275Z >       y.materialize()
2025-04-11T03:52:12.8124279Z 
2025-04-11T03:52:12.8124371Z tests/test_lazy/test_ops.py:33: 
2025-04-11T03:52:12.8124488Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8124607Z colossalai/lazy/lazy_init.py:217: in materialize
2025-04-11T03:52:12.8124705Z     target = self._materialize_data()
2025-04-11T03:52:12.8124836Z colossalai/lazy/lazy_init.py:242: in _materialize_data
2025-04-11T03:52:12.8124974Z     init_val = func(
2025-04-11T03:52:12.8125088Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8125092Z 
2025-04-11T03:52:12.8125180Z t = tensor([[0.8823, 0.9150, 0.3829],
2025-04-11T03:52:12.8125269Z         [0.9593, 0.3904, 0.6009]])
2025-04-11T03:52:12.8125365Z kw = {'device': device(type='cuda')}
2025-04-11T03:52:12.8125371Z 
2025-04-11T03:52:12.8125479Z     def factory_fn(t: torch.Tensor, **kw):
2025-04-11T03:52:12.8125570Z >       return t.to(*args, **kwargs)
2025-04-11T03:52:12.8125685Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8125979Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8126175Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8126338Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8126342Z 
2025-04-11T03:52:12.8126454Z colossalai/lazy/lazy_init.py:380: RuntimeError
2025-04-11T03:52:12.8126595Z _____________________________ test_torch_ddp_lora ______________________________
2025-04-11T03:52:12.8126599Z 
2025-04-11T03:52:12.8126736Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8127349Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8127355Z 
2025-04-11T03:52:12.8127456Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8127541Z         try_count = 0
2025-04-11T03:52:12.8127642Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8127722Z             max_try, int
2025-04-11T03:52:12.8127890Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8127987Z     
2025-04-11T03:52:12.8128138Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8128220Z             try:
2025-04-11T03:52:12.8128314Z                 try_count += 1
2025-04-11T03:52:12.8128406Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8128489Z                 return ret
2025-04-11T03:52:12.8128588Z             except exception_type as e:
2025-04-11T03:52:12.8128755Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8128949Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8129070Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8129219Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8129375Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8129457Z                     continue
2025-04-11T03:52:12.8129537Z                 else:
2025-04-11T03:52:12.8129760Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8129843Z >                   raise e
2025-04-11T03:52:12.8129847Z 
2025-04-11T03:52:12.8129944Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8130057Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8130190Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8130274Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8130413Z tests/test_lora/test_lora.py:108: in test_torch_ddp_lora
2025-04-11T03:52:12.8130495Z     spawn(run_dist, 2)
2025-04-11T03:52:12.8130597Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8130699Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8130958Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8131140Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8131497Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8131593Z     while not context.join():
2025-04-11T03:52:12.8131701Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8131707Z 
2025-04-11T03:52:12.8131907Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be3c5210>
2025-04-11T03:52:12.8131988Z timeout = None
2025-04-11T03:52:12.8131992Z 
2025-04-11T03:52:12.8132086Z     def join(self, timeout=None):
2025-04-11T03:52:12.8132211Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8132284Z     
2025-04-11T03:52:12.8132432Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8132640Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8132807Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8132902Z         of the first process exiting.
2025-04-11T03:52:12.8132971Z     
2025-04-11T03:52:12.8133122Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8133258Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8133392Z     
2025-04-11T03:52:12.8133468Z         Args:
2025-04-11T03:52:12.8133608Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8133683Z         """
2025-04-11T03:52:12.8133823Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8133921Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8134001Z             return True
2025-04-11T03:52:12.8134078Z     
2025-04-11T03:52:12.8134212Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8134331Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8134428Z             self.sentinels.keys(),
2025-04-11T03:52:12.8134512Z             timeout=timeout,
2025-04-11T03:52:12.8134591Z         )
2025-04-11T03:52:12.8134661Z     
2025-04-11T03:52:12.8134749Z         error_index = None
2025-04-11T03:52:12.8134835Z         for sentinel in ready:
2025-04-11T03:52:12.8134944Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8135047Z             process = self.processes[index]
2025-04-11T03:52:12.8135187Z             process.join()
2025-04-11T03:52:12.8135285Z             if process.exitcode != 0:
2025-04-11T03:52:12.8135374Z                 error_index = index
2025-04-11T03:52:12.8135450Z                 break
2025-04-11T03:52:12.8135526Z     
2025-04-11T03:52:12.8135616Z         # Return if there was no error.
2025-04-11T03:52:12.8135705Z         if error_index is None:
2025-04-11T03:52:12.8135841Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8135938Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8136014Z     
2025-04-11T03:52:12.8136155Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8136260Z         for process in self.processes:
2025-04-11T03:52:12.8136350Z             if process.is_alive():
2025-04-11T03:52:12.8136448Z                 process.terminate()
2025-04-11T03:52:12.8136534Z             process.join()
2025-04-11T03:52:12.8136604Z     
2025-04-11T03:52:12.8136748Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8136864Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8136976Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8137097Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8137179Z             if exitcode < 0:
2025-04-11T03:52:12.8137292Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8137398Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8137554Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8137714Z                     error_index=error_index,
2025-04-11T03:52:12.8137821Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8137910Z                     exit_code=exitcode,
2025-04-11T03:52:12.8138000Z                     signal_name=name,
2025-04-11T03:52:12.8138079Z                 )
2025-04-11T03:52:12.8138153Z             else:
2025-04-11T03:52:12.8138263Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8138427Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8138523Z                     error_index=error_index,
2025-04-11T03:52:12.8138626Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8138853Z                     exit_code=exitcode,
2025-04-11T03:52:12.8138937Z                 )
2025-04-11T03:52:12.8139008Z     
2025-04-11T03:52:12.8139147Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8139321Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8139408Z         msg += original_trace
2025-04-11T03:52:12.8139585Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8139800Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8139877Z E       
2025-04-11T03:52:12.8140008Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8140112Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8140420Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8140502Z E           fn(i, *args)
2025-04-11T03:52:12.8140723Z E         File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 103, in run_dist
2025-04-11T03:52:12.8140809Z E           run_lora_test()
2025-04-11T03:52:12.8141032Z E         File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 98, in run_lora_test
2025-04-11T03:52:12.8141211Z E           check_fwd_bwd(model_fn, data_gen_fn, output_transform_fn, loss_fn, task_type)
2025-04-11T03:52:12.8141432Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.8141534Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.8141848Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.8141956Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.8142243Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.8142352Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8142459Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8142746Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8142884Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8143044Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8143051Z 
2025-04-11T03:52:12.8143359Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8143519Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8143681Z [04/11/25 03:46:35] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8143810Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8143921Z                              :75 launch                                         
2025-04-11T03:52:12.8144060Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8144189Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8144440Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8144589Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8145733Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8145906Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8147068Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8147240Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8147994Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8148081Z   warnings.warn(
2025-04-11T03:52:12.8148803Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8148891Z   warnings.warn(
2025-04-11T03:52:12.8149719Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8149801Z   warnings.warn(
2025-04-11T03:52:12.8150665Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8150752Z   warnings.warn(
2025-04-11T03:52:12.8151561Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8151643Z   warnings.warn(
2025-04-11T03:52:12.8152437Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8152518Z   warnings.warn(
2025-04-11T03:52:12.8153315Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8153395Z   warnings.warn(
2025-04-11T03:52:12.8154261Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8154342Z   warnings.warn(
2025-04-11T03:52:12.8154901Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8154980Z   warnings.warn(
2025-04-11T03:52:12.8155511Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8155658Z   warnings.warn(
2025-04-11T03:52:12.8156207Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8156293Z   warnings.warn(
2025-04-11T03:52:12.8156828Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8156991Z   warnings.warn(
2025-04-11T03:52:12.8157141Z _________________________ test_moe_kernel[data_type0] __________________________
2025-04-11T03:52:12.8157146Z 
2025-04-11T03:52:12.8157242Z data_type = torch.float32
2025-04-11T03:52:12.8157247Z 
2025-04-11T03:52:12.8157420Z     @pytest.mark.parametrize("data_type", [torch.float32, torch.float16])
2025-04-11T03:52:12.8157517Z     def test_moe_kernel(data_type):
2025-04-11T03:52:12.8157614Z         torch.manual_seed(1024)
2025-04-11T03:52:12.8157698Z >       run_moe_cumsum()
2025-04-11T03:52:12.8157702Z 
2025-04-11T03:52:12.8157804Z tests/test_moe/test_kernel.py:93: 
2025-04-11T03:52:12.8157919Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8157923Z 
2025-04-11T03:52:12.8158011Z     def run_moe_cumsum():
2025-04-11T03:52:12.8158101Z         test_mask = torch.tensor(
2025-04-11T03:52:12.8158176Z             [
2025-04-11T03:52:12.8158262Z                 [0, 1, 0, 0],
2025-04-11T03:52:12.8158342Z                 [1, 0, 0, 0],
2025-04-11T03:52:12.8158476Z                 [0, 1, 0, 0],
2025-04-11T03:52:12.8158554Z                 [1, 0, 0, 0],
2025-04-11T03:52:12.8158633Z             ],
2025-04-11T03:52:12.8158725Z             dtype=torch.int32,
2025-04-11T03:52:12.8158805Z >       ).to("cuda")
2025-04-11T03:52:12.8158919Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8159211Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8159354Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8159522Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8159526Z 
2025-04-11T03:52:12.8159644Z tests/test_moe/test_kernel.py:29: RuntimeError
2025-04-11T03:52:12.8159784Z _________________________ test_moe_kernel[data_type1] __________________________
2025-04-11T03:52:12.8159790Z 
2025-04-11T03:52:12.8159878Z data_type = torch.float16
2025-04-11T03:52:12.8159882Z 
2025-04-11T03:52:12.8160062Z     @pytest.mark.parametrize("data_type", [torch.float32, torch.float16])
2025-04-11T03:52:12.8160156Z     def test_moe_kernel(data_type):
2025-04-11T03:52:12.8160248Z         torch.manual_seed(1024)
2025-04-11T03:52:12.8160332Z >       run_moe_cumsum()
2025-04-11T03:52:12.8160335Z 
2025-04-11T03:52:12.8160438Z tests/test_moe/test_kernel.py:93: 
2025-04-11T03:52:12.8160550Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8160554Z 
2025-04-11T03:52:12.8160636Z     def run_moe_cumsum():
2025-04-11T03:52:12.8160730Z         test_mask = torch.tensor(
2025-04-11T03:52:12.8160805Z             [
2025-04-11T03:52:12.8160952Z                 [0, 1, 0, 0],
2025-04-11T03:52:12.8161034Z                 [1, 0, 0, 0],
2025-04-11T03:52:12.8161117Z                 [0, 1, 0, 0],
2025-04-11T03:52:12.8161196Z                 [1, 0, 0, 0],
2025-04-11T03:52:12.8161276Z             ],
2025-04-11T03:52:12.8161374Z             dtype=torch.int32,
2025-04-11T03:52:12.8161457Z >       ).to("cuda")
2025-04-11T03:52:12.8161573Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8161857Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8161999Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8162228Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8162233Z 
2025-04-11T03:52:12.8162345Z tests/test_moe/test_kernel.py:29: RuntimeError
2025-04-11T03:52:12.8162492Z __________________________ test_mixtral_moe_layer[4] ___________________________
2025-04-11T03:52:12.8162496Z 
2025-04-11T03:52:12.8162573Z world_size = 4
2025-04-11T03:52:12.8162577Z 
2025-04-11T03:52:12.8162693Z     @pytest.mark.parametrize("world_size", [4])
2025-04-11T03:52:12.8162851Z     def test_mixtral_moe_layer(world_size: int):
2025-04-11T03:52:12.8162945Z >       spawn(run_dist, world_size)
2025-04-11T03:52:12.8162950Z 
2025-04-11T03:52:12.8163058Z tests/test_moe/test_moe_checkpoint.py:171: 
2025-04-11T03:52:12.8163167Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8163272Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8163374Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8163637Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8163816Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8164108Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8164198Z     while not context.join():
2025-04-11T03:52:12.8164305Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8164310Z 
2025-04-11T03:52:12.8164514Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be4edd20>
2025-04-11T03:52:12.8164648Z timeout = None
2025-04-11T03:52:12.8164653Z 
2025-04-11T03:52:12.8164749Z     def join(self, timeout=None):
2025-04-11T03:52:12.8164878Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8164951Z     
2025-04-11T03:52:12.8165096Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8165243Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8165410Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8165504Z         of the first process exiting.
2025-04-11T03:52:12.8165581Z     
2025-04-11T03:52:12.8165730Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8165869Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8165943Z     
2025-04-11T03:52:12.8166018Z         Args:
2025-04-11T03:52:12.8166159Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8166236Z         """
2025-04-11T03:52:12.8166381Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8166475Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8166555Z             return True
2025-04-11T03:52:12.8166629Z     
2025-04-11T03:52:12.8166760Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8166889Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8166982Z             self.sentinels.keys(),
2025-04-11T03:52:12.8167070Z             timeout=timeout,
2025-04-11T03:52:12.8167144Z         )
2025-04-11T03:52:12.8167215Z     
2025-04-11T03:52:12.8167358Z         error_index = None
2025-04-11T03:52:12.8167450Z         for sentinel in ready:
2025-04-11T03:52:12.8167565Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8167666Z             process = self.processes[index]
2025-04-11T03:52:12.8167752Z             process.join()
2025-04-11T03:52:12.8167853Z             if process.exitcode != 0:
2025-04-11T03:52:12.8167939Z                 error_index = index
2025-04-11T03:52:12.8168020Z                 break
2025-04-11T03:52:12.8168091Z     
2025-04-11T03:52:12.8168182Z         # Return if there was no error.
2025-04-11T03:52:12.8168273Z         if error_index is None:
2025-04-11T03:52:12.8168405Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8168564Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8168636Z     
2025-04-11T03:52:12.8168785Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8168884Z         for process in self.processes:
2025-04-11T03:52:12.8168977Z             if process.is_alive():
2025-04-11T03:52:12.8169071Z                 process.terminate()
2025-04-11T03:52:12.8169156Z             process.join()
2025-04-11T03:52:12.8169287Z     
2025-04-11T03:52:12.8169432Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8169554Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8169670Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8169794Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8169881Z             if exitcode < 0:
2025-04-11T03:52:12.8169994Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8170110Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8170263Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8170362Z                     error_index=error_index,
2025-04-11T03:52:12.8170470Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8170561Z                     exit_code=exitcode,
2025-04-11T03:52:12.8170656Z                     signal_name=name,
2025-04-11T03:52:12.8170731Z                 )
2025-04-11T03:52:12.8170807Z             else:
2025-04-11T03:52:12.8170918Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8171139Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8171239Z                     error_index=error_index,
2025-04-11T03:52:12.8171340Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8171429Z                     exit_code=exitcode,
2025-04-11T03:52:12.8171505Z                 )
2025-04-11T03:52:12.8171578Z     
2025-04-11T03:52:12.8171717Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8171890Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8171982Z         msg += original_trace
2025-04-11T03:52:12.8172155Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8172320Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8172401Z E       
2025-04-11T03:52:12.8172527Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8172631Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8172937Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8173025Z E           fn(i, *args)
2025-04-11T03:52:12.8173258Z E         File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 165, in run_dist
2025-04-11T03:52:12.8173354Z E           check_moe_checkpoint()
2025-04-11T03:52:12.8173615Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8173704Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8174024Z E         File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 101, in check_moe_checkpoint
2025-04-11T03:52:12.8174160Z E           dist.broadcast_object_list(broadcast_objects, src=0)
2025-04-11T03:52:12.8174466Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:12.8174559Z E           return func(*args, **kwargs)
2025-04-11T03:52:12.8174923Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in broadcast_object_list
2025-04-11T03:52:12.8175138Z E           tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T03:52:12.8175534Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in <listcomp>
2025-04-11T03:52:12.8175748Z E           tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T03:52:12.8176096Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2115, in _object_to_tensor
2025-04-11T03:52:12.8176296Z E           byte_tensor = torch.ByteTensor(byte_storage).to(device)
2025-04-11T03:52:12.8176405Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8176691Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8176830Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8176993Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8176998Z 
2025-04-11T03:52:12.8177302Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8177457Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8177624Z [04/11/25 03:46:41] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8177752Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8177865Z                              :75 launch                                         
2025-04-11T03:52:12.8178059Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8178191Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8178340Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8178641Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:27296 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8178927Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:27296 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8179100Z _____________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False] ______________
2025-04-11T03:52:12.8179104Z 
2025-04-11T03:52:12.8179252Z adamw = False, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T03:52:12.8179344Z g_dtype = torch.float16
2025-04-11T03:52:12.8179349Z 
2025-04-11T03:52:12.8179480Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8179617Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8179796Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8179958Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8180053Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8180195Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8180291Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8180439Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8180529Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8180677Z >       check_adam_kernel(
2025-04-11T03:52:12.8180939Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8181015Z         )
2025-04-11T03:52:12.8181019Z 
2025-04-11T03:52:12.8181139Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8181250Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8181415Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8181566Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8181681Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8181758Z 
2025-04-11T03:52:12.8181942Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be3c54b0>, lr = 0.001
2025-04-11T03:52:12.8182105Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T03:52:12.8182109Z 
2025-04-11T03:52:12.8182353Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8182501Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8182730Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8182802Z     
2025-04-11T03:52:12.8182922Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8183041Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8183292Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8183403Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8183683Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8183821Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8183979Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8183983Z 
2025-04-11T03:52:12.8184127Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8184301Z ______________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True] ______________
2025-04-11T03:52:12.8184305Z 
2025-04-11T03:52:12.8184496Z adamw = True, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T03:52:12.8184587Z g_dtype = torch.float16
2025-04-11T03:52:12.8184591Z 
2025-04-11T03:52:12.8184720Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8184848Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8185018Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8185176Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8185267Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8185403Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8185491Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8185631Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8185718Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8185803Z >       check_adam_kernel(
2025-04-11T03:52:12.8186065Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8186139Z         )
2025-04-11T03:52:12.8186143Z 
2025-04-11T03:52:12.8186260Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8186372Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8186534Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8186682Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8186846Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8186851Z 
2025-04-11T03:52:12.8187035Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f688e8e6650>, lr = 0.001
2025-04-11T03:52:12.8187190Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T03:52:12.8187195Z 
2025-04-11T03:52:12.8187441Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8187581Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8187749Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8187823Z     
2025-04-11T03:52:12.8187938Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8188122Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8188369Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8188528Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8188813Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8189016Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8189175Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8189179Z 
2025-04-11T03:52:12.8189317Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8189479Z _____________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False] ______________
2025-04-11T03:52:12.8189483Z 
2025-04-11T03:52:12.8189613Z adamw = False, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T03:52:12.8189704Z g_dtype = torch.float16
2025-04-11T03:52:12.8189708Z 
2025-04-11T03:52:12.8189828Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8189957Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8190126Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8190279Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8190366Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8190499Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8190649Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8190786Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8190875Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8190960Z >       check_adam_kernel(
2025-04-11T03:52:12.8191219Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8191294Z         )
2025-04-11T03:52:12.8191298Z 
2025-04-11T03:52:12.8191411Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8191526Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8191685Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8191836Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8191945Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8191949Z 
2025-04-11T03:52:12.8192130Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be78d090>, lr = 0.001
2025-04-11T03:52:12.8192280Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T03:52:12.8192284Z 
2025-04-11T03:52:12.8192527Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8192668Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8192832Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8192909Z     
2025-04-11T03:52:12.8193085Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8193212Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8193456Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8193569Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8193852Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8193985Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8194144Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8194148Z 
2025-04-11T03:52:12.8194348Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8194516Z ______________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True] ______________
2025-04-11T03:52:12.8194520Z 
2025-04-11T03:52:12.8194650Z adamw = True, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T03:52:12.8194742Z g_dtype = torch.float16
2025-04-11T03:52:12.8194746Z 
2025-04-11T03:52:12.8194867Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8194998Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8195230Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8195386Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8195476Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8195606Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8195695Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8195828Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8195917Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8195999Z >       check_adam_kernel(
2025-04-11T03:52:12.8196253Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8196332Z         )
2025-04-11T03:52:12.8196336Z 
2025-04-11T03:52:12.8196452Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8196569Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8196727Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8196949Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8197058Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8197062Z 
2025-04-11T03:52:12.8197239Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be3374f0>, lr = 0.001
2025-04-11T03:52:12.8197397Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T03:52:12.8197401Z 
2025-04-11T03:52:12.8197639Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8197785Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8197950Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8198024Z     
2025-04-11T03:52:12.8198136Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8198258Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8198503Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8198608Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8198894Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8199030Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8199192Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8199196Z 
2025-04-11T03:52:12.8199385Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8199552Z _____________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False] ______________
2025-04-11T03:52:12.8199555Z 
2025-04-11T03:52:12.8199689Z adamw = False, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T03:52:12.8199776Z g_dtype = torch.float32
2025-04-11T03:52:12.8199785Z 
2025-04-11T03:52:12.8199907Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8200033Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8200207Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8200359Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8200520Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8200651Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8200741Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8200878Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8200971Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8201061Z >       check_adam_kernel(
2025-04-11T03:52:12.8201313Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8201447Z         )
2025-04-11T03:52:12.8201451Z 
2025-04-11T03:52:12.8201568Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8201678Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8201837Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8201982Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8202096Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8202100Z 
2025-04-11T03:52:12.8202276Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be7cdb40>, lr = 0.001
2025-04-11T03:52:12.8202429Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T03:52:12.8202433Z 
2025-04-11T03:52:12.8202668Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8202813Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8203029Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8203103Z     
2025-04-11T03:52:12.8203218Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8203333Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8203574Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8203684Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8203970Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8204104Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8204260Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8204270Z 
2025-04-11T03:52:12.8204405Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8204571Z ______________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True] ______________
2025-04-11T03:52:12.8204575Z 
2025-04-11T03:52:12.8204707Z adamw = True, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T03:52:12.8204794Z g_dtype = torch.float32
2025-04-11T03:52:12.8204798Z 
2025-04-11T03:52:12.8204925Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8205053Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8205224Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8205375Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8205529Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8205664Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8205750Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8205890Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8205977Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8206067Z >       check_adam_kernel(
2025-04-11T03:52:12.8206317Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8206390Z         )
2025-04-11T03:52:12.8206394Z 
2025-04-11T03:52:12.8206510Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8206686Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8206847Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8206993Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8207108Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8207111Z 
2025-04-11T03:52:12.8207285Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f688ee27d30>, lr = 0.001
2025-04-11T03:52:12.8207494Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T03:52:12.8207504Z 
2025-04-11T03:52:12.8207742Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8207880Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8208049Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8208122Z     
2025-04-11T03:52:12.8208236Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8208352Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8208596Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8208701Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8208982Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8209175Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8209336Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8209340Z 
2025-04-11T03:52:12.8209482Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8209645Z _____________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False] ______________
2025-04-11T03:52:12.8209651Z 
2025-04-11T03:52:12.8209786Z adamw = False, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T03:52:12.8209875Z g_dtype = torch.float32
2025-04-11T03:52:12.8209879Z 
2025-04-11T03:52:12.8210002Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8210129Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8210296Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8210452Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8210540Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8210676Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8210764Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8210899Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8210988Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8211073Z >       check_adam_kernel(
2025-04-11T03:52:12.8211331Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8211405Z         )
2025-04-11T03:52:12.8211408Z 
2025-04-11T03:52:12.8211525Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8211691Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8211851Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8212002Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8212114Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8212118Z 
2025-04-11T03:52:12.8212300Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be71b100>, lr = 0.001
2025-04-11T03:52:12.8212451Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T03:52:12.8212455Z 
2025-04-11T03:52:12.8212693Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8212892Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8213061Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8213136Z     
2025-04-11T03:52:12.8213247Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8213368Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8213661Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8213776Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8214056Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8214196Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8214352Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8214358Z 
2025-04-11T03:52:12.8214501Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8214662Z ______________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True] ______________
2025-04-11T03:52:12.8214666Z 
2025-04-11T03:52:12.8214795Z adamw = True, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T03:52:12.8214887Z g_dtype = torch.float32
2025-04-11T03:52:12.8214891Z 
2025-04-11T03:52:12.8215013Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8215140Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8215361Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8215519Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8215606Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8215736Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8215827Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8215963Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8216051Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8216134Z >       check_adam_kernel(
2025-04-11T03:52:12.8216389Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8216463Z         )
2025-04-11T03:52:12.8216467Z 
2025-04-11T03:52:12.8216580Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8216695Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8216853Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8217003Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8217110Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8217113Z 
2025-04-11T03:52:12.8217294Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f688e8e6320>, lr = 0.001
2025-04-11T03:52:12.8217444Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T03:52:12.8217448Z 
2025-04-11T03:52:12.8217742Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8217887Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8218052Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8218132Z     
2025-04-11T03:52:12.8218247Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8218372Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8218616Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8218728Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8219013Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8219208Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8219374Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8219378Z 
2025-04-11T03:52:12.8219516Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8219682Z _____________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False] ______________
2025-04-11T03:52:12.8219730Z 
2025-04-11T03:52:12.8219864Z adamw = False, weight_decay = 0.0, p_dtype = torch.float16
2025-04-11T03:52:12.8219957Z g_dtype = torch.float16
2025-04-11T03:52:12.8219961Z 
2025-04-11T03:52:12.8220082Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8220211Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8220375Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8220525Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8220618Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8220747Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8220840Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8220974Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8221057Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8221148Z >       check_adam_kernel(
2025-04-11T03:52:12.8221454Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8221534Z         )
2025-04-11T03:52:12.8221539Z 
2025-04-11T03:52:12.8221655Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8221768Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8221922Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8222072Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8222185Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8222189Z 
2025-04-11T03:52:12.8222370Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be35cb20>, lr = 0.001
2025-04-11T03:52:12.8222522Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T03:52:12.8222526Z 
2025-04-11T03:52:12.8222763Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8222906Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8223069Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8223146Z     
2025-04-11T03:52:12.8223257Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8223376Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8223622Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8223729Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8224072Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8224209Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8224374Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8224378Z 
2025-04-11T03:52:12.8224515Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8224679Z ______________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True] ______________
2025-04-11T03:52:12.8224683Z 
2025-04-11T03:52:12.8224810Z adamw = True, weight_decay = 0.0, p_dtype = torch.float16
2025-04-11T03:52:12.8224898Z g_dtype = torch.float16
2025-04-11T03:52:12.8224964Z 
2025-04-11T03:52:12.8225089Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8225214Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8225380Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8225530Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8225621Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8225750Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8225891Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8226030Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8226115Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8226203Z >       check_adam_kernel(
2025-04-11T03:52:12.8226456Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8226533Z         )
2025-04-11T03:52:12.8226540Z 
2025-04-11T03:52:12.8226653Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8226762Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8226918Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8227063Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8227173Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8227178Z 
2025-04-11T03:52:12.8227351Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be334520>, lr = 0.001
2025-04-11T03:52:12.8227577Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T03:52:12.8227582Z 
2025-04-11T03:52:12.8227821Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8227962Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8228128Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8228201Z     
2025-04-11T03:52:12.8228318Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8228481Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8228792Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8228901Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8229189Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8229322Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8229479Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8229483Z 
2025-04-11T03:52:12.8229626Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8229788Z _____________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False] ______________
2025-04-11T03:52:12.8229792Z 
2025-04-11T03:52:12.8229926Z adamw = False, weight_decay = 0.1, p_dtype = torch.float16
2025-04-11T03:52:12.8230014Z g_dtype = torch.float16
2025-04-11T03:52:12.8230018Z 
2025-04-11T03:52:12.8230213Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8230343Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8230512Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8230664Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8230753Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8230886Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8230971Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8231106Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8231191Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8231344Z >       check_adam_kernel(
2025-04-11T03:52:12.8231603Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8231678Z         )
2025-04-11T03:52:12.8231682Z 
2025-04-11T03:52:12.8231804Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8231915Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8232076Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8232281Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8232392Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8232396Z 
2025-04-11T03:52:12.8232569Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be30af50>, lr = 0.001
2025-04-11T03:52:12.8232716Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T03:52:12.8232722Z 
2025-04-11T03:52:12.8232960Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8233098Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8233263Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8233334Z     
2025-04-11T03:52:12.8233452Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8233571Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8233873Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8233989Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8234266Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8234403Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8234564Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8234569Z 
2025-04-11T03:52:12.8234707Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8234870Z ______________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True] ______________
2025-04-11T03:52:12.8234874Z 
2025-04-11T03:52:12.8235006Z adamw = True, weight_decay = 0.1, p_dtype = torch.float16
2025-04-11T03:52:12.8235094Z g_dtype = torch.float16
2025-04-11T03:52:12.8235098Z 
2025-04-11T03:52:12.8235216Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8235347Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8235515Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8235670Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8235756Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8235893Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8235980Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8236112Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8236201Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8236341Z >       check_adam_kernel(
2025-04-11T03:52:12.8236601Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8236678Z         )
2025-04-11T03:52:12.8236682Z 
2025-04-11T03:52:12.8236801Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8236910Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8237065Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8237213Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8237320Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8237383Z 
2025-04-11T03:52:12.8237563Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be71b100>, lr = 0.001
2025-04-11T03:52:12.8237718Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T03:52:12.8237724Z 
2025-04-11T03:52:12.8237966Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8238105Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8238327Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8238401Z     
2025-04-11T03:52:12.8238512Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8238633Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8238875Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8238986Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8239261Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8239397Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8239555Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8239558Z 
2025-04-11T03:52:12.8239694Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8239859Z _____________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False] ______________
2025-04-11T03:52:12.8239962Z 
2025-04-11T03:52:12.8240094Z adamw = False, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T03:52:12.8240187Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8240191Z 
2025-04-11T03:52:12.8240310Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8240439Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8240606Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8240759Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8240848Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8240978Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8241067Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8241202Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8241292Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8241375Z >       check_adam_kernel(
2025-04-11T03:52:12.8241624Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8241701Z         )
2025-04-11T03:52:12.8241705Z 
2025-04-11T03:52:12.8241819Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8241930Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8242085Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8242234Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8242481Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8242485Z 
2025-04-11T03:52:12.8242667Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be30a350>, lr = 0.001
2025-04-11T03:52:12.8242815Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T03:52:12.8242821Z 
2025-04-11T03:52:12.8243063Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8243202Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8243361Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8243438Z     
2025-04-11T03:52:12.8243548Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8243734Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8243976Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8244087Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8244369Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8244559Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8244722Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8244726Z 
2025-04-11T03:52:12.8244867Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8245029Z ______________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True] ______________
2025-04-11T03:52:12.8245033Z 
2025-04-11T03:52:12.8245163Z adamw = True, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T03:52:12.8245255Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8245259Z 
2025-04-11T03:52:12.8245381Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8245508Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8245681Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8245831Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8245922Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8246053Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8246198Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8246334Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8246419Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8246507Z >       check_adam_kernel(
2025-04-11T03:52:12.8246757Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8246835Z         )
2025-04-11T03:52:12.8246839Z 
2025-04-11T03:52:12.8246951Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8247064Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8247222Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8247370Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8247483Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8247487Z 
2025-04-11T03:52:12.8247661Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be719030>, lr = 0.001
2025-04-11T03:52:12.8247818Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T03:52:12.8247822Z 
2025-04-11T03:52:12.8248055Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8248195Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8248357Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8248433Z     
2025-04-11T03:52:12.8248594Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8248713Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8248958Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8249067Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8249354Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8249491Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8249655Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8249659Z 
2025-04-11T03:52:12.8249855Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8250015Z _____________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False] ______________
2025-04-11T03:52:12.8250023Z 
2025-04-11T03:52:12.8250151Z adamw = False, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T03:52:12.8250241Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8250245Z 
2025-04-11T03:52:12.8250368Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8250492Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8250728Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8250883Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8250976Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8251105Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8251191Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8251326Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8251413Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8251497Z >       check_adam_kernel(
2025-04-11T03:52:12.8251747Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8251819Z         )
2025-04-11T03:52:12.8251827Z 
2025-04-11T03:52:12.8251942Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8252052Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8252213Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8252412Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8252528Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8252532Z 
2025-04-11T03:52:12.8252706Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be308370>, lr = 0.001
2025-04-11T03:52:12.8252860Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T03:52:12.8252863Z 
2025-04-11T03:52:12.8253099Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8253238Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8253402Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8253474Z     
2025-04-11T03:52:12.8253589Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8253707Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8253948Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8254057Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8254333Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8254470Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8254625Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8254629Z 
2025-04-11T03:52:12.8254823Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8254989Z ______________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True] ______________
2025-04-11T03:52:12.8254992Z 
2025-04-11T03:52:12.8255129Z adamw = True, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T03:52:12.8255219Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8255222Z 
2025-04-11T03:52:12.8255347Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8255472Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8255638Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8255790Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8255940Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8256075Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8256159Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8256299Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8256382Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8256467Z >       check_adam_kernel(
2025-04-11T03:52:12.8256726Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8256873Z         )
2025-04-11T03:52:12.8256877Z 
2025-04-11T03:52:12.8256997Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8257105Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8257262Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8257410Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8257519Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8257527Z 
2025-04-11T03:52:12.8257702Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68f04e4eb0>, lr = 0.001
2025-04-11T03:52:12.8257853Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T03:52:12.8257858Z 
2025-04-11T03:52:12.8258098Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8258238Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8258453Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8258529Z     
2025-04-11T03:52:12.8258642Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8258763Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8259001Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8259112Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8259393Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8259530Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8259688Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8259693Z 
2025-04-11T03:52:12.8259834Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8259996Z _____________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False] ______________
2025-04-11T03:52:12.8260000Z 
2025-04-11T03:52:12.8260142Z adamw = False, weight_decay = 0.0, p_dtype = torch.bfloat16
2025-04-11T03:52:12.8260229Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8260233Z 
2025-04-11T03:52:12.8260354Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8260485Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8260651Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8260809Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8260952Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8261087Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8261176Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8261311Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8261403Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8261489Z >       check_adam_kernel(
2025-04-11T03:52:12.8261742Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8261817Z         )
2025-04-11T03:52:12.8261821Z 
2025-04-11T03:52:12.8261937Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8262107Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8262265Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8262418Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8262527Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8262531Z 
2025-04-11T03:52:12.8262713Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be30a3e0>, lr = 0.001
2025-04-11T03:52:12.8262921Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T03:52:12.8262926Z 
2025-04-11T03:52:12.8263165Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8263302Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8263467Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8263540Z     
2025-04-11T03:52:12.8263651Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8263772Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8264015Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8264130Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8264409Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8264600Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8264761Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8264765Z 
2025-04-11T03:52:12.8264901Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8265068Z ______________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True] ______________
2025-04-11T03:52:12.8265073Z 
2025-04-11T03:52:12.8265207Z adamw = True, weight_decay = 0.0, p_dtype = torch.bfloat16
2025-04-11T03:52:12.8265299Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8265302Z 
2025-04-11T03:52:12.8265422Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8265553Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8265718Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8265871Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8265961Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8266090Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8266181Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8266315Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8266404Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8266488Z >       check_adam_kernel(
2025-04-11T03:52:12.8266739Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8266817Z         )
2025-04-11T03:52:12.8266821Z 
2025-04-11T03:52:12.8266931Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8267096Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8267254Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8267404Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8267512Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8267516Z 
2025-04-11T03:52:12.8267691Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f688ee24400>, lr = 0.001
2025-04-11T03:52:12.8267848Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T03:52:12.8267852Z 
2025-04-11T03:52:12.8268086Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8268292Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8268516Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8268594Z     
2025-04-11T03:52:12.8268706Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8268830Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8269135Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8269244Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8269528Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8269661Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8269819Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8269825Z 
2025-04-11T03:52:12.8269961Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8270123Z _____________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False] ______________
2025-04-11T03:52:12.8270129Z 
2025-04-11T03:52:12.8270268Z adamw = False, weight_decay = 0.1, p_dtype = torch.bfloat16
2025-04-11T03:52:12.8270360Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8270363Z 
2025-04-11T03:52:12.8270488Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8270613Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8270844Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8270998Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8271090Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8271220Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8271315Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8271448Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8271533Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8271623Z >       check_adam_kernel(
2025-04-11T03:52:12.8271873Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8271951Z         )
2025-04-11T03:52:12.8271955Z 
2025-04-11T03:52:12.8272071Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8272185Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8272337Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8272482Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8272594Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8272597Z 
2025-04-11T03:52:12.8272774Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be7ccc40>, lr = 0.001
2025-04-11T03:52:12.8272925Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T03:52:12.8272929Z 
2025-04-11T03:52:12.8273220Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8273368Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8273531Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8273605Z     
2025-04-11T03:52:12.8273724Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8273840Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8274086Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8274192Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8274553Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8274685Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8274845Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8274849Z 
2025-04-11T03:52:12.8274990Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8275152Z ______________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True] ______________
2025-04-11T03:52:12.8275208Z 
2025-04-11T03:52:12.8275346Z adamw = True, weight_decay = 0.1, p_dtype = torch.bfloat16
2025-04-11T03:52:12.8275432Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8275436Z 
2025-04-11T03:52:12.8275559Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8275683Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8275851Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8276004Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8276090Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8276225Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8276313Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8276451Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8276537Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8276626Z >       check_adam_kernel(
2025-04-11T03:52:12.8276929Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8277008Z         )
2025-04-11T03:52:12.8277012Z 
2025-04-11T03:52:12.8277134Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8277246Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8277409Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8277557Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8277666Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8277670Z 
2025-04-11T03:52:12.8277846Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be4a4430>, lr = 0.001
2025-04-11T03:52:12.8278000Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T03:52:12.8278006Z 
2025-04-11T03:52:12.8278244Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8278387Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8278554Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8278624Z     
2025-04-11T03:52:12.8278738Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8278855Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8279101Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8279209Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8279548Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8279689Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8279847Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8279851Z 
2025-04-11T03:52:12.8279994Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8280176Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0] ______
2025-04-11T03:52:12.8280180Z 
2025-04-11T03:52:12.8280342Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T03:52:12.8280508Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8280662Z g_dtype = torch.float32
2025-04-11T03:52:12.8280666Z 
2025-04-11T03:52:12.8280833Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8280955Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8281119Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8281214Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8281432Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8281522Z         device: torch.device,
2025-04-11T03:52:12.8281607Z         adamw: bool,
2025-04-11T03:52:12.8281697Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8281783Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8281865Z     ) -> None:
2025-04-11T03:52:12.8282118Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8282223Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8282227Z 
2025-04-11T03:52:12.8282337Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8282450Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8282708Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8282808Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8283048Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8283142Z     return self._apply(convert)
2025-04-11T03:52:12.8283436Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8283523Z     module._apply(fn)
2025-04-11T03:52:12.8283760Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8283846Z     module._apply(fn)
2025-04-11T03:52:12.8284077Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8284162Z     module._apply(fn)
2025-04-11T03:52:12.8284392Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8284490Z     param_applied = fn(param)
2025-04-11T03:52:12.8284603Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8284606Z 
2025-04-11T03:52:12.8284700Z t = Parameter containing:
2025-04-11T03:52:12.8284843Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8284944Z         [ 0.0028, -0.0014,...0,  0.0213, -0.0091],
2025-04-11T03:52:12.8285076Z         [-0.0226, -0.0230, -0.0057,  ..., -0.0094, -0.0239, -0.0399]],
2025-04-11T03:52:12.8285163Z        requires_grad=True)
2025-04-11T03:52:12.8285166Z 
2025-04-11T03:52:12.8285249Z     def convert(t):
2025-04-11T03:52:12.8285382Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8285573Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8285692Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8285953Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8286068Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8286348Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8286492Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8286647Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8286651Z 
2025-04-11T03:52:12.8286910Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8287089Z _______ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2] _______
2025-04-11T03:52:12.8287148Z 
2025-04-11T03:52:12.8287304Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T03:52:12.8287474Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8287563Z g_dtype = torch.float32
2025-04-11T03:52:12.8287567Z 
2025-04-11T03:52:12.8287740Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8287863Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8288072Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8288167Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8288327Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8288414Z         device: torch.device,
2025-04-11T03:52:12.8288495Z         adamw: bool,
2025-04-11T03:52:12.8288589Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8288672Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8288756Z     ) -> None:
2025-04-11T03:52:12.8289012Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8289110Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8289118Z 
2025-04-11T03:52:12.8289231Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8289341Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8289591Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8289743Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8289975Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8290068Z     return self._apply(convert)
2025-04-11T03:52:12.8290303Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8290384Z     module._apply(fn)
2025-04-11T03:52:12.8290617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8290705Z     module._apply(fn)
2025-04-11T03:52:12.8290934Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8291017Z     module._apply(fn)
2025-04-11T03:52:12.8291245Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8291338Z     param_applied = fn(param)
2025-04-11T03:52:12.8291452Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8291458Z 
2025-04-11T03:52:12.8291549Z t = Parameter containing:
2025-04-11T03:52:12.8291688Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8291786Z         [-0.0048,  0.0237,...2, -0.0204,  0.0268],
2025-04-11T03:52:12.8291912Z         [ 0.0211,  0.0139,  0.0082,  ...,  0.0303, -0.0201, -0.0544]],
2025-04-11T03:52:12.8291999Z        requires_grad=True)
2025-04-11T03:52:12.8292003Z 
2025-04-11T03:52:12.8292086Z     def convert(t):
2025-04-11T03:52:12.8292218Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8292451Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8292578Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8292786Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8292897Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8293181Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8293321Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8293478Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8293543Z 
2025-04-11T03:52:12.8293796Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8293982Z _____ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3] ______
2025-04-11T03:52:12.8293986Z 
2025-04-11T03:52:12.8294154Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8294314Z device = device(type='cpu'), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8294402Z g_dtype = torch.float32
2025-04-11T03:52:12.8294461Z 
2025-04-11T03:52:12.8294635Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8294758Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8294915Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8295009Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8295164Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8295257Z         device: torch.device,
2025-04-11T03:52:12.8295340Z         adamw: bool,
2025-04-11T03:52:12.8295432Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8295516Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8295594Z     ) -> None:
2025-04-11T03:52:12.8295853Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8295950Z         torch_model = model_fn().to(device)
2025-04-11T03:52:12.8296064Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T03:52:12.8296146Z         lr = 1e-3
2025-04-11T03:52:12.8296238Z         beta1, beta2 = 0.9, 0.999
2025-04-11T03:52:12.8296370Z         eps = 1e-8
2025-04-11T03:52:12.8296478Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T03:52:12.8296711Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T03:52:12.8296925Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T03:52:12.8296931Z 
2025-04-11T03:52:12.8297045Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T03:52:12.8297157Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8297161Z 
2025-04-11T03:52:12.8297250Z self = HybridAdam (
2025-04-11T03:52:12.8297332Z Parameter Group 0
2025-04-11T03:52:12.8297413Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8297504Z     bias_correction: True
2025-04-11T03:52:12.8297586Z     eps: 1e-08
2025-04-11T03:52:12.8297667Z     lr: 0.001
2025-04-11T03:52:12.8297754Z     weig...arameter Group 1
2025-04-11T03:52:12.8297836Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8297926Z     bias_correction: True
2025-04-11T03:52:12.8298004Z     eps: 1e-08
2025-04-11T03:52:12.8298082Z     lr: 0.001
2025-04-11T03:52:12.8298165Z     weight_decay: 0.0
2025-04-11T03:52:12.8298247Z )
2025-04-11T03:52:12.8298568Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T03:52:12.8298719Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T03:52:12.8298877Z weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
2025-04-11T03:52:12.8299042Z nvme_offload_dir = None, defaults = {}
2025-04-11T03:52:12.8299426Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T03:52:12.8299433Z 
2025-04-11T03:52:12.8299510Z     def __init__(
2025-04-11T03:52:12.8299589Z         self,
2025-04-11T03:52:12.8299668Z         model_params,
2025-04-11T03:52:12.8299743Z         lr=1e-3,
2025-04-11T03:52:12.8299833Z         bias_correction=True,
2025-04-11T03:52:12.8299915Z         betas=(0.9, 0.999),
2025-04-11T03:52:12.8299998Z         eps=1e-8,
2025-04-11T03:52:12.8300077Z         weight_decay=0,
2025-04-11T03:52:12.8300160Z         adamw_mode=True,
2025-04-11T03:52:12.8300323Z         nvme_offload_fraction: float = 0.0,
2025-04-11T03:52:12.8300431Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T03:52:12.8300516Z         **defaults: Any,
2025-04-11T03:52:12.8300586Z     ):
2025-04-11T03:52:12.8300669Z         super().__init__(
2025-04-11T03:52:12.8300757Z             model_params,
2025-04-11T03:52:12.8300832Z             lr,
2025-04-11T03:52:12.8300920Z             bias_correction,
2025-04-11T03:52:12.8300996Z             betas,
2025-04-11T03:52:12.8301137Z             eps,
2025-04-11T03:52:12.8301217Z             weight_decay,
2025-04-11T03:52:12.8301295Z             adamw_mode,
2025-04-11T03:52:12.8301393Z             nvme_offload_fraction,
2025-04-11T03:52:12.8301476Z             nvme_offload_dir,
2025-04-11T03:52:12.8301552Z         )
2025-04-11T03:52:12.8301646Z         if torch.cuda.is_available():
2025-04-11T03:52:12.8301767Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8301899Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8302113Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T03:52:12.8302227Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8302520Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8302666Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8302828Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8302832Z 
2025-04-11T03:52:12.8303025Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T03:52:12.8303216Z _____ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4] ______
2025-04-11T03:52:12.8303220Z 
2025-04-11T03:52:12.8303386Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8303562Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8303650Z g_dtype = torch.float32
2025-04-11T03:52:12.8303654Z 
2025-04-11T03:52:12.8303823Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8303946Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8304100Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8304195Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8304352Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8304444Z         device: torch.device,
2025-04-11T03:52:12.8304525Z         adamw: bool,
2025-04-11T03:52:12.8304613Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8304698Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8304776Z     ) -> None:
2025-04-11T03:52:12.8305029Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8305128Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8305132Z 
2025-04-11T03:52:12.8305246Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8305358Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8305664Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8305761Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8305993Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8306086Z     return self._apply(convert)
2025-04-11T03:52:12.8306321Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8306407Z     module._apply(fn)
2025-04-11T03:52:12.8306641Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8306724Z     module._apply(fn)
2025-04-11T03:52:12.8307015Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8307095Z     module._apply(fn)
2025-04-11T03:52:12.8307323Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8307415Z     param_applied = fn(param)
2025-04-11T03:52:12.8307528Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8307532Z 
2025-04-11T03:52:12.8307621Z t = Parameter containing:
2025-04-11T03:52:12.8307818Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8307921Z         [ 0.0210, -0.0131,...8, -0.0156, -0.0054],
2025-04-11T03:52:12.8308041Z         [ 0.0148,  0.0292,  0.0008,  ...,  0.0355, -0.0048, -0.0186]],
2025-04-11T03:52:12.8308131Z        requires_grad=True)
2025-04-11T03:52:12.8308134Z 
2025-04-11T03:52:12.8308215Z     def convert(t):
2025-04-11T03:52:12.8308350Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8308575Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8308700Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8308907Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8309019Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8309300Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8309497Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8309666Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8309670Z 
2025-04-11T03:52:12.8309924Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8310109Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0] _______
2025-04-11T03:52:12.8310114Z 
2025-04-11T03:52:12.8310279Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T03:52:12.8310448Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8310535Z g_dtype = torch.float32
2025-04-11T03:52:12.8310538Z 
2025-04-11T03:52:12.8310706Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8310830Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8310988Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8311087Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8311244Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8311337Z         device: torch.device,
2025-04-11T03:52:12.8311415Z         adamw: bool,
2025-04-11T03:52:12.8311503Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8311591Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8311671Z     ) -> None:
2025-04-11T03:52:12.8311927Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8312022Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8312026Z 
2025-04-11T03:52:12.8312198Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8312315Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8312557Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8312659Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8312883Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8312982Z     return self._apply(convert)
2025-04-11T03:52:12.8313215Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8313300Z     module._apply(fn)
2025-04-11T03:52:12.8313605Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8313687Z     module._apply(fn)
2025-04-11T03:52:12.8313920Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8314002Z     module._apply(fn)
2025-04-11T03:52:12.8314236Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8314394Z     param_applied = fn(param)
2025-04-11T03:52:12.8314511Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8314517Z 
2025-04-11T03:52:12.8314605Z t = Parameter containing:
2025-04-11T03:52:12.8314741Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8314843Z         [ 0.0168, -0.0116,...1,  0.0247, -0.0168],
2025-04-11T03:52:12.8314963Z         [ 0.0078, -0.0201,  0.0158,  ..., -0.0204,  0.0234,  0.0068]],
2025-04-11T03:52:12.8315055Z        requires_grad=True)
2025-04-11T03:52:12.8315059Z 
2025-04-11T03:52:12.8315138Z     def convert(t):
2025-04-11T03:52:12.8315273Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8315451Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8315573Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8315776Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8315887Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8316235Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8316375Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8316537Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8316543Z 
2025-04-11T03:52:12.8316797Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8316979Z _______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2] ________
2025-04-11T03:52:12.8316983Z 
2025-04-11T03:52:12.8317138Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T03:52:12.8317310Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8317395Z g_dtype = torch.float32
2025-04-11T03:52:12.8317401Z 
2025-04-11T03:52:12.8317567Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8317697Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8317852Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8317946Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8318098Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8318192Z         device: torch.device,
2025-04-11T03:52:12.8318272Z         adamw: bool,
2025-04-11T03:52:12.8318359Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8318447Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8318524Z     ) -> None:
2025-04-11T03:52:12.8318829Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8318929Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8318933Z 
2025-04-11T03:52:12.8319048Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8319164Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8319405Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8319502Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8319729Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8319900Z     return self._apply(convert)
2025-04-11T03:52:12.8320134Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8320215Z     module._apply(fn)
2025-04-11T03:52:12.8320452Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8320533Z     module._apply(fn)
2025-04-11T03:52:12.8320768Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8320908Z     module._apply(fn)
2025-04-11T03:52:12.8321141Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8321231Z     param_applied = fn(param)
2025-04-11T03:52:12.8321343Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8321351Z 
2025-04-11T03:52:12.8321441Z t = Parameter containing:
2025-04-11T03:52:12.8321575Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8321677Z         [ 0.0171, -0.0037,...8, -0.0068,  0.0037],
2025-04-11T03:52:12.8321799Z         [ 0.0260, -0.0271, -0.0247,  ...,  0.0262,  0.0078,  0.0236]],
2025-04-11T03:52:12.8321887Z        requires_grad=True)
2025-04-11T03:52:12.8321891Z 
2025-04-11T03:52:12.8321973Z     def convert(t):
2025-04-11T03:52:12.8322104Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8322287Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8322406Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8322684Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8322794Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8323080Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8323214Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8323378Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8323382Z 
2025-04-11T03:52:12.8323633Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8323815Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3] ______
2025-04-11T03:52:12.8323822Z 
2025-04-11T03:52:12.8323987Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8324139Z device = device(type='cpu'), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8324233Z g_dtype = torch.float32
2025-04-11T03:52:12.8324237Z 
2025-04-11T03:52:12.8324403Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8324526Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8324676Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8324774Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8324928Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8325013Z         device: torch.device,
2025-04-11T03:52:12.8325096Z         adamw: bool,
2025-04-11T03:52:12.8325236Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8325326Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8325405Z     ) -> None:
2025-04-11T03:52:12.8325652Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8325757Z         torch_model = model_fn().to(device)
2025-04-11T03:52:12.8325866Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T03:52:12.8325946Z         lr = 1e-3
2025-04-11T03:52:12.8326034Z         beta1, beta2 = 0.9, 0.999
2025-04-11T03:52:12.8326118Z         eps = 1e-8
2025-04-11T03:52:12.8326224Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T03:52:12.8326509Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T03:52:12.8326730Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T03:52:12.8326734Z 
2025-04-11T03:52:12.8326846Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T03:52:12.8326961Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8326966Z 
2025-04-11T03:52:12.8327104Z self = HybridAdam (
2025-04-11T03:52:12.8327191Z Parameter Group 0
2025-04-11T03:52:12.8327273Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8327361Z     bias_correction: True
2025-04-11T03:52:12.8327443Z     eps: 1e-08
2025-04-11T03:52:12.8327519Z     lr: 0.001
2025-04-11T03:52:12.8327610Z     weig...arameter Group 1
2025-04-11T03:52:12.8327688Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8327772Z     bias_correction: True
2025-04-11T03:52:12.8327858Z     eps: 1e-08
2025-04-11T03:52:12.8327932Z     lr: 0.001
2025-04-11T03:52:12.8328018Z     weight_decay: 0.0
2025-04-11T03:52:12.8328089Z )
2025-04-11T03:52:12.8328402Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T03:52:12.8328554Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T03:52:12.8328705Z weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
2025-04-11T03:52:12.8328810Z nvme_offload_dir = None, defaults = {}
2025-04-11T03:52:12.8329269Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T03:52:12.8329277Z 
2025-04-11T03:52:12.8329366Z     def __init__(
2025-04-11T03:52:12.8329439Z         self,
2025-04-11T03:52:12.8329522Z         model_params,
2025-04-11T03:52:12.8329598Z         lr=1e-3,
2025-04-11T03:52:12.8329682Z         bias_correction=True,
2025-04-11T03:52:12.8329770Z         betas=(0.9, 0.999),
2025-04-11T03:52:12.8329847Z         eps=1e-8,
2025-04-11T03:52:12.8329932Z         weight_decay=0,
2025-04-11T03:52:12.8330013Z         adamw_mode=True,
2025-04-11T03:52:12.8330108Z         nvme_offload_fraction: float = 0.0,
2025-04-11T03:52:12.8330221Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T03:52:12.8330305Z         **defaults: Any,
2025-04-11T03:52:12.8330379Z     ):
2025-04-11T03:52:12.8330460Z         super().__init__(
2025-04-11T03:52:12.8330543Z             model_params,
2025-04-11T03:52:12.8330624Z             lr,
2025-04-11T03:52:12.8330708Z             bias_correction,
2025-04-11T03:52:12.8330789Z             betas,
2025-04-11T03:52:12.8330865Z             eps,
2025-04-11T03:52:12.8330945Z             weight_decay,
2025-04-11T03:52:12.8331027Z             adamw_mode,
2025-04-11T03:52:12.8331116Z             nvme_offload_fraction,
2025-04-11T03:52:12.8331202Z             nvme_offload_dir,
2025-04-11T03:52:12.8331276Z         )
2025-04-11T03:52:12.8331376Z         if torch.cuda.is_available():
2025-04-11T03:52:12.8331496Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8331619Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8331889Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T03:52:12.8332001Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8332293Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8332435Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8332598Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8332602Z 
2025-04-11T03:52:12.8332740Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T03:52:12.8332924Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4] ______
2025-04-11T03:52:12.8332995Z 
2025-04-11T03:52:12.8333160Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8333324Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8333416Z g_dtype = torch.float32
2025-04-11T03:52:12.8333420Z 
2025-04-11T03:52:12.8333586Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8333712Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8333925Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8334022Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8334180Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8334266Z         device: torch.device,
2025-04-11T03:52:12.8334352Z         adamw: bool,
2025-04-11T03:52:12.8334439Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8334527Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8334606Z     ) -> None:
2025-04-11T03:52:12.8334855Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8334957Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8334961Z 
2025-04-11T03:52:12.8335071Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8335189Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8335431Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8335534Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8335816Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8335911Z     return self._apply(convert)
2025-04-11T03:52:12.8336154Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8336236Z     module._apply(fn)
2025-04-11T03:52:12.8336478Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8336558Z     module._apply(fn)
2025-04-11T03:52:12.8336789Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8336870Z     module._apply(fn)
2025-04-11T03:52:12.8337097Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8337192Z     param_applied = fn(param)
2025-04-11T03:52:12.8337306Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8337310Z 
2025-04-11T03:52:12.8337402Z t = Parameter containing:
2025-04-11T03:52:12.8337540Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8337645Z         [-0.0301, -0.0063,...5, -0.0105,  0.0078],
2025-04-11T03:52:12.8337768Z         [-0.0225,  0.0108,  0.0321,  ..., -0.0056, -0.0089, -0.0360]],
2025-04-11T03:52:12.8337854Z        requires_grad=True)
2025-04-11T03:52:12.8337862Z 
2025-04-11T03:52:12.8337942Z     def convert(t):
2025-04-11T03:52:12.8338073Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8338308Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8338431Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8338645Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8338756Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8339044Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8339180Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8339339Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8339401Z 
2025-04-11T03:52:12.8339656Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8339837Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0] ______
2025-04-11T03:52:12.8339841Z 
2025-04-11T03:52:12.8340006Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T03:52:12.8340176Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8340265Z g_dtype = torch.float16
2025-04-11T03:52:12.8340323Z 
2025-04-11T03:52:12.8340490Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8340617Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8340769Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8340863Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8341022Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8341113Z         device: torch.device,
2025-04-11T03:52:12.8341194Z         adamw: bool,
2025-04-11T03:52:12.8341282Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8341364Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8341445Z     ) -> None:
2025-04-11T03:52:12.8341695Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8341794Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8341798Z 
2025-04-11T03:52:12.8341910Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8342156Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8342401Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8342495Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8342720Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8342813Z     return self._apply(convert)
2025-04-11T03:52:12.8343044Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8343123Z     module._apply(fn)
2025-04-11T03:52:12.8343357Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8343437Z     module._apply(fn)
2025-04-11T03:52:12.8343661Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8343747Z     module._apply(fn)
2025-04-11T03:52:12.8343975Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8344066Z     param_applied = fn(param)
2025-04-11T03:52:12.8344175Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8344179Z 
2025-04-11T03:52:12.8344270Z t = Parameter containing:
2025-04-11T03:52:12.8344401Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8344497Z         [-0.0063,  0.0127,...8,  0.0139, -0.0372],
2025-04-11T03:52:12.8344619Z         [-0.0001,  0.0211,  0.0425,  ..., -0.0074,  0.0182,  0.0033]],
2025-04-11T03:52:12.8344704Z        requires_grad=True)
2025-04-11T03:52:12.8344708Z 
2025-04-11T03:52:12.8344848Z     def convert(t):
2025-04-11T03:52:12.8344980Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8345162Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8345286Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8345494Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8345607Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8345894Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8346103Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8346260Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8346264Z 
2025-04-11T03:52:12.8346517Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8346698Z _______ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2] _______
2025-04-11T03:52:12.8346702Z 
2025-04-11T03:52:12.8346927Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T03:52:12.8347097Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8347181Z g_dtype = torch.float16
2025-04-11T03:52:12.8347189Z 
2025-04-11T03:52:12.8347357Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8347477Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8347634Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8347729Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8347887Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8347973Z         device: torch.device,
2025-04-11T03:52:12.8348052Z         adamw: bool,
2025-04-11T03:52:12.8348143Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8348228Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8348306Z     ) -> None:
2025-04-11T03:52:12.8348608Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8348779Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8348784Z 
2025-04-11T03:52:12.8348894Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8349005Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8349248Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8349345Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8349568Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8349659Z     return self._apply(convert)
2025-04-11T03:52:12.8349893Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8349977Z     module._apply(fn)
2025-04-11T03:52:12.8350204Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8350293Z     module._apply(fn)
2025-04-11T03:52:12.8350519Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8350606Z     module._apply(fn)
2025-04-11T03:52:12.8350831Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8350927Z     param_applied = fn(param)
2025-04-11T03:52:12.8351037Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8351043Z 
2025-04-11T03:52:12.8351131Z t = Parameter containing:
2025-04-11T03:52:12.8351266Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8351365Z         [ 0.0058,  0.0119,...4, -0.0198,  0.0151],
2025-04-11T03:52:12.8351552Z         [-0.0479,  0.0136, -0.0425,  ..., -0.0021, -0.0081,  0.0171]],
2025-04-11T03:52:12.8351639Z        requires_grad=True)
2025-04-11T03:52:12.8351643Z 
2025-04-11T03:52:12.8351727Z     def convert(t):
2025-04-11T03:52:12.8351856Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8352033Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8352157Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8352360Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8352469Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8352818Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8352953Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8353114Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8353118Z 
2025-04-11T03:52:12.8353373Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8353621Z _____ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3] ______
2025-04-11T03:52:12.8353627Z 
2025-04-11T03:52:12.8353793Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8353949Z device = device(type='cpu'), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8354036Z g_dtype = torch.float16
2025-04-11T03:52:12.8354040Z 
2025-04-11T03:52:12.8354210Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8354334Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8354491Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8354586Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8354742Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8354833Z         device: torch.device,
2025-04-11T03:52:12.8354911Z         adamw: bool,
2025-04-11T03:52:12.8355005Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8355089Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8355170Z     ) -> None:
2025-04-11T03:52:12.8355477Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8355578Z         torch_model = model_fn().to(device)
2025-04-11T03:52:12.8355693Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T03:52:12.8355772Z         lr = 1e-3
2025-04-11T03:52:12.8355867Z         beta1, beta2 = 0.9, 0.999
2025-04-11T03:52:12.8355947Z         eps = 1e-8
2025-04-11T03:52:12.8356051Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T03:52:12.8356282Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T03:52:12.8356497Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T03:52:12.8356501Z 
2025-04-11T03:52:12.8356615Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T03:52:12.8356727Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8356733Z 
2025-04-11T03:52:12.8356817Z self = HybridAdam (
2025-04-11T03:52:12.8356898Z Parameter Group 0
2025-04-11T03:52:12.8356983Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8357068Z     bias_correction: True
2025-04-11T03:52:12.8357146Z     eps: 1e-08
2025-04-11T03:52:12.8357224Z     lr: 0.001
2025-04-11T03:52:12.8357311Z     weig...arameter Group 1
2025-04-11T03:52:12.8357395Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8357480Z     bias_correction: True
2025-04-11T03:52:12.8357555Z     eps: 1e-08
2025-04-11T03:52:12.8357630Z     lr: 0.001
2025-04-11T03:52:12.8357711Z     weight_decay: 0.0
2025-04-11T03:52:12.8357787Z )
2025-04-11T03:52:12.8358160Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T03:52:12.8358312Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T03:52:12.8358469Z weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
2025-04-11T03:52:12.8358567Z nvme_offload_dir = None, defaults = {}
2025-04-11T03:52:12.8358940Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T03:52:12.8358944Z 
2025-04-11T03:52:12.8359019Z     def __init__(
2025-04-11T03:52:12.8359156Z         self,
2025-04-11T03:52:12.8359236Z         model_params,
2025-04-11T03:52:12.8359309Z         lr=1e-3,
2025-04-11T03:52:12.8359398Z         bias_correction=True,
2025-04-11T03:52:12.8359479Z         betas=(0.9, 0.999),
2025-04-11T03:52:12.8359565Z         eps=1e-8,
2025-04-11T03:52:12.8359647Z         weight_decay=0,
2025-04-11T03:52:12.8359730Z         adamw_mode=True,
2025-04-11T03:52:12.8359827Z         nvme_offload_fraction: float = 0.0,
2025-04-11T03:52:12.8359987Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T03:52:12.8360074Z         **defaults: Any,
2025-04-11T03:52:12.8360147Z     ):
2025-04-11T03:52:12.8360236Z         super().__init__(
2025-04-11T03:52:12.8360318Z             model_params,
2025-04-11T03:52:12.8360393Z             lr,
2025-04-11T03:52:12.8360482Z             bias_correction,
2025-04-11T03:52:12.8360558Z             betas,
2025-04-11T03:52:12.8360636Z             eps,
2025-04-11T03:52:12.8360717Z             weight_decay,
2025-04-11T03:52:12.8360797Z             adamw_mode,
2025-04-11T03:52:12.8360890Z             nvme_offload_fraction,
2025-04-11T03:52:12.8360972Z             nvme_offload_dir,
2025-04-11T03:52:12.8361049Z         )
2025-04-11T03:52:12.8361145Z         if torch.cuda.is_available():
2025-04-11T03:52:12.8361271Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8361397Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8361608Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T03:52:12.8361721Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8362073Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8362217Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8362382Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8362388Z 
2025-04-11T03:52:12.8362536Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T03:52:12.8362724Z _____ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4] ______
2025-04-11T03:52:12.8362728Z 
2025-04-11T03:52:12.8362899Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8363072Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8363162Z g_dtype = torch.float16
2025-04-11T03:52:12.8363168Z 
2025-04-11T03:52:12.8363342Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8363471Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8363630Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8363728Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8363885Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8363981Z         device: torch.device,
2025-04-11T03:52:12.8364062Z         adamw: bool,
2025-04-11T03:52:12.8364157Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8364244Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8364325Z     ) -> None:
2025-04-11T03:52:12.8364628Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8364730Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8364734Z 
2025-04-11T03:52:12.8364850Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8364965Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8365216Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8365313Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8365546Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8365706Z     return self._apply(convert)
2025-04-11T03:52:12.8365943Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8366029Z     module._apply(fn)
2025-04-11T03:52:12.8366264Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8366348Z     module._apply(fn)
2025-04-11T03:52:12.8366580Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8366721Z     module._apply(fn)
2025-04-11T03:52:12.8366953Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8367041Z     param_applied = fn(param)
2025-04-11T03:52:12.8367156Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8367160Z 
2025-04-11T03:52:12.8367248Z t = Parameter containing:
2025-04-11T03:52:12.8367387Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8367488Z         [ 0.0127, -0.0053,...6, -0.0203,  0.0294],
2025-04-11T03:52:12.8367611Z         [ 0.0315,  0.0270, -0.0379,  ...,  0.0044, -0.0077,  0.0209]],
2025-04-11T03:52:12.8367696Z        requires_grad=True)
2025-04-11T03:52:12.8367700Z 
2025-04-11T03:52:12.8367782Z     def convert(t):
2025-04-11T03:52:12.8367917Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8368098Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8368222Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8368487Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8368602Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8368884Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8369022Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8369181Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8369185Z 
2025-04-11T03:52:12.8369440Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8369623Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0] _______
2025-04-11T03:52:12.8369627Z 
2025-04-11T03:52:12.8369789Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T03:52:12.8369964Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8370053Z g_dtype = torch.float16
2025-04-11T03:52:12.8370056Z 
2025-04-11T03:52:12.8370230Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8370351Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8370506Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8370602Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8370763Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8370852Z         device: torch.device,
2025-04-11T03:52:12.8370932Z         adamw: bool,
2025-04-11T03:52:12.8371078Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8371166Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8371243Z     ) -> None:
2025-04-11T03:52:12.8371500Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8371599Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8371603Z 
2025-04-11T03:52:12.8371719Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8371832Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8372076Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8372252Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8372480Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8372574Z     return self._apply(convert)
2025-04-11T03:52:12.8372811Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8372896Z     module._apply(fn)
2025-04-11T03:52:12.8373124Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8373266Z     module._apply(fn)
2025-04-11T03:52:12.8373499Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8373580Z     module._apply(fn)
2025-04-11T03:52:12.8373811Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8373901Z     param_applied = fn(param)
2025-04-11T03:52:12.8374016Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8374022Z 
2025-04-11T03:52:12.8374112Z t = Parameter containing:
2025-04-11T03:52:12.8374249Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8374348Z         [ 0.0126,  0.0307,...5,  0.0153,  0.0116],
2025-04-11T03:52:12.8374472Z         [-0.0007,  0.0044, -0.0020,  ..., -0.0033,  0.0164, -0.0073]],
2025-04-11T03:52:12.8374563Z        requires_grad=True)
2025-04-11T03:52:12.8374567Z 
2025-04-11T03:52:12.8374647Z     def convert(t):
2025-04-11T03:52:12.8374780Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8375007Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8375136Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8375344Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8375451Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8375742Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8375878Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8376041Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8376045Z 
2025-04-11T03:52:12.8376295Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8376481Z _______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2] ________
2025-04-11T03:52:12.8376486Z 
2025-04-11T03:52:12.8376645Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T03:52:12.8376815Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8376903Z g_dtype = torch.float16
2025-04-11T03:52:12.8376906Z 
2025-04-11T03:52:12.8377074Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8377200Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8377350Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8377446Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8377660Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8377753Z         device: torch.device,
2025-04-11T03:52:12.8377835Z         adamw: bool,
2025-04-11T03:52:12.8377922Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8378011Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8378087Z     ) -> None:
2025-04-11T03:52:12.8378338Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8378435Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8378439Z 
2025-04-11T03:52:12.8378552Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8378661Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8378968Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8379065Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8379292Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8379388Z     return self._apply(convert)
2025-04-11T03:52:12.8379617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8379760Z     module._apply(fn)
2025-04-11T03:52:12.8379993Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8380074Z     module._apply(fn)
2025-04-11T03:52:12.8380308Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8380388Z     module._apply(fn)
2025-04-11T03:52:12.8380618Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8380708Z     param_applied = fn(param)
2025-04-11T03:52:12.8380821Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8380825Z 
2025-04-11T03:52:12.8380915Z t = Parameter containing:
2025-04-11T03:52:12.8381049Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8381150Z         [ 0.0062,  0.0098,...3, -0.0036,  0.0170],
2025-04-11T03:52:12.8381270Z         [ 0.0053,  0.0281, -0.0163,  ..., -0.0098, -0.0364,  0.0040]],
2025-04-11T03:52:12.8381359Z        requires_grad=True)
2025-04-11T03:52:12.8381420Z 
2025-04-11T03:52:12.8381505Z     def convert(t):
2025-04-11T03:52:12.8381640Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8381816Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8381934Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8382142Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8382250Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8382538Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8382672Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8382838Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8382841Z 
2025-04-11T03:52:12.8383093Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8383277Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3] ______
2025-04-11T03:52:12.8383280Z 
2025-04-11T03:52:12.8383444Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8383602Z device = device(type='cpu'), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8383694Z g_dtype = torch.float16
2025-04-11T03:52:12.8383697Z 
2025-04-11T03:52:12.8383863Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8384060Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8384216Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8384312Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8384468Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8384554Z         device: torch.device,
2025-04-11T03:52:12.8384640Z         adamw: bool,
2025-04-11T03:52:12.8384727Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8384812Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8384891Z     ) -> None:
2025-04-11T03:52:12.8385143Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8385300Z         torch_model = model_fn().to(device)
2025-04-11T03:52:12.8385410Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T03:52:12.8385492Z         lr = 1e-3
2025-04-11T03:52:12.8385580Z         beta1, beta2 = 0.9, 0.999
2025-04-11T03:52:12.8385664Z         eps = 1e-8
2025-04-11T03:52:12.8385773Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T03:52:12.8385999Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T03:52:12.8386271Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T03:52:12.8386277Z 
2025-04-11T03:52:12.8386387Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T03:52:12.8386505Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8386509Z 
2025-04-11T03:52:12.8386594Z self = HybridAdam (
2025-04-11T03:52:12.8386679Z Parameter Group 0
2025-04-11T03:52:12.8386762Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8386848Z     bias_correction: True
2025-04-11T03:52:12.8386931Z     eps: 1e-08
2025-04-11T03:52:12.8387009Z     lr: 0.001
2025-04-11T03:52:12.8387100Z     weig...arameter Group 1
2025-04-11T03:52:12.8387179Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8387268Z     bias_correction: True
2025-04-11T03:52:12.8387346Z     eps: 1e-08
2025-04-11T03:52:12.8387420Z     lr: 0.001
2025-04-11T03:52:12.8387505Z     weight_decay: 0.0
2025-04-11T03:52:12.8387578Z )
2025-04-11T03:52:12.8387951Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T03:52:12.8388100Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T03:52:12.8388249Z weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
2025-04-11T03:52:12.8388352Z nvme_offload_dir = None, defaults = {}
2025-04-11T03:52:12.8388775Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T03:52:12.8388782Z 
2025-04-11T03:52:12.8388863Z     def __init__(
2025-04-11T03:52:12.8388938Z         self,
2025-04-11T03:52:12.8389025Z         model_params,
2025-04-11T03:52:12.8389103Z         lr=1e-3,
2025-04-11T03:52:12.8389188Z         bias_correction=True,
2025-04-11T03:52:12.8389275Z         betas=(0.9, 0.999),
2025-04-11T03:52:12.8389352Z         eps=1e-8,
2025-04-11T03:52:12.8389439Z         weight_decay=0,
2025-04-11T03:52:12.8389520Z         adamw_mode=True,
2025-04-11T03:52:12.8389624Z         nvme_offload_fraction: float = 0.0,
2025-04-11T03:52:12.8389730Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T03:52:12.8389812Z         **defaults: Any,
2025-04-11T03:52:12.8389890Z     ):
2025-04-11T03:52:12.8389973Z         super().__init__(
2025-04-11T03:52:12.8390058Z             model_params,
2025-04-11T03:52:12.8390134Z             lr,
2025-04-11T03:52:12.8390222Z             bias_correction,
2025-04-11T03:52:12.8390300Z             betas,
2025-04-11T03:52:12.8390376Z             eps,
2025-04-11T03:52:12.8390461Z             weight_decay,
2025-04-11T03:52:12.8390538Z             adamw_mode,
2025-04-11T03:52:12.8390689Z             nvme_offload_fraction,
2025-04-11T03:52:12.8390779Z             nvme_offload_dir,
2025-04-11T03:52:12.8390850Z         )
2025-04-11T03:52:12.8390947Z         if torch.cuda.is_available():
2025-04-11T03:52:12.8391069Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8391193Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8391410Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T03:52:12.8391519Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8391819Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8392025Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8392187Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8392191Z 
2025-04-11T03:52:12.8392330Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T03:52:12.8392516Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4] ______
2025-04-11T03:52:12.8392520Z 
2025-04-11T03:52:12.8392685Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8392909Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8393003Z g_dtype = torch.float16
2025-04-11T03:52:12.8393007Z 
2025-04-11T03:52:12.8393174Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8393300Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8393451Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8393548Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8393704Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8393789Z         device: torch.device,
2025-04-11T03:52:12.8393872Z         adamw: bool,
2025-04-11T03:52:12.8393961Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8394050Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8394127Z     ) -> None:
2025-04-11T03:52:12.8394378Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8394479Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8394540Z 
2025-04-11T03:52:12.8394654Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8394772Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8395018Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8395118Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8395346Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8395439Z     return self._apply(convert)
2025-04-11T03:52:12.8395671Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8395754Z     module._apply(fn)
2025-04-11T03:52:12.8395990Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8396075Z     module._apply(fn)
2025-04-11T03:52:12.8396307Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8396386Z     module._apply(fn)
2025-04-11T03:52:12.8396613Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8396703Z     param_applied = fn(param)
2025-04-11T03:52:12.8396815Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8396821Z 
2025-04-11T03:52:12.8396913Z t = Parameter containing:
2025-04-11T03:52:12.8397045Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8397145Z         [ 0.0145, -0.0268,...4,  0.0235, -0.0067],
2025-04-11T03:52:12.8397329Z         [-0.0276, -0.0061,  0.0080,  ...,  0.0096,  0.0016, -0.0028]],
2025-04-11T03:52:12.8397421Z        requires_grad=True)
2025-04-11T03:52:12.8397426Z 
2025-04-11T03:52:12.8397507Z     def convert(t):
2025-04-11T03:52:12.8397635Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8397821Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8397940Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8398149Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8398256Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8398658Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8398794Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8398953Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8398961Z 
2025-04-11T03:52:12.8399210Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8399452Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0] ______
2025-04-11T03:52:12.8399457Z 
2025-04-11T03:52:12.8399625Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T03:52:12.8399792Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8399884Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8399888Z 
2025-04-11T03:52:12.8400055Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8400181Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8400334Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8400427Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8400587Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8400674Z         device: torch.device,
2025-04-11T03:52:12.8400755Z         adamw: bool,
2025-04-11T03:52:12.8400842Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8400928Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8401008Z     ) -> None:
2025-04-11T03:52:12.8401311Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8401416Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8401420Z 
2025-04-11T03:52:12.8401531Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8401647Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8401897Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8401994Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8402224Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8402315Z     return self._apply(convert)
2025-04-11T03:52:12.8402553Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8402636Z     module._apply(fn)
2025-04-11T03:52:12.8402878Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8402957Z     module._apply(fn)
2025-04-11T03:52:12.8403193Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8403271Z     module._apply(fn)
2025-04-11T03:52:12.8403495Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8403591Z     param_applied = fn(param)
2025-04-11T03:52:12.8403701Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8403705Z 
2025-04-11T03:52:12.8403853Z t = Parameter containing:
2025-04-11T03:52:12.8403988Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8404089Z         [ 0.0360, -0.0060,...2,  0.0336, -0.0315],
2025-04-11T03:52:12.8404209Z         [ 0.0418,  0.0034,  0.0053,  ...,  0.0279, -0.0100,  0.0020]],
2025-04-11T03:52:12.8404297Z        requires_grad=True)
2025-04-11T03:52:12.8404302Z 
2025-04-11T03:52:12.8404386Z     def convert(t):
2025-04-11T03:52:12.8404515Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8404694Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8404811Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8405082Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8405192Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8405480Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8405622Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8405838Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8405842Z 
2025-04-11T03:52:12.8406099Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8406282Z _______ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2] _______
2025-04-11T03:52:12.8406285Z 
2025-04-11T03:52:12.8406442Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T03:52:12.8406610Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8406703Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8406707Z 
2025-04-11T03:52:12.8406879Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8407002Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8407161Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8407253Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8407413Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8407500Z         device: torch.device,
2025-04-11T03:52:12.8407638Z         adamw: bool,
2025-04-11T03:52:12.8407731Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8407816Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8407894Z     ) -> None:
2025-04-11T03:52:12.8408146Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8408249Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8408253Z 
2025-04-11T03:52:12.8408361Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8408476Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8408718Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8408811Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8409038Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8409128Z     return self._apply(convert)
2025-04-11T03:52:12.8409362Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8409442Z     module._apply(fn)
2025-04-11T03:52:12.8409678Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8409760Z     module._apply(fn)
2025-04-11T03:52:12.8409990Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8410074Z     module._apply(fn)
2025-04-11T03:52:12.8410304Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8410449Z     param_applied = fn(param)
2025-04-11T03:52:12.8410560Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8410564Z 
2025-04-11T03:52:12.8410657Z t = Parameter containing:
2025-04-11T03:52:12.8410788Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8410888Z         [-0.0029, -0.0003,...8,  0.0132,  0.0134],
2025-04-11T03:52:12.8411015Z         [-0.0017, -0.0011, -0.0088,  ...,  0.0178,  0.0258,  0.0116]],
2025-04-11T03:52:12.8411099Z        requires_grad=True)
2025-04-11T03:52:12.8411104Z 
2025-04-11T03:52:12.8411188Z     def convert(t):
2025-04-11T03:52:12.8411319Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8411561Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8411678Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8411883Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8411995Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8412277Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8412473Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8412632Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8412636Z 
2025-04-11T03:52:12.8412891Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8413075Z _____ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3] ______
2025-04-11T03:52:12.8413081Z 
2025-04-11T03:52:12.8413254Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8413408Z device = device(type='cpu'), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8413497Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8413500Z 
2025-04-11T03:52:12.8413675Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8413797Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8413955Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8414106Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8414270Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8414358Z         device: torch.device,
2025-04-11T03:52:12.8414438Z         adamw: bool,
2025-04-11T03:52:12.8414531Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8414615Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8414698Z     ) -> None:
2025-04-11T03:52:12.8414946Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8415045Z         torch_model = model_fn().to(device)
2025-04-11T03:52:12.8415156Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T03:52:12.8415234Z         lr = 1e-3
2025-04-11T03:52:12.8415325Z         beta1, beta2 = 0.9, 0.999
2025-04-11T03:52:12.8415405Z         eps = 1e-8
2025-04-11T03:52:12.8415518Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T03:52:12.8415745Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T03:52:12.8415956Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T03:52:12.8415965Z 
2025-04-11T03:52:12.8416073Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T03:52:12.8416185Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8416191Z 
2025-04-11T03:52:12.8416281Z self = HybridAdam (
2025-04-11T03:52:12.8416363Z Parameter Group 0
2025-04-11T03:52:12.8416447Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8416533Z     bias_correction: True
2025-04-11T03:52:12.8416669Z     eps: 1e-08
2025-04-11T03:52:12.8416753Z     lr: 0.001
2025-04-11T03:52:12.8416843Z     weig...arameter Group 1
2025-04-11T03:52:12.8416928Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8417014Z     bias_correction: True
2025-04-11T03:52:12.8417092Z     eps: 1e-08
2025-04-11T03:52:12.8417171Z     lr: 0.001
2025-04-11T03:52:12.8417254Z     weight_decay: 0.0
2025-04-11T03:52:12.8417327Z )
2025-04-11T03:52:12.8417637Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T03:52:12.8417785Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T03:52:12.8418005Z weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
2025-04-11T03:52:12.8418104Z nvme_offload_dir = None, defaults = {}
2025-04-11T03:52:12.8418496Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T03:52:12.8418500Z 
2025-04-11T03:52:12.8418577Z     def __init__(
2025-04-11T03:52:12.8418659Z         self,
2025-04-11T03:52:12.8418737Z         model_params,
2025-04-11T03:52:12.8418875Z         lr=1e-3,
2025-04-11T03:52:12.8418961Z         bias_correction=True,
2025-04-11T03:52:12.8419045Z         betas=(0.9, 0.999),
2025-04-11T03:52:12.8419128Z         eps=1e-8,
2025-04-11T03:52:12.8419208Z         weight_decay=0,
2025-04-11T03:52:12.8419296Z         adamw_mode=True,
2025-04-11T03:52:12.8419394Z         nvme_offload_fraction: float = 0.0,
2025-04-11T03:52:12.8419500Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T03:52:12.8419585Z         **defaults: Any,
2025-04-11T03:52:12.8419661Z     ):
2025-04-11T03:52:12.8419745Z         super().__init__(
2025-04-11T03:52:12.8419826Z             model_params,
2025-04-11T03:52:12.8419905Z             lr,
2025-04-11T03:52:12.8419989Z             bias_correction,
2025-04-11T03:52:12.8420065Z             betas,
2025-04-11T03:52:12.8420144Z             eps,
2025-04-11T03:52:12.8420224Z             weight_decay,
2025-04-11T03:52:12.8420305Z             adamw_mode,
2025-04-11T03:52:12.8420396Z             nvme_offload_fraction,
2025-04-11T03:52:12.8420482Z             nvme_offload_dir,
2025-04-11T03:52:12.8420556Z         )
2025-04-11T03:52:12.8420782Z         if torch.cuda.is_available():
2025-04-11T03:52:12.8420912Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8421041Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8421251Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T03:52:12.8421367Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8421656Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8421802Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8421965Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8421969Z 
2025-04-11T03:52:12.8422107Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T03:52:12.8422293Z _____ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4] ______
2025-04-11T03:52:12.8422299Z 
2025-04-11T03:52:12.8422469Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8422636Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8422724Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8422728Z 
2025-04-11T03:52:12.8422900Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8423025Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8423186Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8423279Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8423498Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8423588Z         device: torch.device,
2025-04-11T03:52:12.8423668Z         adamw: bool,
2025-04-11T03:52:12.8423762Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8423847Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8423925Z     ) -> None:
2025-04-11T03:52:12.8424179Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8424279Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8424283Z 
2025-04-11T03:52:12.8424392Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8424504Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8424825Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8424920Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8425157Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8425251Z     return self._apply(convert)
2025-04-11T03:52:12.8425490Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8425632Z     module._apply(fn)
2025-04-11T03:52:12.8425870Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8425955Z     module._apply(fn)
2025-04-11T03:52:12.8426185Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8426268Z     module._apply(fn)
2025-04-11T03:52:12.8426498Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8426593Z     param_applied = fn(param)
2025-04-11T03:52:12.8426703Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8426707Z 
2025-04-11T03:52:12.8426798Z t = Parameter containing:
2025-04-11T03:52:12.8426938Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8427037Z         [ 0.0281,  0.0026,...4, -0.0037,  0.0294],
2025-04-11T03:52:12.8427161Z         [ 0.0003,  0.0104, -0.0075,  ...,  0.0078,  0.0005, -0.0179]],
2025-04-11T03:52:12.8427247Z        requires_grad=True)
2025-04-11T03:52:12.8427302Z 
2025-04-11T03:52:12.8427390Z     def convert(t):
2025-04-11T03:52:12.8427521Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8427699Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8427821Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8428027Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8428141Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8428473Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8428613Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8428775Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8428778Z 
2025-04-11T03:52:12.8429035Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8429217Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0] _______
2025-04-11T03:52:12.8429221Z 
2025-04-11T03:52:12.8429386Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T03:52:12.8429557Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8429648Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8429652Z 
2025-04-11T03:52:12.8429873Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8430067Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8430229Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8430321Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8430479Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8430569Z         device: torch.device,
2025-04-11T03:52:12.8430649Z         adamw: bool,
2025-04-11T03:52:12.8430739Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8430824Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8430903Z     ) -> None:
2025-04-11T03:52:12.8431153Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8431314Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8431318Z 
2025-04-11T03:52:12.8431433Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8431546Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8431796Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8431890Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8432178Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8432269Z     return self._apply(convert)
2025-04-11T03:52:12.8432501Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8432588Z     module._apply(fn)
2025-04-11T03:52:12.8432814Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8432897Z     module._apply(fn)
2025-04-11T03:52:12.8433124Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8433209Z     module._apply(fn)
2025-04-11T03:52:12.8433432Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8433523Z     param_applied = fn(param)
2025-04-11T03:52:12.8433637Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8433640Z 
2025-04-11T03:52:12.8433731Z t = Parameter containing:
2025-04-11T03:52:12.8433864Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8434024Z         [ 0.0240, -0.0152,...6, -0.0175, -0.0244],
2025-04-11T03:52:12.8434156Z         [-0.0064, -0.0248,  0.0195,  ..., -0.0030, -0.0263,  0.0248]],
2025-04-11T03:52:12.8434243Z        requires_grad=True)
2025-04-11T03:52:12.8434247Z 
2025-04-11T03:52:12.8434326Z     def convert(t):
2025-04-11T03:52:12.8434465Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8434642Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8434763Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8434968Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8435078Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8435359Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8435498Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8435659Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8435663Z 
2025-04-11T03:52:12.8435912Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8436092Z _______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2] ________
2025-04-11T03:52:12.8436097Z 
2025-04-11T03:52:12.8436250Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T03:52:12.8436421Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8436564Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8436569Z 
2025-04-11T03:52:12.8436742Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8436864Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8437022Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8437121Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8437276Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8437365Z         device: torch.device,
2025-04-11T03:52:12.8437444Z         adamw: bool,
2025-04-11T03:52:12.8437537Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8437621Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8437754Z     ) -> None:
2025-04-11T03:52:12.8438010Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8438106Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8438112Z 
2025-04-11T03:52:12.8438224Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8438334Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8438636Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8438732Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8438955Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8439053Z     return self._apply(convert)
2025-04-11T03:52:12.8439281Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8439368Z     module._apply(fn)
2025-04-11T03:52:12.8439594Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8439676Z     module._apply(fn)
2025-04-11T03:52:12.8439902Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8439980Z     module._apply(fn)
2025-04-11T03:52:12.8440208Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8440298Z     param_applied = fn(param)
2025-04-11T03:52:12.8440408Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8440465Z 
2025-04-11T03:52:12.8440555Z t = Parameter containing:
2025-04-11T03:52:12.8440692Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8440789Z         [ 0.0105,  0.0235,...3,  0.0204, -0.0137],
2025-04-11T03:52:12.8440908Z         [ 0.0001, -0.0009, -0.0197,  ...,  0.0352, -0.0017,  0.0075]],
2025-04-11T03:52:12.8441001Z        requires_grad=True)
2025-04-11T03:52:12.8441005Z 
2025-04-11T03:52:12.8441084Z     def convert(t):
2025-04-11T03:52:12.8441217Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8441393Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8441516Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8441717Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8441828Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8442112Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8442245Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8442407Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8442413Z 
2025-04-11T03:52:12.8442663Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8442846Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3] ______
2025-04-11T03:52:12.8442850Z 
2025-04-11T03:52:12.8443065Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8443224Z device = device(type='cpu'), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8443315Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8443318Z 
2025-04-11T03:52:12.8443486Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8443611Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8443766Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8443863Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8444017Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8444169Z         device: torch.device,
2025-04-11T03:52:12.8444250Z         adamw: bool,
2025-04-11T03:52:12.8444337Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8444428Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8444505Z     ) -> None:
2025-04-11T03:52:12.8444758Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8444853Z         torch_model = model_fn().to(device)
2025-04-11T03:52:12.8445105Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T03:52:12.8445186Z         lr = 1e-3
2025-04-11T03:52:12.8445277Z         beta1, beta2 = 0.9, 0.999
2025-04-11T03:52:12.8445362Z         eps = 1e-8
2025-04-11T03:52:12.8445466Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T03:52:12.8445696Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T03:52:12.8445908Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T03:52:12.8445914Z 
2025-04-11T03:52:12.8446025Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T03:52:12.8446140Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8446144Z 
2025-04-11T03:52:12.8446228Z self = HybridAdam (
2025-04-11T03:52:12.8446317Z Parameter Group 0
2025-04-11T03:52:12.8446399Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8446487Z     bias_correction: True
2025-04-11T03:52:12.8446566Z     eps: 1e-08
2025-04-11T03:52:12.8446643Z     lr: 0.001
2025-04-11T03:52:12.8446734Z     weig...arameter Group 1
2025-04-11T03:52:12.8446883Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8446975Z     bias_correction: True
2025-04-11T03:52:12.8447053Z     eps: 1e-08
2025-04-11T03:52:12.8447127Z     lr: 0.001
2025-04-11T03:52:12.8447213Z     weight_decay: 0.0
2025-04-11T03:52:12.8447283Z )
2025-04-11T03:52:12.8447600Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T03:52:12.8447748Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T03:52:12.8447901Z weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
2025-04-11T03:52:12.8448002Z nvme_offload_dir = None, defaults = {}
2025-04-11T03:52:12.8448374Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T03:52:12.8448383Z 
2025-04-11T03:52:12.8448463Z     def __init__(
2025-04-11T03:52:12.8448538Z         self,
2025-04-11T03:52:12.8448620Z         model_params,
2025-04-11T03:52:12.8448695Z         lr=1e-3,
2025-04-11T03:52:12.8448785Z         bias_correction=True,
2025-04-11T03:52:12.8448865Z         betas=(0.9, 0.999),
2025-04-11T03:52:12.8448941Z         eps=1e-8,
2025-04-11T03:52:12.8449027Z         weight_decay=0,
2025-04-11T03:52:12.8449107Z         adamw_mode=True,
2025-04-11T03:52:12.8449209Z         nvme_offload_fraction: float = 0.0,
2025-04-11T03:52:12.8449315Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T03:52:12.8449397Z         **defaults: Any,
2025-04-11T03:52:12.8449476Z     ):
2025-04-11T03:52:12.8449617Z         super().__init__(
2025-04-11T03:52:12.8449705Z             model_params,
2025-04-11T03:52:12.8449781Z             lr,
2025-04-11T03:52:12.8449866Z             bias_correction,
2025-04-11T03:52:12.8449947Z             betas,
2025-04-11T03:52:12.8450023Z             eps,
2025-04-11T03:52:12.8450105Z             weight_decay,
2025-04-11T03:52:12.8450185Z             adamw_mode,
2025-04-11T03:52:12.8450283Z             nvme_offload_fraction,
2025-04-11T03:52:12.8450365Z             nvme_offload_dir,
2025-04-11T03:52:12.8450438Z         )
2025-04-11T03:52:12.8450540Z         if torch.cuda.is_available():
2025-04-11T03:52:12.8450663Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8450792Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8451068Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T03:52:12.8451178Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8451472Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8451612Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8451834Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8451838Z 
2025-04-11T03:52:12.8451977Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T03:52:12.8452160Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4] ______
2025-04-11T03:52:12.8452164Z 
2025-04-11T03:52:12.8452329Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8452496Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8452586Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8452589Z 
2025-04-11T03:52:12.8452758Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8452888Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8453044Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8453141Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8453300Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8453391Z         device: torch.device,
2025-04-11T03:52:12.8453527Z         adamw: bool,
2025-04-11T03:52:12.8453617Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8453708Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8453785Z     ) -> None:
2025-04-11T03:52:12.8454039Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8454138Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8454142Z 
2025-04-11T03:52:12.8454255Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8454368Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8454617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8454715Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8454942Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8455039Z     return self._apply(convert)
2025-04-11T03:52:12.8455273Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8455357Z     module._apply(fn)
2025-04-11T03:52:12.8455588Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8455669Z     module._apply(fn)
2025-04-11T03:52:12.8455905Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8455987Z     module._apply(fn)
2025-04-11T03:52:12.8456221Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8456361Z     param_applied = fn(param)
2025-04-11T03:52:12.8456479Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8456483Z 
2025-04-11T03:52:12.8456574Z t = Parameter containing:
2025-04-11T03:52:12.8456712Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8456819Z         [ 0.0140, -0.0115,...1,  0.0094,  0.0310],
2025-04-11T03:52:12.8456943Z         [ 0.0050,  0.0139, -0.0004,  ...,  0.0203, -0.0216, -0.0075]],
2025-04-11T03:52:12.8457035Z        requires_grad=True)
2025-04-11T03:52:12.8457039Z 
2025-04-11T03:52:12.8457123Z     def convert(t):
2025-04-11T03:52:12.8457261Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8457504Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8457625Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8457834Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8457942Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8458229Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8458423Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8458586Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8458590Z 
2025-04-11T03:52:12.8458848Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8458994Z _____________________________ test_dist_adafactor ______________________________
2025-04-11T03:52:12.8459000Z 
2025-04-11T03:52:12.8459093Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8459710Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8459718Z 
2025-04-11T03:52:12.8459823Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8459902Z         try_count = 0
2025-04-11T03:52:12.8460005Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8460143Z             max_try, int
2025-04-11T03:52:12.8460298Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8460369Z     
2025-04-11T03:52:12.8460487Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8460566Z             try:
2025-04-11T03:52:12.8460652Z                 try_count += 1
2025-04-11T03:52:12.8460746Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8460827Z                 return ret
2025-04-11T03:52:12.8460930Z             except exception_type as e:
2025-04-11T03:52:12.8461032Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8461225Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8461351Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8461502Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8461663Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8461746Z                     continue
2025-04-11T03:52:12.8461823Z                 else:
2025-04-11T03:52:12.8462046Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8462126Z >                   raise e
2025-04-11T03:52:12.8462132Z 
2025-04-11T03:52:12.8462236Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8462347Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8462536Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8462626Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8462803Z tests/test_optimizer/test_dist_adafactor.py:468: in test_dist_adafactor
2025-04-11T03:52:12.8462893Z     spawn(run_dist, nprocs=4)
2025-04-11T03:52:12.8462997Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8463100Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8463355Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8463534Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8463818Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8464023Z     while not context.join():
2025-04-11T03:52:12.8464133Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8464137Z 
2025-04-11T03:52:12.8464340Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f023e410>
2025-04-11T03:52:12.8464424Z timeout = None
2025-04-11T03:52:12.8464429Z 
2025-04-11T03:52:12.8464518Z     def join(self, timeout=None):
2025-04-11T03:52:12.8464648Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8464774Z     
2025-04-11T03:52:12.8464927Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8465074Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8465238Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8465337Z         of the first process exiting.
2025-04-11T03:52:12.8465408Z     
2025-04-11T03:52:12.8465563Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8465703Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8465773Z     
2025-04-11T03:52:12.8465855Z         Args:
2025-04-11T03:52:12.8465994Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8466071Z         """
2025-04-11T03:52:12.8466210Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8466309Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8466388Z             return True
2025-04-11T03:52:12.8466460Z     
2025-04-11T03:52:12.8466656Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8466782Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8466880Z             self.sentinels.keys(),
2025-04-11T03:52:12.8466966Z             timeout=timeout,
2025-04-11T03:52:12.8467039Z         )
2025-04-11T03:52:12.8467113Z     
2025-04-11T03:52:12.8467198Z         error_index = None
2025-04-11T03:52:12.8467289Z         for sentinel in ready:
2025-04-11T03:52:12.8467397Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8467496Z             process = self.processes[index]
2025-04-11T03:52:12.8467585Z             process.join()
2025-04-11T03:52:12.8467680Z             if process.exitcode != 0:
2025-04-11T03:52:12.8467771Z                 error_index = index
2025-04-11T03:52:12.8467847Z                 break
2025-04-11T03:52:12.8467921Z     
2025-04-11T03:52:12.8468014Z         # Return if there was no error.
2025-04-11T03:52:12.8468100Z         if error_index is None:
2025-04-11T03:52:12.8468240Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8468337Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8468447Z     
2025-04-11T03:52:12.8468590Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8468688Z         for process in self.processes:
2025-04-11T03:52:12.8468784Z             if process.is_alive():
2025-04-11T03:52:12.8468877Z                 process.terminate()
2025-04-11T03:52:12.8468965Z             process.join()
2025-04-11T03:52:12.8469036Z     
2025-04-11T03:52:12.8469240Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8469364Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8469470Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8469599Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8469683Z             if exitcode < 0:
2025-04-11T03:52:12.8469795Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8469905Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8470059Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8470162Z                     error_index=error_index,
2025-04-11T03:52:12.8470266Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8470441Z                     exit_code=exitcode,
2025-04-11T03:52:12.8470528Z                     signal_name=name,
2025-04-11T03:52:12.8470609Z                 )
2025-04-11T03:52:12.8470683Z             else:
2025-04-11T03:52:12.8470789Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8470958Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8471050Z                     error_index=error_index,
2025-04-11T03:52:12.8471217Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8471304Z                     exit_code=exitcode,
2025-04-11T03:52:12.8471380Z                 )
2025-04-11T03:52:12.8471458Z     
2025-04-11T03:52:12.8471590Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8471768Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8471854Z         msg += original_trace
2025-04-11T03:52:12.8472036Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8472200Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8472274Z E       
2025-04-11T03:52:12.8472411Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8472512Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8472815Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8472899Z E           fn(i, *args)
2025-04-11T03:52:12.8473217Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 459, in run_dist
2025-04-11T03:52:12.8473313Z E           exam_dist_adafactor_base()
2025-04-11T03:52:12.8473572Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8473666Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8473918Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8474010Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8474295Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 111, in exam_dist_adafactor_base
2025-04-11T03:52:12.8474456Z E           model_col = nn.Linear(H, W).to(local_rank)  # Col parallel weight
2025-04-11T03:52:12.8474721Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:12.8474818Z E           return self._apply(convert)
2025-04-11T03:52:12.8475094Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8475186Z E           param_applied = fn(param)
2025-04-11T03:52:12.8475465Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:12.8475679Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8475792Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8476125Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8476267Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8476430Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8476435Z 
2025-04-11T03:52:12.8476738Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8476896Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8477054Z [04/11/25 03:47:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8477190Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8477357Z                              :75 launch                                         
2025-04-11T03:52:12.8477496Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8477623Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8477822Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8478016Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8479148Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8479324Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8480419Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8480648Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8481740Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8481906Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8482993Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8483155Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8483834Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8483920Z   warnings.warn(
2025-04-11T03:52:12.8484649Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8484732Z   warnings.warn(
2025-04-11T03:52:12.8485389Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8485473Z   warnings.warn(
2025-04-11T03:52:12.8486128Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8486275Z   warnings.warn(
2025-04-11T03:52:12.8487092Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8487232Z   warnings.warn(
2025-04-11T03:52:12.8488037Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8488120Z   warnings.warn(
2025-04-11T03:52:12.8488912Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8488997Z   warnings.warn(
2025-04-11T03:52:12.8489788Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8489873Z   warnings.warn(
2025-04-11T03:52:12.8490711Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8490799Z   warnings.warn(
2025-04-11T03:52:12.8491590Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8491672Z   warnings.warn(
2025-04-11T03:52:12.8492461Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8492548Z   warnings.warn(
2025-04-11T03:52:12.8493336Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8493418Z   warnings.warn(
2025-04-11T03:52:12.8494258Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8494343Z   warnings.warn(
2025-04-11T03:52:12.8495126Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8495204Z   warnings.warn(
2025-04-11T03:52:12.8495983Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8496128Z   warnings.warn(
2025-04-11T03:52:12.8496935Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8497069Z   warnings.warn(
2025-04-11T03:52:12.8497212Z ________________________________ test_dist_came ________________________________
2025-04-11T03:52:12.8497216Z 
2025-04-11T03:52:12.8497311Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8497913Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8497919Z 
2025-04-11T03:52:12.8498027Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8498113Z         try_count = 0
2025-04-11T03:52:12.8498215Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8498297Z             max_try, int
2025-04-11T03:52:12.8498455Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8498528Z     
2025-04-11T03:52:12.8498645Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8498775Z             try:
2025-04-11T03:52:12.8498865Z                 try_count += 1
2025-04-11T03:52:12.8498966Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8499048Z                 return ret
2025-04-11T03:52:12.8499152Z             except exception_type as e:
2025-04-11T03:52:12.8499258Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8499457Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8499579Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8499727Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8499887Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8499970Z                     continue
2025-04-11T03:52:12.8500055Z                 else:
2025-04-11T03:52:12.8500279Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8500364Z >                   raise e
2025-04-11T03:52:12.8500368Z 
2025-04-11T03:52:12.8500463Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8500573Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8500712Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8500802Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8500961Z tests/test_optimizer/test_dist_came.py:357: in test_dist_came
2025-04-11T03:52:12.8501052Z     spawn(run_dist, nprocs=4)
2025-04-11T03:52:12.8501159Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8501324Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8501586Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8501767Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8502053Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8502149Z     while not context.join():
2025-04-11T03:52:12.8502261Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8502265Z 
2025-04-11T03:52:12.8502463Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f023e3b0>
2025-04-11T03:52:12.8502608Z timeout = None
2025-04-11T03:52:12.8502613Z 
2025-04-11T03:52:12.8502705Z     def join(self, timeout=None):
2025-04-11T03:52:12.8502834Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8502905Z     
2025-04-11T03:52:12.8503057Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8503206Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8503429Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8503524Z         of the first process exiting.
2025-04-11T03:52:12.8503597Z     
2025-04-11T03:52:12.8503750Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8503891Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8503964Z     
2025-04-11T03:52:12.8504038Z         Args:
2025-04-11T03:52:12.8504178Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8504259Z         """
2025-04-11T03:52:12.8504398Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8504494Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8504574Z             return True
2025-04-11T03:52:12.8504652Z     
2025-04-11T03:52:12.8504786Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8504909Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8505008Z             self.sentinels.keys(),
2025-04-11T03:52:12.8505092Z             timeout=timeout,
2025-04-11T03:52:12.8505166Z         )
2025-04-11T03:52:12.8505292Z     
2025-04-11T03:52:12.8505381Z         error_index = None
2025-04-11T03:52:12.8505473Z         for sentinel in ready:
2025-04-11T03:52:12.8505583Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8505687Z             process = self.processes[index]
2025-04-11T03:52:12.8505774Z             process.join()
2025-04-11T03:52:12.8505871Z             if process.exitcode != 0:
2025-04-11T03:52:12.8505962Z                 error_index = index
2025-04-11T03:52:12.8506038Z                 break
2025-04-11T03:52:12.8506113Z     
2025-04-11T03:52:12.8506207Z         # Return if there was no error.
2025-04-11T03:52:12.8506300Z         if error_index is None:
2025-04-11T03:52:12.8506434Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8506529Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8506605Z     
2025-04-11T03:52:12.8506744Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8506849Z         for process in self.processes:
2025-04-11T03:52:12.8506939Z             if process.is_alive():
2025-04-11T03:52:12.8507032Z                 process.terminate()
2025-04-11T03:52:12.8507121Z             process.join()
2025-04-11T03:52:12.8507192Z     
2025-04-11T03:52:12.8507334Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8507455Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8507567Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8507688Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8507824Z             if exitcode < 0:
2025-04-11T03:52:12.8507939Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8508047Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8508204Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8508303Z                     error_index=error_index,
2025-04-11T03:52:12.8508459Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8508556Z                     exit_code=exitcode,
2025-04-11T03:52:12.8508646Z                     signal_name=name,
2025-04-11T03:52:12.8508730Z                 )
2025-04-11T03:52:12.8508809Z             else:
2025-04-11T03:52:12.8508920Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8509155Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8509250Z                     error_index=error_index,
2025-04-11T03:52:12.8509355Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8509443Z                     exit_code=exitcode,
2025-04-11T03:52:12.8509523Z                 )
2025-04-11T03:52:12.8509593Z     
2025-04-11T03:52:12.8509725Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8509959Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8510047Z         msg += original_trace
2025-04-11T03:52:12.8510223Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8510383Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8510460Z E       
2025-04-11T03:52:12.8510585Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8510685Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8510988Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8511071Z E           fn(i, *args)
2025-04-11T03:52:12.8511309Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 349, in run_dist
2025-04-11T03:52:12.8511453Z E           exam_bert_test_on_lowlevelzero_plugin()  # err in TODO layer
2025-04-11T03:52:12.8511715Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8511870Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8512178Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 206, in exam_bert_test_on_lowlevelzero_plugin
2025-04-11T03:52:12.8512406Z E           ) = build_model_from_low_level_zero_plugin(model_fn, loss_fn, test_config, CAME, DistributedCAME)
2025-04-11T03:52:12.8512725Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 188, in build_model_from_low_level_zero_plugin
2025-04-11T03:52:12.8512828Z E           org_model = org_model.cuda()
2025-04-11T03:52:12.8513115Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.8513222Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.8513487Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8513612Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8513880Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8513968Z E           module._apply(fn)
2025-04-11T03:52:12.8514234Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8514322Z E           module._apply(fn)
2025-04-11T03:52:12.8514588Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8514683Z E           param_applied = fn(param)
2025-04-11T03:52:12.8515017Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8515138Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8515246Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8515537Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8515674Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8515839Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8515843Z 
2025-04-11T03:52:12.8516202Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8516359Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8516517Z [04/11/25 03:47:13] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8516652Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8516760Z                              :75 launch                                         
2025-04-11T03:52:12.8516967Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8517100Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8517294Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8517446Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8518572Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8518746Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8519906Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8520079Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8521160Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8521328Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8522401Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8522567Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8523297Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8523387Z   warnings.warn(
2025-04-11T03:52:12.8524050Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8524137Z   warnings.warn(
2025-04-11T03:52:12.8524808Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8524956Z   warnings.warn(
2025-04-11T03:52:12.8525613Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8525761Z   warnings.warn(
2025-04-11T03:52:12.8526581Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8526663Z   warnings.warn(
2025-04-11T03:52:12.8527451Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8527536Z   warnings.warn(
2025-04-11T03:52:12.8528336Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8528486Z   warnings.warn(
2025-04-11T03:52:12.8529268Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8529350Z   warnings.warn(
2025-04-11T03:52:12.8530132Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8530210Z   warnings.warn(
2025-04-11T03:52:12.8531061Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8531143Z   warnings.warn(
2025-04-11T03:52:12.8531925Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8532005Z   warnings.warn(
2025-04-11T03:52:12.8532830Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8532915Z   warnings.warn(
2025-04-11T03:52:12.8533706Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8533787Z   warnings.warn(
2025-04-11T03:52:12.8534572Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8534715Z   warnings.warn(
2025-04-11T03:52:12.8535496Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8535634Z   warnings.warn(
2025-04-11T03:52:12.8536416Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8536496Z   warnings.warn(
2025-04-11T03:52:12.8536798Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8537078Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8537218Z _______________________________ test_dist_galore _______________________________
2025-04-11T03:52:12.8537224Z 
2025-04-11T03:52:12.8537320Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8537979Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8537984Z 
2025-04-11T03:52:12.8538096Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8538179Z         try_count = 0
2025-04-11T03:52:12.8538286Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8538368Z             max_try, int
2025-04-11T03:52:12.8538519Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8538590Z     
2025-04-11T03:52:12.8538705Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8538788Z             try:
2025-04-11T03:52:12.8538873Z                 try_count += 1
2025-04-11T03:52:12.8538968Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8539051Z                 return ret
2025-04-11T03:52:12.8539148Z             except exception_type as e:
2025-04-11T03:52:12.8539253Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8539441Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8539564Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8539712Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8539873Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8539956Z                     continue
2025-04-11T03:52:12.8540035Z                 else:
2025-04-11T03:52:12.8540319Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8540403Z >                   raise e
2025-04-11T03:52:12.8540407Z 
2025-04-11T03:52:12.8540514Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8540627Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8540771Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8540860Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8541023Z tests/test_optimizer/test_dist_galore.py:298: in test_dist_galore
2025-04-11T03:52:12.8541125Z     spawn(check_dist_galore, nprocs=4)
2025-04-11T03:52:12.8541226Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8541393Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8541653Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8541834Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8542120Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8542209Z     while not context.join():
2025-04-11T03:52:12.8542381Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8542385Z 
2025-04-11T03:52:12.8542584Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ee240a0>
2025-04-11T03:52:12.8542668Z timeout = None
2025-04-11T03:52:12.8542672Z 
2025-04-11T03:52:12.8542764Z     def join(self, timeout=None):
2025-04-11T03:52:12.8542892Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8542966Z     
2025-04-11T03:52:12.8543113Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8543259Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8543418Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8543517Z         of the first process exiting.
2025-04-11T03:52:12.8543588Z     
2025-04-11T03:52:12.8543738Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8543876Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8543947Z     
2025-04-11T03:52:12.8544082Z         Args:
2025-04-11T03:52:12.8544222Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8544301Z         """
2025-04-11T03:52:12.8544439Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8544532Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8544618Z             return True
2025-04-11T03:52:12.8544690Z     
2025-04-11T03:52:12.8544824Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8544941Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8545037Z             self.sentinels.keys(),
2025-04-11T03:52:12.8545122Z             timeout=timeout,
2025-04-11T03:52:12.8545196Z         )
2025-04-11T03:52:12.8545271Z     
2025-04-11T03:52:12.8545355Z         error_index = None
2025-04-11T03:52:12.8545447Z         for sentinel in ready:
2025-04-11T03:52:12.8545553Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8545655Z             process = self.processes[index]
2025-04-11T03:52:12.8545744Z             process.join()
2025-04-11T03:52:12.8545838Z             if process.exitcode != 0:
2025-04-11T03:52:12.8545931Z                 error_index = index
2025-04-11T03:52:12.8546006Z                 break
2025-04-11T03:52:12.8546077Z     
2025-04-11T03:52:12.8546176Z         # Return if there was no error.
2025-04-11T03:52:12.8546264Z         if error_index is None:
2025-04-11T03:52:12.8546403Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8546501Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8546577Z     
2025-04-11T03:52:12.8546776Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8546876Z         for process in self.processes:
2025-04-11T03:52:12.8546969Z             if process.is_alive():
2025-04-11T03:52:12.8547065Z                 process.terminate()
2025-04-11T03:52:12.8547151Z             process.join()
2025-04-11T03:52:12.8547222Z     
2025-04-11T03:52:12.8547363Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8547487Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8547597Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8547724Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8547870Z             if exitcode < 0:
2025-04-11T03:52:12.8547984Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8548093Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8548247Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8548350Z                     error_index=error_index,
2025-04-11T03:52:12.8548492Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8548587Z                     exit_code=exitcode,
2025-04-11T03:52:12.8548740Z                     signal_name=name,
2025-04-11T03:52:12.8548816Z                 )
2025-04-11T03:52:12.8548898Z             else:
2025-04-11T03:52:12.8549000Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8549169Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8549263Z                     error_index=error_index,
2025-04-11T03:52:12.8549367Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8549456Z                     exit_code=exitcode,
2025-04-11T03:52:12.8549530Z                 )
2025-04-11T03:52:12.8549606Z     
2025-04-11T03:52:12.8549736Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8549915Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8550003Z         msg += original_trace
2025-04-11T03:52:12.8550174Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8550344Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8550418Z E       
2025-04-11T03:52:12.8550609Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8550711Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8551016Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8551098Z E           fn(i, *args)
2025-04-11T03:52:12.8551370Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_galore.py", line 291, in check_dist_galore
2025-04-11T03:52:12.8551458Z E           dist.barrier()
2025-04-11T03:52:12.8551755Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:12.8551857Z E           return func(*args, **kwargs)
2025-04-11T03:52:12.8552180Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T03:52:12.8552292Z E           work = default_pg.barrier(opts=opts)
2025-04-11T03:52:12.8552400Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8552696Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8552836Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8553000Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8553006Z 
2025-04-11T03:52:12.8553313Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8553556Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8553720Z [04/11/25 03:47:21] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8553850Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8553964Z                              :75 launch                                         
2025-04-11T03:52:12.8554098Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8554221Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8554362Z Skipping forward-backward tests due to SVD instability
2025-04-11T03:52:12.8554773Z Running bert tests, which are expected to produce minor errors due to instability in SVD convergence.             For example, a 1e-9 grad diff causes drastic difference in SVD output.
2025-04-11T03:52:12.8554935Z CUDA error: out of memory
2025-04-11T03:52:12.8555215Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8555350Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8555509Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8555579Z 
2025-04-11T03:52:12.8555673Z CUDA error: out of memory
2025-04-11T03:52:12.8555948Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8556071Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8556231Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8556236Z 
2025-04-11T03:52:12.8556323Z CUDA error: out of memory
2025-04-11T03:52:12.8556594Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8556717Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8556873Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8556877Z 
2025-04-11T03:52:12.8557259Z [3] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
2025-04-11T03:52:12.8557724Z Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
2025-04-11T03:52:12.8558191Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f5059469d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T03:52:12.8558541Z frame #1: <unknown function> + 0x5522c2e (0x7f509dd52c2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8559048Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f509dd4d440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8559435Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f509dd4d782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8559800Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f509dd4e5b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8560167Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8560536Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8560880Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8561300Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8561822Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f505a62ea59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8562434Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f505a635a4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8563259Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f505a64be4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8563741Z frame #12: <unknown function> + 0x54c7dbd (0x7f509dcf7dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8564104Z frame #13: <unknown function> + 0x54d1cb8 (0x7f509dd01cb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8564426Z frame #14: <unknown function> + 0x4b16e6c (0x7f509d346e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8564738Z frame #15: <unknown function> + 0x1696528 (0x7f5099ec6528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8565053Z frame #16: <unknown function> + 0x54d94d3 (0x7f509dd094d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8565366Z frame #17: <unknown function> + 0x54e48bf (0x7f509dd148bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8566374Z frame #18: c10d::verify_params_across_processes(c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::optional<std::weak_ptr<c10d::Logger> > const&) + 0x26f (0x7f509dd7af2f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8566707Z frame #19: <unknown function> + 0xc55ad1 (0x7f50a5925ad1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8567034Z frame #20: <unknown function> + 0x413ea4 (0x7f50a50e3ea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8567173Z frame #21: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T03:52:12.8567377Z frame #22: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8567590Z frame #23: _PyEval_EvalFrameDefault + 0x53d6 (0x4f34c6 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8567786Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8567992Z frame #25: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8568184Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8568401Z frame #27: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8568534Z frame #28: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8568673Z frame #29: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T03:52:12.8568850Z frame #30: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8569059Z frame #31: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8569313Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8569525Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8569663Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8569850Z frame #35: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8570054Z frame #36: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8570244Z frame #37: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8570508Z frame #38: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8570695Z frame #39: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8570898Z frame #40: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8571085Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8571280Z frame #42: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8571527Z frame #43: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8571720Z frame #44: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8571908Z frame #45: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8572080Z frame #46: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8572218Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T03:52:12.8572401Z frame #48: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8572575Z frame #49: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8572773Z frame #50: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8572957Z frame #51: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8573218Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8573404Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8573572Z frame #54: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8573700Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T03:52:12.8573888Z frame #56: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8574085Z frame #57: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8574270Z frame #58: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8574471Z frame #59: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8574657Z frame #60: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8574857Z frame #61: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8575041Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8575239Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8575442Z . This may indicate a possible application crash on rank 0 or a network set up issue.
2025-04-11T03:52:12.8575812Z [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
2025-04-11T03:52:12.8576252Z Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
2025-04-11T03:52:12.8576630Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f60e3718d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T03:52:12.8576953Z frame #1: <unknown function> + 0x5522c2e (0x7f6128001c2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8577438Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f6127ffc440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8577914Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f6127ffc782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8578257Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f6127ffd5b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8578611Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f6127fb2a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8579010Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f6127fb2a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8579354Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f6127fb2a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8579780Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f6127fb2a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8580301Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f60e48dda59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8580891Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f60e48e4a4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8581751Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f60e48fae4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8582079Z frame #12: <unknown function> + 0x54c7dbd (0x7f6127fa6dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8582399Z frame #13: <unknown function> + 0x54d1cb8 (0x7f6127fb0cb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8582717Z frame #14: <unknown function> + 0x4b16e6c (0x7f61275f5e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8583036Z frame #15: <unknown function> + 0x1696528 (0x7f6124175528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8583344Z frame #16: <unknown function> + 0x54d94d3 (0x7f6127fb84d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8583659Z frame #17: <unknown function> + 0x54e48bf (0x7f6127fc38bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8584563Z frame #18: c10d::verify_params_across_processes(c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::optional<std::weak_ptr<c10d::Logger> > const&) + 0x26f (0x7f6128029f2f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8584901Z frame #19: <unknown function> + 0xc55ad1 (0x7f612fbd4ad1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8585228Z frame #20: <unknown function> + 0x413ea4 (0x7f612f392ea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8585361Z frame #21: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T03:52:12.8585553Z frame #22: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8585760Z frame #23: _PyEval_EvalFrameDefault + 0x53d6 (0x4f34c6 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8586028Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8586227Z frame #25: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8586417Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8586628Z frame #27: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8586821Z frame #28: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8586951Z frame #29: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T03:52:12.8587127Z frame #30: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8587328Z frame #31: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8587514Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8587726Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8587851Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8588039Z frame #35: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8588239Z frame #36: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8588483Z frame #37: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8588745Z frame #38: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8588942Z frame #39: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8589138Z frame #40: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8589326Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8589525Z frame #42: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8589712Z frame #43: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8589911Z frame #44: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8590095Z frame #45: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8590268Z frame #46: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8590396Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T03:52:12.8590584Z frame #48: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8590752Z frame #49: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8590957Z frame #50: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8591147Z frame #51: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8591409Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8591604Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8591769Z frame #54: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8591901Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T03:52:12.8592086Z frame #56: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8592282Z frame #57: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8592473Z frame #58: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8592748Z frame #59: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8592938Z frame #60: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8593136Z frame #61: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8593324Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8593581Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8593791Z . This may indicate a possible application crash on rank 0 or a network set up issue.
2025-04-11T03:52:12.8594383Z Failed to replace attention.self.query of type Linear with Linear1D_Col with the exception: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
2025-04-11T03:52:12.8594778Z Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
2025-04-11T03:52:12.8595155Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f5059469d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T03:52:12.8595480Z frame #1: <unknown function> + 0x5522c2e (0x7f509dd52c2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8596024Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f509dd4d440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8596380Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f509dd4d782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8596726Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f509dd4e5b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8597075Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8597423Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8597770Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8598116Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8598611Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f505a62ea59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8599262Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f505a635a4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8600073Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f505a64be4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8600409Z frame #12: <unknown function> + 0x54c7dbd (0x7f509dcf7dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8600729Z frame #13: <unknown function> + 0x54d1cb8 (0x7f509dd01cb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8601115Z frame #14: <unknown function> + 0x4b16e6c (0x7f509d346e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8601430Z frame #15: <unknown function> + 0x1696528 (0x7f5099ec6528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8601749Z frame #16: <unknown function> + 0x54d94d3 (0x7f509dd094d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8602125Z frame #17: <unknown function> + 0x54e48bf (0x7f509dd148bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8602459Z frame #18: <unknown function> + 0xca3fae (0x7f50a5973fae in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8602782Z frame #19: <unknown function> + 0x413ea4 (0x7f50a50e3ea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8602917Z frame #20: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T03:52:12.8603109Z frame #21: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8603236Z frame #22: /opt/conda/envs/pytorch/bin/python() [0x509cbf]
2025-04-11T03:52:12.8603444Z frame #23: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8603633Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8603895Z frame #25: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8604086Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8604289Z frame #27: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8604480Z frame #28: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8604678Z frame #29: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8604865Z frame #30: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8605061Z frame #31: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8605252Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8605465Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8605595Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8605718Z frame #35: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T03:52:12.8605896Z frame #36: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8606094Z frame #37: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8606280Z frame #38: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8606505Z frame #39: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8606703Z frame #40: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8606892Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8607091Z frame #42: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8607220Z frame #43: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8607416Z frame #44: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8607548Z frame #45: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8607810Z frame #46: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8607934Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8608135Z frame #48: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8608259Z frame #49: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8608464Z frame #50: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8608647Z frame #51: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8608850Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8609037Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8609229Z frame #54: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8609362Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8609559Z frame #56: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8609753Z frame #57: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8609962Z frame #58: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8610093Z frame #59: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8610281Z frame #60: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8610538Z frame #61: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8610729Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8610923Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8611375Z . This may indicate a possible application crash on rank 0 or a network set up issue.. Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
2025-04-11T03:52:12.8611936Z Failed to replace attention.self.query of type Linear with Linear1D_Col with the exception: [3] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Broken pipe
2025-04-11T03:52:12.8612323Z Exception raised from sendBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:643 (most recent call first):
2025-04-11T03:52:12.8612692Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f5059469d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T03:52:12.8613019Z frame #1: <unknown function> + 0x5521d1f (0x7f509dd51d1f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8613511Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x23f (0x7f509dd4d31f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8614002Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f509dd4d782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8614348Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f509dd4e5b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8614702Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8615046Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8615388Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8615795Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8616287Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f505a62ea59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8616946Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f505a635a4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8617766Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f505a64be4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8618097Z frame #12: <unknown function> + 0x54c7dbd (0x7f509dcf7dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8618417Z frame #13: <unknown function> + 0x54d1cb8 (0x7f509dd01cb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8618787Z frame #14: <unknown function> + 0x4b16e6c (0x7f509d346e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8619109Z frame #15: <unknown function> + 0x1696528 (0x7f5099ec6528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8619420Z frame #16: <unknown function> + 0x54d94d3 (0x7f509dd094d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8619737Z frame #17: <unknown function> + 0x54e48bf (0x7f509dd148bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8620064Z frame #18: <unknown function> + 0xca3fae (0x7f50a5973fae in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8620382Z frame #19: <unknown function> + 0x413ea4 (0x7f50a50e3ea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8620518Z frame #20: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T03:52:12.8620713Z frame #21: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8620843Z frame #22: /opt/conda/envs/pytorch/bin/python() [0x509cbf]
2025-04-11T03:52:12.8621053Z frame #23: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8621244Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8621447Z frame #25: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8621695Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8621900Z frame #27: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8622086Z frame #28: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8622290Z frame #29: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8622477Z frame #30: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8622679Z frame #31: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8622866Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8623142Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8623270Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8623396Z frame #35: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T03:52:12.8623569Z frame #36: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8623767Z frame #37: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8624017Z frame #38: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8624185Z frame #39: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8624386Z frame #40: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8624575Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8624780Z frame #42: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8624910Z frame #43: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8625110Z frame #44: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8625245Z frame #45: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8625443Z frame #46: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8625577Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8625830Z frame #48: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8625960Z frame #49: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8626154Z frame #50: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8626277Z frame #51: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8626478Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8626666Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8626865Z frame #54: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8626989Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8627195Z frame #56: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8627382Z frame #57: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8627594Z frame #58: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8627720Z frame #59: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8627904Z frame #60: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8628108Z frame #61: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8628292Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8628586Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8629041Z . This may indicate a possible application crash on rank 0 or a network set up issue.. Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
2025-04-11T03:52:12.8629246Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8629401Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8630549Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8630786Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8631960Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8632190Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8633290Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8633454Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8634602Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8634769Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8635456Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8635542Z   warnings.warn(
2025-04-11T03:52:12.8636207Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8636293Z   warnings.warn(
2025-04-11T03:52:12.8636961Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8637045Z   warnings.warn(
2025-04-11T03:52:12.8637754Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8637840Z   warnings.warn(
2025-04-11T03:52:12.8638659Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8638743Z   warnings.warn(
2025-04-11T03:52:12.8639542Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8639691Z   warnings.warn(
2025-04-11T03:52:12.8640475Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8640618Z   warnings.warn(
2025-04-11T03:52:12.8641395Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8641481Z   warnings.warn(
2025-04-11T03:52:12.8642252Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8642337Z   warnings.warn(
2025-04-11T03:52:12.8643112Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8643196Z   warnings.warn(
2025-04-11T03:52:12.8644039Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8644126Z   warnings.warn(
2025-04-11T03:52:12.8644904Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8644985Z   warnings.warn(
2025-04-11T03:52:12.8645770Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8645855Z   warnings.warn(
2025-04-11T03:52:12.8646638Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8646721Z   warnings.warn(
2025-04-11T03:52:12.8647548Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8647633Z   warnings.warn(
2025-04-11T03:52:12.8648425Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8648503Z   warnings.warn(
2025-04-11T03:52:12.8648647Z ________________________________ test_dist_lamb ________________________________
2025-04-11T03:52:12.8648719Z 
2025-04-11T03:52:12.8648816Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8649433Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8649439Z 
2025-04-11T03:52:12.8649546Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8649689Z         try_count = 0
2025-04-11T03:52:12.8649796Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8649888Z             max_try, int
2025-04-11T03:52:12.8650041Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8650114Z     
2025-04-11T03:52:12.8650235Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8650309Z             try:
2025-04-11T03:52:12.8650402Z                 try_count += 1
2025-04-11T03:52:12.8650498Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8650580Z                 return ret
2025-04-11T03:52:12.8650682Z             except exception_type as e:
2025-04-11T03:52:12.8650785Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8650980Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8651101Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8651255Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8651468Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8651553Z                     continue
2025-04-11T03:52:12.8651635Z                 else:
2025-04-11T03:52:12.8651856Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8651939Z >                   raise e
2025-04-11T03:52:12.8651945Z 
2025-04-11T03:52:12.8652042Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8652158Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8652294Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8652385Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8652538Z tests/test_optimizer/test_dist_lamb.py:276: in test_dist_lamb
2025-04-11T03:52:12.8652639Z     spawn(check_dist_lamb, nprocs=4)
2025-04-11T03:52:12.8652750Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8652851Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8653114Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8653299Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8653593Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8653691Z     while not context.join():
2025-04-11T03:52:12.8653804Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8653808Z 
2025-04-11T03:52:12.8654012Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ee27f40>
2025-04-11T03:52:12.8654149Z timeout = None
2025-04-11T03:52:12.8654154Z 
2025-04-11T03:52:12.8654254Z     def join(self, timeout=None):
2025-04-11T03:52:12.8654380Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8654454Z     
2025-04-11T03:52:12.8654608Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8654754Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8654920Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8655014Z         of the first process exiting.
2025-04-11T03:52:12.8655091Z     
2025-04-11T03:52:12.8655236Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8655431Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8655507Z     
2025-04-11T03:52:12.8655585Z         Args:
2025-04-11T03:52:12.8655726Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8655802Z         """
2025-04-11T03:52:12.8655940Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8656037Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8656176Z             return True
2025-04-11T03:52:12.8656249Z     
2025-04-11T03:52:12.8656384Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8656511Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8656604Z             self.sentinels.keys(),
2025-04-11T03:52:12.8656690Z             timeout=timeout,
2025-04-11T03:52:12.8656770Z         )
2025-04-11T03:52:12.8656841Z     
2025-04-11T03:52:12.8656932Z         error_index = None
2025-04-11T03:52:12.8657020Z         for sentinel in ready:
2025-04-11T03:52:12.8657130Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8657237Z             process = self.processes[index]
2025-04-11T03:52:12.8657324Z             process.join()
2025-04-11T03:52:12.8657425Z             if process.exitcode != 0:
2025-04-11T03:52:12.8657517Z                 error_index = index
2025-04-11T03:52:12.8657594Z                 break
2025-04-11T03:52:12.8657667Z     
2025-04-11T03:52:12.8657763Z         # Return if there was no error.
2025-04-11T03:52:12.8657856Z         if error_index is None:
2025-04-11T03:52:12.8658048Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8658149Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8658221Z     
2025-04-11T03:52:12.8658365Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8658465Z         for process in self.processes:
2025-04-11T03:52:12.8658556Z             if process.is_alive():
2025-04-11T03:52:12.8658656Z                 process.terminate()
2025-04-11T03:52:12.8658743Z             process.join()
2025-04-11T03:52:12.8658813Z     
2025-04-11T03:52:12.8658960Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8659077Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8659190Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8659313Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8659405Z             if exitcode < 0:
2025-04-11T03:52:12.8659513Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8659623Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8659782Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8659886Z                     error_index=error_index,
2025-04-11T03:52:12.8659993Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8660086Z                     exit_code=exitcode,
2025-04-11T03:52:12.8660173Z                     signal_name=name,
2025-04-11T03:52:12.8660250Z                 )
2025-04-11T03:52:12.8660327Z             else:
2025-04-11T03:52:12.8660437Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8660657Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8660757Z                     error_index=error_index,
2025-04-11T03:52:12.8660861Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8660948Z                     exit_code=exitcode,
2025-04-11T03:52:12.8661029Z                 )
2025-04-11T03:52:12.8661100Z     
2025-04-11T03:52:12.8661238Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8661410Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8661496Z         msg += original_trace
2025-04-11T03:52:12.8661673Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8661897Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8661979Z E       
2025-04-11T03:52:12.8662106Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8662214Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8662514Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8662653Z E           fn(i, *args)
2025-04-11T03:52:12.8662913Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_lamb.py", line 263, in check_dist_lamb
2025-04-11T03:52:12.8663003Z E           run_dist_lamb_basic()
2025-04-11T03:52:12.8663261Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8663353Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8663606Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8663696Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8663939Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8664033Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8664249Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.8664354Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.8664668Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.8664773Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.8665051Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.8665158Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8665271Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8665558Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8665702Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8665868Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8665872Z 
2025-04-11T03:52:12.8666180Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8666343Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8666508Z [04/11/25 03:47:29] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8666638Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8666749Z                              :75 launch                                         
2025-04-11T03:52:12.8683466Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8683676Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8684141Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8684313Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8685482Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8685661Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8686783Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8687030Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8688190Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8688353Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8689453Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8689616Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8690373Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8690463Z   warnings.warn(
2025-04-11T03:52:12.8691126Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8691212Z   warnings.warn(
2025-04-11T03:52:12.8691902Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8691987Z   warnings.warn(
2025-04-11T03:52:12.8692644Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8692725Z   warnings.warn(
2025-04-11T03:52:12.8693597Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8693685Z   warnings.warn(
2025-04-11T03:52:12.8694494Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8694579Z   warnings.warn(
2025-04-11T03:52:12.8695373Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8695523Z   warnings.warn(
2025-04-11T03:52:12.8696318Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8696401Z   warnings.warn(
2025-04-11T03:52:12.8697324Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8697408Z   warnings.warn(
2025-04-11T03:52:12.8698191Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8698274Z   warnings.warn(
2025-04-11T03:52:12.8699058Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8699139Z   warnings.warn(
2025-04-11T03:52:12.8699979Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8700066Z   warnings.warn(
2025-04-11T03:52:12.8700870Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8700952Z   warnings.warn(
2025-04-11T03:52:12.8701730Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8701813Z   warnings.warn(
2025-04-11T03:52:12.8702610Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8702692Z   warnings.warn(
2025-04-11T03:52:12.8703552Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8703633Z   warnings.warn(
2025-04-11T03:52:12.8704191Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8704271Z   warnings.warn(
2025-04-11T03:52:12.8704816Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8704956Z   warnings.warn(
2025-04-11T03:52:12.8705489Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8705567Z   warnings.warn(
2025-04-11T03:52:12.8706104Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8706238Z   warnings.warn(
2025-04-11T03:52:12.8706766Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8706847Z   warnings.warn(
2025-04-11T03:52:12.8707375Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8707461Z   warnings.warn(
2025-04-11T03:52:12.8707986Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8708067Z   warnings.warn(
2025-04-11T03:52:12.8708688Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8708776Z   warnings.warn(
2025-04-11T03:52:12.8708924Z ______________________________ test_pipeline_p2p _______________________________
2025-04-11T03:52:12.8708931Z 
2025-04-11T03:52:12.8709034Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8709636Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8709644Z 
2025-04-11T03:52:12.8709759Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8709844Z         try_count = 0
2025-04-11T03:52:12.8709947Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8710035Z             max_try, int
2025-04-11T03:52:12.8710189Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8710267Z     
2025-04-11T03:52:12.8710384Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8710460Z             try:
2025-04-11T03:52:12.8710552Z                 try_count += 1
2025-04-11T03:52:12.8710647Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8710737Z                 return ret
2025-04-11T03:52:12.8710836Z             except exception_type as e:
2025-04-11T03:52:12.8710940Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8711133Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8711322Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8711484Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8711640Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8711731Z                     continue
2025-04-11T03:52:12.8711809Z                 else:
2025-04-11T03:52:12.8712029Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8712113Z >                   raise e
2025-04-11T03:52:12.8712118Z 
2025-04-11T03:52:12.8712215Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8712336Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8712543Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8712635Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8712815Z tests/test_pipeline/test_p2p_communication.py:79: in test_pipeline_p2p
2025-04-11T03:52:12.8712908Z     spawn(run_dist, WORLD_SIZE)
2025-04-11T03:52:12.8713013Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8713116Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8713378Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8713621Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8713912Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8714005Z     while not context.join():
2025-04-11T03:52:12.8714123Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8714133Z 
2025-04-11T03:52:12.8714337Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ee25510>
2025-04-11T03:52:12.8714416Z timeout = None
2025-04-11T03:52:12.8714420Z 
2025-04-11T03:52:12.8714517Z     def join(self, timeout=None):
2025-04-11T03:52:12.8714648Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8714723Z     
2025-04-11T03:52:12.8714869Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8715019Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8715247Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8715344Z         of the first process exiting.
2025-04-11T03:52:12.8715420Z     
2025-04-11T03:52:12.8715569Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8715712Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8715786Z     
2025-04-11T03:52:12.8715862Z         Args:
2025-04-11T03:52:12.8716003Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8716078Z         """
2025-04-11T03:52:12.8716223Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8716318Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8716401Z             return True
2025-04-11T03:52:12.8716475Z     
2025-04-11T03:52:12.8716609Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8716738Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8716835Z             self.sentinels.keys(),
2025-04-11T03:52:12.8716922Z             timeout=timeout,
2025-04-11T03:52:12.8716997Z         )
2025-04-11T03:52:12.8717067Z     
2025-04-11T03:52:12.8717155Z         error_index = None
2025-04-11T03:52:12.8717240Z         for sentinel in ready:
2025-04-11T03:52:12.8717351Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8717452Z             process = self.processes[index]
2025-04-11T03:52:12.8717537Z             process.join()
2025-04-11T03:52:12.8717636Z             if process.exitcode != 0:
2025-04-11T03:52:12.8717725Z                 error_index = index
2025-04-11T03:52:12.8717807Z                 break
2025-04-11T03:52:12.8717935Z     
2025-04-11T03:52:12.8718032Z         # Return if there was no error.
2025-04-11T03:52:12.8718124Z         if error_index is None:
2025-04-11T03:52:12.8718261Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8718365Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8718438Z     
2025-04-11T03:52:12.8718584Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8718683Z         for process in self.processes:
2025-04-11T03:52:12.8718773Z             if process.is_alive():
2025-04-11T03:52:12.8718869Z                 process.terminate()
2025-04-11T03:52:12.8718955Z             process.join()
2025-04-11T03:52:12.8719094Z     
2025-04-11T03:52:12.8719238Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8719357Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8719469Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8719593Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8719682Z             if exitcode < 0:
2025-04-11T03:52:12.8719791Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8719961Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8720115Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8720212Z                     error_index=error_index,
2025-04-11T03:52:12.8720321Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8720410Z                     exit_code=exitcode,
2025-04-11T03:52:12.8720501Z                     signal_name=name,
2025-04-11T03:52:12.8720576Z                 )
2025-04-11T03:52:12.8720658Z             else:
2025-04-11T03:52:12.8720761Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8720928Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8721028Z                     error_index=error_index,
2025-04-11T03:52:12.8721129Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8721218Z                     exit_code=exitcode,
2025-04-11T03:52:12.8721293Z                 )
2025-04-11T03:52:12.8721365Z     
2025-04-11T03:52:12.8721504Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8721734Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8721829Z         msg += original_trace
2025-04-11T03:52:12.8722003Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8722168Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8722245Z E       
2025-04-11T03:52:12.8722371Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8722477Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8722781Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8722870Z E           fn(i, *args)
2025-04-11T03:52:12.8723128Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 73, in run_dist
2025-04-11T03:52:12.8723226Z E           check_p2p_communication()
2025-04-11T03:52:12.8723519Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 21, in check_p2p_communication
2025-04-11T03:52:12.8723683Z E           tensor = torch.ones(1, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8723796Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8724083Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8724226Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8724387Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8724392Z 
2025-04-11T03:52:12.8724759Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8724916Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8725081Z [04/11/25 03:47:34] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8725210Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8725318Z                              :75 launch                                         
2025-04-11T03:52:12.8725461Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8725585Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8725850Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8725992Z _________________________ test_pipeline_stage_manager __________________________
2025-04-11T03:52:12.8725998Z 
2025-04-11T03:52:12.8726095Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8726686Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8726746Z 
2025-04-11T03:52:12.8726857Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8726941Z         try_count = 0
2025-04-11T03:52:12.8727045Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8727132Z             max_try, int
2025-04-11T03:52:12.8727278Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8727353Z     
2025-04-11T03:52:12.8727467Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8727541Z             try:
2025-04-11T03:52:12.8727629Z                 try_count += 1
2025-04-11T03:52:12.8727722Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8727804Z                 return ret
2025-04-11T03:52:12.8727899Z             except exception_type as e:
2025-04-11T03:52:12.8728001Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8728190Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8728364Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8728518Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8728671Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8728756Z                     continue
2025-04-11T03:52:12.8728837Z                 else:
2025-04-11T03:52:12.8729060Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8729140Z >                   raise e
2025-04-11T03:52:12.8729144Z 
2025-04-11T03:52:12.8729241Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8729358Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8729493Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8729586Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8729772Z tests/test_pipeline/test_stage_manager.py:74: in test_pipeline_stage_manager
2025-04-11T03:52:12.8729859Z     spawn(run_dist, 4)
2025-04-11T03:52:12.8729962Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8730062Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8730323Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8730502Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8730787Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8730932Z     while not context.join():
2025-04-11T03:52:12.8731048Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8731052Z 
2025-04-11T03:52:12.8731249Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ecbf8e0>
2025-04-11T03:52:12.8731329Z timeout = None
2025-04-11T03:52:12.8731333Z 
2025-04-11T03:52:12.8731432Z     def join(self, timeout=None):
2025-04-11T03:52:12.8731586Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8731687Z     
2025-04-11T03:52:12.8731838Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8731987Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8732216Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8732311Z         of the first process exiting.
2025-04-11T03:52:12.8732388Z     
2025-04-11T03:52:12.8732537Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8732679Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8732752Z     
2025-04-11T03:52:12.8732829Z         Args:
2025-04-11T03:52:12.8733031Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8733107Z         """
2025-04-11T03:52:12.8733252Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8733346Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8733432Z             return True
2025-04-11T03:52:12.8733505Z     
2025-04-11T03:52:12.8733640Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8733763Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8733858Z             self.sentinels.keys(),
2025-04-11T03:52:12.8733946Z             timeout=timeout,
2025-04-11T03:52:12.8734021Z         )
2025-04-11T03:52:12.8734094Z     
2025-04-11T03:52:12.8734183Z         error_index = None
2025-04-11T03:52:12.8734269Z         for sentinel in ready:
2025-04-11T03:52:12.8734378Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8734477Z             process = self.processes[index]
2025-04-11T03:52:12.8734570Z             process.join()
2025-04-11T03:52:12.8734664Z             if process.exitcode != 0:
2025-04-11T03:52:12.8734806Z                 error_index = index
2025-04-11T03:52:12.8734891Z                 break
2025-04-11T03:52:12.8734961Z     
2025-04-11T03:52:12.8735059Z         # Return if there was no error.
2025-04-11T03:52:12.8735146Z         if error_index is None:
2025-04-11T03:52:12.8735279Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8735383Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8735453Z     
2025-04-11T03:52:12.8735595Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8735689Z         for process in self.processes:
2025-04-11T03:52:12.8735781Z             if process.is_alive():
2025-04-11T03:52:12.8735878Z                 process.terminate()
2025-04-11T03:52:12.8735961Z             process.join()
2025-04-11T03:52:12.8736034Z     
2025-04-11T03:52:12.8736175Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8736297Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8736406Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8736527Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8736616Z             if exitcode < 0:
2025-04-11T03:52:12.8736721Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8736832Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8736981Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8737077Z                     error_index=error_index,
2025-04-11T03:52:12.8737182Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8737322Z                     exit_code=exitcode,
2025-04-11T03:52:12.8737415Z                     signal_name=name,
2025-04-11T03:52:12.8737489Z                 )
2025-04-11T03:52:12.8737568Z             else:
2025-04-11T03:52:12.8737675Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8737841Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8737937Z                     error_index=error_index,
2025-04-11T03:52:12.8738038Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8738129Z                     exit_code=exitcode,
2025-04-11T03:52:12.8738202Z                 )
2025-04-11T03:52:12.8738273Z     
2025-04-11T03:52:12.8738474Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8738644Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8738736Z         msg += original_trace
2025-04-11T03:52:12.8738914Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8739076Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8739151Z E       
2025-04-11T03:52:12.8739281Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8739458Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8739759Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8739846Z E           fn(i, *args)
2025-04-11T03:52:12.8740087Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 68, in run_dist
2025-04-11T03:52:12.8740179Z E           check_stage_manager()
2025-04-11T03:52:12.8740452Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 56, in check_stage_manager
2025-04-11T03:52:12.8740546Z E           dist.barrier(group=group)
2025-04-11T03:52:12.8740853Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:12.8740947Z E           return func(*args, **kwargs)
2025-04-11T03:52:12.8741273Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3441, in barrier
2025-04-11T03:52:12.8741429Z E           work = group.barrier(opts=opts)
2025-04-11T03:52:12.8741544Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8741831Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8741972Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8742137Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8742142Z 
2025-04-11T03:52:12.8742443Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8742602Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8742759Z [04/11/25 03:47:39] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8742891Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8743003Z                              :75 launch                                         
2025-04-11T03:52:12.8743146Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8743270Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8743465Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8743604Z _______________________________ test_pp[2-12-4] ________________________________
2025-04-11T03:52:12.8743608Z 
2025-04-11T03:52:12.8743686Z args = ()
2025-04-11T03:52:12.8743848Z kwargs = {'batch_size': 12, 'num_microbatch': 4, 'num_model_chunk': 2}
2025-04-11T03:52:12.8743978Z try_count = 1
2025-04-11T03:52:12.8744585Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8744592Z 
2025-04-11T03:52:12.8744695Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8744780Z         try_count = 0
2025-04-11T03:52:12.8744882Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8744964Z             max_try, int
2025-04-11T03:52:12.8745119Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8745250Z     
2025-04-11T03:52:12.8745372Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8745446Z             try:
2025-04-11T03:52:12.8745532Z                 try_count += 1
2025-04-11T03:52:12.8745627Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8745712Z                 return ret
2025-04-11T03:52:12.8745811Z             except exception_type as e:
2025-04-11T03:52:12.8745910Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8746157Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8746278Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8746426Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8746585Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8746666Z                     continue
2025-04-11T03:52:12.8746748Z                 else:
2025-04-11T03:52:12.8746968Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8747051Z >                   raise e
2025-04-11T03:52:12.8747056Z 
2025-04-11T03:52:12.8747150Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8747262Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8747399Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8747488Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8747662Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T03:52:12.8747798Z     spawn(
2025-04-11T03:52:12.8747909Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8748007Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8748264Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8748477Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8748760Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8748850Z     while not context.join():
2025-04-11T03:52:12.8748960Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8748965Z 
2025-04-11T03:52:12.8749168Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f023e2f0>
2025-04-11T03:52:12.8749249Z timeout = None
2025-04-11T03:52:12.8749252Z 
2025-04-11T03:52:12.8749344Z     def join(self, timeout=None):
2025-04-11T03:52:12.8749473Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8749545Z     
2025-04-11T03:52:12.8749695Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8749839Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8750002Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8750098Z         of the first process exiting.
2025-04-11T03:52:12.8750170Z     
2025-04-11T03:52:12.8750320Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8750519Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8750597Z     
2025-04-11T03:52:12.8750672Z         Args:
2025-04-11T03:52:12.8750814Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8750890Z         """
2025-04-11T03:52:12.8751027Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8751126Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8751207Z             return True
2025-04-11T03:52:12.8751282Z     
2025-04-11T03:52:12.8751413Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8751530Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8751628Z             self.sentinels.keys(),
2025-04-11T03:52:12.8751777Z             timeout=timeout,
2025-04-11T03:52:12.8751855Z         )
2025-04-11T03:52:12.8751928Z     
2025-04-11T03:52:12.8752012Z         error_index = None
2025-04-11T03:52:12.8752101Z         for sentinel in ready:
2025-04-11T03:52:12.8752211Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8752314Z             process = self.processes[index]
2025-04-11T03:52:12.8752401Z             process.join()
2025-04-11T03:52:12.8752498Z             if process.exitcode != 0:
2025-04-11T03:52:12.8752652Z                 error_index = index
2025-04-11T03:52:12.8752732Z                 break
2025-04-11T03:52:12.8752811Z     
2025-04-11T03:52:12.8752903Z         # Return if there was no error.
2025-04-11T03:52:12.8752991Z         if error_index is None:
2025-04-11T03:52:12.8753125Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8753222Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8753296Z     
2025-04-11T03:52:12.8753438Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8753540Z         for process in self.processes:
2025-04-11T03:52:12.8753630Z             if process.is_alive():
2025-04-11T03:52:12.8753722Z                 process.terminate()
2025-04-11T03:52:12.8753813Z             process.join()
2025-04-11T03:52:12.8753885Z     
2025-04-11T03:52:12.8754029Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8754147Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8754258Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8754461Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8754547Z             if exitcode < 0:
2025-04-11T03:52:12.8754664Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8754771Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8754928Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8755024Z                     error_index=error_index,
2025-04-11T03:52:12.8755128Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8755218Z                     exit_code=exitcode,
2025-04-11T03:52:12.8755308Z                     signal_name=name,
2025-04-11T03:52:12.8755385Z                 )
2025-04-11T03:52:12.8755461Z             else:
2025-04-11T03:52:12.8755566Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8755729Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8755822Z                     error_index=error_index,
2025-04-11T03:52:12.8755927Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8756015Z                     exit_code=exitcode,
2025-04-11T03:52:12.8756090Z                 )
2025-04-11T03:52:12.8756160Z     
2025-04-11T03:52:12.8756291Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8756470Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8756558Z         msg += original_trace
2025-04-11T03:52:12.8756733Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8756949Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8757030Z E       
2025-04-11T03:52:12.8757158Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8757258Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8757564Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8757646Z E           fn(i, *args)
2025-04-11T03:52:12.8757926Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:12.8758024Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8758297Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8758479Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8758755Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8758847Z E           module._apply(fn)
2025-04-11T03:52:12.8759110Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8759257Z E           module._apply(fn)
2025-04-11T03:52:12.8759529Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8759628Z E           param_applied = fn(param)
2025-04-11T03:52:12.8759901Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8760024Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8760134Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8760424Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8760570Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8760732Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8760737Z 
2025-04-11T03:52:12.8761054Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8761260Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8761425Z [04/11/25 03:47:45] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8761554Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8761668Z                              :75 launch                                         
2025-04-11T03:52:12.8761805Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8761930Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8762132Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8762279Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8762582Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38231 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8762716Z _______________________________ test_pp[2-12-12] _______________________________
2025-04-11T03:52:12.8762720Z 
2025-04-11T03:52:12.8762802Z args = ()
2025-04-11T03:52:12.8762959Z kwargs = {'batch_size': 12, 'num_microbatch': 12, 'num_model_chunk': 2}
2025-04-11T03:52:12.8763038Z try_count = 1
2025-04-11T03:52:12.8763628Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8763635Z 
2025-04-11T03:52:12.8763794Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8763882Z         try_count = 0
2025-04-11T03:52:12.8763982Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8764069Z             max_try, int
2025-04-11T03:52:12.8764218Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8764289Z     
2025-04-11T03:52:12.8764405Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8764482Z             try:
2025-04-11T03:52:12.8764571Z                 try_count += 1
2025-04-11T03:52:12.8764661Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8764744Z                 return ret
2025-04-11T03:52:12.8764836Z             except exception_type as e:
2025-04-11T03:52:12.8765007Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8765202Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8765323Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8765473Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8765625Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8765767Z                     continue
2025-04-11T03:52:12.8765845Z                 else:
2025-04-11T03:52:12.8766066Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8766150Z >                   raise e
2025-04-11T03:52:12.8766154Z 
2025-04-11T03:52:12.8766250Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8766364Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8766500Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8766592Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8766765Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T03:52:12.8766841Z     spawn(
2025-04-11T03:52:12.8766947Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8767048Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8767307Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8767538Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8767832Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8767922Z     while not context.join():
2025-04-11T03:52:12.8768033Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8768036Z 
2025-04-11T03:52:12.8768244Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be334ac0>
2025-04-11T03:52:12.8768322Z timeout = None
2025-04-11T03:52:12.8768326Z 
2025-04-11T03:52:12.8768420Z     def join(self, timeout=None):
2025-04-11T03:52:12.8768550Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8768624Z     
2025-04-11T03:52:12.8768769Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8768915Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8769081Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8769175Z         of the first process exiting.
2025-04-11T03:52:12.8769247Z     
2025-04-11T03:52:12.8769395Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8769533Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8769603Z     
2025-04-11T03:52:12.8769681Z         Args:
2025-04-11T03:52:12.8769822Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8769896Z         """
2025-04-11T03:52:12.8770038Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8770182Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8770266Z             return True
2025-04-11T03:52:12.8770341Z     
2025-04-11T03:52:12.8770474Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8770601Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8770693Z             self.sentinels.keys(),
2025-04-11T03:52:12.8770778Z             timeout=timeout,
2025-04-11T03:52:12.8770857Z         )
2025-04-11T03:52:12.8770929Z     
2025-04-11T03:52:12.8771014Z         error_index = None
2025-04-11T03:52:12.8771100Z         for sentinel in ready:
2025-04-11T03:52:12.8771210Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8771310Z             process = self.processes[index]
2025-04-11T03:52:12.8771461Z             process.join()
2025-04-11T03:52:12.8771559Z             if process.exitcode != 0:
2025-04-11T03:52:12.8771648Z                 error_index = index
2025-04-11T03:52:12.8771730Z                 break
2025-04-11T03:52:12.8771800Z     
2025-04-11T03:52:12.8771894Z         # Return if there was no error.
2025-04-11T03:52:12.8771985Z         if error_index is None:
2025-04-11T03:52:12.8772119Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8772280Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8772351Z     
2025-04-11T03:52:12.8772495Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8772595Z         for process in self.processes:
2025-04-11T03:52:12.8772685Z             if process.is_alive():
2025-04-11T03:52:12.8772779Z                 process.terminate()
2025-04-11T03:52:12.8772863Z             process.join()
2025-04-11T03:52:12.8772937Z     
2025-04-11T03:52:12.8773079Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8773196Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8773305Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8773427Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8773516Z             if exitcode < 0:
2025-04-11T03:52:12.8773622Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8773728Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8773883Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8774031Z                     error_index=error_index,
2025-04-11T03:52:12.8774143Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8774232Z                     exit_code=exitcode,
2025-04-11T03:52:12.8774322Z                     signal_name=name,
2025-04-11T03:52:12.8774396Z                 )
2025-04-11T03:52:12.8774472Z             else:
2025-04-11T03:52:12.8774580Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8774739Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8774837Z                     error_index=error_index,
2025-04-11T03:52:12.8774937Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8775026Z                     exit_code=exitcode,
2025-04-11T03:52:12.8775099Z                 )
2025-04-11T03:52:12.8775170Z     
2025-04-11T03:52:12.8775305Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8775473Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8775563Z         msg += original_trace
2025-04-11T03:52:12.8775736Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8775893Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8775971Z E       
2025-04-11T03:52:12.8776099Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8776200Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8776553Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8776641Z E           fn(i, *args)
2025-04-11T03:52:12.8776911Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:12.8777011Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8777284Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8777403Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8777672Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8777759Z E           module._apply(fn)
2025-04-11T03:52:12.8778094Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8778177Z E           module._apply(fn)
2025-04-11T03:52:12.8778440Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8778538Z E           param_applied = fn(param)
2025-04-11T03:52:12.8778809Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8778988Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8779097Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8779385Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8779520Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8779686Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8779692Z 
2025-04-11T03:52:12.8779995Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8780147Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8780304Z [04/11/25 03:47:51] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8780433Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8780546Z                              :75 launch                                         
2025-04-11T03:52:12.8780737Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8780867Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8781064Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8781213Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8781513Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:20425 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8781651Z _______________________________ test_pp[4-12-4] ________________________________
2025-04-11T03:52:12.8781655Z 
2025-04-11T03:52:12.8781737Z args = ()
2025-04-11T03:52:12.8781891Z kwargs = {'batch_size': 12, 'num_microbatch': 4, 'num_model_chunk': 4}
2025-04-11T03:52:12.8781974Z try_count = 1
2025-04-11T03:52:12.8782580Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8782585Z 
2025-04-11T03:52:12.8782691Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8782771Z         try_count = 0
2025-04-11T03:52:12.8782872Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8782961Z             max_try, int
2025-04-11T03:52:12.8783106Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8783181Z     
2025-04-11T03:52:12.8783347Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8783433Z             try:
2025-04-11T03:52:12.8783518Z                 try_count += 1
2025-04-11T03:52:12.8783610Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8783698Z                 return ret
2025-04-11T03:52:12.8783792Z             except exception_type as e:
2025-04-11T03:52:12.8783897Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8784082Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8784199Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8784348Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8784568Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8784656Z                     continue
2025-04-11T03:52:12.8784735Z                 else:
2025-04-11T03:52:12.8784956Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8785040Z >                   raise e
2025-04-11T03:52:12.8785044Z 
2025-04-11T03:52:12.8785208Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8785319Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8785455Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8785545Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8785716Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T03:52:12.8785796Z     spawn(
2025-04-11T03:52:12.8785896Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8785997Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8786257Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8786435Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8786722Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8786811Z     while not context.join():
2025-04-11T03:52:12.8786924Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8786928Z 
2025-04-11T03:52:12.8787182Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f023dcf0>
2025-04-11T03:52:12.8787268Z timeout = None
2025-04-11T03:52:12.8787272Z 
2025-04-11T03:52:12.8787362Z     def join(self, timeout=None):
2025-04-11T03:52:12.8787488Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8787564Z     
2025-04-11T03:52:12.8787715Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8787866Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8788028Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8788126Z         of the first process exiting.
2025-04-11T03:52:12.8788199Z     
2025-04-11T03:52:12.8788345Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8788521Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8788593Z     
2025-04-11T03:52:12.8788669Z         Args:
2025-04-11T03:52:12.8788808Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8788881Z         """
2025-04-11T03:52:12.8789025Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8789116Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8789197Z             return True
2025-04-11T03:52:12.8789269Z     
2025-04-11T03:52:12.8789399Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8789520Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8789610Z             self.sentinels.keys(),
2025-04-11T03:52:12.8789778Z             timeout=timeout,
2025-04-11T03:52:12.8789855Z         )
2025-04-11T03:52:12.8789927Z     
2025-04-11T03:52:12.8790012Z         error_index = None
2025-04-11T03:52:12.8790099Z         for sentinel in ready:
2025-04-11T03:52:12.8790215Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8790318Z             process = self.processes[index]
2025-04-11T03:52:12.8790408Z             process.join()
2025-04-11T03:52:12.8790503Z             if process.exitcode != 0:
2025-04-11T03:52:12.8790588Z                 error_index = index
2025-04-11T03:52:12.8790668Z                 break
2025-04-11T03:52:12.8790739Z     
2025-04-11T03:52:12.8790835Z         # Return if there was no error.
2025-04-11T03:52:12.8790992Z         if error_index is None:
2025-04-11T03:52:12.8791127Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8791228Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8791300Z     
2025-04-11T03:52:12.8791443Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8791541Z         for process in self.processes:
2025-04-11T03:52:12.8791636Z             if process.is_alive():
2025-04-11T03:52:12.8791726Z                 process.terminate()
2025-04-11T03:52:12.8791873Z             process.join()
2025-04-11T03:52:12.8791949Z     
2025-04-11T03:52:12.8792090Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8792209Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8792318Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8792441Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8792528Z             if exitcode < 0:
2025-04-11T03:52:12.8792639Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8792748Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8792897Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8792997Z                     error_index=error_index,
2025-04-11T03:52:12.8793099Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8793186Z                     exit_code=exitcode,
2025-04-11T03:52:12.8793281Z                     signal_name=name,
2025-04-11T03:52:12.8793356Z                 )
2025-04-11T03:52:12.8793434Z             else:
2025-04-11T03:52:12.8793597Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8793762Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8793856Z                     error_index=error_index,
2025-04-11T03:52:12.8793956Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8794051Z                     exit_code=exitcode,
2025-04-11T03:52:12.8794125Z                 )
2025-04-11T03:52:12.8794198Z     
2025-04-11T03:52:12.8794330Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8794508Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8794599Z         msg += original_trace
2025-04-11T03:52:12.8794775Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8794941Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8795013Z E       
2025-04-11T03:52:12.8795146Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8795245Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8795541Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8795629Z E           fn(i, *args)
2025-04-11T03:52:12.8795901Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:12.8796003Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8796326Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8796448Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8796712Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8796802Z E           module._apply(fn)
2025-04-11T03:52:12.8797071Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8797155Z E           module._apply(fn)
2025-04-11T03:52:12.8797421Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8797514Z E           param_applied = fn(param)
2025-04-11T03:52:12.8797853Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8797970Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8798079Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8798363Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8798553Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8798723Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8798727Z 
2025-04-11T03:52:12.8799027Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8799181Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8799333Z [04/11/25 03:47:55] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8799470Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8799575Z                              :75 launch                                         
2025-04-11T03:52:12.8799710Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8799837Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8800035Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8800219Z _______________________________ test_pp[4-12-12] _______________________________
2025-04-11T03:52:12.8800224Z 
2025-04-11T03:52:12.8800304Z args = ()
2025-04-11T03:52:12.8800465Z kwargs = {'batch_size': 12, 'num_microbatch': 12, 'num_model_chunk': 4}
2025-04-11T03:52:12.8800544Z try_count = 1
2025-04-11T03:52:12.8801130Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8801139Z 
2025-04-11T03:52:12.8801242Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8801325Z         try_count = 0
2025-04-11T03:52:12.8801431Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8801512Z             max_try, int
2025-04-11T03:52:12.8801663Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8801734Z     
2025-04-11T03:52:12.8801849Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8801929Z             try:
2025-04-11T03:52:12.8802013Z                 try_count += 1
2025-04-11T03:52:12.8802105Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8802187Z                 return ret
2025-04-11T03:52:12.8802286Z             except exception_type as e:
2025-04-11T03:52:12.8802387Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8802573Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8802692Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8802897Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8803055Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8803138Z                     continue
2025-04-11T03:52:12.8803217Z                 else:
2025-04-11T03:52:12.8803436Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8803516Z >                   raise e
2025-04-11T03:52:12.8803520Z 
2025-04-11T03:52:12.8803621Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8803730Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8804014Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8804103Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8804279Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T03:52:12.8804355Z     spawn(
2025-04-11T03:52:12.8804455Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8804558Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8804815Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8805051Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8805335Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8805426Z     while not context.join():
2025-04-11T03:52:12.8805535Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8805539Z 
2025-04-11T03:52:12.8805735Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be3353c0>
2025-04-11T03:52:12.8805821Z timeout = None
2025-04-11T03:52:12.8805825Z 
2025-04-11T03:52:12.8805916Z     def join(self, timeout=None):
2025-04-11T03:52:12.8806042Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8806114Z     
2025-04-11T03:52:12.8806262Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8806405Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8806565Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8806716Z         of the first process exiting.
2025-04-11T03:52:12.8806792Z     
2025-04-11T03:52:12.8806941Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8807079Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8807151Z     
2025-04-11T03:52:12.8807231Z         Args:
2025-04-11T03:52:12.8807371Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8807451Z         """
2025-04-11T03:52:12.8807588Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8807688Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8807771Z             return True
2025-04-11T03:52:12.8807842Z     
2025-04-11T03:52:12.8807976Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8808095Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8808189Z             self.sentinels.keys(),
2025-04-11T03:52:12.8808275Z             timeout=timeout,
2025-04-11T03:52:12.8808346Z         )
2025-04-11T03:52:12.8808421Z     
2025-04-11T03:52:12.8808503Z         error_index = None
2025-04-11T03:52:12.8808589Z         for sentinel in ready:
2025-04-11T03:52:12.8808696Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8808798Z             process = self.processes[index]
2025-04-11T03:52:12.8808885Z             process.join()
2025-04-11T03:52:12.8808980Z             if process.exitcode != 0:
2025-04-11T03:52:12.8809070Z                 error_index = index
2025-04-11T03:52:12.8809146Z                 break
2025-04-11T03:52:12.8809220Z     
2025-04-11T03:52:12.8809362Z         # Return if there was no error.
2025-04-11T03:52:12.8809450Z         if error_index is None:
2025-04-11T03:52:12.8809587Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8809687Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8809761Z     
2025-04-11T03:52:12.8809902Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8809997Z         for process in self.processes:
2025-04-11T03:52:12.8810092Z             if process.is_alive():
2025-04-11T03:52:12.8810184Z                 process.terminate()
2025-04-11T03:52:12.8810270Z             process.join()
2025-04-11T03:52:12.8810341Z     
2025-04-11T03:52:12.8810484Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8810663Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8810771Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8810900Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8810983Z             if exitcode < 0:
2025-04-11T03:52:12.8811096Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8811204Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8811408Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8811507Z                     error_index=error_index,
2025-04-11T03:52:12.8811609Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8811700Z                     exit_code=exitcode,
2025-04-11T03:52:12.8811787Z                     signal_name=name,
2025-04-11T03:52:12.8811864Z                 )
2025-04-11T03:52:12.8811936Z             else:
2025-04-11T03:52:12.8812040Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8812211Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8812303Z                     error_index=error_index,
2025-04-11T03:52:12.8812409Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8812495Z                     exit_code=exitcode,
2025-04-11T03:52:12.8812566Z                 )
2025-04-11T03:52:12.8812642Z     
2025-04-11T03:52:12.8812775Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8813014Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8813105Z         msg += original_trace
2025-04-11T03:52:12.8813277Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8813434Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8813507Z E       
2025-04-11T03:52:12.8813643Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8813741Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8814038Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8814122Z E           fn(i, *args)
2025-04-11T03:52:12.8814392Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:12.8814491Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8814762Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8814883Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8815151Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8815242Z E           module._apply(fn)
2025-04-11T03:52:12.8815510Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8815599Z E           module._apply(fn)
2025-04-11T03:52:12.8815924Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8816021Z E           param_applied = fn(param)
2025-04-11T03:52:12.8816304Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8816421Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8816535Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8816822Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8816962Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8817127Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8817193Z 
2025-04-11T03:52:12.8817501Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8817654Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8817809Z [04/11/25 03:48:00] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8817941Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8818120Z                              :75 launch                                         
2025-04-11T03:52:12.8818263Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8818386Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8818583Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8818728Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8819029Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:47696 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8819162Z _______________________________ test_pp[2-12-4] ________________________________
2025-04-11T03:52:12.8819166Z 
2025-04-11T03:52:12.8819321Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 4, 'world_size': 2}
2025-04-11T03:52:12.8819404Z try_count = 1
2025-04-11T03:52:12.8820073Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8820078Z 
2025-04-11T03:52:12.8820187Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8820266Z         try_count = 0
2025-04-11T03:52:12.8820373Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8820457Z             max_try, int
2025-04-11T03:52:12.8820600Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8820681Z     
2025-04-11T03:52:12.8820793Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8820874Z             try:
2025-04-11T03:52:12.8820958Z                 try_count += 1
2025-04-11T03:52:12.8821050Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8821131Z                 return ret
2025-04-11T03:52:12.8821229Z             except exception_type as e:
2025-04-11T03:52:12.8821332Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8821516Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8821637Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8821778Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8821934Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8822018Z                     continue
2025-04-11T03:52:12.8822096Z                 else:
2025-04-11T03:52:12.8822374Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8822459Z >                   raise e
2025-04-11T03:52:12.8822463Z 
2025-04-11T03:52:12.8822562Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8822674Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8822807Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8822895Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8823062Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T03:52:12.8823142Z     spawn(
2025-04-11T03:52:12.8823246Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8823349Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8823672Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8823850Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8824141Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8824229Z     while not context.join():
2025-04-11T03:52:12.8824343Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8824404Z 
2025-04-11T03:52:12.8824610Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ee27fd0>
2025-04-11T03:52:12.8824694Z timeout = None
2025-04-11T03:52:12.8824699Z 
2025-04-11T03:52:12.8824792Z     def join(self, timeout=None):
2025-04-11T03:52:12.8824920Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8824993Z     
2025-04-11T03:52:12.8825139Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8825287Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8825449Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8825545Z         of the first process exiting.
2025-04-11T03:52:12.8825618Z     
2025-04-11T03:52:12.8825764Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8825905Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8825977Z     
2025-04-11T03:52:12.8826059Z         Args:
2025-04-11T03:52:12.8826254Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8826335Z         """
2025-04-11T03:52:12.8826480Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8826577Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8826666Z             return True
2025-04-11T03:52:12.8826741Z     
2025-04-11T03:52:12.8826880Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8827006Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8827102Z             self.sentinels.keys(),
2025-04-11T03:52:12.8827195Z             timeout=timeout,
2025-04-11T03:52:12.8827271Z         )
2025-04-11T03:52:12.8827351Z     
2025-04-11T03:52:12.8827441Z         error_index = None
2025-04-11T03:52:12.8827536Z         for sentinel in ready:
2025-04-11T03:52:12.8827647Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8827753Z             process = self.processes[index]
2025-04-11T03:52:12.8827848Z             process.join()
2025-04-11T03:52:12.8827948Z             if process.exitcode != 0:
2025-04-11T03:52:12.8828045Z                 error_index = index
2025-04-11T03:52:12.8828125Z                 break
2025-04-11T03:52:12.8828198Z     
2025-04-11T03:52:12.8828296Z         # Return if there was no error.
2025-04-11T03:52:12.8828385Z         if error_index is None:
2025-04-11T03:52:12.8828561Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8828658Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8828730Z     
2025-04-11T03:52:12.8828876Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8829041Z         for process in self.processes:
2025-04-11T03:52:12.8829137Z             if process.is_alive():
2025-04-11T03:52:12.8829231Z                 process.terminate()
2025-04-11T03:52:12.8829322Z             process.join()
2025-04-11T03:52:12.8829393Z     
2025-04-11T03:52:12.8829533Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8829656Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8829763Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8829889Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8829972Z             if exitcode < 0:
2025-04-11T03:52:12.8830079Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8830265Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8830418Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8830518Z                     error_index=error_index,
2025-04-11T03:52:12.8830621Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8830711Z                     exit_code=exitcode,
2025-04-11T03:52:12.8830796Z                     signal_name=name,
2025-04-11T03:52:12.8830933Z                 )
2025-04-11T03:52:12.8831013Z             else:
2025-04-11T03:52:12.8831117Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8831285Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8831377Z                     error_index=error_index,
2025-04-11T03:52:12.8831481Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8831567Z                     exit_code=exitcode,
2025-04-11T03:52:12.8831641Z                 )
2025-04-11T03:52:12.8831719Z     
2025-04-11T03:52:12.8831853Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8832028Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8832124Z         msg += original_trace
2025-04-11T03:52:12.8832352Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8832519Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8832595Z E       
2025-04-11T03:52:12.8832730Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8832891Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8833188Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8833269Z E           fn(i, *args)
2025-04-11T03:52:12.8833537Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:12.8833654Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:12.8833917Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:12.8834015Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8834284Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8834408Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8834676Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8834762Z E           module._apply(fn)
2025-04-11T03:52:12.8835029Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8835115Z E           module._apply(fn)
2025-04-11T03:52:12.8835379Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8835474Z E           param_applied = fn(param)
2025-04-11T03:52:12.8835801Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8835920Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8836030Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8836318Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8836454Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8836622Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8836626Z 
2025-04-11T03:52:12.8836925Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8837144Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8837299Z [04/11/25 03:48:06] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8837434Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8837542Z                              :75 launch                                         
2025-04-11T03:52:12.8837679Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8837878Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8838073Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8838220Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8838511Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:42298 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8838652Z _______________________________ test_pp[2-12-6] ________________________________
2025-04-11T03:52:12.8838656Z 
2025-04-11T03:52:12.8838814Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 6, 'world_size': 2}
2025-04-11T03:52:12.8838898Z try_count = 1
2025-04-11T03:52:12.8839490Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8839496Z 
2025-04-11T03:52:12.8839661Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8839746Z         try_count = 0
2025-04-11T03:52:12.8839844Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8839931Z             max_try, int
2025-04-11T03:52:12.8840092Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8840164Z     
2025-04-11T03:52:12.8840278Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8840353Z             try:
2025-04-11T03:52:12.8840439Z                 try_count += 1
2025-04-11T03:52:12.8840530Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8840614Z                 return ret
2025-04-11T03:52:12.8840710Z             except exception_type as e:
2025-04-11T03:52:12.8840812Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8840997Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8841118Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8841267Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8841422Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8841508Z                     continue
2025-04-11T03:52:12.8841587Z                 else:
2025-04-11T03:52:12.8841809Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8841889Z >                   raise e
2025-04-11T03:52:12.8841893Z 
2025-04-11T03:52:12.8841987Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8842156Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8842292Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8842386Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8842553Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T03:52:12.8842634Z     spawn(
2025-04-11T03:52:12.8842736Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8842839Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8843104Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8843281Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8843629Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8843719Z     while not context.join():
2025-04-11T03:52:12.8843828Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8843835Z 
2025-04-11T03:52:12.8844032Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be337550>
2025-04-11T03:52:12.8844112Z timeout = None
2025-04-11T03:52:12.8844172Z 
2025-04-11T03:52:12.8844276Z     def join(self, timeout=None):
2025-04-11T03:52:12.8844404Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8844476Z     
2025-04-11T03:52:12.8844622Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8844768Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8844928Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8845024Z         of the first process exiting.
2025-04-11T03:52:12.8845100Z     
2025-04-11T03:52:12.8845244Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8845383Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8845453Z     
2025-04-11T03:52:12.8845527Z         Args:
2025-04-11T03:52:12.8845670Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8845747Z         """
2025-04-11T03:52:12.8845887Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8846035Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8846122Z             return True
2025-04-11T03:52:12.8846192Z     
2025-04-11T03:52:12.8846326Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8846449Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8846542Z             self.sentinels.keys(),
2025-04-11T03:52:12.8846630Z             timeout=timeout,
2025-04-11T03:52:12.8846703Z         )
2025-04-11T03:52:12.8846773Z     
2025-04-11T03:52:12.8846860Z         error_index = None
2025-04-11T03:52:12.8846946Z         for sentinel in ready:
2025-04-11T03:52:12.8847056Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8847154Z             process = self.processes[index]
2025-04-11T03:52:12.8847238Z             process.join()
2025-04-11T03:52:12.8847339Z             if process.exitcode != 0:
2025-04-11T03:52:12.8847429Z                 error_index = index
2025-04-11T03:52:12.8847510Z                 break
2025-04-11T03:52:12.8847582Z     
2025-04-11T03:52:12.8847678Z         # Return if there was no error.
2025-04-11T03:52:12.8847764Z         if error_index is None:
2025-04-11T03:52:12.8847897Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8848001Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8848071Z     
2025-04-11T03:52:12.8848218Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8848314Z         for process in self.processes:
2025-04-11T03:52:12.8848401Z             if process.is_alive():
2025-04-11T03:52:12.8848496Z                 process.terminate()
2025-04-11T03:52:12.8848635Z             process.join()
2025-04-11T03:52:12.8848714Z     
2025-04-11T03:52:12.8848856Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8848975Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8849083Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8849207Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8849295Z             if exitcode < 0:
2025-04-11T03:52:12.8849399Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8849510Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8849657Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8849821Z                     error_index=error_index,
2025-04-11T03:52:12.8849926Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8850016Z                     exit_code=exitcode,
2025-04-11T03:52:12.8850110Z                     signal_name=name,
2025-04-11T03:52:12.8850187Z                 )
2025-04-11T03:52:12.8850268Z             else:
2025-04-11T03:52:12.8850373Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8850538Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8850692Z                     error_index=error_index,
2025-04-11T03:52:12.8850795Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8850885Z                     exit_code=exitcode,
2025-04-11T03:52:12.8850959Z                 )
2025-04-11T03:52:12.8851032Z     
2025-04-11T03:52:12.8851169Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8851340Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8851431Z         msg += original_trace
2025-04-11T03:52:12.8851600Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8851765Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8851838Z E       
2025-04-11T03:52:12.8851964Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8852068Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8852429Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8852517Z E           fn(i, *args)
2025-04-11T03:52:12.8852788Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:12.8852899Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:12.8853167Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:12.8853266Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8853539Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8853658Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8853931Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8854019Z E           module._apply(fn)
2025-04-11T03:52:12.8854286Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8854370Z E           module._apply(fn)
2025-04-11T03:52:12.8854630Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8854728Z E           param_applied = fn(param)
2025-04-11T03:52:12.8855001Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8855121Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8855229Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8855566Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8855706Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8855872Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8855877Z 
2025-04-11T03:52:12.8856181Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8856331Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8856489Z [04/11/25 03:48:10] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8856680Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8856790Z                              :75 launch                                         
2025-04-11T03:52:12.8856931Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8857061Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8857256Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8857446Z _______________________________ test_pp[4-12-4] ________________________________
2025-04-11T03:52:12.8857454Z 
2025-04-11T03:52:12.8857610Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 4, 'world_size': 4}
2025-04-11T03:52:12.8857687Z try_count = 1
2025-04-11T03:52:12.8858286Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8858292Z 
2025-04-11T03:52:12.8858394Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8858478Z         try_count = 0
2025-04-11T03:52:12.8858580Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8858664Z             max_try, int
2025-04-11T03:52:12.8858809Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8858882Z     
2025-04-11T03:52:12.8858997Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8859129Z             try:
2025-04-11T03:52:12.8859220Z                 try_count += 1
2025-04-11T03:52:12.8859311Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8859391Z                 return ret
2025-04-11T03:52:12.8859488Z             except exception_type as e:
2025-04-11T03:52:12.8859586Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8859777Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8859893Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8860039Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8860191Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8860275Z                     continue
2025-04-11T03:52:12.8860354Z                 else:
2025-04-11T03:52:12.8860572Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8860654Z >                   raise e
2025-04-11T03:52:12.8860658Z 
2025-04-11T03:52:12.8860753Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8860866Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8860998Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8861086Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8861252Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T03:52:12.8861328Z     spawn(
2025-04-11T03:52:12.8861431Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8861585Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8861847Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8862020Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8862304Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8862397Z     while not context.join():
2025-04-11T03:52:12.8862507Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8862510Z 
2025-04-11T03:52:12.8862709Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ee276d0>
2025-04-11T03:52:12.8862866Z timeout = None
2025-04-11T03:52:12.8862871Z 
2025-04-11T03:52:12.8862965Z     def join(self, timeout=None):
2025-04-11T03:52:12.8863094Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8863163Z     
2025-04-11T03:52:12.8863316Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8863460Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8863626Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8863778Z         of the first process exiting.
2025-04-11T03:52:12.8863855Z     
2025-04-11T03:52:12.8864003Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8864145Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8864220Z     
2025-04-11T03:52:12.8864294Z         Args:
2025-04-11T03:52:12.8864433Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8864510Z         """
2025-04-11T03:52:12.8864652Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8864744Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8864823Z             return True
2025-04-11T03:52:12.8864899Z     
2025-04-11T03:52:12.8865030Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8865152Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8865250Z             self.sentinels.keys(),
2025-04-11T03:52:12.8865334Z             timeout=timeout,
2025-04-11T03:52:12.8865412Z         )
2025-04-11T03:52:12.8865534Z     
2025-04-11T03:52:12.8865625Z         error_index = None
2025-04-11T03:52:12.8865713Z         for sentinel in ready:
2025-04-11T03:52:12.8865819Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8865923Z             process = self.processes[index]
2025-04-11T03:52:12.8866007Z             process.join()
2025-04-11T03:52:12.8866105Z             if process.exitcode != 0:
2025-04-11T03:52:12.8866194Z                 error_index = index
2025-04-11T03:52:12.8866275Z                 break
2025-04-11T03:52:12.8866346Z     
2025-04-11T03:52:12.8866437Z         # Return if there was no error.
2025-04-11T03:52:12.8866529Z         if error_index is None:
2025-04-11T03:52:12.8866661Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8866760Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8866832Z     
2025-04-11T03:52:12.8866972Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8867075Z         for process in self.processes:
2025-04-11T03:52:12.8867162Z             if process.is_alive():
2025-04-11T03:52:12.8867257Z                 process.terminate()
2025-04-11T03:52:12.8867341Z             process.join()
2025-04-11T03:52:12.8867412Z     
2025-04-11T03:52:12.8867556Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8867673Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8867782Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8867904Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8867990Z             if exitcode < 0:
2025-04-11T03:52:12.8868153Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8868260Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8868456Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8868556Z                     error_index=error_index,
2025-04-11T03:52:12.8868662Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8868751Z                     exit_code=exitcode,
2025-04-11T03:52:12.8868842Z                     signal_name=name,
2025-04-11T03:52:12.8868917Z                 )
2025-04-11T03:52:12.8868990Z             else:
2025-04-11T03:52:12.8869095Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8869333Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8869428Z                     error_index=error_index,
2025-04-11T03:52:12.8869527Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8869615Z                     exit_code=exitcode,
2025-04-11T03:52:12.8869692Z                 )
2025-04-11T03:52:12.8869761Z     
2025-04-11T03:52:12.8869894Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8870131Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8870223Z         msg += original_trace
2025-04-11T03:52:12.8870396Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8870554Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8870631Z E       
2025-04-11T03:52:12.8870756Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8870858Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8871148Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8871231Z E           fn(i, *args)
2025-04-11T03:52:12.8871503Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:12.8871611Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:12.8871882Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:12.8872032Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8872303Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8872420Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8872690Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8872779Z E           module._apply(fn)
2025-04-11T03:52:12.8873045Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8873138Z E           module._apply(fn)
2025-04-11T03:52:12.8873399Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8873498Z E           param_applied = fn(param)
2025-04-11T03:52:12.8873770Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8873888Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8873997Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8874282Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8874425Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8874587Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8874591Z 
2025-04-11T03:52:12.8874961Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8875113Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8875270Z [04/11/25 03:48:16] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8875400Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8875512Z                              :75 launch                                         
2025-04-11T03:52:12.8875647Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8875770Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8876028Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8876174Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8876470Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30189 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8876605Z _______________________________ test_pp[4-12-6] ________________________________
2025-04-11T03:52:12.8876670Z 
2025-04-11T03:52:12.8876830Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 6, 'world_size': 4}
2025-04-11T03:52:12.8876908Z try_count = 1
2025-04-11T03:52:12.8877507Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8877512Z 
2025-04-11T03:52:12.8877616Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8877698Z         try_count = 0
2025-04-11T03:52:12.8877803Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8877884Z             max_try, int
2025-04-11T03:52:12.8878034Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8878106Z     
2025-04-11T03:52:12.8878223Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8878298Z             try:
2025-04-11T03:52:12.8878385Z                 try_count += 1
2025-04-11T03:52:12.8878483Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8878563Z                 return ret
2025-04-11T03:52:12.8878720Z             except exception_type as e:
2025-04-11T03:52:12.8878821Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8879008Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8879129Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8879276Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8879431Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8879512Z                     continue
2025-04-11T03:52:12.8879595Z                 else:
2025-04-11T03:52:12.8879813Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8879895Z >                   raise e
2025-04-11T03:52:12.8879903Z 
2025-04-11T03:52:12.8879998Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8880108Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8880245Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8880331Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8880499Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T03:52:12.8880575Z     spawn(
2025-04-11T03:52:12.8880679Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8880781Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8881035Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8881265Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8881549Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8881643Z     while not context.join():
2025-04-11T03:52:12.8881751Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8881755Z 
2025-04-11T03:52:12.8881959Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be3341c0>
2025-04-11T03:52:12.8882038Z timeout = None
2025-04-11T03:52:12.8882041Z 
2025-04-11T03:52:12.8882132Z     def join(self, timeout=None):
2025-04-11T03:52:12.8882261Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8882411Z     
2025-04-11T03:52:12.8882564Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8882708Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8882873Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8882971Z         of the first process exiting.
2025-04-11T03:52:12.8883042Z     
2025-04-11T03:52:12.8883192Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8883388Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8883465Z     
2025-04-11T03:52:12.8883539Z         Args:
2025-04-11T03:52:12.8883678Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8883754Z         """
2025-04-11T03:52:12.8883892Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8883990Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8884072Z             return True
2025-04-11T03:52:12.8884141Z     
2025-04-11T03:52:12.8884277Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8884395Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8884495Z             self.sentinels.keys(),
2025-04-11T03:52:12.8884579Z             timeout=timeout,
2025-04-11T03:52:12.8884654Z         )
2025-04-11T03:52:12.8884723Z     
2025-04-11T03:52:12.8884807Z         error_index = None
2025-04-11T03:52:12.8884898Z         for sentinel in ready:
2025-04-11T03:52:12.8885055Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8885160Z             process = self.processes[index]
2025-04-11T03:52:12.8885245Z             process.join()
2025-04-11T03:52:12.8885339Z             if process.exitcode != 0:
2025-04-11T03:52:12.8885432Z                 error_index = index
2025-04-11T03:52:12.8885508Z                 break
2025-04-11T03:52:12.8885580Z     
2025-04-11T03:52:12.8885672Z         # Return if there was no error.
2025-04-11T03:52:12.8885756Z         if error_index is None:
2025-04-11T03:52:12.8885895Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8885990Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8886067Z     
2025-04-11T03:52:12.8886207Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8886308Z         for process in self.processes:
2025-04-11T03:52:12.8886397Z             if process.is_alive():
2025-04-11T03:52:12.8886488Z                 process.terminate()
2025-04-11T03:52:12.8886579Z             process.join()
2025-04-11T03:52:12.8886649Z     
2025-04-11T03:52:12.8886793Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8886907Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8887012Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8887138Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8887222Z             if exitcode < 0:
2025-04-11T03:52:12.8887333Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8887438Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8887661Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8887762Z                     error_index=error_index,
2025-04-11T03:52:12.8887866Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8887962Z                     exit_code=exitcode,
2025-04-11T03:52:12.8888050Z                     signal_name=name,
2025-04-11T03:52:12.8888129Z                 )
2025-04-11T03:52:12.8888206Z             else:
2025-04-11T03:52:12.8888310Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8888476Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8888568Z                     error_index=error_index,
2025-04-11T03:52:12.8888739Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8888825Z                     exit_code=exitcode,
2025-04-11T03:52:12.8888902Z                 )
2025-04-11T03:52:12.8888972Z     
2025-04-11T03:52:12.8889105Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8889277Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8889366Z         msg += original_trace
2025-04-11T03:52:12.8889595Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8889763Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8889841Z E       
2025-04-11T03:52:12.8889968Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8890066Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8890365Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8890454Z E           fn(i, *args)
2025-04-11T03:52:12.8890727Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:12.8890834Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:12.8891104Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:12.8891200Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8891470Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8891646Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8891917Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8892007Z E           module._apply(fn)
2025-04-11T03:52:12.8892270Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8892361Z E           module._apply(fn)
2025-04-11T03:52:12.8892624Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8892719Z E           param_applied = fn(param)
2025-04-11T03:52:12.8892996Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8893112Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8893224Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8893514Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8893653Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8893815Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8893821Z 
2025-04-11T03:52:12.8894136Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8894286Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8894491Z [04/11/25 03:48:21] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8894628Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8894738Z                              :75 launch                                         
2025-04-11T03:52:12.8894882Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8895009Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8895210Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8895357Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8895712Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8896002Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8896134Z ___________________________________ test_pp ____________________________________
2025-04-11T03:52:12.8896137Z 
2025-04-11T03:52:12.8896289Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8896888Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8896892Z 
2025-04-11T03:52:12.8896997Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8897077Z         try_count = 0
2025-04-11T03:52:12.8897179Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8897262Z             max_try, int
2025-04-11T03:52:12.8897408Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8897483Z     
2025-04-11T03:52:12.8897595Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8897676Z             try:
2025-04-11T03:52:12.8897758Z                 try_count += 1
2025-04-11T03:52:12.8897853Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8897935Z                 return ret
2025-04-11T03:52:12.8898028Z             except exception_type as e:
2025-04-11T03:52:12.8898184Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8898373Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8898491Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8898637Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8898796Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8898877Z                     continue
2025-04-11T03:52:12.8898953Z                 else:
2025-04-11T03:52:12.8899175Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8899258Z >                   raise e
2025-04-11T03:52:12.8899262Z 
2025-04-11T03:52:12.8899359Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8899471Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8899608Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8899695Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8899871Z tests/test_pipeline/test_schedule/test_zerobubble_pp.py:1077: in test_pp
2025-04-11T03:52:12.8899953Z     spawn(
2025-04-11T03:52:12.8900054Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8900157Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8900418Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8900591Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8900937Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8901030Z     while not context.join():
2025-04-11T03:52:12.8901139Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8901145Z 
2025-04-11T03:52:12.8901344Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be335f00>
2025-04-11T03:52:12.8901426Z timeout = None
2025-04-11T03:52:12.8901430Z 
2025-04-11T03:52:12.8901520Z     def join(self, timeout=None):
2025-04-11T03:52:12.8901648Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8901719Z     
2025-04-11T03:52:12.8901865Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8902074Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8902235Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8902333Z         of the first process exiting.
2025-04-11T03:52:12.8902404Z     
2025-04-11T03:52:12.8902549Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8902689Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8902815Z     
2025-04-11T03:52:12.8902895Z         Args:
2025-04-11T03:52:12.8903034Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8903112Z         """
2025-04-11T03:52:12.8903249Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8903342Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8903429Z             return True
2025-04-11T03:52:12.8903501Z     
2025-04-11T03:52:12.8903637Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8903754Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8903847Z             self.sentinels.keys(),
2025-04-11T03:52:12.8903938Z             timeout=timeout,
2025-04-11T03:52:12.8904012Z         )
2025-04-11T03:52:12.8904088Z     
2025-04-11T03:52:12.8904170Z         error_index = None
2025-04-11T03:52:12.8904255Z         for sentinel in ready:
2025-04-11T03:52:12.8904368Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8904466Z             process = self.processes[index]
2025-04-11T03:52:12.8904609Z             process.join()
2025-04-11T03:52:12.8904706Z             if process.exitcode != 0:
2025-04-11T03:52:12.8904797Z                 error_index = index
2025-04-11T03:52:12.8904875Z                 break
2025-04-11T03:52:12.8904946Z     
2025-04-11T03:52:12.8905043Z         # Return if there was no error.
2025-04-11T03:52:12.8905130Z         if error_index is None:
2025-04-11T03:52:12.8905269Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8905364Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8905437Z     
2025-04-11T03:52:12.8905581Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8905677Z         for process in self.processes:
2025-04-11T03:52:12.8905771Z             if process.is_alive():
2025-04-11T03:52:12.8905862Z                 process.terminate()
2025-04-11T03:52:12.8905952Z             process.join()
2025-04-11T03:52:12.8906022Z     
2025-04-11T03:52:12.8906162Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8906282Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8906388Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8906514Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8906596Z             if exitcode < 0:
2025-04-11T03:52:12.8906703Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8906810Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8906959Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8907202Z                     error_index=error_index,
2025-04-11T03:52:12.8907305Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8907395Z                     exit_code=exitcode,
2025-04-11T03:52:12.8907486Z                     signal_name=name,
2025-04-11T03:52:12.8907560Z                 )
2025-04-11T03:52:12.8907637Z             else:
2025-04-11T03:52:12.8907740Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8907905Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8907998Z                     error_index=error_index,
2025-04-11T03:52:12.8908098Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8908188Z                     exit_code=exitcode,
2025-04-11T03:52:12.8908326Z                 )
2025-04-11T03:52:12.8908404Z     
2025-04-11T03:52:12.8908582Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8908756Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8908841Z         msg += original_trace
2025-04-11T03:52:12.8909015Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8909245Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8909318Z E       
2025-04-11T03:52:12.8909450Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8909549Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8909844Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8909923Z E           fn(i, *args)
2025-04-11T03:52:12.8910210Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 1070, in run_dist
2025-04-11T03:52:12.8910323Z E           run_with_booster_moehybridplugin()
2025-04-11T03:52:12.8910580Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8910676Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8911013Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 788, in run_with_booster_moehybridplugin
2025-04-11T03:52:12.8911147Z E           torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T03:52:12.8911506Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.8911613Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.8911879Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8911998Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8912271Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8912357Z E           module._apply(fn)
2025-04-11T03:52:12.8912628Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8912722Z E           param_applied = fn(param)
2025-04-11T03:52:12.8913004Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8913121Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8913226Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8913520Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8913658Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8913827Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8913831Z 
2025-04-11T03:52:12.8914192Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8914352Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8914506Z [04/11/25 03:48:27] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8914635Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8914747Z                              :75 launch                                         
2025-04-11T03:52:12.8914882Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8915012Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8915208Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8915422Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8916532Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.8917675Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.8918753Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.8919872Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.8920020Z _____________________________ test_flash_attn_func _____________________________
2025-04-11T03:52:12.8920024Z 
2025-04-11T03:52:12.8920109Z args = (), kwargs = {}
2025-04-11T03:52:12.8920113Z 
2025-04-11T03:52:12.8920212Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.8920310Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.8920420Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.8920541Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.8920648Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.8920743Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.8920747Z 
2025-04-11T03:52:12.8920844Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.8920956Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8921117Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.8921219Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.8921332Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8921336Z 
2025-04-11T03:52:12.8921413Z device = None
2025-04-11T03:52:12.8921417Z 
2025-04-11T03:52:12.8921601Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8921764Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8921841Z     
2025-04-11T03:52:12.8921916Z         Args:
2025-04-11T03:52:12.8922090Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8922264Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8922376Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8922455Z         """
2025-04-11T03:52:12.8922536Z         _lazy_init()
2025-04-11T03:52:12.8922632Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8922805Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8922911Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8923208Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8923346Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8923508Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8923568Z 
2025-04-11T03:52:12.8923817Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8923957Z ______________________________ test_release_layer ______________________________
2025-04-11T03:52:12.8923961Z 
2025-04-11T03:52:12.8924047Z     def test_release_layer():
2025-04-11T03:52:12.8924174Z         orig_cuda_allocated = torch.cuda.memory_allocated()
2025-04-11T03:52:12.8924266Z >       model = Net().cuda()
2025-04-11T03:52:12.8924272Z 
2025-04-11T03:52:12.8924391Z tests/test_shardformer/test_shard_utils.py:16: 
2025-04-11T03:52:12.8924502Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8924736Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: in cuda
2025-04-11T03:52:12.8924851Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8925087Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8925172Z     module._apply(fn)
2025-04-11T03:52:12.8925465Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8925551Z     module._apply(fn)
2025-04-11T03:52:12.8925790Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8925882Z     param_applied = fn(param)
2025-04-11T03:52:12.8925993Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8925999Z 
2025-04-11T03:52:12.8926089Z t = Parameter containing:
2025-04-11T03:52:12.8926171Z tensor([[-0.8151],
2025-04-11T03:52:12.8926262Z         [ 0.1839]], requires_grad=True)
2025-04-11T03:52:12.8926266Z 
2025-04-11T03:52:12.8926375Z >   return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8926481Z E   RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8926769Z E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8926913Z E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8927072Z E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8927076Z 
2025-04-11T03:52:12.8927327Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: RuntimeError
2025-04-11T03:52:12.8927460Z __________________________________ test_gpt2 ___________________________________
2025-04-11T03:52:12.8927466Z 
2025-04-11T03:52:12.8927559Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8927563Z 
2025-04-11T03:52:12.8927667Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8927745Z         try_count = 0
2025-04-11T03:52:12.8927905Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8927991Z             max_try, int
2025-04-11T03:52:12.8928141Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8928214Z     
2025-04-11T03:52:12.8928324Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8928405Z             try:
2025-04-11T03:52:12.8928491Z                 try_count += 1
2025-04-11T03:52:12.8928585Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.8928589Z 
2025-04-11T03:52:12.8928685Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.8928794Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8928973Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.8929066Z     get_accelerator().synchronize()
2025-04-11T03:52:12.8929229Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.8929326Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.8929571Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
2025-04-11T03:52:12.8929667Z     with torch.cuda.device(device):
2025-04-11T03:52:12.8929774Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8929836Z 
2025-04-11T03:52:12.8929966Z self = <torch.cuda.device object at 0x7f68be35f2e0>
2025-04-11T03:52:12.8929970Z 
2025-04-11T03:52:12.8930053Z     def __enter__(self):
2025-04-11T03:52:12.8930189Z >       self.prev_idx = torch.cuda._exchange_device(self.idx)
2025-04-11T03:52:12.8930294Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8930584Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8930722Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8930882Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8930886Z 
2025-04-11T03:52:12.8931121Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
2025-04-11T03:52:12.8931256Z _____________________________ test_grad_clip_norm ______________________________
2025-04-11T03:52:12.8931262Z 
2025-04-11T03:52:12.8931358Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8931362Z 
2025-04-11T03:52:12.8931519Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8931607Z         try_count = 0
2025-04-11T03:52:12.8931708Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8931790Z             max_try, int
2025-04-11T03:52:12.8931933Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8932007Z     
2025-04-11T03:52:12.8932124Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8932199Z             try:
2025-04-11T03:52:12.8932284Z                 try_count += 1
2025-04-11T03:52:12.8932375Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.8932380Z 
2025-04-11T03:52:12.8932479Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.8932586Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8932701Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.8932828Z     get_accelerator().synchronize()
2025-04-11T03:52:12.8933001Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.8933101Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.8933207Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8933211Z 
2025-04-11T03:52:12.8933292Z device = None
2025-04-11T03:52:12.8933295Z 
2025-04-11T03:52:12.8933422Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8933575Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8933651Z     
2025-04-11T03:52:12.8933725Z         Args:
2025-04-11T03:52:12.8933958Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8934130Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8934244Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8934319Z         """
2025-04-11T03:52:12.8934399Z         _lazy_init()
2025-04-11T03:52:12.8934500Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8934603Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8934715Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8935004Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8935204Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8935364Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8935369Z 
2025-04-11T03:52:12.8935607Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8935743Z _____________________________ test_grad_clip_norm ______________________________
2025-04-11T03:52:12.8935815Z 
2025-04-11T03:52:12.8935908Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8935912Z 
2025-04-11T03:52:12.8936018Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8936097Z         try_count = 0
2025-04-11T03:52:12.8936197Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8936277Z             max_try, int
2025-04-11T03:52:12.8936419Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8936494Z     
2025-04-11T03:52:12.8936606Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8936686Z             try:
2025-04-11T03:52:12.8936769Z                 try_count += 1
2025-04-11T03:52:12.8936860Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.8936869Z 
2025-04-11T03:52:12.8936963Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.8937070Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8937186Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.8937280Z     get_accelerator().synchronize()
2025-04-11T03:52:12.8937491Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.8937588Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.8937694Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8937702Z 
2025-04-11T03:52:12.8937780Z device = None
2025-04-11T03:52:12.8937784Z 
2025-04-11T03:52:12.8937903Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8938058Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8938130Z     
2025-04-11T03:52:12.8938209Z         Args:
2025-04-11T03:52:12.8938373Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8938539Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8938650Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8938726Z         """
2025-04-11T03:52:12.8938809Z         _lazy_init()
2025-04-11T03:52:12.8938904Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8939009Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8939113Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8939394Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8939536Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8939692Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8939696Z 
2025-04-11T03:52:12.8939988Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8940124Z _____________________________ test_grad_clip_norm ______________________________
2025-04-11T03:52:12.8940128Z 
2025-04-11T03:52:12.8940222Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8940228Z 
2025-04-11T03:52:12.8940328Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8940411Z         try_count = 0
2025-04-11T03:52:12.8940510Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8940592Z             max_try, int
2025-04-11T03:52:12.8940740Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8940810Z     
2025-04-11T03:52:12.8940924Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8941061Z             try:
2025-04-11T03:52:12.8941146Z                 try_count += 1
2025-04-11T03:52:12.8941241Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.8941246Z 
2025-04-11T03:52:12.8941340Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.8941455Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8941568Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.8941712Z     get_accelerator().synchronize()
2025-04-11T03:52:12.8941867Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.8941966Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.8942077Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8942081Z 
2025-04-11T03:52:12.8942162Z device = None
2025-04-11T03:52:12.8942166Z 
2025-04-11T03:52:12.8942289Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8942441Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8942514Z     
2025-04-11T03:52:12.8942587Z         Args:
2025-04-11T03:52:12.8942757Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8942934Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8943043Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8943121Z         """
2025-04-11T03:52:12.8943202Z         _lazy_init()
2025-04-11T03:52:12.8943301Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8943460Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8943568Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8943854Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8943989Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8944154Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8944158Z 
2025-04-11T03:52:12.8944396Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8944540Z ____________________________ test_dist_crossentropy ____________________________
2025-04-11T03:52:12.8944544Z 
2025-04-11T03:52:12.8944634Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8945252Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8945257Z 
2025-04-11T03:52:12.8945359Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8945437Z         try_count = 0
2025-04-11T03:52:12.8945541Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8945622Z             max_try, int
2025-04-11T03:52:12.8945768Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8945839Z     
2025-04-11T03:52:12.8945954Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8946030Z             try:
2025-04-11T03:52:12.8946181Z                 try_count += 1
2025-04-11T03:52:12.8946281Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8946363Z                 return ret
2025-04-11T03:52:12.8946464Z             except exception_type as e:
2025-04-11T03:52:12.8946562Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8946750Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8946872Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8947015Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8947172Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8947312Z                     continue
2025-04-11T03:52:12.8947392Z                 else:
2025-04-11T03:52:12.8947619Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8947700Z >                   raise e
2025-04-11T03:52:12.8947708Z 
2025-04-11T03:52:12.8947803Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8947915Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8948108Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8948198Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8948463Z tests/test_shardformer/test_layer/test_dist_crossentropy.py:51: in test_dist_crossentropy
2025-04-11T03:52:12.8948613Z     spawn(check_dist_crossentropy, 2, ignore_index=ignore_index)
2025-04-11T03:52:12.8948712Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8948815Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8949065Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8949244Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8949529Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8949622Z     while not context.join():
2025-04-11T03:52:12.8949731Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8949735Z 
2025-04-11T03:52:12.8950001Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ecd9360>
2025-04-11T03:52:12.8950086Z timeout = None
2025-04-11T03:52:12.8950090Z 
2025-04-11T03:52:12.8950180Z     def join(self, timeout=None):
2025-04-11T03:52:12.8950311Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8950382Z     
2025-04-11T03:52:12.8950533Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8950678Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8950844Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8950940Z         of the first process exiting.
2025-04-11T03:52:12.8951010Z     
2025-04-11T03:52:12.8951160Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8951301Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8951375Z     
2025-04-11T03:52:12.8951449Z         Args:
2025-04-11T03:52:12.8951589Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8951665Z         """
2025-04-11T03:52:12.8951803Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8951897Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8951977Z             return True
2025-04-11T03:52:12.8952048Z     
2025-04-11T03:52:12.8952182Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8952303Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8952397Z             self.sentinels.keys(),
2025-04-11T03:52:12.8952539Z             timeout=timeout,
2025-04-11T03:52:12.8952620Z         )
2025-04-11T03:52:12.8952692Z     
2025-04-11T03:52:12.8952776Z         error_index = None
2025-04-11T03:52:12.8952864Z         for sentinel in ready:
2025-04-11T03:52:12.8952971Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8953074Z             process = self.processes[index]
2025-04-11T03:52:12.8953165Z             process.join()
2025-04-11T03:52:12.8953260Z             if process.exitcode != 0:
2025-04-11T03:52:12.8953351Z                 error_index = index
2025-04-11T03:52:12.8953428Z                 break
2025-04-11T03:52:12.8953502Z     
2025-04-11T03:52:12.8953594Z         # Return if there was no error.
2025-04-11T03:52:12.8953747Z         if error_index is None:
2025-04-11T03:52:12.8953887Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8953982Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8954055Z     
2025-04-11T03:52:12.8954196Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8954297Z         for process in self.processes:
2025-04-11T03:52:12.8954387Z             if process.is_alive():
2025-04-11T03:52:12.8954477Z                 process.terminate()
2025-04-11T03:52:12.8954627Z             process.join()
2025-04-11T03:52:12.8954696Z     
2025-04-11T03:52:12.8954840Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8954956Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8955068Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8955195Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8955278Z             if exitcode < 0:
2025-04-11T03:52:12.8955391Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8955497Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8955652Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8955750Z                     error_index=error_index,
2025-04-11T03:52:12.8955851Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8955943Z                     exit_code=exitcode,
2025-04-11T03:52:12.8956031Z                     signal_name=name,
2025-04-11T03:52:12.8956108Z                 )
2025-04-11T03:52:12.8956182Z             else:
2025-04-11T03:52:12.8956338Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8956507Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8956602Z                     error_index=error_index,
2025-04-11T03:52:12.8956708Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8956797Z                     exit_code=exitcode,
2025-04-11T03:52:12.8956872Z                 )
2025-04-11T03:52:12.8956942Z     
2025-04-11T03:52:12.8957073Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8957252Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8957338Z         msg += original_trace
2025-04-11T03:52:12.8957513Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8957677Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8957754Z E       
2025-04-11T03:52:12.8957880Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8957977Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8958279Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8958357Z E           fn(i, *args)
2025-04-11T03:52:12.8958695Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dist_crossentropy.py", line 20, in check_dist_crossentropy
2025-04-11T03:52:12.8958829Z E           pred = torch.randn(2, 4, 8, requires_grad=True).cuda()
2025-04-11T03:52:12.8958996Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8959280Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8959415Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8959579Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8959585Z 
2025-04-11T03:52:12.8959890Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8960042Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8960194Z [04/11/25 03:48:34] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8960409Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8960517Z                              :75 launch                                         
2025-04-11T03:52:12.8960657Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8960782Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8960975Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8961177Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8961473Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26698 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8961607Z _________________________________ test_dropout _________________________________
2025-04-11T03:52:12.8961612Z 
2025-04-11T03:52:12.8961707Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8962306Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8962311Z 
2025-04-11T03:52:12.8962411Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8962498Z         try_count = 0
2025-04-11T03:52:12.8962599Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8962680Z             max_try, int
2025-04-11T03:52:12.8962880Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8962953Z     
2025-04-11T03:52:12.8963070Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8963146Z             try:
2025-04-11T03:52:12.8963230Z                 try_count += 1
2025-04-11T03:52:12.8963325Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8963406Z                 return ret
2025-04-11T03:52:12.8963505Z             except exception_type as e:
2025-04-11T03:52:12.8963605Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8963797Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8963913Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8964061Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8964224Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8964309Z                     continue
2025-04-11T03:52:12.8964390Z                 else:
2025-04-11T03:52:12.8964609Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8964691Z >                   raise e
2025-04-11T03:52:12.8964695Z 
2025-04-11T03:52:12.8964791Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8964903Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8965039Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8965125Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8965354Z tests/test_shardformer/test_layer/test_dropout.py:66: in test_dropout
2025-04-11T03:52:12.8965450Z     spawn(run_dist, nprocs=2)
2025-04-11T03:52:12.8965556Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8965656Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8965911Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8966091Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8966381Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8966473Z     while not context.join():
2025-04-11T03:52:12.8966642Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8966646Z 
2025-04-11T03:52:12.8966853Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be35eb00>
2025-04-11T03:52:12.8966932Z timeout = None
2025-04-11T03:52:12.8966936Z 
2025-04-11T03:52:12.8967028Z     def join(self, timeout=None):
2025-04-11T03:52:12.8967157Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8967230Z     
2025-04-11T03:52:12.8967438Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8967585Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8967752Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8967846Z         of the first process exiting.
2025-04-11T03:52:12.8967919Z     
2025-04-11T03:52:12.8968067Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8968201Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8968276Z     
2025-04-11T03:52:12.8968350Z         Args:
2025-04-11T03:52:12.8968489Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8968562Z         """
2025-04-11T03:52:12.8968701Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8968799Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8968880Z             return True
2025-04-11T03:52:12.8968954Z     
2025-04-11T03:52:12.8969086Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8969259Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8969357Z             self.sentinels.keys(),
2025-04-11T03:52:12.8969443Z             timeout=timeout,
2025-04-11T03:52:12.8969518Z         )
2025-04-11T03:52:12.8969588Z     
2025-04-11T03:52:12.8969670Z         error_index = None
2025-04-11T03:52:12.8969760Z         for sentinel in ready:
2025-04-11T03:52:12.8969868Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8969969Z             process = self.processes[index]
2025-04-11T03:52:12.8970054Z             process.join()
2025-04-11T03:52:12.8970151Z             if process.exitcode != 0:
2025-04-11T03:52:12.8970241Z                 error_index = index
2025-04-11T03:52:12.8970317Z                 break
2025-04-11T03:52:12.8970390Z     
2025-04-11T03:52:12.8970483Z         # Return if there was no error.
2025-04-11T03:52:12.8970575Z         if error_index is None:
2025-04-11T03:52:12.8970709Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8970807Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8970880Z     
2025-04-11T03:52:12.8971021Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8971121Z         for process in self.processes:
2025-04-11T03:52:12.8971208Z             if process.is_alive():
2025-04-11T03:52:12.8971298Z                 process.terminate()
2025-04-11T03:52:12.8971389Z             process.join()
2025-04-11T03:52:12.8971459Z     
2025-04-11T03:52:12.8971603Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8971718Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8971883Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8972009Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8972093Z             if exitcode < 0:
2025-04-11T03:52:12.8972206Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8972311Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8972464Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8972559Z                     error_index=error_index,
2025-04-11T03:52:12.8972661Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8972749Z                     exit_code=exitcode,
2025-04-11T03:52:12.8972895Z                     signal_name=name,
2025-04-11T03:52:12.8972972Z                 )
2025-04-11T03:52:12.8973047Z             else:
2025-04-11T03:52:12.8973152Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8973317Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8973409Z                     error_index=error_index,
2025-04-11T03:52:12.8973512Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8973657Z                     exit_code=exitcode,
2025-04-11T03:52:12.8973733Z                 )
2025-04-11T03:52:12.8973806Z     
2025-04-11T03:52:12.8973944Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8974115Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8974200Z         msg += original_trace
2025-04-11T03:52:12.8974377Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8974539Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8974618Z E       
2025-04-11T03:52:12.8974743Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8974841Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8975147Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8975227Z E           fn(i, *args)
2025-04-11T03:52:12.8975498Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 60, in run_dist
2025-04-11T03:52:12.8975654Z E           check_dropout_parallel_input()
2025-04-11T03:52:12.8975972Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 12, in check_dropout_parallel_input
2025-04-11T03:52:12.8976190Z E           dropout_1d = DropoutForParallelInput.from_native_module(dropout, process_group=None)
2025-04-11T03:52:12.8976467Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 42, in from_native_module
2025-04-11T03:52:12.8976684Z E           return DropoutForParallelInput(p=p, inplace=inplace, process_group=process_group)
2025-04-11T03:52:12.8976927Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 31, in __init__
2025-04-11T03:52:12.8977132Z E           self.randomizer = create_randomizer_with_offset(seed, process_group=process_group)
2025-04-11T03:52:12.8977432Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 318, in create_randomizer_with_offset
2025-04-11T03:52:12.8977621Z E           is_synchronized = Randomizer.is_randomizer_index_synchronized(process_group)
2025-04-11T03:52:12.8977924Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 258, in is_randomizer_index_synchronized
2025-04-11T03:52:12.8978160Z E           index_tensor = torch.tensor(index, dtype=torch.int32, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8978268Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8978555Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8978745Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8978908Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8978912Z 
2025-04-11T03:52:12.8979225Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8979380Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8979542Z [04/11/25 03:48:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8979670Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8979781Z                              :75 launch                                         
2025-04-11T03:52:12.8979975Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8980101Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8980295Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8980427Z ______________________________ test_embedding_1d _______________________________
2025-04-11T03:52:12.8980475Z 
2025-04-11T03:52:12.8980574Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8981167Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8981172Z 
2025-04-11T03:52:12.8981277Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8981357Z         try_count = 0
2025-04-11T03:52:12.8981461Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8981541Z             max_try, int
2025-04-11T03:52:12.8981686Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8981757Z     
2025-04-11T03:52:12.8981868Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8981946Z             try:
2025-04-11T03:52:12.8982031Z                 try_count += 1
2025-04-11T03:52:12.8982125Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8982208Z                 return ret
2025-04-11T03:52:12.8982302Z             except exception_type as e:
2025-04-11T03:52:12.8982459Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8982647Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8982768Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8982915Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8983070Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8983151Z                     continue
2025-04-11T03:52:12.8983229Z                 else:
2025-04-11T03:52:12.8983448Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8983528Z >                   raise e
2025-04-11T03:52:12.8983534Z 
2025-04-11T03:52:12.8983632Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8983742Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8983876Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8983964Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8984149Z tests/test_shardformer/test_layer/test_embedding.py:52: in test_embedding_1d
2025-04-11T03:52:12.8984242Z     spawn(run_dist, nprocs=2)
2025-04-11T03:52:12.8984343Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8984445Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8984698Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8984940Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8985227Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8985318Z     while not context.join():
2025-04-11T03:52:12.8985432Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8985438Z 
2025-04-11T03:52:12.8985632Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfa6e0>
2025-04-11T03:52:12.8985714Z timeout = None
2025-04-11T03:52:12.8985718Z 
2025-04-11T03:52:12.8985809Z     def join(self, timeout=None):
2025-04-11T03:52:12.8985940Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8986071Z     
2025-04-11T03:52:12.8986215Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8986360Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8986523Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8986620Z         of the first process exiting.
2025-04-11T03:52:12.8986691Z     
2025-04-11T03:52:12.8986842Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8987036Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8987107Z     
2025-04-11T03:52:12.8987188Z         Args:
2025-04-11T03:52:12.8987326Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8987401Z         """
2025-04-11T03:52:12.8987538Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8987630Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8987717Z             return True
2025-04-11T03:52:12.8987787Z     
2025-04-11T03:52:12.8987924Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8988044Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8988137Z             self.sentinels.keys(),
2025-04-11T03:52:12.8988225Z             timeout=timeout,
2025-04-11T03:52:12.8988297Z         )
2025-04-11T03:52:12.8988372Z     
2025-04-11T03:52:12.8988493Z         error_index = None
2025-04-11T03:52:12.8988588Z         for sentinel in ready:
2025-04-11T03:52:12.8988692Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8988855Z             process = self.processes[index]
2025-04-11T03:52:12.8988949Z             process.join()
2025-04-11T03:52:12.8989042Z             if process.exitcode != 0:
2025-04-11T03:52:12.8989135Z                 error_index = index
2025-04-11T03:52:12.8989212Z                 break
2025-04-11T03:52:12.8989281Z     
2025-04-11T03:52:12.8989378Z         # Return if there was no error.
2025-04-11T03:52:12.8989469Z         if error_index is None:
2025-04-11T03:52:12.8989608Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8989706Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8989775Z     
2025-04-11T03:52:12.8989919Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8990028Z         for process in self.processes:
2025-04-11T03:52:12.8990120Z             if process.is_alive():
2025-04-11T03:52:12.8990213Z                 process.terminate()
2025-04-11T03:52:12.8990302Z             process.join()
2025-04-11T03:52:12.8990374Z     
2025-04-11T03:52:12.8990514Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8990636Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8990743Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8990870Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8990954Z             if exitcode < 0:
2025-04-11T03:52:12.8991061Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8991170Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8991377Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8991477Z                     error_index=error_index,
2025-04-11T03:52:12.8991579Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8991673Z                     exit_code=exitcode,
2025-04-11T03:52:12.8991762Z                     signal_name=name,
2025-04-11T03:52:12.8991836Z                 )
2025-04-11T03:52:12.8991916Z             else:
2025-04-11T03:52:12.8992016Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8992183Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8992275Z                     error_index=error_index,
2025-04-11T03:52:12.8992376Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8992531Z                     exit_code=exitcode,
2025-04-11T03:52:12.8992605Z                 )
2025-04-11T03:52:12.8992680Z     
2025-04-11T03:52:12.8992812Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8992986Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8993072Z         msg += original_trace
2025-04-11T03:52:12.8993243Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8993470Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8993545Z E       
2025-04-11T03:52:12.8993674Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8993773Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8994073Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8994156Z E           fn(i, *args)
2025-04-11T03:52:12.8994426Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 47, in run_dist
2025-04-11T03:52:12.8994518Z E           check_embedding_1d()
2025-04-11T03:52:12.8994773Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8994866Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8995156Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 18, in check_embedding_1d
2025-04-11T03:52:12.8995326Z E           embedding = nn.Embedding(32, 128).cuda()
2025-04-11T03:52:12.8995598Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8995715Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8995990Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8996084Z E           param_applied = fn(param)
2025-04-11T03:52:12.8996362Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8996479Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8996589Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8996867Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8997008Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8997168Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8997173Z 
2025-04-11T03:52:12.8997476Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8997631Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8997786Z [04/11/25 03:48:43] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8997915Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8998078Z                              :75 launch                                         
2025-04-11T03:52:12.8998220Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8998344Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8998545Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8998688Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8998984Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:44898 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8999123Z _______________________________ test_linearconv ________________________________
2025-04-11T03:52:12.8999190Z 
2025-04-11T03:52:12.8999286Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8999890Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8999894Z 
2025-04-11T03:52:12.8999995Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9000138Z         try_count = 0
2025-04-11T03:52:12.9000238Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9000326Z             max_try, int
2025-04-11T03:52:12.9000472Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9000544Z     
2025-04-11T03:52:12.9000660Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9000735Z             try:
2025-04-11T03:52:12.9000823Z                 try_count += 1
2025-04-11T03:52:12.9000915Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9000995Z                 return ret
2025-04-11T03:52:12.9001094Z             except exception_type as e:
2025-04-11T03:52:12.9001192Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9001381Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9001499Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9001649Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9001858Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9001944Z                     continue
2025-04-11T03:52:12.9002028Z                 else:
2025-04-11T03:52:12.9002248Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9002334Z >                   raise e
2025-04-11T03:52:12.9002339Z 
2025-04-11T03:52:12.9002433Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9002547Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9002681Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9002771Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9002994Z tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py:209: in test_linearconv
2025-04-11T03:52:12.9003085Z     spawn(run_dist, nprocs=2)
2025-04-11T03:52:12.9003188Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9003287Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9003546Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9003720Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9004004Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9004098Z     while not context.join():
2025-04-11T03:52:12.9004205Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9004209Z 
2025-04-11T03:52:12.9004465Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0157970>
2025-04-11T03:52:12.9004549Z timeout = None
2025-04-11T03:52:12.9004553Z 
2025-04-11T03:52:12.9004648Z     def join(self, timeout=None):
2025-04-11T03:52:12.9004776Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9004847Z     
2025-04-11T03:52:12.9004997Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9005140Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9005305Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9005397Z         of the first process exiting.
2025-04-11T03:52:12.9005469Z     
2025-04-11T03:52:12.9005674Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9005810Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9005885Z     
2025-04-11T03:52:12.9005960Z         Args:
2025-04-11T03:52:12.9006102Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9006175Z         """
2025-04-11T03:52:12.9006315Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9006466Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9006549Z             return True
2025-04-11T03:52:12.9006625Z     
2025-04-11T03:52:12.9006761Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9006885Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9006981Z             self.sentinels.keys(),
2025-04-11T03:52:12.9007068Z             timeout=timeout,
2025-04-11T03:52:12.9007147Z         )
2025-04-11T03:52:12.9007219Z     
2025-04-11T03:52:12.9007312Z         error_index = None
2025-04-11T03:52:12.9007400Z         for sentinel in ready:
2025-04-11T03:52:12.9007511Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9007619Z             process = self.processes[index]
2025-04-11T03:52:12.9007710Z             process.join()
2025-04-11T03:52:12.9007811Z             if process.exitcode != 0:
2025-04-11T03:52:12.9007902Z                 error_index = index
2025-04-11T03:52:12.9007980Z                 break
2025-04-11T03:52:12.9008059Z     
2025-04-11T03:52:12.9008153Z         # Return if there was no error.
2025-04-11T03:52:12.9008245Z         if error_index is None:
2025-04-11T03:52:12.9008562Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9008666Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9008737Z     
2025-04-11T03:52:12.9008877Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9008976Z         for process in self.processes:
2025-04-11T03:52:12.9009065Z             if process.is_alive():
2025-04-11T03:52:12.9009162Z                 process.terminate()
2025-04-11T03:52:12.9009244Z             process.join()
2025-04-11T03:52:12.9009313Z     
2025-04-11T03:52:12.9009459Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9009574Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9009683Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9009811Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9009899Z             if exitcode < 0:
2025-04-11T03:52:12.9010005Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9010112Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9010267Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9010361Z                     error_index=error_index,
2025-04-11T03:52:12.9010466Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9010554Z                     exit_code=exitcode,
2025-04-11T03:52:12.9010640Z                     signal_name=name,
2025-04-11T03:52:12.9010719Z                 )
2025-04-11T03:52:12.9010793Z             else:
2025-04-11T03:52:12.9010954Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9011123Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9011219Z                     error_index=error_index,
2025-04-11T03:52:12.9011322Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9011411Z                     exit_code=exitcode,
2025-04-11T03:52:12.9011491Z                 )
2025-04-11T03:52:12.9011560Z     
2025-04-11T03:52:12.9011694Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9011862Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9011947Z         msg += original_trace
2025-04-11T03:52:12.9012176Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9012338Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9012417Z E       
2025-04-11T03:52:12.9012545Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9012645Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9012936Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9013074Z E           fn(i, *args)
2025-04-11T03:52:12.9013388Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 204, in run_dist
2025-04-11T03:52:12.9013486Z E           check_gpt2_qkv_fused_linear_1d()
2025-04-11T03:52:12.9013745Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9013834Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9014088Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9014175Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9014530Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 194, in check_gpt2_qkv_fused_linear_1d
2025-04-11T03:52:12.9014669Z E           check_linear_conv_1d_col(lazy_init, seq_parallel_mode)
2025-04-11T03:52:12.9015054Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 47, in check_linear_conv_1d_col
2025-04-11T03:52:12.9015154Z E           linear = Conv1D(192, 48).cuda()
2025-04-11T03:52:12.9015425Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9015546Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9015825Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9015923Z E           param_applied = fn(param)
2025-04-11T03:52:12.9016207Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9016322Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9016435Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9016724Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9016864Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9017026Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9017030Z 
2025-04-11T03:52:12.9017346Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9017501Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9017659Z [04/11/25 03:48:47] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9017842Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9017950Z                              :75 launch                                         
2025-04-11T03:52:12.9018090Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9018216Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9018412Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9018543Z ________________________________ test_layernorm ________________________________
2025-04-11T03:52:12.9018547Z 
2025-04-11T03:52:12.9018644Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9019234Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9019297Z 
2025-04-11T03:52:12.9019408Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9019490Z         try_count = 0
2025-04-11T03:52:12.9019591Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9019676Z             max_try, int
2025-04-11T03:52:12.9019866Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9019944Z     
2025-04-11T03:52:12.9020061Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9020145Z             try:
2025-04-11T03:52:12.9020234Z                 try_count += 1
2025-04-11T03:52:12.9020329Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9020419Z                 return ret
2025-04-11T03:52:12.9020518Z             except exception_type as e:
2025-04-11T03:52:12.9020624Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9020813Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9020937Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9021086Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9021245Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9021338Z                     continue
2025-04-11T03:52:12.9021418Z                 else:
2025-04-11T03:52:12.9021703Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9021789Z >                   raise e
2025-04-11T03:52:12.9021793Z 
2025-04-11T03:52:12.9021890Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9022003Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9022136Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9022228Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9022406Z tests/test_shardformer/test_layer/test_layernorm.py:50: in test_layernorm
2025-04-11T03:52:12.9022504Z     spawn(run_dist, nprocs=2)
2025-04-11T03:52:12.9022605Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9022703Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9022961Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9023140Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9023427Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9023517Z     while not context.join():
2025-04-11T03:52:12.9023631Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9023637Z 
2025-04-11T03:52:12.9023837Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be336b30>
2025-04-11T03:52:12.9023917Z timeout = None
2025-04-11T03:52:12.9023925Z 
2025-04-11T03:52:12.9024016Z     def join(self, timeout=None):
2025-04-11T03:52:12.9024197Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9024277Z     
2025-04-11T03:52:12.9024426Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9024575Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9024736Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9024829Z         of the first process exiting.
2025-04-11T03:52:12.9024903Z     
2025-04-11T03:52:12.9025049Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9025188Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9025326Z     
2025-04-11T03:52:12.9025405Z         Args:
2025-04-11T03:52:12.9025546Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9025620Z         """
2025-04-11T03:52:12.9025762Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9025856Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9025940Z             return True
2025-04-11T03:52:12.9026010Z     
2025-04-11T03:52:12.9026145Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9026325Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9026421Z             self.sentinels.keys(),
2025-04-11T03:52:12.9026511Z             timeout=timeout,
2025-04-11T03:52:12.9026584Z         )
2025-04-11T03:52:12.9026654Z     
2025-04-11T03:52:12.9026739Z         error_index = None
2025-04-11T03:52:12.9026823Z         for sentinel in ready:
2025-04-11T03:52:12.9026936Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9027037Z             process = self.processes[index]
2025-04-11T03:52:12.9027125Z             process.join()
2025-04-11T03:52:12.9027219Z             if process.exitcode != 0:
2025-04-11T03:52:12.9027309Z                 error_index = index
2025-04-11T03:52:12.9027389Z                 break
2025-04-11T03:52:12.9027461Z     
2025-04-11T03:52:12.9027557Z         # Return if there was no error.
2025-04-11T03:52:12.9027643Z         if error_index is None:
2025-04-11T03:52:12.9027777Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9027879Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9027950Z     
2025-04-11T03:52:12.9028150Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9028249Z         for process in self.processes:
2025-04-11T03:52:12.9028344Z             if process.is_alive():
2025-04-11T03:52:12.9028488Z                 process.terminate()
2025-04-11T03:52:12.9028574Z             process.join()
2025-04-11T03:52:12.9028650Z     
2025-04-11T03:52:12.9028793Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9028914Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9029023Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9029147Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9029237Z             if exitcode < 0:
2025-04-11T03:52:12.9029344Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9029454Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9029605Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9029703Z                     error_index=error_index,
2025-04-11T03:52:12.9029803Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9029890Z                     exit_code=exitcode,
2025-04-11T03:52:12.9029982Z                     signal_name=name,
2025-04-11T03:52:12.9030056Z                 )
2025-04-11T03:52:12.9030136Z             else:
2025-04-11T03:52:12.9030240Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9030403Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9030559Z                     error_index=error_index,
2025-04-11T03:52:12.9030665Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9030754Z                     exit_code=exitcode,
2025-04-11T03:52:12.9030828Z                 )
2025-04-11T03:52:12.9030905Z     
2025-04-11T03:52:12.9031039Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9031207Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9031297Z         msg += original_trace
2025-04-11T03:52:12.9031468Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9031628Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9031768Z E       
2025-04-11T03:52:12.9031896Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9032000Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9032296Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9032382Z E           fn(i, *args)
2025-04-11T03:52:12.9032650Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 45, in run_dist
2025-04-11T03:52:12.9032811Z E           check_layernorm()
2025-04-11T03:52:12.9033069Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9033158Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9033497Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 17, in check_layernorm
2025-04-11T03:52:12.9033605Z E           norm = nn.LayerNorm(128, 0.00001).cuda()
2025-04-11T03:52:12.9033879Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9033997Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9034267Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9034363Z E           param_applied = fn(param)
2025-04-11T03:52:12.9034634Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9034814Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9034927Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9035211Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9035346Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9035512Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9035516Z 
2025-04-11T03:52:12.9035819Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9035976Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9036133Z [04/11/25 03:48:52] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9036270Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9036377Z                              :75 launch                                         
2025-04-11T03:52:12.9036513Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9036641Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9036834Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9036982Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9037276Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43526 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9037465Z _________________________________ test_linear __________________________________
2025-04-11T03:52:12.9037469Z 
2025-04-11T03:52:12.9037566Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9038166Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9038171Z 
2025-04-11T03:52:12.9038274Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9038354Z         try_count = 0
2025-04-11T03:52:12.9038456Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9038603Z             max_try, int
2025-04-11T03:52:12.9038757Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9038829Z     
2025-04-11T03:52:12.9038944Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9039022Z             try:
2025-04-11T03:52:12.9039106Z                 try_count += 1
2025-04-11T03:52:12.9039201Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9039281Z                 return ret
2025-04-11T03:52:12.9039439Z             except exception_type as e:
2025-04-11T03:52:12.9039539Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9039727Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9039847Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9039995Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9040153Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9040238Z                     continue
2025-04-11T03:52:12.9040319Z                 else:
2025-04-11T03:52:12.9040538Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9040618Z >                   raise e
2025-04-11T03:52:12.9040622Z 
2025-04-11T03:52:12.9040721Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9040833Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9040974Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9041112Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9041293Z tests/test_shardformer/test_layer/test_linear_1d.py:284: in test_linear
2025-04-11T03:52:12.9041393Z     spawn(check_dist_linear, nprocs=2)
2025-04-11T03:52:12.9041494Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9041598Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9041854Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9042032Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9042315Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9042407Z     while not context.join():
2025-04-11T03:52:12.9042517Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9042523Z 
2025-04-11T03:52:12.9042722Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be309f30>
2025-04-11T03:52:12.9042806Z timeout = None
2025-04-11T03:52:12.9042810Z 
2025-04-11T03:52:12.9042903Z     def join(self, timeout=None):
2025-04-11T03:52:12.9043030Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9043100Z     
2025-04-11T03:52:12.9043246Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9043391Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9043554Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9043651Z         of the first process exiting.
2025-04-11T03:52:12.9043774Z     
2025-04-11T03:52:12.9043923Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9044058Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9044132Z     
2025-04-11T03:52:12.9044206Z         Args:
2025-04-11T03:52:12.9044344Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9044422Z         """
2025-04-11T03:52:12.9044560Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9044658Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9044738Z             return True
2025-04-11T03:52:12.9044807Z     
2025-04-11T03:52:12.9045011Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9045128Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9045222Z             self.sentinels.keys(),
2025-04-11T03:52:12.9045305Z             timeout=timeout,
2025-04-11T03:52:12.9045382Z         )
2025-04-11T03:52:12.9045451Z     
2025-04-11T03:52:12.9045533Z         error_index = None
2025-04-11T03:52:12.9045622Z         for sentinel in ready:
2025-04-11T03:52:12.9045729Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9045888Z             process = self.processes[index]
2025-04-11T03:52:12.9045975Z             process.join()
2025-04-11T03:52:12.9046071Z             if process.exitcode != 0:
2025-04-11T03:52:12.9046162Z                 error_index = index
2025-04-11T03:52:12.9046238Z                 break
2025-04-11T03:52:12.9046310Z     
2025-04-11T03:52:12.9046401Z         # Return if there was no error.
2025-04-11T03:52:12.9046486Z         if error_index is None:
2025-04-11T03:52:12.9046627Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9046725Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9046797Z     
2025-04-11T03:52:12.9046937Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9047036Z         for process in self.processes:
2025-04-11T03:52:12.9047124Z             if process.is_alive():
2025-04-11T03:52:12.9047214Z                 process.terminate()
2025-04-11T03:52:12.9047303Z             process.join()
2025-04-11T03:52:12.9047371Z     
2025-04-11T03:52:12.9047571Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9047689Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9047798Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9047922Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9048006Z             if exitcode < 0:
2025-04-11T03:52:12.9048120Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9048224Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9048377Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9048472Z                     error_index=error_index,
2025-04-11T03:52:12.9048576Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9048667Z                     exit_code=exitcode,
2025-04-11T03:52:12.9048755Z                     signal_name=name,
2025-04-11T03:52:12.9048835Z                 )
2025-04-11T03:52:12.9048908Z             else:
2025-04-11T03:52:12.9049011Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9049179Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9049269Z                     error_index=error_index,
2025-04-11T03:52:12.9049373Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9049457Z                     exit_code=exitcode,
2025-04-11T03:52:12.9049534Z                 )
2025-04-11T03:52:12.9049604Z     
2025-04-11T03:52:12.9049733Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9049905Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9050042Z         msg += original_trace
2025-04-11T03:52:12.9050218Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9050380Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9050456Z E       
2025-04-11T03:52:12.9050591Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9050688Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9050984Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9051064Z E           fn(i, *args)
2025-04-11T03:52:12.9051362Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 279, in check_dist_linear
2025-04-11T03:52:12.9051515Z E           run_dist_linear_test()
2025-04-11T03:52:12.9051775Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9051869Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9052126Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9052279Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9052529Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9052619Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9052916Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 270, in run_dist_linear_test
2025-04-11T03:52:12.9053063Z E           check_linear_1d_col(lazy_init, seq_parallel_mode, overlap)
2025-04-11T03:52:12.9053351Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T03:52:12.9053451Z E           linear = nn.Linear(32, 128).cuda()
2025-04-11T03:52:12.9053726Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9053844Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9054122Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9054272Z E           param_applied = fn(param)
2025-04-11T03:52:12.9054555Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9054672Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9054780Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9055066Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9055202Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9055370Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9055374Z 
2025-04-11T03:52:12.9055671Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9055827Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9055981Z [04/11/25 03:48:56] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9056113Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9056219Z                              :75 launch                                         
2025-04-11T03:52:12.9056354Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9056485Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9056678Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9056869Z _______________________________ test_linearconv ________________________________
2025-04-11T03:52:12.9056874Z 
2025-04-11T03:52:12.9056968Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9057562Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9057569Z 
2025-04-11T03:52:12.9057670Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9057752Z         try_count = 0
2025-04-11T03:52:12.9057850Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9057931Z             max_try, int
2025-04-11T03:52:12.9058154Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9058225Z     
2025-04-11T03:52:12.9058339Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9058413Z             try:
2025-04-11T03:52:12.9058503Z                 try_count += 1
2025-04-11T03:52:12.9058593Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9058673Z                 return ret
2025-04-11T03:52:12.9058772Z             except exception_type as e:
2025-04-11T03:52:12.9058929Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9059119Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9059235Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9059378Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9059535Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9059617Z                     continue
2025-04-11T03:52:12.9059696Z                 else:
2025-04-11T03:52:12.9059916Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9059999Z >                   raise e
2025-04-11T03:52:12.9060003Z 
2025-04-11T03:52:12.9060099Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9060211Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9060349Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9060487Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9060696Z tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py:166: in test_linearconv
2025-04-11T03:52:12.9060790Z     spawn(run_dist, nprocs=2)
2025-04-11T03:52:12.9060893Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9060994Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9061246Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9061425Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9061709Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9061802Z     while not context.join():
2025-04-11T03:52:12.9061910Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9061915Z 
2025-04-11T03:52:12.9062119Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be334940>
2025-04-11T03:52:12.9062200Z timeout = None
2025-04-11T03:52:12.9062204Z 
2025-04-11T03:52:12.9062299Z     def join(self, timeout=None):
2025-04-11T03:52:12.9062421Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9062492Z     
2025-04-11T03:52:12.9062640Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9062783Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9062950Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9063042Z         of the first process exiting.
2025-04-11T03:52:12.9063115Z     
2025-04-11T03:52:12.9063315Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9063456Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9063536Z     
2025-04-11T03:52:12.9063615Z         Args:
2025-04-11T03:52:12.9063757Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9063835Z         """
2025-04-11T03:52:12.9063978Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9064080Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9064166Z             return True
2025-04-11T03:52:12.9064248Z     
2025-04-11T03:52:12.9064381Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9064563Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9064658Z             self.sentinels.keys(),
2025-04-11T03:52:12.9064742Z             timeout=timeout,
2025-04-11T03:52:12.9064819Z         )
2025-04-11T03:52:12.9064890Z     
2025-04-11T03:52:12.9064976Z         error_index = None
2025-04-11T03:52:12.9065065Z         for sentinel in ready:
2025-04-11T03:52:12.9065173Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9065339Z             process = self.processes[index]
2025-04-11T03:52:12.9065422Z             process.join()
2025-04-11T03:52:12.9065523Z             if process.exitcode != 0:
2025-04-11T03:52:12.9065611Z                 error_index = index
2025-04-11T03:52:12.9065688Z                 break
2025-04-11T03:52:12.9065763Z     
2025-04-11T03:52:12.9065858Z         # Return if there was no error.
2025-04-11T03:52:12.9065946Z         if error_index is None:
2025-04-11T03:52:12.9066082Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9066179Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9066252Z     
2025-04-11T03:52:12.9066397Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9066499Z         for process in self.processes:
2025-04-11T03:52:12.9066587Z             if process.is_alive():
2025-04-11T03:52:12.9066685Z                 process.terminate()
2025-04-11T03:52:12.9066768Z             process.join()
2025-04-11T03:52:12.9066841Z     
2025-04-11T03:52:12.9066988Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9067156Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9067269Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9067391Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9067475Z             if exitcode < 0:
2025-04-11T03:52:12.9067585Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9067692Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9067841Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9067935Z                     error_index=error_index,
2025-04-11T03:52:12.9068039Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9068125Z                     exit_code=exitcode,
2025-04-11T03:52:12.9068213Z                     signal_name=name,
2025-04-11T03:52:12.9068297Z                 )
2025-04-11T03:52:12.9068372Z             else:
2025-04-11T03:52:12.9068529Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9068693Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9068788Z                     error_index=error_index,
2025-04-11T03:52:12.9068890Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9068978Z                     exit_code=exitcode,
2025-04-11T03:52:12.9069055Z                 )
2025-04-11T03:52:12.9069130Z     
2025-04-11T03:52:12.9069270Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9069436Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9069587Z         msg += original_trace
2025-04-11T03:52:12.9069767Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9069928Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9070009Z E       
2025-04-11T03:52:12.9070135Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9070235Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9070534Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9070614Z E           fn(i, *args)
2025-04-11T03:52:12.9070914Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 158, in run_dist
2025-04-11T03:52:12.9071070Z E           check_linear_1d_col()
2025-04-11T03:52:12.9071334Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9071429Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9071751Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T03:52:12.9071928Z E           linear = nn.Linear(8, 80).cuda()
2025-04-11T03:52:12.9072204Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9072325Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9072592Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9072690Z E           param_applied = fn(param)
2025-04-11T03:52:12.9072965Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9073083Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9073188Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9073474Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9073615Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9073779Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9073783Z 
2025-04-11T03:52:12.9074149Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9074301Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9074462Z [04/11/25 03:49:01] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9074592Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9074703Z                              :75 launch                                         
2025-04-11T03:52:12.9074841Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9074964Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9075159Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9075306Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9075609Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:59565 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9075740Z ________________________________ test_ring_attn ________________________________
2025-04-11T03:52:12.9075743Z 
2025-04-11T03:52:12.9075840Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9076434Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9076499Z 
2025-04-11T03:52:12.9076603Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9076685Z         try_count = 0
2025-04-11T03:52:12.9076785Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9076875Z             max_try, int
2025-04-11T03:52:12.9077020Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9077095Z     
2025-04-11T03:52:12.9077205Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9077284Z             try:
2025-04-11T03:52:12.9077367Z                 try_count += 1
2025-04-11T03:52:12.9077458Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9077542Z                 return ret
2025-04-11T03:52:12.9077702Z             except exception_type as e:
2025-04-11T03:52:12.9077803Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9077991Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9078109Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9078259Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9078469Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9078557Z                     continue
2025-04-11T03:52:12.9078640Z                 else:
2025-04-11T03:52:12.9078863Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9078945Z >                   raise e
2025-04-11T03:52:12.9078949Z 
2025-04-11T03:52:12.9079050Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9079164Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9079301Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9079395Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9079555Z colossalai/testing/utils.py:64: in _execute_function_by_param
2025-04-11T03:52:12.9079649Z     partial_func(**kwargs)
2025-04-11T03:52:12.9079829Z tests/test_shardformer/test_layer/test_ring_attn.py:181: in test_ring_attn
2025-04-11T03:52:12.9079948Z     spawn(launch_single_ring, nprocs=world_size)
2025-04-11T03:52:12.9080055Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9080211Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9080476Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9080649Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9080936Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9081030Z     while not context.join():
2025-04-11T03:52:12.9081137Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9081144Z 
2025-04-11T03:52:12.9081340Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f05ddba0>
2025-04-11T03:52:12.9081420Z timeout = None
2025-04-11T03:52:12.9081424Z 
2025-04-11T03:52:12.9081520Z     def join(self, timeout=None):
2025-04-11T03:52:12.9081646Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9081720Z     
2025-04-11T03:52:12.9081868Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9082009Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9082172Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9082265Z         of the first process exiting.
2025-04-11T03:52:12.9082343Z     
2025-04-11T03:52:12.9082488Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9082626Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9082695Z     
2025-04-11T03:52:12.9082768Z         Args:
2025-04-11T03:52:12.9082983Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9083059Z         """
2025-04-11T03:52:12.9083202Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9083298Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9083378Z             return True
2025-04-11T03:52:12.9083455Z     
2025-04-11T03:52:12.9083586Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9083708Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9083802Z             self.sentinels.keys(),
2025-04-11T03:52:12.9083888Z             timeout=timeout,
2025-04-11T03:52:12.9083961Z         )
2025-04-11T03:52:12.9084095Z     
2025-04-11T03:52:12.9084184Z         error_index = None
2025-04-11T03:52:12.9084270Z         for sentinel in ready:
2025-04-11T03:52:12.9084379Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9084477Z             process = self.processes[index]
2025-04-11T03:52:12.9084566Z             process.join()
2025-04-11T03:52:12.9084664Z             if process.exitcode != 0:
2025-04-11T03:52:12.9084751Z                 error_index = index
2025-04-11T03:52:12.9084886Z                 break
2025-04-11T03:52:12.9084957Z     
2025-04-11T03:52:12.9085049Z         # Return if there was no error.
2025-04-11T03:52:12.9085139Z         if error_index is None:
2025-04-11T03:52:12.9085275Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9085375Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9085446Z     
2025-04-11T03:52:12.9085589Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9085690Z         for process in self.processes:
2025-04-11T03:52:12.9085777Z             if process.is_alive():
2025-04-11T03:52:12.9085874Z                 process.terminate()
2025-04-11T03:52:12.9085957Z             process.join()
2025-04-11T03:52:12.9086029Z     
2025-04-11T03:52:12.9086173Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9086287Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9086400Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9086528Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9086614Z             if exitcode < 0:
2025-04-11T03:52:12.9086772Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9086886Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9087037Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9087133Z                     error_index=error_index,
2025-04-11T03:52:12.9087241Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9087331Z                     exit_code=exitcode,
2025-04-11T03:52:12.9087420Z                     signal_name=name,
2025-04-11T03:52:12.9087493Z                 )
2025-04-11T03:52:12.9087568Z             else:
2025-04-11T03:52:12.9087674Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9087840Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9087936Z                     error_index=error_index,
2025-04-11T03:52:12.9088033Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9088125Z                     exit_code=exitcode,
2025-04-11T03:52:12.9088198Z                 )
2025-04-11T03:52:12.9088268Z     
2025-04-11T03:52:12.9088402Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9088569Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9088660Z         msg += original_trace
2025-04-11T03:52:12.9088834Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9088995Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9089067Z E       
2025-04-11T03:52:12.9089251Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9089357Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9089651Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9089738Z E           fn(i, *args)
2025-04-11T03:52:12.9090043Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 169, in launch_single_ring
2025-04-11T03:52:12.9090133Z E           check_packed_seq()
2025-04-11T03:52:12.9090390Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9090478Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9090794Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9090881Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9091129Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9091215Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9091324Z E         [Previous line repeated 2 more times]
2025-04-11T03:52:12.9091664Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 94, in check_packed_seq
2025-04-11T03:52:12.9091842Z E           padding_mask = torch.ones((bs, seqlen), dtype=torch.int, device=device)
2025-04-11T03:52:12.9091950Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9092234Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9092379Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9092541Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9092545Z 
2025-04-11T03:52:12.9092856Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9093009Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9093165Z [04/11/25 03:49:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9093348Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9093455Z                              :75 launch                                         
2025-04-11T03:52:12.9093595Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9093718Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9093919Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9094055Z _______________________________ test_double_ring _______________________________
2025-04-11T03:52:12.9094058Z 
2025-04-11T03:52:12.9094154Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9094754Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9094761Z 
2025-04-11T03:52:12.9094867Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9094948Z         try_count = 0
2025-04-11T03:52:12.9095049Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9095136Z             max_try, int
2025-04-11T03:52:12.9095281Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9095355Z     
2025-04-11T03:52:12.9095468Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9095543Z             try:
2025-04-11T03:52:12.9095630Z                 try_count += 1
2025-04-11T03:52:12.9095721Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9095859Z                 return ret
2025-04-11T03:52:12.9095955Z             except exception_type as e:
2025-04-11T03:52:12.9096056Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9096244Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9096365Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9096515Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9096665Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9096752Z                     continue
2025-04-11T03:52:12.9096829Z                 else:
2025-04-11T03:52:12.9097112Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9097192Z >                   raise e
2025-04-11T03:52:12.9097196Z 
2025-04-11T03:52:12.9097291Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9097406Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9097540Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9097690Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9097842Z colossalai/testing/utils.py:64: in _execute_function_by_param
2025-04-11T03:52:12.9097933Z     partial_func(**kwargs)
2025-04-11T03:52:12.9098115Z tests/test_shardformer/test_layer/test_ring_attn.py:187: in test_double_ring
2025-04-11T03:52:12.9098228Z     spawn(launch_double_ring, nprocs=world_size)
2025-04-11T03:52:12.9098332Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9098430Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9098690Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9098863Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9099147Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9099236Z     while not context.join():
2025-04-11T03:52:12.9099345Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9099351Z 
2025-04-11T03:52:12.9099552Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be336b30>
2025-04-11T03:52:12.9099688Z timeout = None
2025-04-11T03:52:12.9099693Z 
2025-04-11T03:52:12.9099791Z     def join(self, timeout=None):
2025-04-11T03:52:12.9099914Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9099988Z     
2025-04-11T03:52:12.9100134Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9100274Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9100433Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9100527Z         of the first process exiting.
2025-04-11T03:52:12.9100603Z     
2025-04-11T03:52:12.9100746Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9100886Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9100957Z     
2025-04-11T03:52:12.9101031Z         Args:
2025-04-11T03:52:12.9101176Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9101249Z         """
2025-04-11T03:52:12.9101391Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9101487Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9101567Z             return True
2025-04-11T03:52:12.9101640Z     
2025-04-11T03:52:12.9101771Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9101893Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9101984Z             self.sentinels.keys(),
2025-04-11T03:52:12.9102067Z             timeout=timeout,
2025-04-11T03:52:12.9102146Z         )
2025-04-11T03:52:12.9102273Z     
2025-04-11T03:52:12.9102361Z         error_index = None
2025-04-11T03:52:12.9102446Z         for sentinel in ready:
2025-04-11T03:52:12.9102558Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9102657Z             process = self.processes[index]
2025-04-11T03:52:12.9102743Z             process.join()
2025-04-11T03:52:12.9102843Z             if process.exitcode != 0:
2025-04-11T03:52:12.9102929Z                 error_index = index
2025-04-11T03:52:12.9103009Z                 break
2025-04-11T03:52:12.9103078Z     
2025-04-11T03:52:12.9103169Z         # Return if there was no error.
2025-04-11T03:52:12.9103259Z         if error_index is None:
2025-04-11T03:52:12.9103455Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9103556Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9103627Z     
2025-04-11T03:52:12.9103766Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9103868Z         for process in self.processes:
2025-04-11T03:52:12.9103958Z             if process.is_alive():
2025-04-11T03:52:12.9104054Z                 process.terminate()
2025-04-11T03:52:12.9104136Z             process.join()
2025-04-11T03:52:12.9104272Z     
2025-04-11T03:52:12.9104412Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9104530Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9104641Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9104762Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9104851Z             if exitcode < 0:
2025-04-11T03:52:12.9104958Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9105066Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9105218Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9105313Z                     error_index=error_index,
2025-04-11T03:52:12.9105419Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9105507Z                     exit_code=exitcode,
2025-04-11T03:52:12.9105598Z                     signal_name=name,
2025-04-11T03:52:12.9105673Z                 )
2025-04-11T03:52:12.9105749Z             else:
2025-04-11T03:52:12.9105858Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9106095Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9106194Z                     error_index=error_index,
2025-04-11T03:52:12.9106298Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9106390Z                     exit_code=exitcode,
2025-04-11T03:52:12.9106465Z                 )
2025-04-11T03:52:12.9106535Z     
2025-04-11T03:52:12.9106671Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9106842Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9106932Z         msg += original_trace
2025-04-11T03:52:12.9107108Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9107267Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9107348Z E       
2025-04-11T03:52:12.9107475Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9107577Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9107879Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9107964Z E           fn(i, *args)
2025-04-11T03:52:12.9108266Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 175, in launch_double_ring
2025-04-11T03:52:12.9108369Z E           check_ring_attn(inner_ring_size=2)
2025-04-11T03:52:12.9108665Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9108820Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9109079Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9109169Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9109422Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9109511Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9109616Z E         [Previous line repeated 2 more times]
2025-04-11T03:52:12.9109899Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 36, in check_ring_attn
2025-04-11T03:52:12.9110102Z E           qkv = torch.randn(bs, seq_len, 3, nheads, d, device=device, dtype=dtype, requires_grad=True)
2025-04-11T03:52:12.9110280Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9110568Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9110709Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9110869Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9111016Z 
2025-04-11T03:52:12.9111330Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9111482Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9111635Z [04/11/25 03:49:11] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9111768Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9111877Z                              :75 launch                                         
2025-04-11T03:52:12.9112018Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9112143Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9112341Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9112483Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9112840Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:22847 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9113131Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:22847 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9113277Z __________________________ test_all_to_all_attention ___________________________
2025-04-11T03:52:12.9113281Z 
2025-04-11T03:52:12.9113380Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9113986Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9113991Z 
2025-04-11T03:52:12.9114098Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9114177Z         try_count = 0
2025-04-11T03:52:12.9114283Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9114365Z             max_try, int
2025-04-11T03:52:12.9114515Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9114586Z     
2025-04-11T03:52:12.9114700Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9114781Z             try:
2025-04-11T03:52:12.9114866Z                 try_count += 1
2025-04-11T03:52:12.9114962Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9115043Z                 return ret
2025-04-11T03:52:12.9115136Z             except exception_type as e:
2025-04-11T03:52:12.9115241Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9115483Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9115604Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9115748Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9115907Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9115993Z                     continue
2025-04-11T03:52:12.9116068Z                 else:
2025-04-11T03:52:12.9116288Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9116367Z >                   raise e
2025-04-11T03:52:12.9116371Z 
2025-04-11T03:52:12.9116534Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9116643Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9116780Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9116865Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9117097Z tests/test_shardformer/test_layer/test_sequence_parallel.py:174: in test_all_to_all_attention
2025-04-11T03:52:12.9117198Z     spawn(check_all2all_attn, nprocs=4)
2025-04-11T03:52:12.9117353Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9117455Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9117714Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9117891Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9118174Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9118266Z     while not context.join():
2025-04-11T03:52:12.9118380Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9118383Z 
2025-04-11T03:52:12.9118580Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be4a41c0>
2025-04-11T03:52:12.9118664Z timeout = None
2025-04-11T03:52:12.9118668Z 
2025-04-11T03:52:12.9118758Z     def join(self, timeout=None):
2025-04-11T03:52:12.9118887Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9118958Z     
2025-04-11T03:52:12.9119102Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9119306Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9119470Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9119567Z         of the first process exiting.
2025-04-11T03:52:12.9119639Z     
2025-04-11T03:52:12.9119787Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9119924Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9119994Z     
2025-04-11T03:52:12.9120073Z         Args:
2025-04-11T03:52:12.9120208Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9120285Z         """
2025-04-11T03:52:12.9120424Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9120518Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9120604Z             return True
2025-04-11T03:52:12.9120676Z     
2025-04-11T03:52:12.9120812Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9120929Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9121024Z             self.sentinels.keys(),
2025-04-11T03:52:12.9121107Z             timeout=timeout,
2025-04-11T03:52:12.9121179Z         )
2025-04-11T03:52:12.9121255Z     
2025-04-11T03:52:12.9121337Z         error_index = None
2025-04-11T03:52:12.9121427Z         for sentinel in ready:
2025-04-11T03:52:12.9121530Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9121628Z             process = self.processes[index]
2025-04-11T03:52:12.9121717Z             process.join()
2025-04-11T03:52:12.9121865Z             if process.exitcode != 0:
2025-04-11T03:52:12.9121958Z                 error_index = index
2025-04-11T03:52:12.9122035Z                 break
2025-04-11T03:52:12.9122104Z     
2025-04-11T03:52:12.9122201Z         # Return if there was no error.
2025-04-11T03:52:12.9122287Z         if error_index is None:
2025-04-11T03:52:12.9122425Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9122521Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9122594Z     
2025-04-11T03:52:12.9122733Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9122832Z         for process in self.processes:
2025-04-11T03:52:12.9122924Z             if process.is_alive():
2025-04-11T03:52:12.9123077Z                 process.terminate()
2025-04-11T03:52:12.9123165Z             process.join()
2025-04-11T03:52:12.9123235Z     
2025-04-11T03:52:12.9123376Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9123498Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9123607Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9123731Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9123871Z             if exitcode < 0:
2025-04-11T03:52:12.9123984Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9124090Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9124238Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9124336Z                     error_index=error_index,
2025-04-11T03:52:12.9124435Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9124528Z                     exit_code=exitcode,
2025-04-11T03:52:12.9124615Z                     signal_name=name,
2025-04-11T03:52:12.9124688Z                 )
2025-04-11T03:52:12.9124765Z             else:
2025-04-11T03:52:12.9124865Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9125029Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9125122Z                     error_index=error_index,
2025-04-11T03:52:12.9125224Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9125311Z                     exit_code=exitcode,
2025-04-11T03:52:12.9125383Z                 )
2025-04-11T03:52:12.9125511Z     
2025-04-11T03:52:12.9125645Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9125814Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9125899Z         msg += original_trace
2025-04-11T03:52:12.9126069Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9126230Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9126304Z E       
2025-04-11T03:52:12.9126433Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9126534Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9126833Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9126917Z E           fn(i, *args)
2025-04-11T03:52:12.9127240Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 169, in check_all2all_attn
2025-04-11T03:52:12.9127334Z E           run_seq_parallel_attn()
2025-04-11T03:52:12.9127596Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9127690Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9127938Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9128031Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9128278Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9128434Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9128539Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.9128865Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 164, in run_seq_parallel_attn
2025-04-11T03:52:12.9129017Z E           seq_parallel_attn(seq_len, hidden_dim, head_num, batch_size)
2025-04-11T03:52:12.9129329Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 101, in seq_parallel_attn
2025-04-11T03:52:12.9129464Z E           x = torch.randn(batch_size, seq_len, hidden_dim).cuda()
2025-04-11T03:52:12.9129570Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9129916Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9130052Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9130215Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9130219Z 
2025-04-11T03:52:12.9130527Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9130742Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9130900Z [04/11/25 03:49:16] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9131026Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9131135Z                              :75 launch                                         
2025-04-11T03:52:12.9131271Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9131402Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9131597Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9131735Z _____________________________ test_vocab_embedding _____________________________
2025-04-11T03:52:12.9131739Z 
2025-04-11T03:52:12.9131838Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9132483Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9132489Z 
2025-04-11T03:52:12.9132597Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9132679Z         try_count = 0
2025-04-11T03:52:12.9132784Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9132866Z             max_try, int
2025-04-11T03:52:12.9133015Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9133086Z     
2025-04-11T03:52:12.9133198Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9133280Z             try:
2025-04-11T03:52:12.9133366Z                 try_count += 1
2025-04-11T03:52:12.9133460Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9133539Z                 return ret
2025-04-11T03:52:12.9133634Z             except exception_type as e:
2025-04-11T03:52:12.9133740Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9133935Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9134093Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9134237Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9134393Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9134477Z                     continue
2025-04-11T03:52:12.9134552Z                 else:
2025-04-11T03:52:12.9134829Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9134912Z >                   raise e
2025-04-11T03:52:12.9134916Z 
2025-04-11T03:52:12.9135014Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9135127Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9135272Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9135360Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9135602Z tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py:54: in test_vocab_embedding
2025-04-11T03:52:12.9135697Z     spawn(run_dist, nprocs=2)
2025-04-11T03:52:12.9135800Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9135902Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9136237Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9136414Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9136701Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9136789Z     while not context.join():
2025-04-11T03:52:12.9136904Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9136966Z 
2025-04-11T03:52:12.9137168Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be337400>
2025-04-11T03:52:12.9137253Z timeout = None
2025-04-11T03:52:12.9137257Z 
2025-04-11T03:52:12.9137348Z     def join(self, timeout=None):
2025-04-11T03:52:12.9137477Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9137548Z     
2025-04-11T03:52:12.9137693Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9137839Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9138002Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9138099Z         of the first process exiting.
2025-04-11T03:52:12.9138168Z     
2025-04-11T03:52:12.9138318Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9138453Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9138524Z     
2025-04-11T03:52:12.9138602Z         Args:
2025-04-11T03:52:12.9138793Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9138874Z         """
2025-04-11T03:52:12.9139013Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9139107Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9139193Z             return True
2025-04-11T03:52:12.9139262Z     
2025-04-11T03:52:12.9139399Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9139517Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9139611Z             self.sentinels.keys(),
2025-04-11T03:52:12.9139694Z             timeout=timeout,
2025-04-11T03:52:12.9139767Z         )
2025-04-11T03:52:12.9139840Z     
2025-04-11T03:52:12.9139922Z         error_index = None
2025-04-11T03:52:12.9140009Z         for sentinel in ready:
2025-04-11T03:52:12.9140118Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9140215Z             process = self.processes[index]
2025-04-11T03:52:12.9140304Z             process.join()
2025-04-11T03:52:12.9140397Z             if process.exitcode != 0:
2025-04-11T03:52:12.9140487Z                 error_index = index
2025-04-11T03:52:12.9140561Z                 break
2025-04-11T03:52:12.9140631Z     
2025-04-11T03:52:12.9140725Z         # Return if there was no error.
2025-04-11T03:52:12.9140812Z         if error_index is None:
2025-04-11T03:52:12.9140950Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9141046Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9141118Z     
2025-04-11T03:52:12.9141308Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9141408Z         for process in self.processes:
2025-04-11T03:52:12.9141500Z             if process.is_alive():
2025-04-11T03:52:12.9141588Z                 process.terminate()
2025-04-11T03:52:12.9141678Z             process.join()
2025-04-11T03:52:12.9141748Z     
2025-04-11T03:52:12.9141890Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9142009Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9142114Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9142237Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9142320Z             if exitcode < 0:
2025-04-11T03:52:12.9142491Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9142595Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9142745Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9142844Z                     error_index=error_index,
2025-04-11T03:52:12.9142944Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9143035Z                     exit_code=exitcode,
2025-04-11T03:52:12.9143179Z                     signal_name=name,
2025-04-11T03:52:12.9143255Z                 )
2025-04-11T03:52:12.9143334Z             else:
2025-04-11T03:52:12.9143438Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9143608Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9143699Z                     error_index=error_index,
2025-04-11T03:52:12.9143802Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9143886Z                     exit_code=exitcode,
2025-04-11T03:52:12.9143959Z                 )
2025-04-11T03:52:12.9144035Z     
2025-04-11T03:52:12.9144167Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9144341Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9144425Z         msg += original_trace
2025-04-11T03:52:12.9144600Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9144768Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9144842Z E       
2025-04-11T03:52:12.9145025Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9145128Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9145425Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9145506Z E           fn(i, *args)
2025-04-11T03:52:12.9145823Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 49, in run_dist
2025-04-11T03:52:12.9145922Z E           check_vocab_embedding_1d()
2025-04-11T03:52:12.9146177Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9146273Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9146644Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 18, in check_vocab_embedding_1d
2025-04-11T03:52:12.9146763Z E           embedding = nn.Embedding(128, 32).to("cuda")
2025-04-11T03:52:12.9147030Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:12.9147127Z E           return self._apply(convert)
2025-04-11T03:52:12.9147397Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9147491Z E           param_applied = fn(param)
2025-04-11T03:52:12.9147764Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:12.9148029Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.9148144Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9148464Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9148610Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9148774Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9148778Z 
2025-04-11T03:52:12.9149084Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9149234Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9149465Z [04/11/25 03:49:20] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9149597Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9149705Z                              :75 launch                                         
2025-04-11T03:52:12.9149846Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9149968Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9150229Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9150357Z __________________________________ test_bert ___________________________________
2025-04-11T03:52:12.9150361Z 
2025-04-11T03:52:12.9150456Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9150463Z 
2025-04-11T03:52:12.9150560Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9150643Z         try_count = 0
2025-04-11T03:52:12.9150746Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9150828Z             max_try, int
2025-04-11T03:52:12.9150976Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9151046Z     
2025-04-11T03:52:12.9151161Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9151241Z             try:
2025-04-11T03:52:12.9151326Z                 try_count += 1
2025-04-11T03:52:12.9151424Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9151430Z 
2025-04-11T03:52:12.9151525Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9151710Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9151830Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9151929Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9152088Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9152188Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9152299Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9152303Z 
2025-04-11T03:52:12.9152381Z device = None
2025-04-11T03:52:12.9152385Z 
2025-04-11T03:52:12.9152514Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9152668Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9152737Z     
2025-04-11T03:52:12.9152819Z         Args:
2025-04-11T03:52:12.9152995Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9153170Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9153279Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9153355Z         """
2025-04-11T03:52:12.9153435Z         _lazy_init()
2025-04-11T03:52:12.9153531Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9153639Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9153747Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9154041Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9154239Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9154402Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9154412Z 
2025-04-11T03:52:12.9154656Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9154791Z __________________________________ test_blip2 __________________________________
2025-04-11T03:52:12.9154795Z 
2025-04-11T03:52:12.9154893Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9154897Z 
2025-04-11T03:52:12.9154995Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9155078Z         try_count = 0
2025-04-11T03:52:12.9155255Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9155338Z             max_try, int
2025-04-11T03:52:12.9155486Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9155559Z     
2025-04-11T03:52:12.9155677Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9155752Z             try:
2025-04-11T03:52:12.9155840Z                 try_count += 1
2025-04-11T03:52:12.9155932Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9155995Z 
2025-04-11T03:52:12.9156096Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9156207Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9156323Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9156423Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9156579Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9156680Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9156786Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9156790Z 
2025-04-11T03:52:12.9156871Z device = None
2025-04-11T03:52:12.9156874Z 
2025-04-11T03:52:12.9156996Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9157150Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9157229Z     
2025-04-11T03:52:12.9157303Z         Args:
2025-04-11T03:52:12.9157475Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9157700Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9157810Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9157891Z         """
2025-04-11T03:52:12.9157968Z         _lazy_init()
2025-04-11T03:52:12.9158066Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9158166Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9158280Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9158566Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9158705Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9158866Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9158869Z 
2025-04-11T03:52:12.9159109Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9159244Z __________________________________ test_bloom __________________________________
2025-04-11T03:52:12.9159248Z 
2025-04-11T03:52:12.9159338Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9159342Z 
2025-04-11T03:52:12.9159447Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9159527Z         try_count = 0
2025-04-11T03:52:12.9159629Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9159709Z             max_try, int
2025-04-11T03:52:12.9159851Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9159929Z     
2025-04-11T03:52:12.9160038Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9160179Z             try:
2025-04-11T03:52:12.9160264Z                 try_count += 1
2025-04-11T03:52:12.9160354Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9160361Z 
2025-04-11T03:52:12.9160458Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9160567Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9160686Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9160781Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9160940Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9161036Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9161144Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9161215Z 
2025-04-11T03:52:12.9161296Z device = None
2025-04-11T03:52:12.9161300Z 
2025-04-11T03:52:12.9161418Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9161576Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9161648Z     
2025-04-11T03:52:12.9161723Z         Args:
2025-04-11T03:52:12.9161888Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9162109Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9162224Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9162298Z         """
2025-04-11T03:52:12.9162380Z         _lazy_init()
2025-04-11T03:52:12.9162474Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9162579Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9162687Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9162973Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9163115Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9163274Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9163277Z 
2025-04-11T03:52:12.9163519Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9163651Z _________________________________ test_chatglm _________________________________
2025-04-11T03:52:12.9163709Z 
2025-04-11T03:52:12.9163810Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9163814Z 
2025-04-11T03:52:12.9163912Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9163992Z         try_count = 0
2025-04-11T03:52:12.9164090Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9164172Z             max_try, int
2025-04-11T03:52:12.9164319Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9164392Z     
2025-04-11T03:52:12.9164506Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9164581Z             try:
2025-04-11T03:52:12.9164666Z                 try_count += 1
2025-04-11T03:52:12.9164762Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9164766Z 
2025-04-11T03:52:12.9164861Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9164974Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9165089Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9165185Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9165339Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9165434Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9165544Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9165550Z 
2025-04-11T03:52:12.9165628Z device = None
2025-04-11T03:52:12.9165631Z 
2025-04-11T03:52:12.9165756Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9165906Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9166030Z     
2025-04-11T03:52:12.9166110Z         Args:
2025-04-11T03:52:12.9166278Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9166450Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9166559Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9166637Z         """
2025-04-11T03:52:12.9166715Z         _lazy_init()
2025-04-11T03:52:12.9166811Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9166912Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9167018Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9167373Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9167510Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9167672Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9167675Z 
2025-04-11T03:52:12.9167912Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9168105Z _________________________________ test_command _________________________________
2025-04-11T03:52:12.9168109Z 
2025-04-11T03:52:12.9168206Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9168210Z 
2025-04-11T03:52:12.9168316Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9168395Z         try_count = 0
2025-04-11T03:52:12.9168495Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9168579Z             max_try, int
2025-04-11T03:52:12.9168723Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9168798Z     
2025-04-11T03:52:12.9168908Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9168983Z             try:
2025-04-11T03:52:12.9169070Z                 try_count += 1
2025-04-11T03:52:12.9169161Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9169165Z 
2025-04-11T03:52:12.9169263Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9169370Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9169488Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9169631Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9169788Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9169887Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9169993Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9169997Z 
2025-04-11T03:52:12.9170083Z device = None
2025-04-11T03:52:12.9170086Z 
2025-04-11T03:52:12.9170207Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9170362Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9170434Z     
2025-04-11T03:52:12.9170509Z         Args:
2025-04-11T03:52:12.9170679Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9170842Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9170954Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9171028Z         """
2025-04-11T03:52:12.9171112Z         _lazy_init()
2025-04-11T03:52:12.9171206Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9171305Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9171413Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9171696Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9171837Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9172050Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9172055Z 
2025-04-11T03:52:12.9172295Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9172427Z _______________________________ test_deepseek[4] _______________________________
2025-04-11T03:52:12.9172432Z 
2025-04-11T03:52:12.9172551Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9173150Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9173154Z 
2025-04-11T03:52:12.9173316Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9173401Z         try_count = 0
2025-04-11T03:52:12.9173499Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9173583Z             max_try, int
2025-04-11T03:52:12.9173725Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9173798Z     
2025-04-11T03:52:12.9173906Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9173981Z             try:
2025-04-11T03:52:12.9174120Z                 try_count += 1
2025-04-11T03:52:12.9174210Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9174295Z                 return ret
2025-04-11T03:52:12.9174388Z             except exception_type as e:
2025-04-11T03:52:12.9174486Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9174677Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9174793Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9174942Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9175097Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9175185Z                     continue
2025-04-11T03:52:12.9175261Z                 else:
2025-04-11T03:52:12.9175482Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9175569Z >                   raise e
2025-04-11T03:52:12.9175573Z 
2025-04-11T03:52:12.9175667Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9175834Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9175965Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9176056Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9176246Z tests/test_shardformer/test_model/test_shard_deepseek.py:228: in test_deepseek
2025-04-11T03:52:12.9176343Z     spawn(check_deepseek, world_size)
2025-04-11T03:52:12.9176447Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9176546Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9176807Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9176983Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9177269Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9177360Z     while not context.join():
2025-04-11T03:52:12.9177469Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9177474Z 
2025-04-11T03:52:12.9177670Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ecb27d0>
2025-04-11T03:52:12.9177750Z timeout = None
2025-04-11T03:52:12.9177754Z 
2025-04-11T03:52:12.9177846Z     def join(self, timeout=None):
2025-04-11T03:52:12.9177969Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9178046Z     
2025-04-11T03:52:12.9178191Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9178409Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9178580Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9178675Z         of the first process exiting.
2025-04-11T03:52:12.9178751Z     
2025-04-11T03:52:12.9178899Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9179044Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9179115Z     
2025-04-11T03:52:12.9179191Z         Args:
2025-04-11T03:52:12.9179337Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9179410Z         """
2025-04-11T03:52:12.9179552Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9179705Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9179785Z             return True
2025-04-11T03:52:12.9179859Z     
2025-04-11T03:52:12.9179992Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9180115Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9180207Z             self.sentinels.keys(),
2025-04-11T03:52:12.9180298Z             timeout=timeout,
2025-04-11T03:52:12.9180372Z         )
2025-04-11T03:52:12.9180496Z     
2025-04-11T03:52:12.9180585Z         error_index = None
2025-04-11T03:52:12.9180669Z         for sentinel in ready:
2025-04-11T03:52:12.9180784Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9180886Z             process = self.processes[index]
2025-04-11T03:52:12.9180973Z             process.join()
2025-04-11T03:52:12.9181073Z             if process.exitcode != 0:
2025-04-11T03:52:12.9181162Z                 error_index = index
2025-04-11T03:52:12.9181243Z                 break
2025-04-11T03:52:12.9181313Z     
2025-04-11T03:52:12.9181407Z         # Return if there was no error.
2025-04-11T03:52:12.9181500Z         if error_index is None:
2025-04-11T03:52:12.9181634Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9181735Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9181805Z     
2025-04-11T03:52:12.9181948Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9182046Z         for process in self.processes:
2025-04-11T03:52:12.9182136Z             if process.is_alive():
2025-04-11T03:52:12.9182283Z                 process.terminate()
2025-04-11T03:52:12.9182370Z             process.join()
2025-04-11T03:52:12.9182444Z     
2025-04-11T03:52:12.9182585Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9182700Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9182811Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9182932Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9183023Z             if exitcode < 0:
2025-04-11T03:52:12.9183128Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9183237Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9183387Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9183481Z                     error_index=error_index,
2025-04-11T03:52:12.9183587Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9183674Z                     exit_code=exitcode,
2025-04-11T03:52:12.9183768Z                     signal_name=name,
2025-04-11T03:52:12.9183842Z                 )
2025-04-11T03:52:12.9183917Z             else:
2025-04-11T03:52:12.9184023Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9184183Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9184281Z                     error_index=error_index,
2025-04-11T03:52:12.9184379Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9184469Z                     exit_code=exitcode,
2025-04-11T03:52:12.9184542Z                 )
2025-04-11T03:52:12.9184611Z     
2025-04-11T03:52:12.9184800Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9184968Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9185058Z         msg += original_trace
2025-04-11T03:52:12.9185227Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9185386Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9185464Z E       
2025-04-11T03:52:12.9185589Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9185691Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9185986Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9186131Z E           fn(i, *args)
2025-04-11T03:52:12.9186437Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 216, in check_deepseek
2025-04-11T03:52:12.9186527Z E           run_deepseek_test()
2025-04-11T03:52:12.9186788Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9186935Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9187243Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 187, in run_deepseek_test
2025-04-11T03:52:12.9187337Z E           run_deepseek_commom(config)
2025-04-11T03:52:12.9187641Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 77, in run_deepseek_commom
2025-04-11T03:52:12.9187844Z E           torch_model = AutoModel.from_config(config, trust_remote_code=True).cuda().to(dtype)
2025-04-11T03:52:12.9188137Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9188238Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9188581Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9188707Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9188974Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9189126Z E           module._apply(fn)
2025-04-11T03:52:12.9189393Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9189490Z E           param_applied = fn(param)
2025-04-11T03:52:12.9189763Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9189879Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9189989Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9190272Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9190410Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9190571Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9190577Z 
2025-04-11T03:52:12.9190881Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9191033Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9191192Z [04/11/25 03:49:26] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9191323Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9191432Z                              :75 launch                                         
2025-04-11T03:52:12.9191573Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9191752Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9191848Z rank 0 testing (0, 1, 4, 1, 1)
2025-04-11T03:52:12.9192050Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9192200Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9192904Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9192993Z   warnings.warn(
2025-04-11T03:52:12.9193749Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9193833Z   warnings.warn(
2025-04-11T03:52:12.9194516Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9194662Z   warnings.warn(
2025-04-11T03:52:12.9195340Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9195419Z   warnings.warn(
2025-04-11T03:52:12.9196257Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9196336Z   warnings.warn(
2025-04-11T03:52:12.9197192Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9197278Z   warnings.warn(
2025-04-11T03:52:12.9198100Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9198181Z   warnings.warn(
2025-04-11T03:52:12.9199005Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9199084Z   warnings.warn(
2025-04-11T03:52:12.9199901Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9199978Z   warnings.warn(
2025-04-11T03:52:12.9200794Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9200874Z   warnings.warn(
2025-04-11T03:52:12.9201763Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9201846Z   warnings.warn(
2025-04-11T03:52:12.9202644Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9202788Z   warnings.warn(
2025-04-11T03:52:12.9203587Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9203669Z   warnings.warn(
2025-04-11T03:52:12.9204467Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9204605Z   warnings.warn(
2025-04-11T03:52:12.9204891Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9204990Z - configuration_deepseek.py
2025-04-11T03:52:12.9205341Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9206135Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9206214Z   warnings.warn(
2025-04-11T03:52:12.9206557Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9206651Z - configuration_deepseek.py
2025-04-11T03:52:12.9206990Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9207796Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9207877Z   warnings.warn(
2025-04-11T03:52:12.9208157Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9208250Z - configuration_deepseek.py
2025-04-11T03:52:12.9208590Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9209381Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9209464Z   warnings.warn(
2025-04-11T03:52:12.9209732Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9209825Z - configuration_deepseek.py
2025-04-11T03:52:12.9210213Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9211006Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9211085Z   warnings.warn(
2025-04-11T03:52:12.9211861Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9212009Z   warnings.warn(
2025-04-11T03:52:12.9212795Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9212936Z   warnings.warn(
2025-04-11T03:52:12.9213719Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9213802Z   warnings.warn(
2025-04-11T03:52:12.9214582Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9214667Z   warnings.warn(
2025-04-11T03:52:12.9215453Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9215538Z   warnings.warn(
2025-04-11T03:52:12.9215855Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9215947Z - modeling_deepseek.py
2025-04-11T03:52:12.9216284Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9216557Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9216644Z - modeling_deepseek.py
2025-04-11T03:52:12.9216981Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9217250Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9217335Z - modeling_deepseek.py
2025-04-11T03:52:12.9217670Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9217935Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9218023Z - modeling_deepseek.py
2025-04-11T03:52:12.9218351Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9219219Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9219302Z   warnings.warn(
2025-04-11T03:52:12.9220091Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9220170Z   warnings.warn(
2025-04-11T03:52:12.9220955Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9221186Z   warnings.warn(
2025-04-11T03:52:12.9221328Z _____________________________ test_deepseek_v3[4] ______________________________
2025-04-11T03:52:12.9221332Z 
2025-04-11T03:52:12.9221456Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9222062Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9222120Z 
2025-04-11T03:52:12.9222230Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9222314Z         try_count = 0
2025-04-11T03:52:12.9222421Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9222503Z             max_try, int
2025-04-11T03:52:12.9222654Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9222733Z     
2025-04-11T03:52:12.9222844Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9222924Z             try:
2025-04-11T03:52:12.9223010Z                 try_count += 1
2025-04-11T03:52:12.9223107Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9223187Z                 return ret
2025-04-11T03:52:12.9223282Z             except exception_type as e:
2025-04-11T03:52:12.9223391Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9223638Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9223762Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9223912Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9224071Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9224157Z                     continue
2025-04-11T03:52:12.9224234Z                 else:
2025-04-11T03:52:12.9224455Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9224534Z >                   raise e
2025-04-11T03:52:12.9224539Z 
2025-04-11T03:52:12.9224637Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9224748Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9224885Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9224975Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9225184Z tests/test_shardformer/test_model/test_shard_deepseek_v3.py:100: in test_deepseek_v3
2025-04-11T03:52:12.9225287Z     spawn(check_deepseek_v3, world_size)
2025-04-11T03:52:12.9225390Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9225495Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9225755Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9225935Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9226274Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9226367Z     while not context.join():
2025-04-11T03:52:12.9226478Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9226484Z 
2025-04-11T03:52:12.9226684Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ec753f0>
2025-04-11T03:52:12.9226765Z timeout = None
2025-04-11T03:52:12.9226769Z 
2025-04-11T03:52:12.9226863Z     def join(self, timeout=None):
2025-04-11T03:52:12.9226995Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9227065Z     
2025-04-11T03:52:12.9227209Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9227417Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9227580Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9227675Z         of the first process exiting.
2025-04-11T03:52:12.9227750Z     
2025-04-11T03:52:12.9227902Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9228041Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9228168Z     
2025-04-11T03:52:12.9228249Z         Args:
2025-04-11T03:52:12.9228388Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9228503Z         """
2025-04-11T03:52:12.9228643Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9228738Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9228824Z             return True
2025-04-11T03:52:12.9228894Z     
2025-04-11T03:52:12.9229028Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9229147Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9229240Z             self.sentinels.keys(),
2025-04-11T03:52:12.9229329Z             timeout=timeout,
2025-04-11T03:52:12.9229402Z         )
2025-04-11T03:52:12.9229478Z     
2025-04-11T03:52:12.9229561Z         error_index = None
2025-04-11T03:52:12.9229652Z         for sentinel in ready:
2025-04-11T03:52:12.9229759Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9229861Z             process = self.processes[index]
2025-04-11T03:52:12.9229949Z             process.join()
2025-04-11T03:52:12.9230101Z             if process.exitcode != 0:
2025-04-11T03:52:12.9230196Z                 error_index = index
2025-04-11T03:52:12.9230273Z                 break
2025-04-11T03:52:12.9230342Z     
2025-04-11T03:52:12.9230440Z         # Return if there was no error.
2025-04-11T03:52:12.9230523Z         if error_index is None:
2025-04-11T03:52:12.9230664Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9230759Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9230829Z     
2025-04-11T03:52:12.9230973Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9231071Z         for process in self.processes:
2025-04-11T03:52:12.9231165Z             if process.is_alive():
2025-04-11T03:52:12.9231257Z                 process.terminate()
2025-04-11T03:52:12.9231347Z             process.join()
2025-04-11T03:52:12.9231416Z     
2025-04-11T03:52:12.9231555Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9231676Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9231785Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9231908Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9231994Z             if exitcode < 0:
2025-04-11T03:52:12.9232103Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9232215Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9232366Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9232465Z                     error_index=error_index,
2025-04-11T03:52:12.9232625Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9232719Z                     exit_code=exitcode,
2025-04-11T03:52:12.9232810Z                     signal_name=name,
2025-04-11T03:52:12.9232887Z                 )
2025-04-11T03:52:12.9232965Z             else:
2025-04-11T03:52:12.9233069Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9233235Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9233328Z                     error_index=error_index,
2025-04-11T03:52:12.9233432Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9233518Z                     exit_code=exitcode,
2025-04-11T03:52:12.9233658Z                 )
2025-04-11T03:52:12.9233732Z     
2025-04-11T03:52:12.9233866Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9234042Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9234127Z         msg += original_trace
2025-04-11T03:52:12.9234300Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9234467Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9234657Z E       
2025-04-11T03:52:12.9234796Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9234898Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9235197Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9235277Z E           fn(i, *args)
2025-04-11T03:52:12.9235594Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 93, in check_deepseek_v3
2025-04-11T03:52:12.9235691Z E           run_deepseek_v3_test()
2025-04-11T03:52:12.9235948Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9236044Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9236359Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 82, in run_deepseek_v3_test
2025-04-11T03:52:12.9236455Z E           check_forward_backward(
2025-04-11T03:52:12.9236824Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 29, in check_forward_backward
2025-04-11T03:52:12.9237106Z E           org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
2025-04-11T03:52:12.9237407Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 138, in build_model_from_hybrid_plugin
2025-04-11T03:52:12.9237505Z E           org_model = org_model.cuda()
2025-04-11T03:52:12.9237796Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9237901Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9238173Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9238295Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9238570Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9238661Z E           module._apply(fn)
2025-04-11T03:52:12.9238928Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9239021Z E           module._apply(fn)
2025-04-11T03:52:12.9239289Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9239389Z E           param_applied = fn(param)
2025-04-11T03:52:12.9239727Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9239853Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9239964Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9240254Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9240395Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9240555Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9240560Z 
2025-04-11T03:52:12.9240863Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9241078Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9241238Z [04/11/25 03:49:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9241368Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9241481Z                              :75 launch                                         
2025-04-11T03:52:12.9241619Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9241804Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9242004Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9242150Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9243278Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9243450Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9244607Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9244781Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9245892Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9246055Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9247153Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9247312Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9248066Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9248152Z   warnings.warn(
2025-04-11T03:52:12.9248827Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9248909Z   warnings.warn(
2025-04-11T03:52:12.9249578Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9249724Z   warnings.warn(
2025-04-11T03:52:12.9250381Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9250461Z   warnings.warn(
2025-04-11T03:52:12.9251277Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9251413Z   warnings.warn(
2025-04-11T03:52:12.9252224Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9252305Z   warnings.warn(
2025-04-11T03:52:12.9253115Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9253193Z   warnings.warn(
2025-04-11T03:52:12.9254050Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9254132Z   warnings.warn(
2025-04-11T03:52:12.9254911Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9254991Z   warnings.warn(
2025-04-11T03:52:12.9255774Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9255860Z   warnings.warn(
2025-04-11T03:52:12.9256639Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9256722Z   warnings.warn(
2025-04-11T03:52:12.9257553Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9257637Z   warnings.warn(
2025-04-11T03:52:12.9258420Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9258504Z   warnings.warn(
2025-04-11T03:52:12.9259281Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9259428Z   warnings.warn(
2025-04-11T03:52:12.9260211Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9260352Z   warnings.warn(
2025-04-11T03:52:12.9261134Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9261215Z   warnings.warn(
2025-04-11T03:52:12.9261511Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:44807 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9261796Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:44807 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9262583Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9262665Z   warnings.warn(
2025-04-11T03:52:12.9263495Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9263580Z   warnings.warn(
2025-04-11T03:52:12.9264386Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9264465Z   warnings.warn(
2025-04-11T03:52:12.9265262Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9265342Z   warnings.warn(
2025-04-11T03:52:12.9266145Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9266222Z   warnings.warn(
2025-04-11T03:52:12.9267077Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9267158Z   warnings.warn(
2025-04-11T03:52:12.9267943Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9268025Z   warnings.warn(
2025-04-11T03:52:12.9268852Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9269013Z   warnings.warn(
2025-04-11T03:52:12.9269803Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9269944Z   warnings.warn(
2025-04-11T03:52:12.9270733Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9270814Z   warnings.warn(
2025-04-11T03:52:12.9271078Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9271173Z - configuration_deepseek.py
2025-04-11T03:52:12.9271517Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9272370Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9272455Z   warnings.warn(
2025-04-11T03:52:12.9272708Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9272804Z - configuration_deepseek.py
2025-04-11T03:52:12.9273150Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9273967Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9274047Z   warnings.warn(
2025-04-11T03:52:12.9274296Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9274390Z - configuration_deepseek.py
2025-04-11T03:52:12.9274726Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9275533Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9275618Z   warnings.warn(
2025-04-11T03:52:12.9275921Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9276017Z - configuration_deepseek.py
2025-04-11T03:52:12.9276345Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9277134Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9277212Z   warnings.warn(
2025-04-11T03:52:12.9277999Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9278146Z   warnings.warn(
2025-04-11T03:52:12.9278921Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9279058Z   warnings.warn(
2025-04-11T03:52:12.9279841Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9279920Z   warnings.warn(
2025-04-11T03:52:12.9280698Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9280778Z   warnings.warn(
2025-04-11T03:52:12.9281632Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9281715Z   warnings.warn(
2025-04-11T03:52:12.9281960Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9282051Z - modeling_deepseek.py
2025-04-11T03:52:12.9282388Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9282633Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9282720Z - modeling_deepseek.py
2025-04-11T03:52:12.9283055Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9283295Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9283387Z - modeling_deepseek.py
2025-04-11T03:52:12.9283721Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9283957Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9284052Z - modeling_deepseek.py
2025-04-11T03:52:12.9284384Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9284586Z _________________________________ test_falcon __________________________________
2025-04-11T03:52:12.9284590Z 
2025-04-11T03:52:12.9284689Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9284694Z 
2025-04-11T03:52:12.9284805Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9284890Z         try_count = 0
2025-04-11T03:52:12.9284997Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9285088Z             max_try, int
2025-04-11T03:52:12.9285238Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9285317Z     
2025-04-11T03:52:12.9285435Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9285516Z             try:
2025-04-11T03:52:12.9285667Z                 try_count += 1
2025-04-11T03:52:12.9285761Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9285765Z 
2025-04-11T03:52:12.9285867Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9285983Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9286107Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9286205Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9286368Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9286520Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9286635Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9286639Z 
2025-04-11T03:52:12.9286723Z device = None
2025-04-11T03:52:12.9286727Z 
2025-04-11T03:52:12.9286849Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9287007Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9287079Z     
2025-04-11T03:52:12.9287157Z         Args:
2025-04-11T03:52:12.9287328Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9287500Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9287615Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9287688Z         """
2025-04-11T03:52:12.9287768Z         _lazy_init()
2025-04-11T03:52:12.9287868Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9287972Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9288136Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9288434Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9288577Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9288738Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9288744Z 
2025-04-11T03:52:12.9288992Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9289127Z __________________________________ test_gpt2 ___________________________________
2025-04-11T03:52:12.9289131Z 
2025-04-11T03:52:12.9289229Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9289233Z 
2025-04-11T03:52:12.9289335Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9289416Z         try_count = 0
2025-04-11T03:52:12.9289520Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9289605Z             max_try, int
2025-04-11T03:52:12.9289752Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9289822Z     
2025-04-11T03:52:12.9289938Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9290014Z             try:
2025-04-11T03:52:12.9290099Z                 try_count += 1
2025-04-11T03:52:12.9290196Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9290200Z 
2025-04-11T03:52:12.9290294Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9290405Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9290580Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9290676Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9290834Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9290931Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9291045Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9291050Z 
2025-04-11T03:52:12.9291127Z device = None
2025-04-11T03:52:12.9291131Z 
2025-04-11T03:52:12.9291252Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9291404Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9291477Z     
2025-04-11T03:52:12.9291613Z         Args:
2025-04-11T03:52:12.9291781Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9291954Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9292064Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9292140Z         """
2025-04-11T03:52:12.9292218Z         _lazy_init()
2025-04-11T03:52:12.9292313Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9292478Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9292585Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9292875Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9293015Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9293174Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9293181Z 
2025-04-11T03:52:12.9293420Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9293553Z __________________________________ test_llama __________________________________
2025-04-11T03:52:12.9293558Z 
2025-04-11T03:52:12.9293650Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9293654Z 
2025-04-11T03:52:12.9293755Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9293841Z         try_count = 0
2025-04-11T03:52:12.9293939Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9294023Z             max_try, int
2025-04-11T03:52:12.9294223Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9294302Z     
2025-04-11T03:52:12.9294413Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9294491Z             try:
2025-04-11T03:52:12.9294578Z                 try_count += 1
2025-04-11T03:52:12.9294672Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9294677Z 
2025-04-11T03:52:12.9294773Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9294881Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9294998Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9295095Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9295249Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9295348Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9295456Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9295461Z 
2025-04-11T03:52:12.9295542Z device = None
2025-04-11T03:52:12.9295546Z 
2025-04-11T03:52:12.9295662Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9295817Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9295889Z     
2025-04-11T03:52:12.9295962Z         Args:
2025-04-11T03:52:12.9296137Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9296303Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9296486Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9296563Z         """
2025-04-11T03:52:12.9296643Z         _lazy_init()
2025-04-11T03:52:12.9296740Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9296842Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9296951Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9297237Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9297378Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9297536Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9297599Z 
2025-04-11T03:52:12.9297837Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9297970Z _________________________________ test_mistral _________________________________
2025-04-11T03:52:12.9297974Z 
2025-04-11T03:52:12.9298067Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9298072Z 
2025-04-11T03:52:12.9298175Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9298253Z         try_count = 0
2025-04-11T03:52:12.9298416Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9298496Z             max_try, int
2025-04-11T03:52:12.9298643Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9298713Z     
2025-04-11T03:52:12.9298825Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9298905Z             try:
2025-04-11T03:52:12.9298987Z                 try_count += 1
2025-04-11T03:52:12.9299081Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9299086Z 
2025-04-11T03:52:12.9299180Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9299286Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9299405Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9299500Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9299656Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9299750Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9299863Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9299867Z 
2025-04-11T03:52:12.9299943Z device = None
2025-04-11T03:52:12.9300003Z 
2025-04-11T03:52:12.9300122Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9300277Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9300346Z     
2025-04-11T03:52:12.9300426Z         Args:
2025-04-11T03:52:12.9300592Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9300760Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9300864Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9300937Z         """
2025-04-11T03:52:12.9301023Z         _lazy_init()
2025-04-11T03:52:12.9301120Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9301224Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9301332Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9301614Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9301752Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9301906Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9301910Z 
2025-04-11T03:52:12.9302152Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9302283Z _______________________________ test_mixtral[4] ________________________________
2025-04-11T03:52:12.9302287Z 
2025-04-11T03:52:12.9302405Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9303062Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9303069Z 
2025-04-11T03:52:12.9303176Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9303257Z         try_count = 0
2025-04-11T03:52:12.9303360Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9303440Z             max_try, int
2025-04-11T03:52:12.9303583Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9303657Z     
2025-04-11T03:52:12.9303831Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9303908Z             try:
2025-04-11T03:52:12.9303991Z                 try_count += 1
2025-04-11T03:52:12.9304081Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9304165Z                 return ret
2025-04-11T03:52:12.9304261Z             except exception_type as e:
2025-04-11T03:52:12.9304362Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9304548Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9304724Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9304873Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9305026Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9305112Z                     continue
2025-04-11T03:52:12.9305190Z                 else:
2025-04-11T03:52:12.9305415Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9305496Z >                   raise e
2025-04-11T03:52:12.9305500Z 
2025-04-11T03:52:12.9305597Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9305709Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9305841Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9305932Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9306118Z tests/test_shardformer/test_model/test_shard_mixtral.py:222: in test_mixtral
2025-04-11T03:52:12.9306268Z     spawn(check_mixtral, world_size)
2025-04-11T03:52:12.9306372Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9306477Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9306736Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9306914Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9307210Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9307300Z     while not context.join():
2025-04-11T03:52:12.9307415Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9307420Z 
2025-04-11T03:52:12.9307620Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ec76cb0>
2025-04-11T03:52:12.9307704Z timeout = None
2025-04-11T03:52:12.9307708Z 
2025-04-11T03:52:12.9307797Z     def join(self, timeout=None):
2025-04-11T03:52:12.9307931Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9308002Z     
2025-04-11T03:52:12.9308148Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9308297Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9308492Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9308593Z         of the first process exiting.
2025-04-11T03:52:12.9308663Z     
2025-04-11T03:52:12.9308811Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9309014Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9309088Z     
2025-04-11T03:52:12.9309169Z         Args:
2025-04-11T03:52:12.9309309Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9309389Z         """
2025-04-11T03:52:12.9309529Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9309622Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9309710Z             return True
2025-04-11T03:52:12.9309779Z     
2025-04-11T03:52:12.9309919Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9310036Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9310128Z             self.sentinels.keys(),
2025-04-11T03:52:12.9310291Z             timeout=timeout,
2025-04-11T03:52:12.9310364Z         )
2025-04-11T03:52:12.9310436Z     
2025-04-11T03:52:12.9310521Z         error_index = None
2025-04-11T03:52:12.9310607Z         for sentinel in ready:
2025-04-11T03:52:12.9310720Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9310818Z             process = self.processes[index]
2025-04-11T03:52:12.9310908Z             process.join()
2025-04-11T03:52:12.9311005Z             if process.exitcode != 0:
2025-04-11T03:52:12.9311160Z                 error_index = index
2025-04-11T03:52:12.9311236Z                 break
2025-04-11T03:52:12.9311310Z     
2025-04-11T03:52:12.9311406Z         # Return if there was no error.
2025-04-11T03:52:12.9311492Z         if error_index is None:
2025-04-11T03:52:12.9311631Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9311727Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9311800Z     
2025-04-11T03:52:12.9311947Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9312044Z         for process in self.processes:
2025-04-11T03:52:12.9312136Z             if process.is_alive():
2025-04-11T03:52:12.9312228Z                 process.terminate()
2025-04-11T03:52:12.9312316Z             process.join()
2025-04-11T03:52:12.9312389Z     
2025-04-11T03:52:12.9312530Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9312652Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9312759Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9312949Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9313038Z             if exitcode < 0:
2025-04-11T03:52:12.9313148Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9313261Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9313412Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9313513Z                     error_index=error_index,
2025-04-11T03:52:12.9313614Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9313707Z                     exit_code=exitcode,
2025-04-11T03:52:12.9313796Z                     signal_name=name,
2025-04-11T03:52:12.9313870Z                 )
2025-04-11T03:52:12.9313952Z             else:
2025-04-11T03:52:12.9314056Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9314227Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9314321Z                     error_index=error_index,
2025-04-11T03:52:12.9314422Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9314512Z                     exit_code=exitcode,
2025-04-11T03:52:12.9314583Z                 )
2025-04-11T03:52:12.9314658Z     
2025-04-11T03:52:12.9314790Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9314964Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9315051Z         msg += original_trace
2025-04-11T03:52:12.9315226Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9315446Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9315525Z E       
2025-04-11T03:52:12.9315658Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9315758Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9316060Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9316144Z E           fn(i, *args)
2025-04-11T03:52:12.9316440Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 210, in check_mixtral
2025-04-11T03:52:12.9316530Z E           run_mixtral_test()
2025-04-11T03:52:12.9316786Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9316945Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9317247Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 180, in run_mixtral_test
2025-04-11T03:52:12.9317344Z E           run_mixtral_commom(config)
2025-04-11T03:52:12.9317642Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 70, in run_mixtral_commom
2025-04-11T03:52:12.9317834Z E           torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T03:52:12.9318128Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9318229Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9318496Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9318615Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9318888Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9318976Z E           module._apply(fn)
2025-04-11T03:52:12.9319243Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9319341Z E           param_applied = fn(param)
2025-04-11T03:52:12.9319616Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9319805Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9319919Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9320205Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9320341Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9320509Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9320513Z 
2025-04-11T03:52:12.9320815Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9320976Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9321137Z [04/11/25 03:49:48] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9321269Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9321387Z                              :75 launch                                         
2025-04-11T03:52:12.9321524Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9321655Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9321847Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9321994Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9322344Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25164 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9322628Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25164 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9323739Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.9324816Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.9325951Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.9327070Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.9327206Z ________________________________ test_OPTModel _________________________________
2025-04-11T03:52:12.9327212Z 
2025-04-11T03:52:12.9327310Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9327313Z 
2025-04-11T03:52:12.9327469Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9327555Z         try_count = 0
2025-04-11T03:52:12.9327655Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9327743Z             max_try, int
2025-04-11T03:52:12.9327891Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9327961Z     
2025-04-11T03:52:12.9328079Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9328154Z             try:
2025-04-11T03:52:12.9328243Z                 try_count += 1
2025-04-11T03:52:12.9328332Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9328336Z 
2025-04-11T03:52:12.9328433Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9328548Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9328663Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9328762Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9328918Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9329017Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9329124Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9329128Z 
2025-04-11T03:52:12.9329204Z device = None
2025-04-11T03:52:12.9329208Z 
2025-04-11T03:52:12.9329331Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9329486Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9329560Z     
2025-04-11T03:52:12.9329635Z         Args:
2025-04-11T03:52:12.9329865Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9330037Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9330143Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9330223Z         """
2025-04-11T03:52:12.9330302Z         _lazy_init()
2025-04-11T03:52:12.9330405Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9330506Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9330614Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9330903Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9331109Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9331272Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9331276Z 
2025-04-11T03:52:12.9331523Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9331658Z __________________________________ test_qwen2 __________________________________
2025-04-11T03:52:12.9331662Z 
2025-04-11T03:52:12.9331754Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9331820Z 
2025-04-11T03:52:12.9331927Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9332008Z         try_count = 0
2025-04-11T03:52:12.9332108Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9332193Z             max_try, int
2025-04-11T03:52:12.9332335Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9332410Z     
2025-04-11T03:52:12.9332519Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9332599Z             try:
2025-04-11T03:52:12.9332681Z                 try_count += 1
2025-04-11T03:52:12.9332770Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9332774Z 
2025-04-11T03:52:12.9332874Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9332985Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9333103Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9333197Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9333355Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9333506Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9333617Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9333621Z 
2025-04-11T03:52:12.9333703Z device = None
2025-04-11T03:52:12.9333707Z 
2025-04-11T03:52:12.9333826Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9333984Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9334057Z     
2025-04-11T03:52:12.9334137Z         Args:
2025-04-11T03:52:12.9334303Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9334470Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9334580Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9334652Z         """
2025-04-11T03:52:12.9334735Z         _lazy_init()
2025-04-11T03:52:12.9334831Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9334935Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9335044Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9335390Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9335532Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9335691Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9335696Z 
2025-04-11T03:52:12.9335944Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9336214Z ___________________________________ test_sam ___________________________________
2025-04-11T03:52:12.9336219Z 
2025-04-11T03:52:12.9336318Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9336325Z 
2025-04-11T03:52:12.9336423Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9336505Z         try_count = 0
2025-04-11T03:52:12.9336610Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9336692Z             max_try, int
2025-04-11T03:52:12.9336841Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9336913Z     
2025-04-11T03:52:12.9337026Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9337163Z             try:
2025-04-11T03:52:12.9337247Z                 try_count += 1
2025-04-11T03:52:12.9337341Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9337345Z 
2025-04-11T03:52:12.9337442Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9337557Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9337671Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9337762Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9337975Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9338071Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9338185Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9338189Z 
2025-04-11T03:52:12.9338268Z device = None
2025-04-11T03:52:12.9338272Z 
2025-04-11T03:52:12.9338391Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9338541Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9338618Z     
2025-04-11T03:52:12.9338692Z         Args:
2025-04-11T03:52:12.9338856Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9339026Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9339133Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9339209Z         """
2025-04-11T03:52:12.9339288Z         _lazy_init()
2025-04-11T03:52:12.9339387Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9339490Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9339650Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9339937Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9340075Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9340241Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9340245Z 
2025-04-11T03:52:12.9340482Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9340614Z ___________________________________ test_t5 ____________________________________
2025-04-11T03:52:12.9340617Z 
2025-04-11T03:52:12.9340709Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9340713Z 
2025-04-11T03:52:12.9340816Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9340899Z         try_count = 0
2025-04-11T03:52:12.9341000Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9341082Z             max_try, int
2025-04-11T03:52:12.9341223Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9341296Z     
2025-04-11T03:52:12.9341407Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9341484Z             try:
2025-04-11T03:52:12.9341573Z                 try_count += 1
2025-04-11T03:52:12.9341664Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9341668Z 
2025-04-11T03:52:12.9341765Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9341925Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9342043Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9342138Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9342294Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9342393Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9342500Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9342504Z 
2025-04-11T03:52:12.9342587Z device = None
2025-04-11T03:52:12.9342591Z 
2025-04-11T03:52:12.9342706Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9342857Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9342995Z     
2025-04-11T03:52:12.9343069Z         Args:
2025-04-11T03:52:12.9343239Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9343406Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9343519Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9343591Z         """
2025-04-11T03:52:12.9343669Z         _lazy_init()
2025-04-11T03:52:12.9343852Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9343954Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9344063Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9344343Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9344481Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9344638Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9344644Z 
2025-04-11T03:52:12.9344879Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9345011Z ___________________________________ test_vit ___________________________________
2025-04-11T03:52:12.9345014Z 
2025-04-11T03:52:12.9345106Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9345110Z 
2025-04-11T03:52:12.9345213Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9345295Z         try_count = 0
2025-04-11T03:52:12.9345396Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9345529Z             max_try, int
2025-04-11T03:52:12.9345673Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9345750Z     
2025-04-11T03:52:12.9345858Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9345939Z             try:
2025-04-11T03:52:12.9346022Z                 try_count += 1
2025-04-11T03:52:12.9346119Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9346123Z 
2025-04-11T03:52:12.9346219Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9346325Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9346444Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9346534Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9346694Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9346788Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9346900Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9346904Z 
2025-04-11T03:52:12.9346980Z device = None
2025-04-11T03:52:12.9346984Z 
2025-04-11T03:52:12.9347103Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9347255Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9347325Z     
2025-04-11T03:52:12.9347402Z         Args:
2025-04-11T03:52:12.9347568Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9347735Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9347897Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9347972Z         """
2025-04-11T03:52:12.9348056Z         _lazy_init()
2025-04-11T03:52:12.9348150Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9348256Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9348361Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9348698Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9348835Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9348991Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9349062Z 
2025-04-11T03:52:12.9349311Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9349446Z _________________________________ test_whisper _________________________________
2025-04-11T03:52:12.9349450Z 
2025-04-11T03:52:12.9349553Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9349557Z 
2025-04-11T03:52:12.9349661Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9349757Z         try_count = 0
2025-04-11T03:52:12.9349921Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9350007Z             max_try, int
2025-04-11T03:52:12.9350160Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9350234Z     
2025-04-11T03:52:12.9350353Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9350431Z             try:
2025-04-11T03:52:12.9350521Z                 try_count += 1
2025-04-11T03:52:12.9350615Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9350621Z 
2025-04-11T03:52:12.9350714Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9350832Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9350946Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9351050Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9351212Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9351313Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9351428Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9351432Z 
2025-04-11T03:52:12.9351566Z device = None
2025-04-11T03:52:12.9351571Z 
2025-04-11T03:52:12.9351701Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9351850Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9351926Z     
2025-04-11T03:52:12.9352001Z         Args:
2025-04-11T03:52:12.9352174Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9352339Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9352445Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9352530Z         """
2025-04-11T03:52:12.9352609Z         _lazy_init()
2025-04-11T03:52:12.9352712Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9352817Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9352925Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9353211Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9353347Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9353508Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9353512Z 
2025-04-11T03:52:12.9353750Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9353885Z ________________________________ test_comm_spec ________________________________
2025-04-11T03:52:12.9353889Z 
2025-04-11T03:52:12.9354048Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9354665Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9354672Z 
2025-04-11T03:52:12.9354775Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9354859Z         try_count = 0
2025-04-11T03:52:12.9354964Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9355047Z             max_try, int
2025-04-11T03:52:12.9355196Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9355267Z     
2025-04-11T03:52:12.9355446Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9355520Z             try:
2025-04-11T03:52:12.9355602Z                 try_count += 1
2025-04-11T03:52:12.9355697Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9355780Z                 return ret
2025-04-11T03:52:12.9355879Z             except exception_type as e:
2025-04-11T03:52:12.9355982Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9356176Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9356353Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9356503Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9356659Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9356742Z                     continue
2025-04-11T03:52:12.9356823Z                 else:
2025-04-11T03:52:12.9357051Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9357135Z >                   raise e
2025-04-11T03:52:12.9357139Z 
2025-04-11T03:52:12.9357232Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9357348Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9357483Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9357571Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9357732Z tests/test_tensor/test_comm_spec_apply.py:211: in test_comm_spec
2025-04-11T03:52:12.9357882Z     spawn(check_comm, world_size)
2025-04-11T03:52:12.9357993Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9358093Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9358353Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9358534Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9358825Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9358921Z     while not context.join():
2025-04-11T03:52:12.9359030Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9359034Z 
2025-04-11T03:52:12.9359244Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfaef0>
2025-04-11T03:52:12.9359327Z timeout = None
2025-04-11T03:52:12.9359331Z 
2025-04-11T03:52:12.9359419Z     def join(self, timeout=None):
2025-04-11T03:52:12.9359550Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9359622Z     
2025-04-11T03:52:12.9359775Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9359918Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9360083Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9360179Z         of the first process exiting.
2025-04-11T03:52:12.9360254Z     
2025-04-11T03:52:12.9360409Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9360606Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9360683Z     
2025-04-11T03:52:12.9360758Z         Args:
2025-04-11T03:52:12.9360898Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9360975Z         """
2025-04-11T03:52:12.9361114Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9361213Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9361298Z             return True
2025-04-11T03:52:12.9361373Z     
2025-04-11T03:52:12.9361508Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9361626Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9361723Z             self.sentinels.keys(),
2025-04-11T03:52:12.9361877Z             timeout=timeout,
2025-04-11T03:52:12.9361954Z         )
2025-04-11T03:52:12.9362025Z     
2025-04-11T03:52:12.9362108Z         error_index = None
2025-04-11T03:52:12.9362202Z         for sentinel in ready:
2025-04-11T03:52:12.9362312Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9362417Z             process = self.processes[index]
2025-04-11T03:52:12.9362502Z             process.join()
2025-04-11T03:52:12.9362596Z             if process.exitcode != 0:
2025-04-11T03:52:12.9362747Z                 error_index = index
2025-04-11T03:52:12.9362823Z                 break
2025-04-11T03:52:12.9362900Z     
2025-04-11T03:52:12.9362994Z         # Return if there was no error.
2025-04-11T03:52:12.9363084Z         if error_index is None:
2025-04-11T03:52:12.9363220Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9363316Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9363389Z     
2025-04-11T03:52:12.9363530Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9363629Z         for process in self.processes:
2025-04-11T03:52:12.9363715Z             if process.is_alive():
2025-04-11T03:52:12.9363805Z                 process.terminate()
2025-04-11T03:52:12.9363897Z             process.join()
2025-04-11T03:52:12.9363966Z     
2025-04-11T03:52:12.9364111Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9364227Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9364339Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9364512Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9364599Z             if exitcode < 0:
2025-04-11T03:52:12.9364710Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9364816Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9364972Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9365069Z                     error_index=error_index,
2025-04-11T03:52:12.9365172Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9365263Z                     exit_code=exitcode,
2025-04-11T03:52:12.9365355Z                     signal_name=name,
2025-04-11T03:52:12.9365434Z                 )
2025-04-11T03:52:12.9365510Z             else:
2025-04-11T03:52:12.9365618Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9365784Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9365882Z                     error_index=error_index,
2025-04-11T03:52:12.9365986Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9366072Z                     exit_code=exitcode,
2025-04-11T03:52:12.9366151Z                 )
2025-04-11T03:52:12.9366223Z     
2025-04-11T03:52:12.9366357Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9366534Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9366621Z         msg += original_trace
2025-04-11T03:52:12.9366798Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9367017Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9367098Z E       
2025-04-11T03:52:12.9367224Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9367325Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9367639Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9367722Z E           fn(i, *args)
2025-04-11T03:52:12.9367981Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 191, in check_comm
2025-04-11T03:52:12.9368085Z E           check_all_gather(device_mesh, rank)
2025-04-11T03:52:12.9368349Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 16, in check_all_gather
2025-04-11T03:52:12.9368543Z E           sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T03:52:12.9368650Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9368946Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9369082Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9369301Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9369306Z 
2025-04-11T03:52:12.9369613Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9369773Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9369927Z [04/11/25 03:49:54] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9370063Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9370169Z                              :75 launch                                         
2025-04-11T03:52:12.9370309Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9370442Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9370638Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9370786Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9371128Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56178 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9371417Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56178 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9371553Z ______________________________ test_padded_tensor ______________________________
2025-04-11T03:52:12.9371559Z 
2025-04-11T03:52:12.9371657Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9372251Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9372255Z 
2025-04-11T03:52:12.9372360Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9372442Z         try_count = 0
2025-04-11T03:52:12.9372544Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9372636Z             max_try, int
2025-04-11T03:52:12.9372784Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9372859Z     
2025-04-11T03:52:12.9372969Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9373045Z             try:
2025-04-11T03:52:12.9373134Z                 try_count += 1
2025-04-11T03:52:12.9373226Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9373320Z                 return ret
2025-04-11T03:52:12.9373414Z             except exception_type as e:
2025-04-11T03:52:12.9373518Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9373755Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9373875Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9374033Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9374188Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9374277Z                     continue
2025-04-11T03:52:12.9374354Z                 else:
2025-04-11T03:52:12.9374581Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9374722Z >                   raise e
2025-04-11T03:52:12.9374726Z 
2025-04-11T03:52:12.9374824Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9374942Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9375077Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9375167Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9375330Z tests/test_tensor/test_padded_tensor.py:42: in test_padded_tensor
2025-04-11T03:52:12.9375441Z     spawn(check_padded_tensor, world_size)
2025-04-11T03:52:12.9375598Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9375699Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9375963Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9376142Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9376430Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9376523Z     while not context.join():
2025-04-11T03:52:12.9376636Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9376640Z 
2025-04-11T03:52:12.9376843Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be337ee0>
2025-04-11T03:52:12.9376926Z timeout = None
2025-04-11T03:52:12.9376930Z 
2025-04-11T03:52:12.9377027Z     def join(self, timeout=None):
2025-04-11T03:52:12.9377155Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9377232Z     
2025-04-11T03:52:12.9377436Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9377590Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9377750Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9377848Z         of the first process exiting.
2025-04-11T03:52:12.9377925Z     
2025-04-11T03:52:12.9378076Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9378224Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9378295Z     
2025-04-11T03:52:12.9378372Z         Args:
2025-04-11T03:52:12.9378518Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9378597Z         """
2025-04-11T03:52:12.9378744Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9378839Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9378925Z             return True
2025-04-11T03:52:12.9378997Z     
2025-04-11T03:52:12.9379133Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9384688Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9384830Z             self.sentinels.keys(),
2025-04-11T03:52:12.9384933Z             timeout=timeout,
2025-04-11T03:52:12.9385015Z         )
2025-04-11T03:52:12.9385110Z     
2025-04-11T03:52:12.9385227Z         error_index = None
2025-04-11T03:52:12.9385351Z         for sentinel in ready:
2025-04-11T03:52:12.9385475Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9385585Z             process = self.processes[index]
2025-04-11T03:52:12.9385755Z             process.join()
2025-04-11T03:52:12.9385862Z             if process.exitcode != 0:
2025-04-11T03:52:12.9385955Z                 error_index = index
2025-04-11T03:52:12.9386040Z                 break
2025-04-11T03:52:12.9386116Z     
2025-04-11T03:52:12.9386217Z         # Return if there was no error.
2025-04-11T03:52:12.9386304Z         if error_index is None:
2025-04-11T03:52:12.9386456Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9386557Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9386631Z     
2025-04-11T03:52:12.9386783Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9386885Z         for process in self.processes:
2025-04-11T03:52:12.9387048Z             if process.is_alive():
2025-04-11T03:52:12.9387142Z                 process.terminate()
2025-04-11T03:52:12.9387229Z             process.join()
2025-04-11T03:52:12.9387308Z     
2025-04-11T03:52:12.9387459Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9387588Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9387699Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9387886Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9387977Z             if exitcode < 0:
2025-04-11T03:52:12.9388092Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9388209Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9388376Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9388529Z                     error_index=error_index,
2025-04-11T03:52:12.9388639Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9388735Z                     exit_code=exitcode,
2025-04-11T03:52:12.9388835Z                     signal_name=name,
2025-04-11T03:52:12.9388914Z                 )
2025-04-11T03:52:12.9388995Z             else:
2025-04-11T03:52:12.9389114Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9389287Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9389384Z                     error_index=error_index,
2025-04-11T03:52:12.9389493Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9389651Z                     exit_code=exitcode,
2025-04-11T03:52:12.9389731Z                 )
2025-04-11T03:52:12.9389808Z     
2025-04-11T03:52:12.9389944Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9390118Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9390211Z         msg += original_trace
2025-04-11T03:52:12.9390394Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9390563Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9390642Z E       
2025-04-11T03:52:12.9390779Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9390880Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9391198Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9391288Z E           fn(i, *args)
2025-04-11T03:52:12.9391558Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_padded_tensor.py", line 14, in check_padded_tensor
2025-04-11T03:52:12.9391685Z E           original_tensor = torch.rand(32, 64).to("cuda")
2025-04-11T03:52:12.9391793Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9392089Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9392231Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9392398Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9392409Z 
2025-04-11T03:52:12.9392795Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9392961Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9393131Z [04/11/25 03:49:59] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9393267Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9393387Z                              :75 launch                                         
2025-04-11T03:52:12.9393528Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9393659Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9393929Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9394065Z __________________________________ test_apply __________________________________
2025-04-11T03:52:12.9394069Z 
2025-04-11T03:52:12.9394172Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9394782Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9394851Z 
2025-04-11T03:52:12.9394966Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9395050Z         try_count = 0
2025-04-11T03:52:12.9395158Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9395244Z             max_try, int
2025-04-11T03:52:12.9395398Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9395474Z     
2025-04-11T03:52:12.9395589Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9395675Z             try:
2025-04-11T03:52:12.9395761Z                 try_count += 1
2025-04-11T03:52:12.9395860Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9395944Z                 return ret
2025-04-11T03:52:12.9396042Z             except exception_type as e:
2025-04-11T03:52:12.9396148Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9396341Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9396517Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9396670Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9396831Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9396914Z                     continue
2025-04-11T03:52:12.9396994Z                 else:
2025-04-11T03:52:12.9397222Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9397306Z >                   raise e
2025-04-11T03:52:12.9397310Z 
2025-04-11T03:52:12.9397417Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9397533Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9397671Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9397761Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9418520Z tests/test_tensor/test_shape_consistency_apply.py:72: in test_apply
2025-04-11T03:52:12.9418963Z     spawn(check_apply, world_size)
2025-04-11T03:52:12.9419229Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9419499Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9419927Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9420447Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9420990Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9421510Z     while not context.join():
2025-04-11T03:52:12.9421777Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9421962Z 
2025-04-11T03:52:12.9422170Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cf8c70>
2025-04-11T03:52:12.9422523Z timeout = None
2025-04-11T03:52:12.9422639Z 
2025-04-11T03:52:12.9422737Z     def join(self, timeout=None):
2025-04-11T03:52:12.9423022Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9423286Z     
2025-04-11T03:52:12.9423530Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9423882Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9424317Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9424636Z         of the first process exiting.
2025-04-11T03:52:12.9424863Z     
2025-04-11T03:52:12.9425107Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9425463Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9425733Z     
2025-04-11T03:52:12.9425895Z         Args:
2025-04-11T03:52:12.9426204Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9426479Z         """
2025-04-11T03:52:12.9426721Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9427011Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9427249Z             return True
2025-04-11T03:52:12.9427447Z     
2025-04-11T03:52:12.9427669Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9427982Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9428259Z             self.sentinels.keys(),
2025-04-11T03:52:12.9428538Z             timeout=timeout,
2025-04-11T03:52:12.9428749Z         )
2025-04-11T03:52:12.9428927Z     
2025-04-11T03:52:12.9429106Z         error_index = None
2025-04-11T03:52:12.9449869Z         for sentinel in ready:
2025-04-11T03:52:12.9450130Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9450404Z             process = self.processes[index]
2025-04-11T03:52:12.9450659Z             process.join()
2025-04-11T03:52:12.9450939Z             if process.exitcode != 0:
2025-04-11T03:52:12.9451186Z                 error_index = index
2025-04-11T03:52:12.9451481Z                 break
2025-04-11T03:52:12.9451675Z     
2025-04-11T03:52:12.9451860Z         # Return if there was no error.
2025-04-11T03:52:12.9452097Z         if error_index is None:
2025-04-11T03:52:12.9452375Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9452664Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9452897Z     
2025-04-11T03:52:12.9453133Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9453429Z         for process in self.processes:
2025-04-11T03:52:12.9453673Z             if process.is_alive():
2025-04-11T03:52:12.9453909Z                 process.terminate()
2025-04-11T03:52:12.9454143Z             process.join()
2025-04-11T03:52:12.9454343Z     
2025-04-11T03:52:12.9454570Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9454890Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9455178Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9455475Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9455744Z             if exitcode < 0:
2025-04-11T03:52:12.9455983Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9456261Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9456580Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9456888Z                     error_index=error_index,
2025-04-11T03:52:12.9457146Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9457461Z                     exit_code=exitcode,
2025-04-11T03:52:12.9457701Z                     signal_name=name,
2025-04-11T03:52:12.9457922Z                 )
2025-04-11T03:52:12.9458103Z             else:
2025-04-11T03:52:12.9458314Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9458635Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9458955Z                     error_index=error_index,
2025-04-11T03:52:12.9459211Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9459465Z                     exit_code=exitcode,
2025-04-11T03:52:12.9459687Z                 )
2025-04-11T03:52:12.9459866Z     
2025-04-11T03:52:12.9460090Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9460533Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9460858Z         msg += original_trace
2025-04-11T03:52:12.9461173Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9461576Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9461865Z E       
2025-04-11T03:52:12.9462095Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9462464Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9462931Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9463374Z E           fn(i, *args)
2025-04-11T03:52:12.9463774Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_shape_consistency_apply.py", line 41, in check_apply
2025-04-11T03:52:12.9464283Z E           tensor_to_comm = torch.cat((sharded_tensor_0, sharded_tensor_1), 1).cuda()
2025-04-11T03:52:12.9464633Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9465092Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9465582Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9465943Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9466169Z 
2025-04-11T03:52:12.9466490Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9467079Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9467459Z [04/11/25 03:50:04] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9467812Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9468114Z                              :75 launch                                         
2025-04-11T03:52:12.9468462Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9468784Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9469172Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9469578Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9470095Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:61198 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9470586Z ________________________________ test_comm_spec ________________________________
2025-04-11T03:52:12.9470784Z 
2025-04-11T03:52:12.9470885Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9471642Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9472301Z 
2025-04-11T03:52:12.9472407Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9472651Z         try_count = 0
2025-04-11T03:52:12.9472937Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9473188Z             max_try, int
2025-04-11T03:52:12.9473464Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9473746Z     
2025-04-11T03:52:12.9473953Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9474208Z             try:
2025-04-11T03:52:12.9474408Z                 try_count += 1
2025-04-11T03:52:12.9474638Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9474873Z                 return ret
2025-04-11T03:52:12.9475098Z             except exception_type as e:
2025-04-11T03:52:12.9475359Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9475796Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9476165Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9476495Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9476857Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9477155Z                     continue
2025-04-11T03:52:12.9477434Z                 else:
2025-04-11T03:52:12.9477782Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9478149Z >                   raise e
2025-04-11T03:52:12.9478282Z 
2025-04-11T03:52:12.9478385Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9478652Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9478971Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9479262Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9479581Z tests/test_tensor/test_dtensor/test_comm_spec.py:157: in test_comm_spec
2025-04-11T03:52:12.9479907Z     spawn(check_comm, world_size)
2025-04-11T03:52:12.9480160Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9480428Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9480858Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9481366Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9481960Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9482405Z     while not context.join():
2025-04-11T03:52:12.9482664Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9482846Z 
2025-04-11T03:52:12.9483050Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cf8dc0>
2025-04-11T03:52:12.9483406Z timeout = None
2025-04-11T03:52:12.9483527Z 
2025-04-11T03:52:12.9483628Z     def join(self, timeout=None):
2025-04-11T03:52:12.9483916Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9484178Z     
2025-04-11T03:52:12.9484430Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9484790Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9485173Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9485501Z         of the first process exiting.
2025-04-11T03:52:12.9485729Z     
2025-04-11T03:52:12.9485978Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9486330Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9486604Z     
2025-04-11T03:52:12.9486780Z         Args:
2025-04-11T03:52:12.9487032Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9487318Z         """
2025-04-11T03:52:12.9487569Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9487867Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9488170Z             return True
2025-04-11T03:52:12.9488365Z     
2025-04-11T03:52:12.9488589Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9488899Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9489182Z             self.sentinels.keys(),
2025-04-11T03:52:12.9489420Z             timeout=timeout,
2025-04-11T03:52:12.9489632Z         )
2025-04-11T03:52:12.9489805Z     
2025-04-11T03:52:12.9489986Z         error_index = None
2025-04-11T03:52:12.9490216Z         for sentinel in ready:
2025-04-11T03:52:12.9490463Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9490732Z             process = self.processes[index]
2025-04-11T03:52:12.9491039Z             process.join()
2025-04-11T03:52:12.9491268Z             if process.exitcode != 0:
2025-04-11T03:52:12.9491506Z                 error_index = index
2025-04-11T03:52:12.9491730Z                 break
2025-04-11T03:52:12.9491922Z     
2025-04-11T03:52:12.9492105Z         # Return if there was no error.
2025-04-11T03:52:12.9492347Z         if error_index is None:
2025-04-11T03:52:12.9492623Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9492914Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9493203Z     
2025-04-11T03:52:12.9493447Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9493751Z         for process in self.processes:
2025-04-11T03:52:12.9494002Z             if process.is_alive():
2025-04-11T03:52:12.9494244Z                 process.terminate()
2025-04-11T03:52:12.9494477Z             process.join()
2025-04-11T03:52:12.9494684Z     
2025-04-11T03:52:12.9494922Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9495247Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9495538Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9495838Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9496110Z             if exitcode < 0:
2025-04-11T03:52:12.9496357Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9496638Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9496962Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9497339Z                     error_index=error_index,
2025-04-11T03:52:12.9497599Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9497855Z                     exit_code=exitcode,
2025-04-11T03:52:12.9498092Z                     signal_name=name,
2025-04-11T03:52:12.9498312Z                 )
2025-04-11T03:52:12.9498494Z             else:
2025-04-11T03:52:12.9498704Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9499031Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9499352Z                     error_index=error_index,
2025-04-11T03:52:12.9499608Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9499863Z                     exit_code=exitcode,
2025-04-11T03:52:12.9500081Z                 )
2025-04-11T03:52:12.9500258Z     
2025-04-11T03:52:12.9500479Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9500844Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9501161Z         msg += original_trace
2025-04-11T03:52:12.9501462Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9501857Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9502148Z E       
2025-04-11T03:52:12.9502373Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9502658Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9503116Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9503605Z E           fn(i, *args)
2025-04-11T03:52:12.9504006Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 140, in check_comm
2025-04-11T03:52:12.9504446Z E           check_all_gather(process_group_dict, rank)
2025-04-11T03:52:12.9504908Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 15, in check_all_gather
2025-04-11T03:52:12.9505367Z E           sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T03:52:12.9505654Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9506111Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9506661Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9507022Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9507248Z 
2025-04-11T03:52:12.9507561Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9508086Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9508503Z [04/11/25 03:50:09] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9508915Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9509218Z                              :75 launch                                         
2025-04-11T03:52:12.9509529Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9509855Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9510247Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9510646Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9511155Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60535 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9511787Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60535 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9512266Z _________________________________ test_dtensor _________________________________
2025-04-11T03:52:12.9512528Z 
2025-04-11T03:52:12.9512630Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9513385Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9514054Z 
2025-04-11T03:52:12.9514162Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9514411Z         try_count = 0
2025-04-11T03:52:12.9514633Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9514882Z             max_try, int
2025-04-11T03:52:12.9515158Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9515444Z     
2025-04-11T03:52:12.9515656Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9515908Z             try:
2025-04-11T03:52:12.9516106Z                 try_count += 1
2025-04-11T03:52:12.9516338Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9516576Z                 return ret
2025-04-11T03:52:12.9516799Z             except exception_type as e:
2025-04-11T03:52:12.9517058Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9517413Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9517786Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9518117Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9518545Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9518849Z                     continue
2025-04-11T03:52:12.9519059Z                 else:
2025-04-11T03:52:12.9519394Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9519756Z >                   raise e
2025-04-11T03:52:12.9519884Z 
2025-04-11T03:52:12.9519986Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9520250Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9520558Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9520836Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9521141Z tests/test_tensor/test_dtensor/test_dtensor.py:83: in test_dtensor
2025-04-11T03:52:12.9521535Z     spawn(check_dtensor, world_size)
2025-04-11T03:52:12.9521791Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9522048Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9522479Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9522977Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9523565Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9524028Z     while not context.join():
2025-04-11T03:52:12.9524275Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9524452Z 
2025-04-11T03:52:12.9524652Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ecd9240>
2025-04-11T03:52:12.9525011Z timeout = None
2025-04-11T03:52:12.9525125Z 
2025-04-11T03:52:12.9525219Z     def join(self, timeout=None):
2025-04-11T03:52:12.9525494Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9525752Z     
2025-04-11T03:52:12.9525990Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9526343Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9526711Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9527038Z         of the first process exiting.
2025-04-11T03:52:12.9527261Z     
2025-04-11T03:52:12.9527557Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9527911Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9528185Z     
2025-04-11T03:52:12.9528352Z         Args:
2025-04-11T03:52:12.9528599Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9528870Z         """
2025-04-11T03:52:12.9529117Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9529406Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9529641Z             return True
2025-04-11T03:52:12.9529832Z     
2025-04-11T03:52:12.9530055Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9530364Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9530643Z             self.sentinels.keys(),
2025-04-11T03:52:12.9530880Z             timeout=timeout,
2025-04-11T03:52:12.9531090Z         )
2025-04-11T03:52:12.9531257Z     
2025-04-11T03:52:12.9531433Z         error_index = None
2025-04-11T03:52:12.9531656Z         for sentinel in ready:
2025-04-11T03:52:12.9531908Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9532175Z             process = self.processes[index]
2025-04-11T03:52:12.9532416Z             process.join()
2025-04-11T03:52:12.9532644Z             if process.exitcode != 0:
2025-04-11T03:52:12.9532885Z                 error_index = index
2025-04-11T03:52:12.9533109Z                 break
2025-04-11T03:52:12.9533299Z     
2025-04-11T03:52:12.9533483Z         # Return if there was no error.
2025-04-11T03:52:12.9533724Z         if error_index is None:
2025-04-11T03:52:12.9534055Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9534349Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9534575Z     
2025-04-11T03:52:12.9534804Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9535102Z         for process in self.processes:
2025-04-11T03:52:12.9535350Z             if process.is_alive():
2025-04-11T03:52:12.9535589Z                 process.terminate()
2025-04-11T03:52:12.9535818Z             process.join()
2025-04-11T03:52:12.9536015Z     
2025-04-11T03:52:12.9536247Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9536634Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9536988Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9537280Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9537544Z             if exitcode < 0:
2025-04-11T03:52:12.9537785Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9538062Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9538376Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9538747Z                     error_index=error_index,
2025-04-11T03:52:12.9539001Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9539256Z                     exit_code=exitcode,
2025-04-11T03:52:12.9539493Z                     signal_name=name,
2025-04-11T03:52:12.9539714Z                 )
2025-04-11T03:52:12.9539895Z             else:
2025-04-11T03:52:12.9540101Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9540425Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9540749Z                     error_index=error_index,
2025-04-11T03:52:12.9541007Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9541255Z                     exit_code=exitcode,
2025-04-11T03:52:12.9541473Z                 )
2025-04-11T03:52:12.9541651Z     
2025-04-11T03:52:12.9541874Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9542239Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9542563Z         msg += original_trace
2025-04-11T03:52:12.9543006Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9543404Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9543698Z E       
2025-04-11T03:52:12.9543922Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9544207Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9544667Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9545103Z E           fn(i, *args)
2025-04-11T03:52:12.9545500Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py", line 25, in check_dtensor
2025-04-11T03:52:12.9545938Z E           test_model = TestModel(8, 8).to("cuda")
2025-04-11T03:52:12.9546378Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:12.9546799Z E           return self._apply(convert)
2025-04-11T03:52:12.9547227Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9547646Z E           module._apply(fn)
2025-04-11T03:52:12.9548046Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9548501Z E           param_applied = fn(param)
2025-04-11T03:52:12.9548939Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:12.9549490Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.9549947Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9550408Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9550893Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9551256Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9551478Z 
2025-04-11T03:52:12.9551789Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9552311Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9552768Z [04/11/25 03:50:13] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9553110Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9553415Z                              :75 launch                                         
2025-04-11T03:52:12.9553725Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9554052Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9554503Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9554895Z ____________________________ test_layout_converter _____________________________
2025-04-11T03:52:12.9555095Z 
2025-04-11T03:52:12.9555189Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9555931Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9556580Z 
2025-04-11T03:52:12.9556682Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9556926Z         try_count = 0
2025-04-11T03:52:12.9557149Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9557392Z             max_try, int
2025-04-11T03:52:12.9557661Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9557941Z     
2025-04-11T03:52:12.9558149Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9558399Z             try:
2025-04-11T03:52:12.9558652Z                 try_count += 1
2025-04-11T03:52:12.9558882Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9559119Z                 return ret
2025-04-11T03:52:12.9559339Z             except exception_type as e:
2025-04-11T03:52:12.9559597Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9559948Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9560307Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9560633Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9560998Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9561296Z                     continue
2025-04-11T03:52:12.9561507Z                 else:
2025-04-11T03:52:12.9561839Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9562198Z >                   raise e
2025-04-11T03:52:12.9562332Z 
2025-04-11T03:52:12.9562429Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9562694Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9563001Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9563282Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9563618Z tests/test_tensor/test_dtensor/test_layout_converter.py:180: in test_layout_converter
2025-04-11T03:52:12.9564003Z     spawn(check_layout_converting_apply, world_size)
2025-04-11T03:52:12.9564346Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9564610Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9565039Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9565537Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9566062Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9566525Z     while not context.join():
2025-04-11T03:52:12.9566774Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9566951Z 
2025-04-11T03:52:12.9567156Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfaf80>
2025-04-11T03:52:12.9567574Z timeout = None
2025-04-11T03:52:12.9567687Z 
2025-04-11T03:52:12.9567781Z     def join(self, timeout=None):
2025-04-11T03:52:12.9568055Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9568315Z     
2025-04-11T03:52:12.9568558Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9568902Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9569337Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9569658Z         of the first process exiting.
2025-04-11T03:52:12.9569885Z     
2025-04-11T03:52:12.9570124Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9570467Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9570731Z     
2025-04-11T03:52:12.9570893Z         Args:
2025-04-11T03:52:12.9571141Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9571418Z         """
2025-04-11T03:52:12.9571657Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9571948Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9572185Z             return True
2025-04-11T03:52:12.9572377Z     
2025-04-11T03:52:12.9572599Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9572908Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9573183Z             self.sentinels.keys(),
2025-04-11T03:52:12.9573417Z             timeout=timeout,
2025-04-11T03:52:12.9573686Z         )
2025-04-11T03:52:12.9573860Z     
2025-04-11T03:52:12.9574037Z         error_index = None
2025-04-11T03:52:12.9574259Z         for sentinel in ready:
2025-04-11T03:52:12.9574507Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9574778Z             process = self.processes[index]
2025-04-11T03:52:12.9575028Z             process.join()
2025-04-11T03:52:12.9575258Z             if process.exitcode != 0:
2025-04-11T03:52:12.9575501Z                 error_index = index
2025-04-11T03:52:12.9575724Z                 break
2025-04-11T03:52:12.9575911Z     
2025-04-11T03:52:12.9576096Z         # Return if there was no error.
2025-04-11T03:52:12.9576336Z         if error_index is None:
2025-04-11T03:52:12.9576603Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9576892Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9577121Z     
2025-04-11T03:52:12.9577355Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9577651Z         for process in self.processes:
2025-04-11T03:52:12.9577897Z             if process.is_alive():
2025-04-11T03:52:12.9578131Z                 process.terminate()
2025-04-11T03:52:12.9578367Z             process.join()
2025-04-11T03:52:12.9578564Z     
2025-04-11T03:52:12.9578800Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9579119Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9579398Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9579690Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9580034Z             if exitcode < 0:
2025-04-11T03:52:12.9580279Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9580560Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9580881Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9581196Z                     error_index=error_index,
2025-04-11T03:52:12.9581465Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9581726Z                     exit_code=exitcode,
2025-04-11T03:52:12.9581970Z                     signal_name=name,
2025-04-11T03:52:12.9582196Z                 )
2025-04-11T03:52:12.9582386Z             else:
2025-04-11T03:52:12.9582660Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9582985Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9583303Z                     error_index=error_index,
2025-04-11T03:52:12.9583554Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9583806Z                     exit_code=exitcode,
2025-04-11T03:52:12.9584026Z                 )
2025-04-11T03:52:12.9584205Z     
2025-04-11T03:52:12.9584431Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9584851Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9585174Z         msg += original_trace
2025-04-11T03:52:12.9585480Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9585871Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9586162Z E       
2025-04-11T03:52:12.9586384Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9586673Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9587136Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9587580Z E           fn(i, *args)
2025-04-11T03:52:12.9588047Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_layout_converter.py", line 162, in check_layout_converting_apply
2025-04-11T03:52:12.9588610Z E           original_tensor = torch.rand(global_shape).cuda()
2025-04-11T03:52:12.9588903Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9589419Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9589905Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9590270Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9590496Z 
2025-04-11T03:52:12.9590807Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9591329Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9591698Z [04/11/25 03:50:18] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9592041Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9592344Z                              :75 launch                                         
2025-04-11T03:52:12.9592654Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9592982Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9593314Z [04/11/25 03:50:22] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9593642Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9593938Z                              :75 launch                                         
2025-04-11T03:52:12.9594242Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9594436Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9594580Z [04/11/25 03:50:26] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9594705Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9594813Z                              :75 launch                                         
2025-04-11T03:52:12.9594947Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9595073Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9595270Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9595489Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9595788Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57837 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9596071Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57837 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9596211Z ____________________________ test_chunk_manager[2] _____________________________
2025-04-11T03:52:12.9596274Z 
2025-04-11T03:52:12.9596394Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T03:52:12.9596999Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9597004Z 
2025-04-11T03:52:12.9597109Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9597196Z         try_count = 0
2025-04-11T03:52:12.9597299Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9597383Z             max_try, int
2025-04-11T03:52:12.9597528Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9597599Z     
2025-04-11T03:52:12.9597714Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9597789Z             try:
2025-04-11T03:52:12.9597876Z                 try_count += 1
2025-04-11T03:52:12.9597969Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9598055Z                 return ret
2025-04-11T03:52:12.9598204Z             except exception_type as e:
2025-04-11T03:52:12.9598308Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9598500Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9598617Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9598767Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9598921Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9599007Z                     continue
2025-04-11T03:52:12.9599086Z                 else:
2025-04-11T03:52:12.9599300Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9599385Z >                   raise e
2025-04-11T03:52:12.9599391Z 
2025-04-11T03:52:12.9599488Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9599603Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9599734Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9599824Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9599998Z tests/test_zero/test_gemini/test_chunk_mgrv2.py:60: in test_chunk_manager
2025-04-11T03:52:12.9600091Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9600199Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9600299Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9600563Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9600794Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9601085Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9601177Z     while not context.join():
2025-04-11T03:52:12.9601289Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9601293Z 
2025-04-11T03:52:12.9601495Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ece1210>
2025-04-11T03:52:12.9601574Z timeout = None
2025-04-11T03:52:12.9601578Z 
2025-04-11T03:52:12.9601671Z     def join(self, timeout=None):
2025-04-11T03:52:12.9601794Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9601933Z     
2025-04-11T03:52:12.9602082Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9602226Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9602399Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9602492Z         of the first process exiting.
2025-04-11T03:52:12.9602567Z     
2025-04-11T03:52:12.9602716Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9602915Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9602992Z     
2025-04-11T03:52:12.9603069Z         Args:
2025-04-11T03:52:12.9603211Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9603288Z         """
2025-04-11T03:52:12.9603432Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9603528Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9603615Z             return True
2025-04-11T03:52:12.9603694Z     
2025-04-11T03:52:12.9603828Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9603956Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9604054Z             self.sentinels.keys(),
2025-04-11T03:52:12.9604143Z             timeout=timeout,
2025-04-11T03:52:12.9604223Z         )
2025-04-11T03:52:12.9604296Z     
2025-04-11T03:52:12.9604387Z         error_index = None
2025-04-11T03:52:12.9604476Z         for sentinel in ready:
2025-04-11T03:52:12.9604586Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9604745Z             process = self.processes[index]
2025-04-11T03:52:12.9604834Z             process.join()
2025-04-11T03:52:12.9604934Z             if process.exitcode != 0:
2025-04-11T03:52:12.9605021Z                 error_index = index
2025-04-11T03:52:12.9605102Z                 break
2025-04-11T03:52:12.9605173Z     
2025-04-11T03:52:12.9605265Z         # Return if there was no error.
2025-04-11T03:52:12.9605354Z         if error_index is None:
2025-04-11T03:52:12.9605490Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9605590Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9605664Z     
2025-04-11T03:52:12.9605806Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9605907Z         for process in self.processes:
2025-04-11T03:52:12.9605997Z             if process.is_alive():
2025-04-11T03:52:12.9606092Z                 process.terminate()
2025-04-11T03:52:12.9606180Z             process.join()
2025-04-11T03:52:12.9606254Z     
2025-04-11T03:52:12.9606393Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9606508Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9606617Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9606739Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9606826Z             if exitcode < 0:
2025-04-11T03:52:12.9606934Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9607042Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9607272Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9607375Z                     error_index=error_index,
2025-04-11T03:52:12.9607480Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9607570Z                     exit_code=exitcode,
2025-04-11T03:52:12.9607659Z                     signal_name=name,
2025-04-11T03:52:12.9607736Z                 )
2025-04-11T03:52:12.9607812Z             else:
2025-04-11T03:52:12.9607921Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9608084Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9608182Z                     error_index=error_index,
2025-04-11T03:52:12.9608346Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9608431Z                     exit_code=exitcode,
2025-04-11T03:52:12.9608508Z                 )
2025-04-11T03:52:12.9608577Z     
2025-04-11T03:52:12.9608714Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9608883Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9608972Z         msg += original_trace
2025-04-11T03:52:12.9609206Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9609372Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9609450Z E       
2025-04-11T03:52:12.9609577Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9609678Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9609975Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9610063Z E           fn(i, *args)
2025-04-11T03:52:12.9610319Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 53, in run_dist
2025-04-11T03:52:12.9610407Z E           exam_chunk_memory()
2025-04-11T03:52:12.9610671Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9610761Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9611015Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9611105Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9611447Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 21, in exam_chunk_memory
2025-04-11T03:52:12.9611554Z E           chunk_manager = ChunkManager(config)
2025-04-11T03:52:12.9611796Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T03:52:12.9612052Z E           self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.9612156Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9612444Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9612581Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9612750Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9612754Z 
2025-04-11T03:52:12.9613058Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9613218Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9613370Z [04/11/25 03:50:30] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9613496Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9613608Z                              :75 launch                                         
2025-04-11T03:52:12.9613744Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9613929Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9614134Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9614285Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9615183Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T03:52:12.9615350Z   return tensor.storage().size() == 0
2025-04-11T03:52:12.9615486Z ____________________________ test_chunk_function[1] ____________________________
2025-04-11T03:52:12.9615491Z 
2025-04-11T03:52:12.9615611Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T03:52:12.9616205Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9616266Z 
2025-04-11T03:52:12.9616371Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9616456Z         try_count = 0
2025-04-11T03:52:12.9616556Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9616641Z             max_try, int
2025-04-11T03:52:12.9616787Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9616861Z     
2025-04-11T03:52:12.9616972Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9617051Z             try:
2025-04-11T03:52:12.9617141Z                 try_count += 1
2025-04-11T03:52:12.9617230Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9617313Z                 return ret
2025-04-11T03:52:12.9617409Z             except exception_type as e:
2025-04-11T03:52:12.9617507Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9617696Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9617812Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9618012Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9618166Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9618253Z                     continue
2025-04-11T03:52:12.9618328Z                 else:
2025-04-11T03:52:12.9618544Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9618629Z >                   raise e
2025-04-11T03:52:12.9618633Z 
2025-04-11T03:52:12.9618726Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9618842Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9618974Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9619063Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9619236Z tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
2025-04-11T03:52:12.9619324Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9619431Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9619530Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9619793Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9619969Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9620259Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9620350Z     while not context.join():
2025-04-11T03:52:12.9620514Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9620523Z 
2025-04-11T03:52:12.9620721Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cf9840>
2025-04-11T03:52:12.9620799Z timeout = None
2025-04-11T03:52:12.9620805Z 
2025-04-11T03:52:12.9620902Z     def join(self, timeout=None):
2025-04-11T03:52:12.9621030Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9621106Z     
2025-04-11T03:52:12.9621251Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9621394Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9621557Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9621711Z         of the first process exiting.
2025-04-11T03:52:12.9621786Z     
2025-04-11T03:52:12.9621932Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9622073Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9622146Z     
2025-04-11T03:52:12.9622220Z         Args:
2025-04-11T03:52:12.9622361Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9622434Z         """
2025-04-11T03:52:12.9622631Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9622728Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9622809Z             return True
2025-04-11T03:52:12.9622884Z     
2025-04-11T03:52:12.9623016Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9623138Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9623230Z             self.sentinels.keys(),
2025-04-11T03:52:12.9623321Z             timeout=timeout,
2025-04-11T03:52:12.9623393Z         )
2025-04-11T03:52:12.9623463Z     
2025-04-11T03:52:12.9623551Z         error_index = None
2025-04-11T03:52:12.9623636Z         for sentinel in ready:
2025-04-11T03:52:12.9623746Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9623845Z             process = self.processes[index]
2025-04-11T03:52:12.9623929Z             process.join()
2025-04-11T03:52:12.9624027Z             if process.exitcode != 0:
2025-04-11T03:52:12.9624116Z                 error_index = index
2025-04-11T03:52:12.9624194Z                 break
2025-04-11T03:52:12.9624265Z     
2025-04-11T03:52:12.9624411Z         # Return if there was no error.
2025-04-11T03:52:12.9624501Z         if error_index is None:
2025-04-11T03:52:12.9624637Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9624740Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9624810Z     
2025-04-11T03:52:12.9624953Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9625052Z         for process in self.processes:
2025-04-11T03:52:12.9625139Z             if process.is_alive():
2025-04-11T03:52:12.9625237Z                 process.terminate()
2025-04-11T03:52:12.9625322Z             process.join()
2025-04-11T03:52:12.9625396Z     
2025-04-11T03:52:12.9625538Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9625656Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9625771Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9625891Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9625978Z             if exitcode < 0:
2025-04-11T03:52:12.9626085Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9626195Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9626342Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9626443Z                     error_index=error_index,
2025-04-11T03:52:12.9626552Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9626639Z                     exit_code=exitcode,
2025-04-11T03:52:12.9626728Z                     signal_name=name,
2025-04-11T03:52:12.9626858Z                 )
2025-04-11T03:52:12.9626938Z             else:
2025-04-11T03:52:12.9627047Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9627213Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9627315Z                     error_index=error_index,
2025-04-11T03:52:12.9627420Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9627512Z                     exit_code=exitcode,
2025-04-11T03:52:12.9627589Z                 )
2025-04-11T03:52:12.9627662Z     
2025-04-11T03:52:12.9627802Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9627974Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9628127Z         msg += original_trace
2025-04-11T03:52:12.9628301Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9628510Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9628584Z E       
2025-04-11T03:52:12.9628714Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9628817Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9629173Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9629261Z E           fn(i, *args)
2025-04-11T03:52:12.9629507Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T03:52:12.9629596Z E           exam_chunk_basic()
2025-04-11T03:52:12.9629849Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9629939Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9630194Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9630280Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9630533Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9630617Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9630727Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.9631045Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T03:52:12.9631135Z E           my_chunk = Chunk(
2025-04-11T03:52:12.9631371Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T03:52:12.9631572Z E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T03:52:12.9631688Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9631969Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9632110Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9632271Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9632276Z 
2025-04-11T03:52:12.9632578Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9632735Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9632890Z [04/11/25 03:50:34] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9633021Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9633128Z                              :75 launch                                         
2025-04-11T03:52:12.9633268Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9633392Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.9633650Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9633789Z ____________________________ test_chunk_function[2] ____________________________
2025-04-11T03:52:12.9633795Z 
2025-04-11T03:52:12.9633912Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T03:52:12.9634510Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9634515Z 
2025-04-11T03:52:12.9634617Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9634701Z         try_count = 0
2025-04-11T03:52:12.9634871Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9634957Z             max_try, int
2025-04-11T03:52:12.9635105Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9635178Z     
2025-04-11T03:52:12.9635291Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9635368Z             try:
2025-04-11T03:52:12.9635456Z                 try_count += 1
2025-04-11T03:52:12.9635547Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9635701Z                 return ret
2025-04-11T03:52:12.9635796Z             except exception_type as e:
2025-04-11T03:52:12.9635896Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9636086Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9636201Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9636349Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9636504Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9636589Z                     continue
2025-04-11T03:52:12.9636666Z                 else:
2025-04-11T03:52:12.9636896Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9637015Z >                   raise e
2025-04-11T03:52:12.9637020Z 
2025-04-11T03:52:12.9637120Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9637289Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9637428Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9637516Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9637685Z tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
2025-04-11T03:52:12.9637773Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9637877Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9637979Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9638240Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9638418Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9638707Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9638796Z     while not context.join():
2025-04-11T03:52:12.9638907Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9638913Z 
2025-04-11T03:52:12.9639117Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ecbfe80>
2025-04-11T03:52:12.9639196Z timeout = None
2025-04-11T03:52:12.9639200Z 
2025-04-11T03:52:12.9639295Z     def join(self, timeout=None):
2025-04-11T03:52:12.9639423Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9639500Z     
2025-04-11T03:52:12.9639646Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9639788Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9640006Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9640101Z         of the first process exiting.
2025-04-11T03:52:12.9640174Z     
2025-04-11T03:52:12.9640320Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9640460Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9640529Z     
2025-04-11T03:52:12.9640606Z         Args:
2025-04-11T03:52:12.9640749Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9640822Z         """
2025-04-11T03:52:12.9640963Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9641056Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9641203Z             return True
2025-04-11T03:52:12.9641278Z     
2025-04-11T03:52:12.9641411Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9641531Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9641624Z             self.sentinels.keys(),
2025-04-11T03:52:12.9641707Z             timeout=timeout,
2025-04-11T03:52:12.9641785Z         )
2025-04-11T03:52:12.9641855Z     
2025-04-11T03:52:12.9641941Z         error_index = None
2025-04-11T03:52:12.9642083Z         for sentinel in ready:
2025-04-11T03:52:12.9642195Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9642293Z             process = self.processes[index]
2025-04-11T03:52:12.9642377Z             process.join()
2025-04-11T03:52:12.9642475Z             if process.exitcode != 0:
2025-04-11T03:52:12.9642562Z                 error_index = index
2025-04-11T03:52:12.9642643Z                 break
2025-04-11T03:52:12.9642712Z     
2025-04-11T03:52:12.9642804Z         # Return if there was no error.
2025-04-11T03:52:12.9642896Z         if error_index is None:
2025-04-11T03:52:12.9643027Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9643123Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9643193Z     
2025-04-11T03:52:12.9643333Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9643433Z         for process in self.processes:
2025-04-11T03:52:12.9643520Z             if process.is_alive():
2025-04-11T03:52:12.9643616Z                 process.terminate()
2025-04-11T03:52:12.9643700Z             process.join()
2025-04-11T03:52:12.9643773Z     
2025-04-11T03:52:12.9643965Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9644081Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9644194Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9644315Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9644407Z             if exitcode < 0:
2025-04-11T03:52:12.9644513Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9644619Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9644772Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9644867Z                     error_index=error_index,
2025-04-11T03:52:12.9644972Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9645062Z                     exit_code=exitcode,
2025-04-11T03:52:12.9645151Z                     signal_name=name,
2025-04-11T03:52:12.9645224Z                 )
2025-04-11T03:52:12.9645298Z             else:
2025-04-11T03:52:12.9645405Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9645569Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9645665Z                     error_index=error_index,
2025-04-11T03:52:12.9645765Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9645857Z                     exit_code=exitcode,
2025-04-11T03:52:12.9645930Z                 )
2025-04-11T03:52:12.9646002Z     
2025-04-11T03:52:12.9646139Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9646361Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9646453Z         msg += original_trace
2025-04-11T03:52:12.9646628Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9646794Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9646876Z E       
2025-04-11T03:52:12.9647004Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9647105Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9647401Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9647486Z E           fn(i, *args)
2025-04-11T03:52:12.9647797Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T03:52:12.9647882Z E           exam_chunk_basic()
2025-04-11T03:52:12.9648148Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9648238Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9648493Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9648639Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9648898Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9648984Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9649089Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.9649346Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T03:52:12.9649436Z E           my_chunk = Chunk(
2025-04-11T03:52:12.9649676Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T03:52:12.9649880Z E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T03:52:12.9649991Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9650297Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9650494Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9650659Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9650663Z 
2025-04-11T03:52:12.9650968Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9651123Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9651279Z [04/11/25 03:50:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9651410Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9651517Z                              :75 launch                                         
2025-04-11T03:52:12.9651654Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9651778Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9651975Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9652114Z ____________________________ test_chunk_function[4] ____________________________
2025-04-11T03:52:12.9652118Z 
2025-04-11T03:52:12.9652232Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9652835Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9652841Z 
2025-04-11T03:52:12.9652943Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9653084Z         try_count = 0
2025-04-11T03:52:12.9653187Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9653271Z             max_try, int
2025-04-11T03:52:12.9653419Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9653491Z     
2025-04-11T03:52:12.9653608Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9653687Z             try:
2025-04-11T03:52:12.9653772Z                 try_count += 1
2025-04-11T03:52:12.9653864Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9653948Z                 return ret
2025-04-11T03:52:12.9654045Z             except exception_type as e:
2025-04-11T03:52:12.9654207Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9654393Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9654508Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9654655Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9654807Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9654945Z                     continue
2025-04-11T03:52:12.9655026Z                 else:
2025-04-11T03:52:12.9655245Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9655327Z >                   raise e
2025-04-11T03:52:12.9655331Z 
2025-04-11T03:52:12.9655425Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9655539Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9655673Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9655761Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9655934Z tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
2025-04-11T03:52:12.9656021Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9656126Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9656226Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9656483Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9656660Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9657000Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9657094Z     while not context.join():
2025-04-11T03:52:12.9657203Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9657207Z 
2025-04-11T03:52:12.9657409Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be4edb40>
2025-04-11T03:52:12.9657489Z timeout = None
2025-04-11T03:52:12.9657493Z 
2025-04-11T03:52:12.9657585Z     def join(self, timeout=None):
2025-04-11T03:52:12.9657711Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9657786Z     
2025-04-11T03:52:12.9657930Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9658073Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9658238Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9658334Z         of the first process exiting.
2025-04-11T03:52:12.9658406Z     
2025-04-11T03:52:12.9658549Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9658683Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9658755Z     
2025-04-11T03:52:12.9658829Z         Args:
2025-04-11T03:52:12.9658970Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9659044Z         """
2025-04-11T03:52:12.9659184Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9659332Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9659415Z             return True
2025-04-11T03:52:12.9659491Z     
2025-04-11T03:52:12.9659622Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9659746Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9659838Z             self.sentinels.keys(),
2025-04-11T03:52:12.9659923Z             timeout=timeout,
2025-04-11T03:52:12.9659999Z         )
2025-04-11T03:52:12.9660068Z     
2025-04-11T03:52:12.9660154Z         error_index = None
2025-04-11T03:52:12.9660238Z         for sentinel in ready:
2025-04-11T03:52:12.9660343Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9660443Z             process = self.processes[index]
2025-04-11T03:52:12.9660606Z             process.join()
2025-04-11T03:52:12.9660704Z             if process.exitcode != 0:
2025-04-11T03:52:12.9660792Z                 error_index = index
2025-04-11T03:52:12.9660872Z                 break
2025-04-11T03:52:12.9660942Z     
2025-04-11T03:52:12.9661035Z         # Return if there was no error.
2025-04-11T03:52:12.9661124Z         if error_index is None:
2025-04-11T03:52:12.9661256Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9661412Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9661482Z     
2025-04-11T03:52:12.9661622Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9661722Z         for process in self.processes:
2025-04-11T03:52:12.9661810Z             if process.is_alive():
2025-04-11T03:52:12.9661904Z                 process.terminate()
2025-04-11T03:52:12.9661986Z             process.join()
2025-04-11T03:52:12.9662055Z     
2025-04-11T03:52:12.9662202Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9662319Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9662431Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9662553Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9662638Z             if exitcode < 0:
2025-04-11T03:52:12.9662744Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9662848Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9663005Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9663156Z                     error_index=error_index,
2025-04-11T03:52:12.9663264Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9663354Z                     exit_code=exitcode,
2025-04-11T03:52:12.9663443Z                     signal_name=name,
2025-04-11T03:52:12.9663516Z                 )
2025-04-11T03:52:12.9663590Z             else:
2025-04-11T03:52:12.9663702Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9663866Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9663962Z                     error_index=error_index,
2025-04-11T03:52:12.9664062Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9664148Z                     exit_code=exitcode,
2025-04-11T03:52:12.9664224Z                 )
2025-04-11T03:52:12.9664296Z     
2025-04-11T03:52:12.9664428Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9664596Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9664685Z         msg += original_trace
2025-04-11T03:52:12.9664857Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9665019Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9665096Z E       
2025-04-11T03:52:12.9665223Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9665327Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9665623Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9665764Z E           fn(i, *args)
2025-04-11T03:52:12.9666012Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T03:52:12.9666099Z E           exam_chunk_basic()
2025-04-11T03:52:12.9666361Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9666451Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9666701Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9666788Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9667035Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9667186Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9667294Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.9667558Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T03:52:12.9667642Z E           my_chunk = Chunk(
2025-04-11T03:52:12.9667884Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T03:52:12.9668147Z E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T03:52:12.9668262Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9668599Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9668738Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9668903Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9668907Z 
2025-04-11T03:52:12.9669213Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9669368Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9669521Z [04/11/25 03:50:43] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9669652Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9669842Z                              :75 launch                                         
2025-04-11T03:52:12.9669986Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9670109Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9670307Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9670454Z ____________________________ test_grad_accumulation ____________________________
2025-04-11T03:52:12.9670458Z 
2025-04-11T03:52:12.9670552Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9671144Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9671150Z 
2025-04-11T03:52:12.9671253Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9671335Z         try_count = 0
2025-04-11T03:52:12.9671435Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9671520Z             max_try, int
2025-04-11T03:52:12.9671667Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9671737Z     
2025-04-11T03:52:12.9671851Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9671929Z             try:
2025-04-11T03:52:12.9672015Z                 try_count += 1
2025-04-11T03:52:12.9672111Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9672192Z                 return ret
2025-04-11T03:52:12.9672289Z             except exception_type as e:
2025-04-11T03:52:12.9672450Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9672641Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9672758Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9672911Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9673066Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9673150Z                     continue
2025-04-11T03:52:12.9673231Z                 else:
2025-04-11T03:52:12.9673449Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9673600Z >                   raise e
2025-04-11T03:52:12.9673604Z 
2025-04-11T03:52:12.9673700Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9673815Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9673950Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9674038Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9674223Z tests/test_zero/test_gemini/test_grad_accum.py:158: in test_grad_accumulation
2025-04-11T03:52:12.9674450Z     spawn(run_dist, 2)
2025-04-11T03:52:12.9674555Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9674658Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9674924Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9675103Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9675391Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9675489Z     while not context.join():
2025-04-11T03:52:12.9675597Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9675601Z 
2025-04-11T03:52:12.9675805Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ec751b0>
2025-04-11T03:52:12.9675882Z timeout = None
2025-04-11T03:52:12.9675886Z 
2025-04-11T03:52:12.9675980Z     def join(self, timeout=None):
2025-04-11T03:52:12.9676106Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9676177Z     
2025-04-11T03:52:12.9676386Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9676531Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9676697Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9676793Z         of the first process exiting.
2025-04-11T03:52:12.9676868Z     
2025-04-11T03:52:12.9677018Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9677156Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9677232Z     
2025-04-11T03:52:12.9677308Z         Args:
2025-04-11T03:52:12.9677447Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9677519Z         """
2025-04-11T03:52:12.9677661Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9677759Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9677840Z             return True
2025-04-11T03:52:12.9677911Z     
2025-04-11T03:52:12.9678040Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9678162Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9678253Z             self.sentinels.keys(),
2025-04-11T03:52:12.9678335Z             timeout=timeout,
2025-04-11T03:52:12.9678414Z         )
2025-04-11T03:52:12.9678484Z     
2025-04-11T03:52:12.9678572Z         error_index = None
2025-04-11T03:52:12.9678656Z         for sentinel in ready:
2025-04-11T03:52:12.9678763Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9678921Z             process = self.processes[index]
2025-04-11T03:52:12.9679007Z             process.join()
2025-04-11T03:52:12.9679103Z             if process.exitcode != 0:
2025-04-11T03:52:12.9679190Z                 error_index = index
2025-04-11T03:52:12.9679269Z                 break
2025-04-11T03:52:12.9679342Z     
2025-04-11T03:52:12.9679434Z         # Return if there was no error.
2025-04-11T03:52:12.9679523Z         if error_index is None:
2025-04-11T03:52:12.9679657Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9679758Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9679827Z     
2025-04-11T03:52:12.9679967Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9680129Z         for process in self.processes:
2025-04-11T03:52:12.9680217Z             if process.is_alive():
2025-04-11T03:52:12.9680311Z                 process.terminate()
2025-04-11T03:52:12.9680393Z             process.join()
2025-04-11T03:52:12.9680465Z     
2025-04-11T03:52:12.9680609Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9680724Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9680834Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9681018Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9681108Z             if exitcode < 0:
2025-04-11T03:52:12.9681213Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9681317Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9681470Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9681565Z                     error_index=error_index,
2025-04-11T03:52:12.9681672Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9681760Z                     exit_code=exitcode,
2025-04-11T03:52:12.9681846Z                     signal_name=name,
2025-04-11T03:52:12.9681924Z                 )
2025-04-11T03:52:12.9681999Z             else:
2025-04-11T03:52:12.9682105Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9682268Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9682367Z                     error_index=error_index,
2025-04-11T03:52:12.9682467Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9682608Z                     exit_code=exitcode,
2025-04-11T03:52:12.9682690Z                 )
2025-04-11T03:52:12.9682763Z     
2025-04-11T03:52:12.9682901Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9683073Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9683164Z         msg += original_trace
2025-04-11T03:52:12.9683342Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9683509Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9683589Z E       
2025-04-11T03:52:12.9683721Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9683825Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9684134Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9684221Z E           fn(i, *args)
2025-04-11T03:52:12.9684486Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 152, in run_dist
2025-04-11T03:52:12.9684578Z E           exam_gemini_grad_acc()
2025-04-11T03:52:12.9684846Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9684941Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9685201Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9685290Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9685610Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9685703Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9685809Z E         [Previous line repeated 4 more times]
2025-04-11T03:52:12.9686094Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 71, in exam_gemini_grad_acc
2025-04-11T03:52:12.9686196Z E           torch_model = model_builder().cuda()
2025-04-11T03:52:12.9686485Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9686584Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9686925Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9687045Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9687314Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9687406Z E           module._apply(fn)
2025-04-11T03:52:12.9687671Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9687822Z E           module._apply(fn)
2025-04-11T03:52:12.9688096Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9688197Z E           param_applied = fn(param)
2025-04-11T03:52:12.9688475Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9688591Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9688704Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9688991Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9689134Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9689299Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9689303Z 
2025-04-11T03:52:12.9689616Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9689821Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9689984Z [04/11/25 03:50:49] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9690110Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9690217Z                              :75 launch                                         
2025-04-11T03:52:12.9690358Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9690482Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9690680Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9690823Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9691957Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9692128Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9693297Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9693468Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9694165Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9694250Z   warnings.warn(
2025-04-11T03:52:12.9694938Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9695084Z   warnings.warn(
2025-04-11T03:52:12.9695912Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9696050Z   warnings.warn(
2025-04-11T03:52:12.9696861Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9696942Z   warnings.warn(
2025-04-11T03:52:12.9697741Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9697823Z   warnings.warn(
2025-04-11T03:52:12.9698677Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9698762Z   warnings.warn(
2025-04-11T03:52:12.9699555Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9699640Z   warnings.warn(
2025-04-11T03:52:12.9700437Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9700522Z   warnings.warn(
2025-04-11T03:52:12.9701400Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T03:52:12.9701506Z   return tensor.storage().size() == 0
2025-04-11T03:52:12.9702516Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9702693Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9702831Z ______________________________ test_grad_clip[1] _______________________________
2025-04-11T03:52:12.9702837Z 
2025-04-11T03:52:12.9702963Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T03:52:12.9703554Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9703559Z 
2025-04-11T03:52:12.9703730Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9703812Z         try_count = 0
2025-04-11T03:52:12.9703920Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9704003Z             max_try, int
2025-04-11T03:52:12.9704153Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9704232Z     
2025-04-11T03:52:12.9704345Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9704423Z             try:
2025-04-11T03:52:12.9704569Z                 try_count += 1
2025-04-11T03:52:12.9704663Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9704751Z                 return ret
2025-04-11T03:52:12.9704848Z             except exception_type as e:
2025-04-11T03:52:12.9704950Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9705134Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9705254Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9705400Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9705551Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9705638Z                     continue
2025-04-11T03:52:12.9705713Z                 else:
2025-04-11T03:52:12.9705940Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9706021Z >                   raise e
2025-04-11T03:52:12.9706025Z 
2025-04-11T03:52:12.9706121Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9706287Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9706424Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9706516Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9706681Z tests/test_zero/test_gemini/test_grad_clip.py:134: in test_grad_clip
2025-04-11T03:52:12.9706783Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9706883Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9706983Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9707248Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9707425Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9707713Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9707803Z     while not context.join():
2025-04-11T03:52:12.9707917Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9707921Z 
2025-04-11T03:52:12.9708122Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be4ef6a0>
2025-04-11T03:52:12.9708204Z timeout = None
2025-04-11T03:52:12.9708208Z 
2025-04-11T03:52:12.9708299Z     def join(self, timeout=None):
2025-04-11T03:52:12.9708464Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9708543Z     
2025-04-11T03:52:12.9708689Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9708902Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9709072Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9709170Z         of the first process exiting.
2025-04-11T03:52:12.9709242Z     
2025-04-11T03:52:12.9709389Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9709529Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9709599Z     
2025-04-11T03:52:12.9709677Z         Args:
2025-04-11T03:52:12.9709812Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9709885Z         """
2025-04-11T03:52:12.9710029Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9710190Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9710278Z             return True
2025-04-11T03:52:12.9710349Z     
2025-04-11T03:52:12.9710482Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9710604Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9710697Z             self.sentinels.keys(),
2025-04-11T03:52:12.9710787Z             timeout=timeout,
2025-04-11T03:52:12.9710859Z         )
2025-04-11T03:52:12.9710998Z     
2025-04-11T03:52:12.9711082Z         error_index = None
2025-04-11T03:52:12.9711169Z         for sentinel in ready:
2025-04-11T03:52:12.9711281Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9711382Z             process = self.processes[index]
2025-04-11T03:52:12.9711470Z             process.join()
2025-04-11T03:52:12.9711564Z             if process.exitcode != 0:
2025-04-11T03:52:12.9711650Z                 error_index = index
2025-04-11T03:52:12.9711731Z                 break
2025-04-11T03:52:12.9711802Z     
2025-04-11T03:52:12.9711898Z         # Return if there was no error.
2025-04-11T03:52:12.9711984Z         if error_index is None:
2025-04-11T03:52:12.9712118Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9712220Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9712289Z     
2025-04-11T03:52:12.9712434Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9712530Z         for process in self.processes:
2025-04-11T03:52:12.9712624Z             if process.is_alive():
2025-04-11T03:52:12.9712717Z                 process.terminate()
2025-04-11T03:52:12.9712880Z             process.join()
2025-04-11T03:52:12.9712956Z     
2025-04-11T03:52:12.9713098Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9713217Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9713327Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9713449Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9713535Z             if exitcode < 0:
2025-04-11T03:52:12.9713642Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9713751Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9713903Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9714000Z                     error_index=error_index,
2025-04-11T03:52:12.9714104Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9714191Z                     exit_code=exitcode,
2025-04-11T03:52:12.9714283Z                     signal_name=name,
2025-04-11T03:52:12.9714357Z                 )
2025-04-11T03:52:12.9714435Z             else:
2025-04-11T03:52:12.9714536Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9714704Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9714799Z                     error_index=error_index,
2025-04-11T03:52:12.9714900Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9714990Z                     exit_code=exitcode,
2025-04-11T03:52:12.9715062Z                 )
2025-04-11T03:52:12.9715138Z     
2025-04-11T03:52:12.9715324Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9715497Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9715590Z         msg += original_trace
2025-04-11T03:52:12.9715768Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9715934Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9716008Z E       
2025-04-11T03:52:12.9716142Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9716241Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9716539Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9716691Z E           fn(i, *args)
2025-04-11T03:52:12.9716946Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T03:52:12.9717045Z E           exam_grad_clipping()
2025-04-11T03:52:12.9717307Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9717400Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9717707Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9717796Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9718046Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9718132Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9718241Z E         [Previous line repeated 2 more times]
2025-04-11T03:52:12.9718510Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T03:52:12.9718616Z E           torch_model = model_builder().cuda()
2025-04-11T03:52:12.9718903Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9719002Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9719268Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9719388Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9719710Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9719800Z E           module._apply(fn)
2025-04-11T03:52:12.9720070Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9720161Z E           module._apply(fn)
2025-04-11T03:52:12.9720426Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9720526Z E           param_applied = fn(param)
2025-04-11T03:52:12.9720802Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9720924Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9721033Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9721321Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9721459Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9721625Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9721629Z 
2025-04-11T03:52:12.9721930Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9722086Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9722246Z [04/11/25 03:50:56] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9722433Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9722548Z                              :75 launch                                         
2025-04-11T03:52:12.9722687Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9722814Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.9723007Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9723160Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9724273Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9724510Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9725236Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9725320Z   warnings.warn(
2025-04-11T03:52:12.9726129Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9726210Z   warnings.warn(
2025-04-11T03:52:12.9727008Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9727089Z   warnings.warn(
2025-04-11T03:52:12.9727941Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9728022Z   warnings.warn(
2025-04-11T03:52:12.9728160Z ______________________________ test_grad_clip[2] _______________________________
2025-04-11T03:52:12.9728166Z 
2025-04-11T03:52:12.9728287Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T03:52:12.9728886Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9728890Z 
2025-04-11T03:52:12.9728995Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9729078Z         try_count = 0
2025-04-11T03:52:12.9729180Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9729264Z             max_try, int
2025-04-11T03:52:12.9729413Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9729483Z     
2025-04-11T03:52:12.9729599Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9729673Z             try:
2025-04-11T03:52:12.9729763Z                 try_count += 1
2025-04-11T03:52:12.9729854Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9729934Z                 return ret
2025-04-11T03:52:12.9730031Z             except exception_type as e:
2025-04-11T03:52:12.9730186Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9730379Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9730498Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9730648Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9730802Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9730884Z                     continue
2025-04-11T03:52:12.9730966Z                 else:
2025-04-11T03:52:12.9731182Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9731332Z >                   raise e
2025-04-11T03:52:12.9731336Z 
2025-04-11T03:52:12.9731431Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9731545Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9731681Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9731768Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9731937Z tests/test_zero/test_gemini/test_grad_clip.py:134: in test_grad_clip
2025-04-11T03:52:12.9732086Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9732196Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9732297Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9732552Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9732730Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9733015Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9733110Z     while not context.join():
2025-04-11T03:52:12.9733220Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9733224Z 
2025-04-11T03:52:12.9733425Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ecb3490>
2025-04-11T03:52:12.9733503Z timeout = None
2025-04-11T03:52:12.9733506Z 
2025-04-11T03:52:12.9733600Z     def join(self, timeout=None):
2025-04-11T03:52:12.9733730Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9733801Z     
2025-04-11T03:52:12.9734002Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9734153Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9734321Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9734416Z         of the first process exiting.
2025-04-11T03:52:12.9734489Z     
2025-04-11T03:52:12.9734638Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9734773Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9734847Z     
2025-04-11T03:52:12.9734921Z         Args:
2025-04-11T03:52:12.9735061Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9735135Z         """
2025-04-11T03:52:12.9735278Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9735380Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9735461Z             return True
2025-04-11T03:52:12.9735537Z     
2025-04-11T03:52:12.9735669Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9735786Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9735880Z             self.sentinels.keys(),
2025-04-11T03:52:12.9735963Z             timeout=timeout,
2025-04-11T03:52:12.9736039Z         )
2025-04-11T03:52:12.9736110Z     
2025-04-11T03:52:12.9736195Z         error_index = None
2025-04-11T03:52:12.9736280Z         for sentinel in ready:
2025-04-11T03:52:12.9736384Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9736486Z             process = self.processes[index]
2025-04-11T03:52:12.9736624Z             process.join()
2025-04-11T03:52:12.9736725Z             if process.exitcode != 0:
2025-04-11T03:52:12.9736814Z                 error_index = index
2025-04-11T03:52:12.9736890Z                 break
2025-04-11T03:52:12.9736966Z     
2025-04-11T03:52:12.9737059Z         # Return if there was no error.
2025-04-11T03:52:12.9737148Z         if error_index is None:
2025-04-11T03:52:12.9737284Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9737380Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9737460Z     
2025-04-11T03:52:12.9737646Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9737750Z         for process in self.processes:
2025-04-11T03:52:12.9737906Z             if process.is_alive():
2025-04-11T03:52:12.9738004Z                 process.terminate()
2025-04-11T03:52:12.9738092Z             process.join()
2025-04-11T03:52:12.9738162Z     
2025-04-11T03:52:12.9738310Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9738427Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9738537Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9738741Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9738825Z             if exitcode < 0:
2025-04-11T03:52:12.9738938Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9739045Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9739199Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9739295Z                     error_index=error_index,
2025-04-11T03:52:12.9739402Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9739489Z                     exit_code=exitcode,
2025-04-11T03:52:12.9739580Z                     signal_name=name,
2025-04-11T03:52:12.9739662Z                 )
2025-04-11T03:52:12.9739735Z             else:
2025-04-11T03:52:12.9739843Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9740005Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9740099Z                     error_index=error_index,
2025-04-11T03:52:12.9740203Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9740345Z                     exit_code=exitcode,
2025-04-11T03:52:12.9740427Z                 )
2025-04-11T03:52:12.9740500Z     
2025-04-11T03:52:12.9740636Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9740805Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9740893Z         msg += original_trace
2025-04-11T03:52:12.9741069Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9741235Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9741316Z E       
2025-04-11T03:52:12.9741443Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9741545Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9741839Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9741922Z E           fn(i, *args)
2025-04-11T03:52:12.9742178Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T03:52:12.9742270Z E           exam_grad_clipping()
2025-04-11T03:52:12.9742529Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9742616Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9742875Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9742963Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9743266Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9743361Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9743468Z E         [Previous line repeated 2 more times]
2025-04-11T03:52:12.9743746Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T03:52:12.9743849Z E           torch_model = model_builder().cuda()
2025-04-11T03:52:12.9744137Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9744238Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9744499Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9744683Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9744953Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9745044Z E           module._apply(fn)
2025-04-11T03:52:12.9745309Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9745456Z E           module._apply(fn)
2025-04-11T03:52:12.9745725Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9745819Z E           param_applied = fn(param)
2025-04-11T03:52:12.9746094Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9746210Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9746325Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9746612Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9746754Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9746915Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9746919Z 
2025-04-11T03:52:12.9747227Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9747435Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9747598Z [04/11/25 03:51:03] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9747733Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9747841Z                              :75 launch                                         
2025-04-11T03:52:12.9747982Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9748105Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9748303Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9748487Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9749621Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9749793Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9750963Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9751137Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9751837Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9751920Z   warnings.warn(
2025-04-11T03:52:12.9752610Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9752763Z   warnings.warn(
2025-04-11T03:52:12.9753601Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9753746Z   warnings.warn(
2025-04-11T03:52:12.9754555Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9754638Z   warnings.warn(
2025-04-11T03:52:12.9755439Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9755521Z   warnings.warn(
2025-04-11T03:52:12.9756383Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9756473Z   warnings.warn(
2025-04-11T03:52:12.9757270Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9757353Z   warnings.warn(
2025-04-11T03:52:12.9758171Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9758252Z   warnings.warn(
2025-04-11T03:52:12.9758556Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26619 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9759506Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9759680Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9759822Z ______________________________ test_inference[1] _______________________________
2025-04-11T03:52:12.9759826Z 
2025-04-11T03:52:12.9760004Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T03:52:12.9760602Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9760610Z 
2025-04-11T03:52:12.9760714Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9760796Z         try_count = 0
2025-04-11T03:52:12.9760901Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9760985Z             max_try, int
2025-04-11T03:52:12.9761135Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9761269Z     
2025-04-11T03:52:12.9761385Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9761459Z             try:
2025-04-11T03:52:12.9761544Z                 try_count += 1
2025-04-11T03:52:12.9761640Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9761725Z                 return ret
2025-04-11T03:52:12.9761825Z             except exception_type as e:
2025-04-11T03:52:12.9761926Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9762168Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9762290Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9762435Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9762595Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9762678Z                     continue
2025-04-11T03:52:12.9762759Z                 else:
2025-04-11T03:52:12.9762980Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9763061Z >                   raise e
2025-04-11T03:52:12.9763069Z 
2025-04-11T03:52:12.9763165Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9763277Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9763414Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9763503Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9763726Z tests/test_zero/test_gemini/test_inference.py:118: in test_inference
2025-04-11T03:52:12.9763819Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9763922Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9764029Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9764289Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9764468Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9764751Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9764841Z     while not context.join():
2025-04-11T03:52:12.9764953Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9764957Z 
2025-04-11T03:52:12.9765157Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfa290>
2025-04-11T03:52:12.9765237Z timeout = None
2025-04-11T03:52:12.9765241Z 
2025-04-11T03:52:12.9765333Z     def join(self, timeout=None):
2025-04-11T03:52:12.9765463Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9765533Z     
2025-04-11T03:52:12.9765680Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9765823Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9765989Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9766082Z         of the first process exiting.
2025-04-11T03:52:12.9766151Z     
2025-04-11T03:52:12.9766301Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9766492Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9766569Z     
2025-04-11T03:52:12.9766645Z         Args:
2025-04-11T03:52:12.9766779Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9766862Z         """
2025-04-11T03:52:12.9767000Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9767095Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9767178Z             return True
2025-04-11T03:52:12.9767248Z     
2025-04-11T03:52:12.9767380Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9767498Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9767666Z             self.sentinels.keys(),
2025-04-11T03:52:12.9767750Z             timeout=timeout,
2025-04-11T03:52:12.9767825Z         )
2025-04-11T03:52:12.9767897Z     
2025-04-11T03:52:12.9767982Z         error_index = None
2025-04-11T03:52:12.9768073Z         for sentinel in ready:
2025-04-11T03:52:12.9768181Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9768284Z             process = self.processes[index]
2025-04-11T03:52:12.9768366Z             process.join()
2025-04-11T03:52:12.9768525Z             if process.exitcode != 0:
2025-04-11T03:52:12.9768619Z                 error_index = index
2025-04-11T03:52:12.9768696Z                 break
2025-04-11T03:52:12.9768769Z     
2025-04-11T03:52:12.9768861Z         # Return if there was no error.
2025-04-11T03:52:12.9768946Z         if error_index is None:
2025-04-11T03:52:12.9769085Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9769183Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9769260Z     
2025-04-11T03:52:12.9769401Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9769501Z         for process in self.processes:
2025-04-11T03:52:12.9769590Z             if process.is_alive():
2025-04-11T03:52:12.9769681Z                 process.terminate()
2025-04-11T03:52:12.9769769Z             process.join()
2025-04-11T03:52:12.9769840Z     
2025-04-11T03:52:12.9769985Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9770101Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9770263Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9770388Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9770472Z             if exitcode < 0:
2025-04-11T03:52:12.9770583Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9770688Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9770844Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9770938Z                     error_index=error_index,
2025-04-11T03:52:12.9771039Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9771133Z                     exit_code=exitcode,
2025-04-11T03:52:12.9771219Z                     signal_name=name,
2025-04-11T03:52:12.9771293Z                 )
2025-04-11T03:52:12.9771366Z             else:
2025-04-11T03:52:12.9771471Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9771642Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9771736Z                     error_index=error_index,
2025-04-11T03:52:12.9771840Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9771928Z                     exit_code=exitcode,
2025-04-11T03:52:12.9772002Z                 )
2025-04-11T03:52:12.9772073Z     
2025-04-11T03:52:12.9772201Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9772378Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9772463Z         msg += original_trace
2025-04-11T03:52:12.9772692Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9772857Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9772933Z E       
2025-04-11T03:52:12.9773061Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9773162Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9773467Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9773549Z E           fn(i, *args)
2025-04-11T03:52:12.9773805Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T03:52:12.9773890Z E           exam_inference()
2025-04-11T03:52:12.9774212Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9774302Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9774551Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9774644Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9774887Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9775039Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9775302Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T03:52:12.9775406Z E           torch_model = model_builder().cuda()
2025-04-11T03:52:12.9775692Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9775791Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9776059Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9776179Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9776447Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9776535Z E           module._apply(fn)
2025-04-11T03:52:12.9776799Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9776887Z E           module._apply(fn)
2025-04-11T03:52:12.9777203Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9777303Z E           param_applied = fn(param)
2025-04-11T03:52:12.9777572Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9777694Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9777803Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9778097Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9778238Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9778402Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9778408Z 
2025-04-11T03:52:12.9778714Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9778869Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9779030Z [04/11/25 03:51:10] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9779158Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9779269Z                              :75 launch                                         
2025-04-11T03:52:12.9779405Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9779585Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.9779786Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9779932Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9781068Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9781304Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9782007Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9782088Z   warnings.warn(
2025-04-11T03:52:12.9782911Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9783051Z   warnings.warn(
2025-04-11T03:52:12.9783854Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9783935Z   warnings.warn(
2025-04-11T03:52:12.9784743Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9784821Z   warnings.warn(
2025-04-11T03:52:12.9785014Z ______________________________ test_inference[4] _______________________________
2025-04-11T03:52:12.9785019Z 
2025-04-11T03:52:12.9785137Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9785728Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9785735Z 
2025-04-11T03:52:12.9785838Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9785919Z         try_count = 0
2025-04-11T03:52:12.9786021Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9786106Z             max_try, int
2025-04-11T03:52:12.9786257Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9786327Z     
2025-04-11T03:52:12.9786442Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9786519Z             try:
2025-04-11T03:52:12.9786609Z                 try_count += 1
2025-04-11T03:52:12.9786702Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9786781Z                 return ret
2025-04-11T03:52:12.9786879Z             except exception_type as e:
2025-04-11T03:52:12.9786980Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9787169Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9787289Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9787433Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9787643Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9787728Z                     continue
2025-04-11T03:52:12.9787808Z                 else:
2025-04-11T03:52:12.9788027Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9788111Z >                   raise e
2025-04-11T03:52:12.9788117Z 
2025-04-11T03:52:12.9788213Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9788322Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9788486Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9788576Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9788909Z tests/test_zero/test_gemini/test_inference.py:118: in test_inference
2025-04-11T03:52:12.9788998Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9789103Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9789202Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9789464Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9789640Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9789991Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9790085Z     while not context.join():
2025-04-11T03:52:12.9790195Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9790199Z 
2025-04-11T03:52:12.9790402Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ec01e70>
2025-04-11T03:52:12.9790480Z timeout = None
2025-04-11T03:52:12.9790486Z 
2025-04-11T03:52:12.9790580Z     def join(self, timeout=None):
2025-04-11T03:52:12.9790706Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9790779Z     
2025-04-11T03:52:12.9790932Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9791076Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9791244Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9791339Z         of the first process exiting.
2025-04-11T03:52:12.9791410Z     
2025-04-11T03:52:12.9791621Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9791760Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9791836Z     
2025-04-11T03:52:12.9791911Z         Args:
2025-04-11T03:52:12.9792052Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9792127Z         """
2025-04-11T03:52:12.9792267Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9792363Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9792445Z             return True
2025-04-11T03:52:12.9792520Z     
2025-04-11T03:52:12.9792653Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9792772Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9792867Z             self.sentinels.keys(),
2025-04-11T03:52:12.9792951Z             timeout=timeout,
2025-04-11T03:52:12.9793025Z         )
2025-04-11T03:52:12.9793097Z     
2025-04-11T03:52:12.9793182Z         error_index = None
2025-04-11T03:52:12.9793272Z         for sentinel in ready:
2025-04-11T03:52:12.9793378Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9793480Z             process = self.processes[index]
2025-04-11T03:52:12.9793564Z             process.join()
2025-04-11T03:52:12.9793662Z             if process.exitcode != 0:
2025-04-11T03:52:12.9793751Z                 error_index = index
2025-04-11T03:52:12.9793827Z                 break
2025-04-11T03:52:12.9793901Z     
2025-04-11T03:52:12.9793993Z         # Return if there was no error.
2025-04-11T03:52:12.9794081Z         if error_index is None:
2025-04-11T03:52:12.9794286Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9794385Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9794460Z     
2025-04-11T03:52:12.9794602Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9794701Z         for process in self.processes:
2025-04-11T03:52:12.9794792Z             if process.is_alive():
2025-04-11T03:52:12.9794887Z                 process.terminate()
2025-04-11T03:52:12.9794969Z             process.join()
2025-04-11T03:52:12.9795038Z     
2025-04-11T03:52:12.9795182Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9795298Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9795474Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9795600Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9795687Z             if exitcode < 0:
2025-04-11T03:52:12.9795801Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9795910Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9796063Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9796214Z                     error_index=error_index,
2025-04-11T03:52:12.9796320Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9796408Z                     exit_code=exitcode,
2025-04-11T03:52:12.9796495Z                     signal_name=name,
2025-04-11T03:52:12.9796573Z                 )
2025-04-11T03:52:12.9796647Z             else:
2025-04-11T03:52:12.9796752Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9796913Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9797008Z                     error_index=error_index,
2025-04-11T03:52:12.9797112Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9797197Z                     exit_code=exitcode,
2025-04-11T03:52:12.9797275Z                 )
2025-04-11T03:52:12.9797346Z     
2025-04-11T03:52:12.9797482Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9797652Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9797741Z         msg += original_trace
2025-04-11T03:52:12.9797973Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9798135Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9798211Z E       
2025-04-11T03:52:12.9798337Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9798437Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9798737Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9798837Z E           fn(i, *args)
2025-04-11T03:52:12.9799097Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T03:52:12.9799181Z E           exam_inference()
2025-04-11T03:52:12.9799439Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9799529Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9799781Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9799870Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9800111Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9800202Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9800457Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T03:52:12.9800562Z E           torch_model = model_builder().cuda()
2025-04-11T03:52:12.9800899Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9801003Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9801269Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9801391Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9801660Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9801746Z E           module._apply(fn)
2025-04-11T03:52:12.9802014Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9802160Z E           module._apply(fn)
2025-04-11T03:52:12.9802428Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9802523Z E           param_applied = fn(param)
2025-04-11T03:52:12.9802796Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9802917Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9803079Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9803373Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9803511Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9803675Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9803679Z 
2025-04-11T03:52:12.9803980Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9804137Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9804296Z [04/11/25 03:51:18] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9804426Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9804537Z                              :75 launch                                         
2025-04-11T03:52:12.9804675Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9804859Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9805058Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9805205Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9806316Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9806490Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9807586Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9807756Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9808911Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9809081Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9810152Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9810396Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9811067Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9811155Z   warnings.warn(
2025-04-11T03:52:12.9811870Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9811954Z   warnings.warn(
2025-04-11T03:52:12.9812609Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9812693Z   warnings.warn(
2025-04-11T03:52:12.9813345Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9813427Z   warnings.warn(
2025-04-11T03:52:12.9814287Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9814369Z   warnings.warn(
2025-04-11T03:52:12.9815162Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9815242Z   warnings.warn(
2025-04-11T03:52:12.9816032Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9816109Z   warnings.warn(
2025-04-11T03:52:12.9816921Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9817000Z   warnings.warn(
2025-04-11T03:52:12.9817867Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9817948Z   warnings.warn(
2025-04-11T03:52:12.9818735Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9818813Z   warnings.warn(
2025-04-11T03:52:12.9819597Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9819741Z   warnings.warn(
2025-04-11T03:52:12.9820533Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9820671Z   warnings.warn(
2025-04-11T03:52:12.9821461Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9821542Z   warnings.warn(
2025-04-11T03:52:12.9822333Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9822415Z   warnings.warn(
2025-04-11T03:52:12.9823209Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9823294Z   warnings.warn(
2025-04-11T03:52:12.9823659Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9824451Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9824530Z   warnings.warn(
2025-04-11T03:52:12.9824827Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9825105Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9826048Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9826223Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9827196Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9827362Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9828274Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9828481Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9828692Z ________________________________ test_optim[4] _________________________________
2025-04-11T03:52:12.9828697Z 
2025-04-11T03:52:12.9828815Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9829411Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9829478Z 
2025-04-11T03:52:12.9829584Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9829669Z         try_count = 0
2025-04-11T03:52:12.9829771Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9829853Z             max_try, int
2025-04-11T03:52:12.9830004Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9830075Z     
2025-04-11T03:52:12.9830194Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9830270Z             try:
2025-04-11T03:52:12.9830359Z                 try_count += 1
2025-04-11T03:52:12.9830450Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9830532Z                 return ret
2025-04-11T03:52:12.9830632Z             except exception_type as e:
2025-04-11T03:52:12.9830733Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9830924Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9831044Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9831249Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9831411Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9831493Z                     continue
2025-04-11T03:52:12.9831577Z                 else:
2025-04-11T03:52:12.9831796Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9831883Z >                   raise e
2025-04-11T03:52:12.9831887Z 
2025-04-11T03:52:12.9831982Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9832098Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9832233Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9832320Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9832472Z tests/test_zero/test_gemini/test_optim.py:193: in test_optim
2025-04-11T03:52:12.9832566Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9832673Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9832775Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9833032Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9833212Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9833495Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9833591Z     while not context.join():
2025-04-11T03:52:12.9833701Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9833705Z 
2025-04-11T03:52:12.9833964Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfa290>
2025-04-11T03:52:12.9834044Z timeout = None
2025-04-11T03:52:12.9834048Z 
2025-04-11T03:52:12.9834145Z     def join(self, timeout=None):
2025-04-11T03:52:12.9834274Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9834345Z     
2025-04-11T03:52:12.9834498Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9834646Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9834814Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9834909Z         of the first process exiting.
2025-04-11T03:52:12.9835046Z     
2025-04-11T03:52:12.9835201Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9835337Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9835414Z     
2025-04-11T03:52:12.9835492Z         Args:
2025-04-11T03:52:12.9835638Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9835711Z         """
2025-04-11T03:52:12.9835852Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9836007Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9836087Z             return True
2025-04-11T03:52:12.9836163Z     
2025-04-11T03:52:12.9836295Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9836412Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9836511Z             self.sentinels.keys(),
2025-04-11T03:52:12.9836596Z             timeout=timeout,
2025-04-11T03:52:12.9836676Z         )
2025-04-11T03:52:12.9836744Z     
2025-04-11T03:52:12.9836830Z         error_index = None
2025-04-11T03:52:12.9836915Z         for sentinel in ready:
2025-04-11T03:52:12.9837026Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9837131Z             process = self.processes[index]
2025-04-11T03:52:12.9837216Z             process.join()
2025-04-11T03:52:12.9837316Z             if process.exitcode != 0:
2025-04-11T03:52:12.9837404Z                 error_index = index
2025-04-11T03:52:12.9837483Z                 break
2025-04-11T03:52:12.9837555Z     
2025-04-11T03:52:12.9837648Z         # Return if there was no error.
2025-04-11T03:52:12.9837790Z         if error_index is None:
2025-04-11T03:52:12.9837928Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9838026Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9838121Z     
2025-04-11T03:52:12.9838337Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9838471Z         for process in self.processes:
2025-04-11T03:52:12.9838564Z             if process.is_alive():
2025-04-11T03:52:12.9838657Z                 process.terminate()
2025-04-11T03:52:12.9838742Z             process.join()
2025-04-11T03:52:12.9838813Z     
2025-04-11T03:52:12.9838960Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9839078Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9839189Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9839313Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9839399Z             if exitcode < 0:
2025-04-11T03:52:12.9839510Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9839617Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9839768Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9839862Z                     error_index=error_index,
2025-04-11T03:52:12.9839970Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9840058Z                     exit_code=exitcode,
2025-04-11T03:52:12.9840144Z                     signal_name=name,
2025-04-11T03:52:12.9840222Z                 )
2025-04-11T03:52:12.9840351Z             else:
2025-04-11T03:52:12.9840459Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9840623Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9840722Z                     error_index=error_index,
2025-04-11T03:52:12.9840823Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9840911Z                     exit_code=exitcode,
2025-04-11T03:52:12.9840988Z                 )
2025-04-11T03:52:12.9841057Z     
2025-04-11T03:52:12.9841193Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9841362Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9841510Z         msg += original_trace
2025-04-11T03:52:12.9841691Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9841854Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9841931Z E       
2025-04-11T03:52:12.9842062Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9842165Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9842466Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9842612Z E           fn(i, *args)
2025-04-11T03:52:12.9842860Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 185, in run_dist
2025-04-11T03:52:12.9842946Z E           exam_model_step()
2025-04-11T03:52:12.9843203Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9843293Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9843545Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9843633Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9843879Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9843971Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9844078Z E         [Previous line repeated 2 more times]
2025-04-11T03:52:12.9844331Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 75, in exam_model_step
2025-04-11T03:52:12.9844486Z E           torch_model = model_builder().cuda()
2025-04-11T03:52:12.9844777Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9844877Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9845146Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9845273Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9845543Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9845638Z E           module._apply(fn)
2025-04-11T03:52:12.9845901Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9845993Z E           module._apply(fn)
2025-04-11T03:52:12.9846260Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9846356Z E           param_applied = fn(param)
2025-04-11T03:52:12.9846629Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9846746Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9846856Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9847142Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9847336Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9847502Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9847507Z 
2025-04-11T03:52:12.9847822Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9847981Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9848138Z [04/11/25 03:51:26] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9848268Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9848374Z                              :75 launch                                         
2025-04-11T03:52:12.9848578Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9848702Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9848904Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9849048Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9850176Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9850414Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9851518Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9851685Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9852847Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9853016Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9854109Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9854271Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9854956Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9855040Z   warnings.warn(
2025-04-11T03:52:12.9855724Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9855860Z   warnings.warn(
2025-04-11T03:52:12.9856545Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9856628Z   warnings.warn(
2025-04-11T03:52:12.9857298Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9857443Z   warnings.warn(
2025-04-11T03:52:12.9858271Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9858354Z   warnings.warn(
2025-04-11T03:52:12.9859161Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9859303Z   warnings.warn(
2025-04-11T03:52:12.9860119Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9860204Z   warnings.warn(
2025-04-11T03:52:12.9861016Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9861100Z   warnings.warn(
2025-04-11T03:52:12.9861970Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9862058Z   warnings.warn(
2025-04-11T03:52:12.9862840Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9862923Z   warnings.warn(
2025-04-11T03:52:12.9863707Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9863791Z   warnings.warn(
2025-04-11T03:52:12.9864580Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9864663Z   warnings.warn(
2025-04-11T03:52:12.9865508Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9865595Z   warnings.warn(
2025-04-11T03:52:12.9866388Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9866472Z   warnings.warn(
2025-04-11T03:52:12.9867265Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9867406Z   warnings.warn(
2025-04-11T03:52:12.9868204Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9868342Z   warnings.warn(
2025-04-11T03:52:12.9869322Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9869494Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9870419Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9870588Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9871562Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9871725Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9871867Z ________________________________ test_search[1] ________________________________
2025-04-11T03:52:12.9871871Z 
2025-04-11T03:52:12.9871989Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T03:52:12.9872589Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9872595Z 
2025-04-11T03:52:12.9872699Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9872786Z         try_count = 0
2025-04-11T03:52:12.9872886Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9872972Z             max_try, int
2025-04-11T03:52:12.9873119Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9873189Z     
2025-04-11T03:52:12.9873305Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9873382Z             try:
2025-04-11T03:52:12.9873470Z                 try_count += 1
2025-04-11T03:52:12.9873562Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9873643Z                 return ret
2025-04-11T03:52:12.9873810Z             except exception_type as e:
2025-04-11T03:52:12.9873913Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9874102Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9874226Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9874377Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9874530Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9874614Z                     continue
2025-04-11T03:52:12.9874695Z                 else:
2025-04-11T03:52:12.9874914Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9875071Z >                   raise e
2025-04-11T03:52:12.9875074Z 
2025-04-11T03:52:12.9875170Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9875285Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9875417Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9875506Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9875657Z tests/test_zero/test_gemini/test_search.py:68: in test_search
2025-04-11T03:52:12.9875809Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9875917Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9876015Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9876278Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9876457Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9876751Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9876845Z     while not context.join():
2025-04-11T03:52:12.9876955Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9876961Z 
2025-04-11T03:52:12.9877161Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfa4d0>
2025-04-11T03:52:12.9877240Z timeout = None
2025-04-11T03:52:12.9877244Z 
2025-04-11T03:52:12.9877340Z     def join(self, timeout=None):
2025-04-11T03:52:12.9877466Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9877590Z     
2025-04-11T03:52:12.9877743Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9877888Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9878055Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9878148Z         of the first process exiting.
2025-04-11T03:52:12.9878224Z     
2025-04-11T03:52:12.9878370Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9878504Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9878578Z     
2025-04-11T03:52:12.9878654Z         Args:
2025-04-11T03:52:12.9878793Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9878867Z         """
2025-04-11T03:52:12.9879005Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9879103Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9879187Z             return True
2025-04-11T03:52:12.9879262Z     
2025-04-11T03:52:12.9879391Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9879510Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9879602Z             self.sentinels.keys(),
2025-04-11T03:52:12.9879685Z             timeout=timeout,
2025-04-11T03:52:12.9879762Z         )
2025-04-11T03:52:12.9879832Z     
2025-04-11T03:52:12.9879917Z         error_index = None
2025-04-11T03:52:12.9880000Z         for sentinel in ready:
2025-04-11T03:52:12.9880109Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9880271Z             process = self.processes[index]
2025-04-11T03:52:12.9880360Z             process.join()
2025-04-11T03:52:12.9880461Z             if process.exitcode != 0:
2025-04-11T03:52:12.9880550Z                 error_index = index
2025-04-11T03:52:12.9880626Z                 break
2025-04-11T03:52:12.9880700Z     
2025-04-11T03:52:12.9880793Z         # Return if there was no error.
2025-04-11T03:52:12.9880880Z         if error_index is None:
2025-04-11T03:52:12.9881014Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9881115Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9881187Z     
2025-04-11T03:52:12.9881327Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9881508Z         for process in self.processes:
2025-04-11T03:52:12.9881596Z             if process.is_alive():
2025-04-11T03:52:12.9881691Z                 process.terminate()
2025-04-11T03:52:12.9881775Z             process.join()
2025-04-11T03:52:12.9881846Z     
2025-04-11T03:52:12.9881990Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9882106Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9882275Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9882397Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9882486Z             if exitcode < 0:
2025-04-11T03:52:12.9882594Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9882700Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9882855Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9882951Z                     error_index=error_index,
2025-04-11T03:52:12.9883057Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9883147Z                     exit_code=exitcode,
2025-04-11T03:52:12.9883233Z                     signal_name=name,
2025-04-11T03:52:12.9883312Z                 )
2025-04-11T03:52:12.9883387Z             else:
2025-04-11T03:52:12.9883492Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9883658Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9883758Z                     error_index=error_index,
2025-04-11T03:52:12.9883910Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9884000Z                     exit_code=exitcode,
2025-04-11T03:52:12.9884078Z                 )
2025-04-11T03:52:12.9884149Z     
2025-04-11T03:52:12.9884285Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9884456Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9884550Z         msg += original_trace
2025-04-11T03:52:12.9884725Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9884885Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9884963Z E       
2025-04-11T03:52:12.9885088Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9885188Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9885492Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9885577Z E           fn(i, *args)
2025-04-11T03:52:12.9885826Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T03:52:12.9885914Z E           exam_chunk_manager()
2025-04-11T03:52:12.9886183Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T03:52:12.9886289Z E           chunk_manager = init_chunk_manager(
2025-04-11T03:52:12.9886563Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T03:52:12.9886649Z E           dist.barrier()
2025-04-11T03:52:12.9887023Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:12.9887127Z E           return func(*args, **kwargs)
2025-04-11T03:52:12.9887448Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T03:52:12.9887560Z E           work = default_pg.barrier(opts=opts)
2025-04-11T03:52:12.9887665Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9887952Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9888088Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9888313Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9888317Z 
2025-04-11T03:52:12.9888620Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9888773Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9888931Z [04/11/25 03:51:30] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9889115Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9889228Z                              :75 launch                                         
2025-04-11T03:52:12.9889364Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9889491Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.9889685Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9889820Z ________________________________ test_search[4] ________________________________
2025-04-11T03:52:12.9889824Z 
2025-04-11T03:52:12.9889939Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9890520Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9890530Z 
2025-04-11T03:52:12.9890682Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9890766Z         try_count = 0
2025-04-11T03:52:12.9890869Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9890951Z             max_try, int
2025-04-11T03:52:12.9891100Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9891171Z     
2025-04-11T03:52:12.9891285Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9891365Z             try:
2025-04-11T03:52:12.9891448Z                 try_count += 1
2025-04-11T03:52:12.9891542Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9891621Z                 return ret
2025-04-11T03:52:12.9891721Z             except exception_type as e:
2025-04-11T03:52:12.9891819Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9892006Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9892128Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9892275Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9892432Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9892512Z                     continue
2025-04-11T03:52:12.9892591Z                 else:
2025-04-11T03:52:12.9892808Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9892889Z >                   raise e
2025-04-11T03:52:12.9892894Z 
2025-04-11T03:52:12.9892991Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9893160Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9893298Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9893384Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9893536Z tests/test_zero/test_gemini/test_search.py:68: in test_search
2025-04-11T03:52:12.9893628Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9893732Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9893833Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9894091Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9894269Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9894619Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9894712Z     while not context.join():
2025-04-11T03:52:12.9894820Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9894824Z 
2025-04-11T03:52:12.9895023Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfa7d0>
2025-04-11T03:52:12.9895108Z timeout = None
2025-04-11T03:52:12.9895166Z 
2025-04-11T03:52:12.9895259Z     def join(self, timeout=None):
2025-04-11T03:52:12.9895391Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9895461Z     
2025-04-11T03:52:12.9895608Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9895752Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9895912Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9896013Z         of the first process exiting.
2025-04-11T03:52:12.9896082Z     
2025-04-11T03:52:12.9896232Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9896369Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9896438Z     
2025-04-11T03:52:12.9896517Z         Args:
2025-04-11T03:52:12.9896653Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9896731Z         """
2025-04-11T03:52:12.9896871Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9897025Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9897108Z             return True
2025-04-11T03:52:12.9897180Z     
2025-04-11T03:52:12.9897319Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9897436Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9897531Z             self.sentinels.keys(),
2025-04-11T03:52:12.9897617Z             timeout=timeout,
2025-04-11T03:52:12.9897690Z         )
2025-04-11T03:52:12.9897763Z     
2025-04-11T03:52:12.9897845Z         error_index = None
2025-04-11T03:52:12.9897933Z         for sentinel in ready:
2025-04-11T03:52:12.9898043Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9898143Z             process = self.processes[index]
2025-04-11T03:52:12.9898233Z             process.join()
2025-04-11T03:52:12.9898327Z             if process.exitcode != 0:
2025-04-11T03:52:12.9898420Z                 error_index = index
2025-04-11T03:52:12.9898498Z                 break
2025-04-11T03:52:12.9898572Z     
2025-04-11T03:52:12.9898667Z         # Return if there was no error.
2025-04-11T03:52:12.9898753Z         if error_index is None:
2025-04-11T03:52:12.9898888Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9898992Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9899066Z     
2025-04-11T03:52:12.9899207Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9899303Z         for process in self.processes:
2025-04-11T03:52:12.9899396Z             if process.is_alive():
2025-04-11T03:52:12.9899487Z                 process.terminate()
2025-04-11T03:52:12.9899633Z             process.join()
2025-04-11T03:52:12.9899707Z     
2025-04-11T03:52:12.9899853Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9899971Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9900082Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9900208Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9900292Z             if exitcode < 0:
2025-04-11T03:52:12.9900406Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9900512Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9900660Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9900824Z                     error_index=error_index,
2025-04-11T03:52:12.9900927Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9901020Z                     exit_code=exitcode,
2025-04-11T03:52:12.9901107Z                     signal_name=name,
2025-04-11T03:52:12.9901184Z                 )
2025-04-11T03:52:12.9901258Z             else:
2025-04-11T03:52:12.9901361Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9901531Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9901681Z                     error_index=error_index,
2025-04-11T03:52:12.9901788Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9901876Z                     exit_code=exitcode,
2025-04-11T03:52:12.9901948Z                 )
2025-04-11T03:52:12.9902021Z     
2025-04-11T03:52:12.9902151Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9902326Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9902415Z         msg += original_trace
2025-04-11T03:52:12.9902587Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9902751Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9902823Z E       
2025-04-11T03:52:12.9902953Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9903050Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9903354Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9903495Z E           fn(i, *args)
2025-04-11T03:52:12.9903739Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T03:52:12.9903828Z E           exam_chunk_manager()
2025-04-11T03:52:12.9904084Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T03:52:12.9904193Z E           chunk_manager = init_chunk_manager(
2025-04-11T03:52:12.9904453Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T03:52:12.9904541Z E           dist.barrier()
2025-04-11T03:52:12.9904834Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:12.9904932Z E           return func(*args, **kwargs)
2025-04-11T03:52:12.9905246Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T03:52:12.9905349Z E           work = default_pg.barrier(opts=opts)
2025-04-11T03:52:12.9905456Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9905734Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9905877Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9906037Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9906041Z 
2025-04-11T03:52:12.9906492Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9906650Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9906812Z [04/11/25 03:51:36] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9906946Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9907054Z                              :75 launch                                         
2025-04-11T03:52:12.9907195Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9907320Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9907585Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9907729Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9908026Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43909 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9908304Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43909 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9908685Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43909 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9908820Z _______________________________ test_zero_ddp[4] _______________________________
2025-04-11T03:52:12.9908825Z 
2025-04-11T03:52:12.9908940Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9909527Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9909534Z 
2025-04-11T03:52:12.9909637Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9909723Z         try_count = 0
2025-04-11T03:52:12.9909829Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9909912Z             max_try, int
2025-04-11T03:52:12.9910060Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9910136Z     
2025-04-11T03:52:12.9910315Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9910395Z             try:
2025-04-11T03:52:12.9910485Z                 try_count += 1
2025-04-11T03:52:12.9910577Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9910661Z                 return ret
2025-04-11T03:52:12.9910756Z             except exception_type as e:
2025-04-11T03:52:12.9910857Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9911046Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9911162Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9911311Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9911463Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9911549Z                     continue
2025-04-11T03:52:12.9911625Z                 else:
2025-04-11T03:52:12.9911841Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9911927Z >                   raise e
2025-04-11T03:52:12.9911931Z 
2025-04-11T03:52:12.9912026Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9912141Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9912276Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9912367Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9912548Z tests/test_zero/test_gemini/test_zeroddp_state_dict.py:85: in test_zero_ddp
2025-04-11T03:52:12.9912637Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9912814Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9912917Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9913176Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9913356Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9913646Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9913737Z     while not context.join():
2025-04-11T03:52:12.9913847Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9913851Z 
2025-04-11T03:52:12.9914123Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be1e89a0>
2025-04-11T03:52:12.9914205Z timeout = None
2025-04-11T03:52:12.9914210Z 
2025-04-11T03:52:12.9914303Z     def join(self, timeout=None):
2025-04-11T03:52:12.9914429Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9914503Z     
2025-04-11T03:52:12.9914649Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9914793Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9915023Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9915120Z         of the first process exiting.
2025-04-11T03:52:12.9915197Z     
2025-04-11T03:52:12.9915346Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9915488Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9915563Z     
2025-04-11T03:52:12.9915642Z         Args:
2025-04-11T03:52:12.9915785Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9915862Z         """
2025-04-11T03:52:12.9916007Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9916104Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9916189Z             return True
2025-04-11T03:52:12.9916267Z     
2025-04-11T03:52:12.9916401Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9916527Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9916622Z             self.sentinels.keys(),
2025-04-11T03:52:12.9916765Z             timeout=timeout,
2025-04-11T03:52:12.9916843Z         )
2025-04-11T03:52:12.9916914Z     
2025-04-11T03:52:12.9917000Z         error_index = None
2025-04-11T03:52:12.9917085Z         for sentinel in ready:
2025-04-11T03:52:12.9917192Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9917289Z             process = self.processes[index]
2025-04-11T03:52:12.9917375Z             process.join()
2025-04-11T03:52:12.9917474Z             if process.exitcode != 0:
2025-04-11T03:52:12.9917561Z                 error_index = index
2025-04-11T03:52:12.9917639Z                 break
2025-04-11T03:52:12.9917709Z     
2025-04-11T03:52:12.9917802Z         # Return if there was no error.
2025-04-11T03:52:12.9917892Z         if error_index is None:
2025-04-11T03:52:12.9918025Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9918129Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9918199Z     
2025-04-11T03:52:12.9918344Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9918438Z         for process in self.processes:
2025-04-11T03:52:12.9918525Z             if process.is_alive():
2025-04-11T03:52:12.9918619Z                 process.terminate()
2025-04-11T03:52:12.9918702Z             process.join()
2025-04-11T03:52:12.9918777Z     
2025-04-11T03:52:12.9918920Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9919036Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9919148Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9919320Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9919412Z             if exitcode < 0:
2025-04-11T03:52:12.9919520Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9919634Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9919783Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9919880Z                     error_index=error_index,
2025-04-11T03:52:12.9919985Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9920072Z                     exit_code=exitcode,
2025-04-11T03:52:12.9920162Z                     signal_name=name,
2025-04-11T03:52:12.9920235Z                 )
2025-04-11T03:52:12.9920374Z             else:
2025-04-11T03:52:12.9920479Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9920645Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9920743Z                     error_index=error_index,
2025-04-11T03:52:12.9920846Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9920937Z                     exit_code=exitcode,
2025-04-11T03:52:12.9921009Z                 )
2025-04-11T03:52:12.9921141Z     
2025-04-11T03:52:12.9921278Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9921448Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9921541Z         msg += original_trace
2025-04-11T03:52:12.9921714Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9921877Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9921956Z E       
2025-04-11T03:52:12.9922084Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9922186Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9922486Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9922571Z E           fn(i, *args)
2025-04-11T03:52:12.9922851Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 78, in run_dist
2025-04-11T03:52:12.9922939Z E           exam_state_dict()
2025-04-11T03:52:12.9923259Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9923354Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9923609Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9923697Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9923941Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9924028Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9924131Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.9924418Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 45, in exam_state_dict
2025-04-11T03:52:12.9924665Z E           model = GeminiDDP(model, config_dict, **placement_config, pin_memory=True, master_weights=master_weights)
2025-04-11T03:52:12.9924904Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 109, in __init__
2025-04-11T03:52:12.9925009Z E           self.chunk_manager = ChunkManager(
2025-04-11T03:52:12.9925249Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T03:52:12.9925493Z E           self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.9925604Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9925881Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9926066Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9926235Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9926240Z 
2025-04-11T03:52:12.9926547Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9926706Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9926863Z [04/11/25 03:51:45] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9926999Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9927107Z                              :75 launch                                         
2025-04-11T03:52:12.9927310Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9927436Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9927636Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9927782Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9928907Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9929141Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9930248Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9930420Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9931590Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9931764Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9932859Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9933025Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9933712Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9933797Z   warnings.warn(
2025-04-11T03:52:12.9934469Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9934613Z   warnings.warn(
2025-04-11T03:52:12.9935289Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9935376Z   warnings.warn(
2025-04-11T03:52:12.9936042Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9936188Z   warnings.warn(
2025-04-11T03:52:12.9937027Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9937109Z   warnings.warn(
2025-04-11T03:52:12.9937920Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9938062Z   warnings.warn(
2025-04-11T03:52:12.9939016Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9939103Z   warnings.warn(
2025-04-11T03:52:12.9939897Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9939981Z   warnings.warn(
2025-04-11T03:52:12.9940819Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9940905Z   warnings.warn(
2025-04-11T03:52:12.9941702Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9941782Z   warnings.warn(
2025-04-11T03:52:12.9942586Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9942664Z   warnings.warn(
2025-04-11T03:52:12.9943466Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9943546Z   warnings.warn(
2025-04-11T03:52:12.9944392Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9944474Z   warnings.warn(
2025-04-11T03:52:12.9945294Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9945374Z   warnings.warn(
2025-04-11T03:52:12.9946185Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9946331Z   warnings.warn(
2025-04-11T03:52:12.9947131Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9947268Z   warnings.warn(
2025-04-11T03:52:12.9947571Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30659 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9947856Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30659 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9948787Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T03:52:12.9948892Z   return tensor.storage().size() == 0
2025-04-11T03:52:12.9949067Z Exception ignored in: <function GeminiDDP.__del__ at 0x7efd22975f30>
2025-04-11T03:52:12.9949162Z Traceback (most recent call last):
2025-04-11T03:52:12.9949397Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T03:52:12.9949490Z     self.remove_hooks()
2025-04-11T03:52:12.9949791Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T03:52:12.9949901Z     for p in self.module.parameters():
2025-04-11T03:52:12.9950206Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T03:52:12.9950400Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T03:52:12.9950555Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T03:52:12.9951447Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T03:52:12.9951551Z   return tensor.storage().size() == 0
2025-04-11T03:52:12.9952422Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T03:52:12.9952518Z   return tensor.storage().size() == 0
2025-04-11T03:52:12.9952661Z _________________________________ test_comm_nd _________________________________
2025-04-11T03:52:12.9952666Z 
2025-04-11T03:52:12.9952759Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9953418Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9953425Z 
2025-04-11T03:52:12.9953533Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9953615Z         try_count = 0
2025-04-11T03:52:12.9953722Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9953806Z             max_try, int
2025-04-11T03:52:12.9953954Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9954028Z     
2025-04-11T03:52:12.9954143Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9954291Z             try:
2025-04-11T03:52:12.9954378Z                 try_count += 1
2025-04-11T03:52:12.9954472Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9954553Z                 return ret
2025-04-11T03:52:12.9954649Z             except exception_type as e:
2025-04-11T03:52:12.9954755Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9954944Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9955132Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9955279Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9955439Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9955520Z                     continue
2025-04-11T03:52:12.9955597Z                 else:
2025-04-11T03:52:12.9955820Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9955905Z >                   raise e
2025-04-11T03:52:12.9955909Z 
2025-04-11T03:52:12.9956008Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9956121Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9956258Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9956347Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9956508Z tests/test_zero/test_low_level/test_coll_nd.py:38: in test_comm_nd
2025-04-11T03:52:12.9956598Z     spawn(run_dist, 4)
2025-04-11T03:52:12.9956771Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9956875Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9957135Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9957311Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9957601Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9957692Z     while not context.join():
2025-04-11T03:52:12.9957805Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9957809Z 
2025-04-11T03:52:12.9958009Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f038ba90>
2025-04-11T03:52:12.9958089Z timeout = None
2025-04-11T03:52:12.9958094Z 
2025-04-11T03:52:12.9958187Z     def join(self, timeout=None):
2025-04-11T03:52:12.9958317Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9958387Z     
2025-04-11T03:52:12.9958536Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9958686Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9958846Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9958941Z         of the first process exiting.
2025-04-11T03:52:12.9959013Z     
2025-04-11T03:52:12.9959157Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9959296Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9959365Z     
2025-04-11T03:52:12.9959500Z         Args:
2025-04-11T03:52:12.9959640Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9959718Z         """
2025-04-11T03:52:12.9959855Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9959950Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9960036Z             return True
2025-04-11T03:52:12.9960106Z     
2025-04-11T03:52:12.9960240Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9960360Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9960451Z             self.sentinels.keys(),
2025-04-11T03:52:12.9960542Z             timeout=timeout,
2025-04-11T03:52:12.9960671Z         )
2025-04-11T03:52:12.9960743Z     
2025-04-11T03:52:12.9960828Z         error_index = None
2025-04-11T03:52:12.9960913Z         for sentinel in ready:
2025-04-11T03:52:12.9961022Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9961125Z             process = self.processes[index]
2025-04-11T03:52:12.9961216Z             process.join()
2025-04-11T03:52:12.9961308Z             if process.exitcode != 0:
2025-04-11T03:52:12.9961399Z                 error_index = index
2025-04-11T03:52:12.9961544Z                 break
2025-04-11T03:52:12.9961615Z     
2025-04-11T03:52:12.9961712Z         # Return if there was no error.
2025-04-11T03:52:12.9961799Z         if error_index is None:
2025-04-11T03:52:12.9961937Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9962035Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9962106Z     
2025-04-11T03:52:12.9962249Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9962347Z         for process in self.processes:
2025-04-11T03:52:12.9962436Z             if process.is_alive():
2025-04-11T03:52:12.9962527Z                 process.terminate()
2025-04-11T03:52:12.9962614Z             process.join()
2025-04-11T03:52:12.9962683Z     
2025-04-11T03:52:12.9962823Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9962944Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9963053Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9963182Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9963396Z             if exitcode < 0:
2025-04-11T03:52:12.9963506Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9963620Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9963769Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9963869Z                     error_index=error_index,
2025-04-11T03:52:12.9963978Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9964071Z                     exit_code=exitcode,
2025-04-11T03:52:12.9964158Z                     signal_name=name,
2025-04-11T03:52:12.9964231Z                 )
2025-04-11T03:52:12.9964313Z             else:
2025-04-11T03:52:12.9964418Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9964587Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9964683Z                     error_index=error_index,
2025-04-11T03:52:12.9964788Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9964880Z                     exit_code=exitcode,
2025-04-11T03:52:12.9964952Z                 )
2025-04-11T03:52:12.9965026Z     
2025-04-11T03:52:12.9965157Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9965331Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9965421Z         msg += original_trace
2025-04-11T03:52:12.9965596Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9965761Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9965833Z E       
2025-04-11T03:52:12.9966018Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9966120Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9966423Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9966507Z E           fn(i, *args)
2025-04-11T03:52:12.9966759Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 32, in run_dist
2025-04-11T03:52:12.9966852Z E           check_all_gather_2d()
2025-04-11T03:52:12.9967123Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 16, in check_all_gather_2d
2025-04-11T03:52:12.9967322Z E           tensor = torch.rand(128, device=get_current_device())
2025-04-11T03:52:12.9967428Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9967717Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9967855Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9968015Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9968077Z 
2025-04-11T03:52:12.9968391Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9968544Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9968706Z [04/11/25 03:51:50] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9968832Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9968943Z                              :75 launch                                         
2025-04-11T03:52:12.9969081Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9969210Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9969405Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9969544Z ____________________________ test_grad_accumulation ____________________________
2025-04-11T03:52:12.9969550Z 
2025-04-11T03:52:12.9969640Z     @pytest.mark.dist
2025-04-11T03:52:12.9969787Z     def test_grad_accumulation():
2025-04-11T03:52:12.9969878Z >       spawn(run_dist, 2)
2025-04-11T03:52:12.9969883Z 
2025-04-11T03:52:12.9970016Z tests/test_zero/test_low_level/test_grad_acc.py:146: 
2025-04-11T03:52:12.9970127Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9970231Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9970332Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9970590Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9970768Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9971056Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9971147Z     while not context.join():
2025-04-11T03:52:12.9971263Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9971267Z 
2025-04-11T03:52:12.9971464Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be1ebf40>
2025-04-11T03:52:12.9971545Z timeout = None
2025-04-11T03:52:12.9971553Z 
2025-04-11T03:52:12.9971643Z     def join(self, timeout=None):
2025-04-11T03:52:12.9971768Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9971847Z     
2025-04-11T03:52:12.9971991Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9972139Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9972357Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9972452Z         of the first process exiting.
2025-04-11T03:52:12.9972525Z     
2025-04-11T03:52:12.9972673Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9972813Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9972884Z     
2025-04-11T03:52:12.9972965Z         Args:
2025-04-11T03:52:12.9973104Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9973177Z         """
2025-04-11T03:52:12.9973320Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9973413Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9973564Z             return True
2025-04-11T03:52:12.9973635Z     
2025-04-11T03:52:12.9973767Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9973890Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9973982Z             self.sentinels.keys(),
2025-04-11T03:52:12.9974072Z             timeout=timeout,
2025-04-11T03:52:12.9974145Z         )
2025-04-11T03:52:12.9974214Z     
2025-04-11T03:52:12.9974306Z         error_index = None
2025-04-11T03:52:12.9974449Z         for sentinel in ready:
2025-04-11T03:52:12.9974560Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9974662Z             process = self.processes[index]
2025-04-11T03:52:12.9974753Z             process.join()
2025-04-11T03:52:12.9974848Z             if process.exitcode != 0:
2025-04-11T03:52:12.9974935Z                 error_index = index
2025-04-11T03:52:12.9975016Z                 break
2025-04-11T03:52:12.9975085Z     
2025-04-11T03:52:12.9975179Z         # Return if there was no error.
2025-04-11T03:52:12.9975267Z         if error_index is None:
2025-04-11T03:52:12.9975399Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9975501Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9975571Z     
2025-04-11T03:52:12.9975713Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9975809Z         for process in self.processes:
2025-04-11T03:52:12.9975900Z             if process.is_alive():
2025-04-11T03:52:12.9975992Z                 process.terminate()
2025-04-11T03:52:12.9976076Z             process.join()
2025-04-11T03:52:12.9976149Z     
2025-04-11T03:52:12.9976347Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9976470Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9976578Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9976701Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9976792Z             if exitcode < 0:
2025-04-11T03:52:12.9976898Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9977007Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9977159Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9977255Z                     error_index=error_index,
2025-04-11T03:52:12.9977357Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9977444Z                     exit_code=exitcode,
2025-04-11T03:52:12.9977538Z                     signal_name=name,
2025-04-11T03:52:12.9977614Z                 )
2025-04-11T03:52:12.9977694Z             else:
2025-04-11T03:52:12.9977797Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9977960Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9978056Z                     error_index=error_index,
2025-04-11T03:52:12.9978158Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9978248Z                     exit_code=exitcode,
2025-04-11T03:52:12.9978321Z                 )
2025-04-11T03:52:12.9978393Z     
2025-04-11T03:52:12.9978523Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9978749Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9978842Z         msg += original_trace
2025-04-11T03:52:12.9979014Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9979177Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9979253Z E       
2025-04-11T03:52:12.9979380Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9979480Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9979773Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9979858Z E           fn(i, *args)
2025-04-11T03:52:12.9980174Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 139, in run_dist
2025-04-11T03:52:12.9980272Z E           exam_zero_1_grad_acc(sync=True)
2025-04-11T03:52:12.9980550Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 86, in exam_zero_1_grad_acc
2025-04-11T03:52:12.9980655Z E           zero_model = zero_model.to(device)
2025-04-11T03:52:12.9980923Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:12.9981075Z E           return self._apply(convert)
2025-04-11T03:52:12.9981359Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9981448Z E           module._apply(fn)
2025-04-11T03:52:12.9981725Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9981824Z E           param_applied = fn(param)
2025-04-11T03:52:12.9982105Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:12.9982319Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.9982428Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9982717Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9982909Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9983076Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9983080Z 
2025-04-11T03:52:12.9983381Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9983535Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9983691Z [04/11/25 03:51:54] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9983823Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9983931Z                              :75 launch                                         
2025-04-11T03:52:12.9984069Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9984197Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9984329Z ________________________________ test_zero_1_2 _________________________________
2025-04-11T03:52:12.9984333Z 
2025-04-11T03:52:12.9984431Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9985021Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9985027Z 
2025-04-11T03:52:12.9985135Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9985217Z         try_count = 0
2025-04-11T03:52:12.9985319Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9985460Z             max_try, int
2025-04-11T03:52:12.9985609Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9985683Z     
2025-04-11T03:52:12.9985799Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9985880Z             try:
2025-04-11T03:52:12.9985968Z                 try_count += 1
2025-04-11T03:52:12.9986060Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9986147Z                 return ret
2025-04-11T03:52:12.9986242Z             except exception_type as e:
2025-04-11T03:52:12.9986346Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9986528Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9986719Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9986864Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9987016Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9987105Z                     continue
2025-04-11T03:52:12.9987183Z                 else:
2025-04-11T03:52:12.9987407Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9987547Z >                   raise e
2025-04-11T03:52:12.9987552Z 
2025-04-11T03:52:12.9987651Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9987762Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9987893Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9987984Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9988151Z tests/test_zero/test_low_level/test_mem_leak.py:57: in test_zero_1_2
2025-04-11T03:52:12.9988236Z     spawn(run_dist, 2)
2025-04-11T03:52:12.9988337Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9988482Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9988744Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9988919Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9989213Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9989368Z     while not context.join():
2025-04-11T03:52:12.9989485Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9989489Z 
2025-04-11T03:52:12.9989685Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be1eb2b0>
2025-04-11T03:52:12.9989767Z timeout = None
2025-04-11T03:52:12.9989773Z 
2025-04-11T03:52:12.9989863Z     def join(self, timeout=None):
2025-04-11T03:52:12.9989987Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9990062Z     
2025-04-11T03:52:12.9990206Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9990354Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9990512Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9990609Z         of the first process exiting.
2025-04-11T03:52:12.9990680Z     
2025-04-11T03:52:12.9990825Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9990963Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9991033Z     
2025-04-11T03:52:12.9991109Z         Args:
2025-04-11T03:52:12.9991245Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9991320Z         """
2025-04-11T03:52:12.9991462Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9991555Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9991640Z             return True
2025-04-11T03:52:12.9991711Z     
2025-04-11T03:52:12.9991909Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9992029Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9992120Z             self.sentinels.keys(),
2025-04-11T03:52:12.9992213Z             timeout=timeout,
2025-04-11T03:52:12.9992286Z         )
2025-04-11T03:52:12.9992358Z     
2025-04-11T03:52:12.9992443Z         error_index = None
2025-04-11T03:52:12.9992528Z         for sentinel in ready:
2025-04-11T03:52:12.9992638Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9992736Z             process = self.processes[index]
2025-04-11T03:52:12.9992824Z             process.join()
2025-04-11T03:52:12.9992918Z             if process.exitcode != 0:
2025-04-11T03:52:12.9993075Z                 error_index = index
2025-04-11T03:52:12.9993157Z                 break
2025-04-11T03:52:12.9993227Z     
2025-04-11T03:52:12.9993322Z         # Return if there was no error.
2025-04-11T03:52:12.9993409Z         if error_index is None:
2025-04-11T03:52:12.9993553Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9993650Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9993718Z     
2025-04-11T03:52:12.9993925Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9994021Z         for process in self.processes:
2025-04-11T03:52:12.9994119Z             if process.is_alive():
2025-04-11T03:52:12.9994210Z                 process.terminate()
2025-04-11T03:52:12.9994293Z             process.join()
2025-04-11T03:52:12.9994367Z     
2025-04-11T03:52:12.9994506Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9994626Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9994735Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9994859Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9994944Z             if exitcode < 0:
2025-04-11T03:52:12.9995052Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9995162Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9995312Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9995412Z                     error_index=error_index,
2025-04-11T03:52:12.9995515Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9995655Z                     exit_code=exitcode,
2025-04-11T03:52:12.9995751Z                     signal_name=name,
2025-04-11T03:52:12.9995825Z                 )
2025-04-11T03:52:12.9995906Z             else:
2025-04-11T03:52:12.9996007Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9996175Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9996270Z                     error_index=error_index,
2025-04-11T03:52:12.9996371Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9996462Z                     exit_code=exitcode,
2025-04-11T03:52:12.9996536Z                 )
2025-04-11T03:52:12.9996611Z     
2025-04-11T03:52:12.9996743Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9996910Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9997002Z         msg += original_trace
2025-04-11T03:52:12.9997178Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9997344Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9997417Z E       
2025-04-11T03:52:12.9997547Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9997646Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9997947Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9998034Z E           fn(i, *args)
2025-04-11T03:52:12.9998339Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 51, in run_dist
2025-04-11T03:52:12.9998447Z E           exam_mem_leak(world_size=world_size)
2025-04-11T03:52:12.9998701Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 36, in exam_mem_leak
2025-04-11T03:52:12.9998803Z E           zero_model = MlpModel().cuda()
2025-04-11T03:52:12.9999068Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9999186Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9999459Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9999606Z E           module._apply(fn)
2025-04-11T03:52:12.9999881Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9999975Z E           param_applied = fn(param)
2025-04-11T03:52:13.0000259Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.0000379Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.0000545Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.0000831Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.0000968Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.0001134Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.0001138Z 
2025-04-11T03:52:13.0001439Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:13.0001597Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:13.0001754Z [04/11/25 03:51:59] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:13.0001886Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:13.0001991Z                              :75 launch                                         
2025-04-11T03:52:13.0002136Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:13.0002314Z                              environment is initialized, world size: 2          
2025-04-11T03:52:13.0002513Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:13.0002666Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:13.0002963Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:33730 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:13.0003097Z ________________________________ test_zero_1_2 _________________________________
2025-04-11T03:52:13.0003101Z 
2025-04-11T03:52:13.0003194Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:13.0003787Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:13.0003792Z 
2025-04-11T03:52:13.0003896Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:13.0003982Z         try_count = 0
2025-04-11T03:52:13.0004081Z         assert max_try is None or isinstance(
2025-04-11T03:52:13.0004162Z             max_try, int
2025-04-11T03:52:13.0004314Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:13.0004384Z     
2025-04-11T03:52:13.0004502Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:13.0004578Z             try:
2025-04-11T03:52:13.0004663Z                 try_count += 1
2025-04-11T03:52:13.0004758Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:13.0004891Z                 return ret
2025-04-11T03:52:13.0004991Z             except exception_type as e:
2025-04-11T03:52:13.0005091Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:13.0005279Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:13.0005401Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:13.0005544Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:13.0005705Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:13.0005785Z                     continue
2025-04-11T03:52:13.0005864Z                 else:
2025-04-11T03:52:13.0006150Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:13.0006238Z >                   raise e
2025-04-11T03:52:13.0006242Z 
2025-04-11T03:52:13.0006341Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:13.0006457Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:13.0006596Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:13.0006740Z     ret = func(*args, **kwargs)
2025-04-11T03:52:13.0006909Z tests/test_zero/test_low_level/test_zero1_2.py:224: in test_zero_1_2
2025-04-11T03:52:13.0006995Z     spawn(run_dist, 4)
2025-04-11T03:52:13.0007104Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:13.0007206Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:13.0007463Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:13.0007645Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:13.0007930Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:13.0008027Z     while not context.join():
2025-04-11T03:52:13.0008139Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:13.0008143Z 
2025-04-11T03:52:13.0008347Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be1ebbb0>
2025-04-11T03:52:13.0008428Z timeout = None
2025-04-11T03:52:13.0008432Z 
2025-04-11T03:52:13.0008523Z     def join(self, timeout=None):
2025-04-11T03:52:13.0008705Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:13.0008778Z     
2025-04-11T03:52:13.0008930Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:13.0009073Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:13.0009238Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:13.0009336Z         of the first process exiting.
2025-04-11T03:52:13.0009407Z     
2025-04-11T03:52:13.0009557Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:13.0009696Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:13.0009768Z     
2025-04-11T03:52:13.0009844Z         Args:
2025-04-11T03:52:13.0009980Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:13.0010060Z         """
2025-04-11T03:52:13.0010200Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:13.0010302Z         if len(self.sentinels) == 0:
2025-04-11T03:52:13.0010382Z             return True
2025-04-11T03:52:13.0010457Z     
2025-04-11T03:52:13.0010587Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:13.0010704Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:13.0010799Z             self.sentinels.keys(),
2025-04-11T03:52:13.0010886Z             timeout=timeout,
2025-04-11T03:52:13.0010964Z         )
2025-04-11T03:52:13.0011035Z     
2025-04-11T03:52:13.0011118Z         error_index = None
2025-04-11T03:52:13.0011207Z         for sentinel in ready:
2025-04-11T03:52:13.0011385Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:13.0011490Z             process = self.processes[index]
2025-04-11T03:52:13.0011582Z             process.join()
2025-04-11T03:52:13.0011682Z             if process.exitcode != 0:
2025-04-11T03:52:13.0011773Z                 error_index = index
2025-04-11T03:52:13.0011849Z                 break
2025-04-11T03:52:13.0011925Z     
2025-04-11T03:52:13.0012015Z         # Return if there was no error.
2025-04-11T03:52:13.0012103Z         if error_index is None:
2025-04-11T03:52:13.0012237Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:13.0012334Z             return len(self.sentinels) == 0
2025-04-11T03:52:13.0012408Z     
2025-04-11T03:52:13.0012619Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:13.0012720Z         for process in self.processes:
2025-04-11T03:52:13.0012807Z             if process.is_alive():
2025-04-11T03:52:13.0012898Z                 process.terminate()
2025-04-11T03:52:13.0012987Z             process.join()
2025-04-11T03:52:13.0013057Z     
2025-04-11T03:52:13.0013204Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:13.0013380Z         failed_process = self.processes[error_index]
2025-04-11T03:52:13.0013491Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:13.0013615Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:13.0013697Z             if exitcode < 0:
2025-04-11T03:52:13.0013810Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:13.0013916Z                 raise ProcessExitedException(
2025-04-11T03:52:13.0014071Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:13.0014171Z                     error_index=error_index,
2025-04-11T03:52:13.0014277Z                     error_pid=failed_process.pid,
2025-04-11T03:52:13.0014368Z                     exit_code=exitcode,
2025-04-11T03:52:13.0014457Z                     signal_name=name,
2025-04-11T03:52:13.0014538Z                 )
2025-04-11T03:52:13.0014613Z             else:
2025-04-11T03:52:13.0014720Z                 raise ProcessExitedException(
2025-04-11T03:52:13.0014889Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:13.0015118Z                     error_index=error_index,
2025-04-11T03:52:13.0015225Z                     error_pid=failed_process.pid,
2025-04-11T03:52:13.0015313Z                     exit_code=exitcode,
2025-04-11T03:52:13.0015391Z                 )
2025-04-11T03:52:13.0015464Z     
2025-04-11T03:52:13.0015599Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:13.0015775Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:13.0015861Z         msg += original_trace
2025-04-11T03:52:13.0016039Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:13.0016203Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.0016279Z E       
2025-04-11T03:52:13.0016407Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:13.0016505Z E       Traceback (most recent call last):
2025-04-11T03:52:13.0016812Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.0016893Z E           fn(i, *args)
2025-04-11T03:52:13.0017148Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 217, in run_dist
2025-04-11T03:52:13.0017240Z E           exam_zero_1_torch_ddp()
2025-04-11T03:52:13.0017502Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.0017594Z E           partial_func(**kwargs)
2025-04-11T03:52:13.0017844Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.0017996Z E           partial_func(**kwargs)
2025-04-11T03:52:13.0018244Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.0018335Z E           partial_func(**kwargs)
2025-04-11T03:52:13.0018614Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 151, in exam_zero_1_torch_ddp
2025-04-11T03:52:13.0018729Z E           torch_model = MlpModel().cuda().to(dtype)
2025-04-11T03:52:13.0018995Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.0019116Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.0019442Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.0019530Z E           module._apply(fn)
2025-04-11T03:52:13.0019803Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.0019896Z E           param_applied = fn(param)
2025-04-11T03:52:13.0020171Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.0020362Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.0020475Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.0020755Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.0020892Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.0021057Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.0021063Z 
2025-04-11T03:52:13.0021363Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:13.0021519Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:13.0021671Z [04/11/25 03:52:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:13.0021801Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:13.0021910Z                              :75 launch                                         
2025-04-11T03:52:13.0022111Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:13.0022239Z                              environment is initialized, world size: 4          
2025-04-11T03:52:13.0022436Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:13.0022587Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:13.0022881Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:63920 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:13.0023018Z ________________________________ test_zero_ckpt ________________________________
2025-04-11T03:52:13.0023022Z 
2025-04-11T03:52:13.0023113Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:13.0023701Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:13.0023707Z 
2025-04-11T03:52:13.0023809Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:13.0023894Z         try_count = 0
2025-04-11T03:52:13.0023997Z         assert max_try is None or isinstance(
2025-04-11T03:52:13.0024079Z             max_try, int
2025-04-11T03:52:13.0024233Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:13.0024305Z     
2025-04-11T03:52:13.0024419Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:13.0024496Z             try:
2025-04-11T03:52:13.0024635Z                 try_count += 1
2025-04-11T03:52:13.0024731Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:13.0024813Z                 return ret
2025-04-11T03:52:13.0024913Z             except exception_type as e:
2025-04-11T03:52:13.0025015Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:13.0025210Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:13.0025327Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:13.0025471Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:13.0025629Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:13.0025773Z                     continue
2025-04-11T03:52:13.0025856Z                 else:
2025-04-11T03:52:13.0026076Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:13.0026161Z >                   raise e
2025-04-11T03:52:13.0026165Z 
2025-04-11T03:52:13.0026261Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:13.0026371Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:13.0026568Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:13.0026658Z     ret = func(*args, **kwargs)
2025-04-11T03:52:13.0026831Z tests/test_zero/test_low_level/test_zero_ckpt.py:129: in test_zero_ckpt
2025-04-11T03:52:13.0026913Z     spawn(run_dist, 4)
2025-04-11T03:52:13.0027016Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:13.0027117Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:13.0027371Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:13.0027551Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:13.0027838Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:13.0027934Z     while not context.join():
2025-04-11T03:52:13.0028041Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:13.0028047Z 
2025-04-11T03:52:13.0028248Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0389b10>
2025-04-11T03:52:13.0028379Z timeout = None
2025-04-11T03:52:13.0028384Z 
2025-04-11T03:52:13.0028526Z     def join(self, timeout=None):
2025-04-11T03:52:13.0028656Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:13.0028726Z     
2025-04-11T03:52:13.0028876Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:13.0029019Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:13.0029186Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:13.0029280Z         of the first process exiting.
2025-04-11T03:52:13.0029349Z     
2025-04-11T03:52:13.0029500Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:13.0029639Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:13.0029714Z     
2025-04-11T03:52:13.0029789Z         Args:
2025-04-11T03:52:13.0029932Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:13.0030007Z         """
2025-04-11T03:52:13.0030149Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:13.0030248Z         if len(self.sentinels) == 0:
2025-04-11T03:52:13.0030328Z             return True
2025-04-11T03:52:13.0030404Z     
2025-04-11T03:52:13.0030535Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:13.0030654Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:13.0030749Z             self.sentinels.keys(),
2025-04-11T03:52:13.0030833Z             timeout=timeout,
2025-04-11T03:52:13.0030911Z         )
2025-04-11T03:52:13.0030982Z     
2025-04-11T03:52:13.0031132Z         error_index = None
2025-04-11T03:52:13.0031225Z         for sentinel in ready:
2025-04-11T03:52:13.0031332Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:13.0031437Z             process = self.processes[index]
2025-04-11T03:52:13.0031523Z             process.join()
2025-04-11T03:52:13.0031622Z             if process.exitcode != 0:
2025-04-11T03:52:13.0031710Z                 error_index = index
2025-04-11T03:52:13.0031785Z                 break
2025-04-11T03:52:13.0031860Z     
2025-04-11T03:52:13.0031951Z         # Return if there was no error.
2025-04-11T03:52:13.0032039Z         if error_index is None:
2025-04-11T03:52:13.0032173Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:13.0032341Z             return len(self.sentinels) == 0
2025-04-11T03:52:13.0032416Z     
2025-04-11T03:52:13.0032556Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:13.0032658Z         for process in self.processes:
2025-04-11T03:52:13.0032749Z             if process.is_alive():
2025-04-11T03:52:13.0032839Z                 process.terminate()
2025-04-11T03:52:13.0032928Z             process.join()
2025-04-11T03:52:13.0033058Z     
2025-04-11T03:52:13.0033204Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:13.0033324Z         failed_process = self.processes[error_index]
2025-04-11T03:52:13.0033440Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:13.0033565Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:13.0033654Z             if exitcode < 0:
2025-04-11T03:52:13.0033769Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:13.0033878Z                 raise ProcessExitedException(
2025-04-11T03:52:13.0034030Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:13.0034125Z                     error_index=error_index,
2025-04-11T03:52:13.0034231Z                     error_pid=failed_process.pid,
2025-04-11T03:52:13.0034320Z                     exit_code=exitcode,
2025-04-11T03:52:13.0034409Z                     signal_name=name,
2025-04-11T03:52:13.0034487Z                 )
2025-04-11T03:52:13.0034563Z             else:
2025-04-11T03:52:13.0034668Z                 raise ProcessExitedException(
2025-04-11T03:52:13.0034903Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:13.0034999Z                     error_index=error_index,
2025-04-11T03:52:13.0035107Z                     error_pid=failed_process.pid,
2025-04-11T03:52:13.0035194Z                     exit_code=exitcode,
2025-04-11T03:52:13.0035271Z                 )
2025-04-11T03:52:13.0035345Z     
2025-04-11T03:52:13.0035482Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:13.0035650Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:13.0035737Z         msg += original_trace
2025-04-11T03:52:13.0035915Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:13.0036074Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.0036153Z E       
2025-04-11T03:52:13.0036279Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:13.0036377Z E       Traceback (most recent call last):
2025-04-11T03:52:13.0036676Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.0036758Z E           fn(i, *args)
2025-04-11T03:52:13.0037017Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 123, in run_dist
2025-04-11T03:52:13.0037114Z E           exam_zero_1_torch_ddp_ckpt()
2025-04-11T03:52:13.0037373Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.0037463Z E           partial_func(**kwargs)
2025-04-11T03:52:13.0037817Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 62, in exam_zero_1_torch_ddp_ckpt
2025-04-11T03:52:13.0037919Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:13.0038187Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.0038311Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.0038577Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.0038670Z E           module._apply(fn)
2025-04-11T03:52:13.0038934Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.0039096Z E           param_applied = fn(param)
2025-04-11T03:52:13.0039433Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.0039554Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.0039665Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.0039947Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.0040146Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.0040308Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.0040313Z 
2025-04-11T03:52:13.0040614Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:13.0040764Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:13.0040925Z [04/11/25 03:52:11] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:13.0041053Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:13.0041161Z                              :75 launch                                         
2025-04-11T03:52:13.0041302Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:13.0041425Z                              environment is initialized, world size: 4          
2025-04-11T03:52:13.0041677Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:13.0041825Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:13.0042124Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:37496 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:13.0042411Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:37496 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:13.0042521Z =============================== warnings summary ===============================
2025-04-11T03:52:13.0042625Z colossalai/interface/model.py:45
2025-04-11T03:52:13.0042924Z   /__w/ColossalAI/ColossalAI/colossalai/interface/model.py:45: DeprecationWarning: invalid escape sequence '\S'
2025-04-11T03:52:13.0043141Z     to_return = {re.sub(f"lora_\S\.{adapter_name}\.(weight|bias)", "base_layer", k) for k in to_return}
2025-04-11T03:52:13.0043147Z 
2025-04-11T03:52:13.0043246Z colossalai/interface/model.py:45
2025-04-11T03:52:13.0043530Z   /__w/ColossalAI/ColossalAI/colossalai/interface/model.py:45: DeprecationWarning: invalid escape sequence '\.'
2025-04-11T03:52:13.0043735Z     to_return = {re.sub(f"lora_\S\.{adapter_name}\.(weight|bias)", "base_layer", k) for k in to_return}
2025-04-11T03:52:13.0043740Z 
2025-04-11T03:52:13.0043843Z colossalai/checkpoint_io/utils.py:862
2025-04-11T03:52:13.0044140Z   /__w/ColossalAI/ColossalAI/colossalai/checkpoint_io/utils.py:862: DeprecationWarning: invalid escape sequence '\.'
2025-04-11T03:52:13.0044249Z     reg = re.compile("(.*?).index((\..*)?).json")
2025-04-11T03:52:13.0044258Z 
2025-04-11T03:52:13.0044414Z colossalai/nn/optimizer/cpu_adam.py:12
2025-04-11T03:52:13.0044716Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/cpu_adam.py:12: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T03:52:13.0044798Z     """
2025-04-11T03:52:13.0044802Z 
2025-04-11T03:52:13.0044904Z colossalai/nn/optimizer/fused_adam.py:15
2025-04-11T03:52:13.0045210Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/fused_adam.py:15: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T03:52:13.0045302Z     """Implements Adam algorithm.
2025-04-11T03:52:13.0045306Z 
2025-04-11T03:52:13.0045410Z colossalai/nn/optimizer/hybrid_adam.py:12
2025-04-11T03:52:13.0045722Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/hybrid_adam.py:12: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T03:52:13.0045878Z     """Implements Adam algorithm.
2025-04-11T03:52:13.0045886Z 
2025-04-11T03:52:13.0046169Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34
2025-04-11T03:52:13.0047315Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:13.0047548Z     deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:13.0047553Z 
2025-04-11T03:52:13.0047789Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896
2025-04-11T03:52:13.0047938Z tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T03:52:13.0048103Z tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T03:52:13.0048267Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.0048964Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:13.0049055Z     warnings.warn(
2025-04-11T03:52:13.0049059Z 
2025-04-11T03:52:13.0049330Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T03:52:13.0049549Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T03:52:13.0049757Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T03:52:13.0049964Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T03:52:13.0050171Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T03:52:13.0050292Z tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T03:52:13.0050458Z tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T03:52:13.0050612Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.0051434Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:13.0051519Z     warnings.warn(
2025-04-11T03:52:13.0051523Z 
2025-04-11T03:52:13.0051621Z <frozen importlib._bootstrap>:283
2025-04-11T03:52:13.0051761Z tests/test_config/test_load_config.py::test_load_config
2025-04-11T03:52:13.0051914Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.0052067Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.0052269Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.0052424Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.0052801Z   <frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead
2025-04-11T03:52:13.0052807Z 
2025-04-11T03:52:13.0052912Z colossalai/fx/profiler/dataflow.py:20
2025-04-11T03:52:13.0053217Z   /__w/ColossalAI/ColossalAI/colossalai/fx/profiler/dataflow.py:20: DeprecationWarning: invalid escape sequence '\_'
2025-04-11T03:52:13.0053294Z     """
2025-04-11T03:52:13.0053298Z 
2025-04-11T03:52:13.0053397Z colossalai/fx/profiler/dataflow.py:77
2025-04-11T03:52:13.0053758Z   /__w/ColossalAI/ColossalAI/colossalai/fx/profiler/dataflow.py:77: DeprecationWarning: invalid escape sequence '\_'
2025-04-11T03:52:13.0053935Z     """Analyze the autograd node dependencies and find out the memory usage.
2025-04-11T03:52:13.0053939Z 
2025-04-11T03:52:13.0054091Z colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py:31
2025-04-11T03:52:13.0054450Z   /__w/ColossalAI/ColossalAI/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py:31: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T03:52:13.0054781Z     """A wrapper for optimizer. ``ShardedOptimizerV2`` and ``ShardedModelV2`` implement Zero Redundancy Optimizer (ZeRO).
2025-04-11T03:52:13.0054785Z 
2025-04-11T03:52:13.1416603Z colossalai/inference/utils.py:80
2025-04-11T03:52:13.1416959Z   /__w/ColossalAI/ColossalAI/colossalai/inference/utils.py:80: DeprecationWarning: invalid escape sequence '\.'
2025-04-11T03:52:13.1417099Z     reg = re.compile("(.*?).index((\..*)?).json")
2025-04-11T03:52:13.1417107Z 
2025-04-11T03:52:13.1417234Z colossalai/inference/executor/rpc_worker.py:188
2025-04-11T03:52:13.1417594Z   /__w/ColossalAI/ColossalAI/colossalai/inference/executor/rpc_worker.py:188: SyntaxWarning: "is" with a literal. Did you mean "=="?
2025-04-11T03:52:13.1417697Z     if arch is "BaichuanForCausalLM":
2025-04-11T03:52:13.1417704Z 
2025-04-11T03:52:13.1417859Z tests/test_infer/test_async_engine/test_async_engine.py:49
2025-04-11T03:52:13.1418648Z   /__w/ColossalAI/ColossalAI/tests/test_infer/test_async_engine/test_async_engine.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
2025-04-11T03:52:13.1418753Z     @pytest.mark.asyncio
2025-04-11T03:52:13.1418757Z 
2025-04-11T03:52:13.1418879Z tests/test_tensor/test_dtensor/test_dtensor.py:10
2025-04-11T03:52:13.1419473Z   /__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py:10: PytestCollectionWarning: cannot collect test class 'TestModel' because it has a __init__ constructor (from: tests/test_tensor/test_dtensor/test_dtensor.py)
2025-04-11T03:52:13.1419580Z     class TestModel(torch.nn.Module):
2025-04-11T03:52:13.1419584Z 
2025-04-11T03:52:13.1419706Z tests/test_zero/test_low_level/test_mem_leak.py:23
2025-04-11T03:52:13.1420352Z   /__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py:23: PytestCollectionWarning: cannot collect test class 'TestLowLevelZeroOptimizer' because it has a __init__ constructor (from: tests/test_zero/test_low_level/test_mem_leak.py)
2025-04-11T03:52:13.1420509Z     class TestLowLevelZeroOptimizer(LowLevelZeroOptimizer):
2025-04-11T03:52:13.1420513Z 
2025-04-11T03:52:13.1420635Z tests/test_booster/test_accelerator.py: 1 warning
2025-04-11T03:52:13.1420795Z tests/test_checkpoint_io/test_general_checkpoint_io.py: 1 warning
2025-04-11T03:52:13.1420975Z tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py: 1 warning
2025-04-11T03:52:13.1421128Z tests/test_checkpoint_io/test_safetensors_async_io.py: 1 warning
2025-04-11T03:52:13.1421232Z tests/test_fp8/test_fp8_cast.py: 1 warning
2025-04-11T03:52:13.1421440Z tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py: 2 warnings
2025-04-11T03:52:13.1421650Z tests/test_shardformer/test_flash_attention.py: 1 warning
2025-04-11T03:52:13.1421874Z tests/test_shardformer/test_with_torch_ddp.py: 1 warning
2025-04-11T03:52:13.1422091Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py: 1 warning
2025-04-11T03:52:13.1422318Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py: 1 warning
2025-04-11T03:52:13.1422537Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py: 1 warning
2025-04-11T03:52:13.1422691Z tests/test_shardformer/test_model/test_shard_bert.py: 1 warning
2025-04-11T03:52:13.1422840Z tests/test_shardformer/test_model/test_shard_blip2.py: 1 warning
2025-04-11T03:52:13.1422986Z tests/test_shardformer/test_model/test_shard_bloom.py: 1 warning
2025-04-11T03:52:13.1423222Z tests/test_shardformer/test_model/test_shard_chatglm2.py: 1 warning
2025-04-11T03:52:13.1423373Z tests/test_shardformer/test_model/test_shard_command.py: 1 warning
2025-04-11T03:52:13.1423541Z tests/test_shardformer/test_model/test_shard_falcon.py: 1 warning
2025-04-11T03:52:13.1423696Z tests/test_shardformer/test_model/test_shard_gpt2.py: 1 warning
2025-04-11T03:52:13.1423854Z tests/test_shardformer/test_model/test_shard_llama.py: 1 warning
2025-04-11T03:52:13.1424093Z tests/test_shardformer/test_model/test_shard_mistral.py: 1 warning
2025-04-11T03:52:13.1424242Z tests/test_shardformer/test_model/test_shard_opt.py: 1 warning
2025-04-11T03:52:13.1424393Z tests/test_shardformer/test_model/test_shard_qwen2.py: 1 warning
2025-04-11T03:52:13.1424537Z tests/test_shardformer/test_model/test_shard_sam.py: 1 warning
2025-04-11T03:52:13.1424682Z tests/test_shardformer/test_model/test_shard_t5.py: 1 warning
2025-04-11T03:52:13.1424822Z tests/test_shardformer/test_model/test_shard_vit.py: 1 warning
2025-04-11T03:52:13.1424980Z tests/test_shardformer/test_model/test_shard_whisper.py: 1 warning
2025-04-11T03:52:13.1425562Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:13.1425654Z     warnings.warn(
2025-04-11T03:52:13.1425658Z 
2025-04-11T03:52:13.1425777Z tests/test_booster/test_accelerator.py: 1 warning
2025-04-11T03:52:13.1425931Z tests/test_checkpoint_io/test_general_checkpoint_io.py: 1 warning
2025-04-11T03:52:13.1426168Z tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py: 1 warning
2025-04-11T03:52:13.1426314Z tests/test_checkpoint_io/test_safetensors_async_io.py: 1 warning
2025-04-11T03:52:13.1426422Z tests/test_fp8/test_fp8_cast.py: 1 warning
2025-04-11T03:52:13.1426608Z tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py: 2 warnings
2025-04-11T03:52:13.1426749Z tests/test_shardformer/test_flash_attention.py: 1 warning
2025-04-11T03:52:13.1426881Z tests/test_shardformer/test_with_torch_ddp.py: 1 warning
2025-04-11T03:52:13.1427106Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py: 1 warning
2025-04-11T03:52:13.1427340Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py: 1 warning
2025-04-11T03:52:13.1427560Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py: 1 warning
2025-04-11T03:52:13.1427715Z tests/test_shardformer/test_model/test_shard_bert.py: 1 warning
2025-04-11T03:52:13.1427861Z tests/test_shardformer/test_model/test_shard_blip2.py: 1 warning
2025-04-11T03:52:13.1428008Z tests/test_shardformer/test_model/test_shard_bloom.py: 1 warning
2025-04-11T03:52:13.1428171Z tests/test_shardformer/test_model/test_shard_chatglm2.py: 1 warning
2025-04-11T03:52:13.1428327Z tests/test_shardformer/test_model/test_shard_command.py: 1 warning
2025-04-11T03:52:13.1428566Z tests/test_shardformer/test_model/test_shard_falcon.py: 1 warning
2025-04-11T03:52:13.1428718Z tests/test_shardformer/test_model/test_shard_gpt2.py: 1 warning
2025-04-11T03:52:13.1428869Z tests/test_shardformer/test_model/test_shard_llama.py: 1 warning
2025-04-11T03:52:13.1429097Z tests/test_shardformer/test_model/test_shard_mistral.py: 1 warning
2025-04-11T03:52:13.1429252Z tests/test_shardformer/test_model/test_shard_opt.py: 1 warning
2025-04-11T03:52:13.1429397Z tests/test_shardformer/test_model/test_shard_qwen2.py: 1 warning
2025-04-11T03:52:13.1429539Z tests/test_shardformer/test_model/test_shard_sam.py: 1 warning
2025-04-11T03:52:13.1429688Z tests/test_shardformer/test_model/test_shard_t5.py: 1 warning
2025-04-11T03:52:13.1429829Z tests/test_shardformer/test_model/test_shard_vit.py: 1 warning
2025-04-11T03:52:13.1429994Z tests/test_shardformer/test_model/test_shard_whisper.py: 1 warning
2025-04-11T03:52:13.1430545Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:13.1430708Z     warnings.warn(
2025-04-11T03:52:13.1430712Z 
2025-04-11T03:52:13.1430911Z tests/test_infer/test_async_engine/test_async_engine.py::test_new_requests_event
2025-04-11T03:52:13.1431392Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/_pytest/python.py:183: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
2025-04-11T03:52:13.1431634Z   You need to install a suitable plugin for your async framework, for example:
2025-04-11T03:52:13.1431718Z     - anyio
2025-04-11T03:52:13.1431822Z     - pytest-asyncio
2025-04-11T03:52:13.1431909Z     - pytest-tornasync
2025-04-11T03:52:13.1431998Z     - pytest-trio
2025-04-11T03:52:13.1432082Z     - pytest-twisted
2025-04-11T03:52:13.1432266Z     warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))
2025-04-11T03:52:13.1432270Z 
2025-04-11T03:52:13.1432535Z tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
2025-04-11T03:52:13.1433431Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T03:52:13.1433531Z     numel += p.storage().size()
2025-04-11T03:52:13.1433534Z 
2025-04-11T03:52:13.1433770Z tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
2025-04-11T03:52:13.1434949Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
2025-04-11T03:52:13.1435163Z     warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
2025-04-11T03:52:13.1435167Z 
2025-04-11T03:52:13.1435338Z tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
2025-04-11T03:52:13.1435800Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
2025-04-11T03:52:13.1435970Z     warnings.warn("To get the last learning rate computed by the scheduler, "
2025-04-11T03:52:13.1435974Z 
2025-04-11T03:52:13.1436156Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2025-04-11T03:52:13.1436284Z ============================== slowest durations ===============================
2025-04-11T03:52:13.1436479Z 16.61s call     tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.1436684Z 15.30s call     tests/test_infer/test_continuous_batching.py::test_continuous_batching
2025-04-11T03:52:13.1436941Z 12.61s call     tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter
2025-04-11T03:52:13.1437160Z 10.71s call     tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4]
2025-04-11T03:52:13.1437353Z 9.38s call     tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin
2025-04-11T03:52:13.1437502Z 8.94s call     tests/test_optimizer/test_dist_came.py::test_dist_came
2025-04-11T03:52:13.1437675Z 8.61s call     tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin
2025-04-11T03:52:13.1437842Z 8.47s call     tests/test_zero/test_gemini/test_inference.py::test_inference[4]
2025-04-11T03:52:13.1438049Z 8.47s call     tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO
2025-04-11T03:52:13.1438307Z 8.34s call     tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4]
2025-04-11T03:52:13.1438465Z 8.11s call     tests/test_zero/test_gemini/test_optim.py::test_optim[4]
2025-04-11T03:52:13.1438651Z 8.04s call     tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4]
2025-04-11T03:52:13.1438981Z 7.74s call     tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4]
2025-04-11T03:52:13.1439131Z 7.65s call     tests/test_optimizer/test_dist_lamb.py::test_dist_lamb
2025-04-11T03:52:13.1439397Z 7.65s call     tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin
2025-04-11T03:52:13.1439570Z 7.64s call     tests/test_optimizer/test_dist_galore.py::test_dist_galore
2025-04-11T03:52:13.1439776Z 7.58s call     tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2]
2025-04-11T03:52:13.1439948Z 7.43s call     tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2]
2025-04-11T03:52:13.1440120Z 7.30s call     tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor
2025-04-11T03:52:13.1440333Z 7.25s call     tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin
2025-04-11T03:52:13.1440481Z 7.11s call     tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp
2025-04-11T03:52:13.1440707Z 6.73s call     tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin
2025-04-11T03:52:13.1440985Z 6.52s call     tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2]
2025-04-11T03:52:13.1441196Z 6.48s call     tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all
2025-04-11T03:52:13.1441347Z 6.46s call     tests/test_lora/test_lora.py::test_torch_ddp_lora
2025-04-11T03:52:13.1441490Z 6.43s call     tests/test_infer/test_streamingllm.py::test_engine
2025-04-11T03:52:13.1441679Z 6.42s call     tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation
2025-04-11T03:52:13.1441842Z 6.32s call     tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single
2025-04-11T03:52:13.1442007Z 6.24s call     tests/test_zero/test_gemini/test_inference.py::test_inference[1]
2025-04-11T03:52:13.1442155Z 6.19s call     tests/test_fp8/test_fp8_allreduce.py::test_all_reduce
2025-04-11T03:52:13.1442310Z 6.19s call     tests/test_zero/test_gemini/test_search.py::test_search[4]
2025-04-11T03:52:13.1442506Z 6.15s call     tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4]
2025-04-11T03:52:13.1442679Z 6.06s call     tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4]
2025-04-11T03:52:13.1442849Z 5.99s call     tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1]
2025-04-11T03:52:13.1442993Z 5.91s call     tests/test_fp8/test_fp8_allgather.py::test_all_gather
2025-04-11T03:52:13.1443155Z 5.90s call     tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt
2025-04-11T03:52:13.1443335Z 5.89s call     tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring
2025-04-11T03:52:13.1443523Z 5.86s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6]
2025-04-11T03:52:13.1443709Z 5.83s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4]
2025-04-11T03:52:13.1443951Z 5.81s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4]
2025-04-11T03:52:13.1444127Z 5.79s call     tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2
2025-04-11T03:52:13.1444301Z 5.78s call     tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single
2025-04-11T03:52:13.1444512Z 5.76s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12]
2025-04-11T03:52:13.1444673Z 5.69s call     tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter
2025-04-11T03:52:13.1444828Z 5.40s call     tests/test_device/test_init_logical_pg.py::test_logical_pg
2025-04-11T03:52:13.1444964Z 5.35s call     tests/test_infer/test_drafter.py::test_spec_dec
2025-04-11T03:52:13.1445235Z 5.08s call     tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader
2025-04-11T03:52:13.1445394Z 5.04s call     tests/test_tensor/test_comm_spec_apply.py::test_comm_spec
2025-04-11T03:52:13.1445582Z 5.01s call     tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm
2025-04-11T03:52:13.1445750Z 4.97s call     tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2
2025-04-11T03:52:13.1446069Z 4.96s call     tests/test_cluster/test_process_group_mesh.py::test_process_group_mesh
2025-04-11T03:52:13.1446386Z 4.96s call     tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy
2025-04-11T03:52:13.1446567Z 4.93s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4]
2025-04-11T03:52:13.1446763Z 4.93s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12]
2025-04-11T03:52:13.1446952Z 4.92s call     tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d
2025-04-11T03:52:13.1447230Z 4.91s call     tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention
2025-04-11T03:52:13.1447448Z 4.90s call     tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T03:52:13.1447616Z 4.89s call     tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec
2025-04-11T03:52:13.1447793Z 4.88s call     tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp
2025-04-11T03:52:13.1447965Z 4.87s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4]
2025-04-11T03:52:13.1448211Z 4.85s call     tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager
2025-04-11T03:52:13.1448376Z 4.81s call     tests/test_tensor/test_shape_consistency_apply.py::test_apply
2025-04-11T03:52:13.1448543Z 4.80s call     tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd
2025-04-11T03:52:13.1448735Z 4.71s call     tests/test_device/test_device_mesh.py::test_device_mesh_from_process_group
2025-04-11T03:52:13.1448876Z 4.71s call     tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T03:52:13.1449101Z 4.70s call     tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO
2025-04-11T03:52:13.1449287Z 4.65s call     tests/test_cluster/test_device_mesh_manager.py::test_device_mesh_manager
2025-04-11T03:52:13.1449495Z 4.58s call     tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt
2025-04-11T03:52:13.1449666Z 4.45s call     tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T03:52:13.1449853Z 4.28s call     tests/test_infer/test_config_and_struct.py::test_config_and_inference
2025-04-11T03:52:13.1450010Z 4.23s call     tests/test_tensor/test_padded_tensor.py::test_padded_tensor
2025-04-11T03:52:13.1450213Z 4.18s call     tests/test_infer/test_request_handler.py::test_running_list_and_request_handler
2025-04-11T03:52:13.1450388Z 4.08s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2]
2025-04-11T03:52:13.1450567Z 4.07s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6]
2025-04-11T03:52:13.1450750Z 4.07s call     tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation
2025-04-11T03:52:13.1451039Z 4.07s call     tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding
2025-04-11T03:52:13.1451227Z 4.07s call     tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2]
2025-04-11T03:52:13.1451408Z 4.05s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4]
2025-04-11T03:52:13.1451628Z 4.04s call     tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T03:52:13.1451804Z 4.00s call     tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn
2025-04-11T03:52:13.1451971Z 3.97s call     tests/test_shardformer/test_layer/test_linear_1d.py::test_linear
2025-04-11T03:52:13.1452150Z 3.97s call     tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p
2025-04-11T03:52:13.1452470Z 3.93s call     tests/test_infer/test_kvcache_manager.py::test_cache_manager
2025-04-11T03:52:13.1452679Z 3.90s call     tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor
2025-04-11T03:52:13.1452911Z 3.89s call     tests/test_shardformer/test_layer/test_dropout.py::test_dropout
2025-04-11T03:52:13.1453096Z 3.88s call     tests/test_zero/test_gemini/test_search.py::test_search[1]
2025-04-11T03:52:13.1453266Z 3.69s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1]
2025-04-11T03:52:13.1453630Z 0.78s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1453910Z 0.76s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
2025-04-11T03:52:13.1454167Z 0.68s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-True]
2025-04-11T03:52:13.1454421Z 0.64s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-False]
2025-04-11T03:52:13.1454674Z 0.59s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False]
2025-04-11T03:52:13.1454824Z 0.51s setup    tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T03:52:13.1455111Z 0.47s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False]
2025-04-11T03:52:13.1455488Z 0.37s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32]
2025-04-11T03:52:13.1455754Z 0.35s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True]
2025-04-11T03:52:13.1456023Z 0.35s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True]
2025-04-11T03:52:13.1456272Z 0.32s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False]
2025-04-11T03:52:13.1456565Z 0.32s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1456808Z 0.31s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True]
2025-04-11T03:52:13.1457098Z 0.30s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1457389Z 0.28s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1457549Z 0.28s call     tests/test_shardformer/test_with_torch_ddp.py::test_gpt2
2025-04-11T03:52:13.1457756Z 0.27s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False]
2025-04-11T03:52:13.1457957Z 0.26s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True]
2025-04-11T03:52:13.1458240Z 0.22s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2]
2025-04-11T03:52:13.1458407Z 0.22s setup    tests/test_infer/test_kvcache_manager.py::test_logical_blocks
2025-04-11T03:52:13.1458750Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3]
2025-04-11T03:52:13.1459024Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0]
2025-04-11T03:52:13.1459300Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device1]
2025-04-11T03:52:13.1459573Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3]
2025-04-11T03:52:13.1459847Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0]
2025-04-11T03:52:13.1460181Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2]
2025-04-11T03:52:13.1460334Z 0.21s setup    tests/test_fp8/test_fp8_allreduce.py::test_all_reduce
2025-04-11T03:52:13.1460602Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2]
2025-04-11T03:52:13.1460875Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4]
2025-04-11T03:52:13.1461209Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3]
2025-04-11T03:52:13.1461486Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4]
2025-04-11T03:52:13.1461755Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4]
2025-04-11T03:52:13.1462029Z 0.20s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4]
2025-04-11T03:52:13.1462172Z 0.20s call     tests/test_fp8/test_fp8_hook.py::test_fp8_hook
2025-04-11T03:52:13.1462336Z 0.20s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True]
2025-04-11T03:52:13.1462473Z 0.20s setup    tests/test_fp8/test_fp8_cast.py::test_fp8_cast
2025-04-11T03:52:13.1462639Z 0.19s setup    tests/test_infer/test_kvcache_manager.py::test_cache_manager
2025-04-11T03:52:13.1462985Z 0.18s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1463278Z 0.18s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1463560Z 0.18s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1463852Z 0.17s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1464112Z 0.17s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7]
2025-04-11T03:52:13.1464404Z 0.17s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1464621Z 0.17s setup    tests/test_infer/test_async_engine/test_async_engine.py::test_new_requests_event
2025-04-11T03:52:13.1464952Z 0.16s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32]
2025-04-11T03:52:13.1465236Z 0.16s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1465488Z 0.16s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4]
2025-04-11T03:52:13.1465754Z 0.16s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device1]
2025-04-11T03:52:13.1466059Z 0.16s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32]
2025-04-11T03:52:13.1466320Z 0.16s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7]
2025-04-11T03:52:13.1466581Z 0.16s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device1]
2025-04-11T03:52:13.1466773Z 0.15s setup    tests/test_tensor/test_padded_tensor.py::test_padded_tensor
2025-04-11T03:52:13.1466921Z 0.15s setup    tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp
2025-04-11T03:52:13.1467174Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4]
2025-04-11T03:52:13.1467425Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4]
2025-04-11T03:52:13.1467750Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention
2025-04-11T03:52:13.1468098Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32]
2025-04-11T03:52:13.1468480Z 0.15s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0]
2025-04-11T03:52:13.1468756Z 0.15s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True]
2025-04-11T03:52:13.1469063Z 0.15s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1469319Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4]
2025-04-11T03:52:13.1469606Z 0.15s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1469948Z 0.15s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7]
2025-04-11T03:52:13.1470221Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3]
2025-04-11T03:52:13.1470507Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device1]
2025-04-11T03:52:13.1470782Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3]
2025-04-11T03:52:13.1470983Z 0.15s setup    tests/test_fp8/test_fp8_hook.py::test_fp8_hook
2025-04-11T03:52:13.1471255Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3]
2025-04-11T03:52:13.1471405Z 0.15s setup    tests/test_infer/test_drafter.py::test_spec_dec
2025-04-11T03:52:13.1471690Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3]
2025-04-11T03:52:13.1471949Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7]
2025-04-11T03:52:13.1472196Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4]
2025-04-11T03:52:13.1472466Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3]
2025-04-11T03:52:13.1472761Z 0.15s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1472914Z 0.15s setup    tests/test_fp8/test_fp8_allgather.py::test_all_gather
2025-04-11T03:52:13.1473185Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3]
2025-04-11T03:52:13.1473444Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32]
2025-04-11T03:52:13.1473659Z 0.15s setup    tests/test_infer/test_request_handler.py::test_running_list_and_request_handler
2025-04-11T03:52:13.1473965Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4]
2025-04-11T03:52:13.1474137Z 0.15s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True]
2025-04-11T03:52:13.1474291Z 0.15s setup    tests/test_infer/test_streamingllm.py::test_engine
2025-04-11T03:52:13.1474548Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4]
2025-04-11T03:52:13.1474822Z 0.15s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4]
2025-04-11T03:52:13.1475070Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4]
2025-04-11T03:52:13.1475393Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32]
2025-04-11T03:52:13.1475649Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4]
2025-04-11T03:52:13.1475900Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32]
2025-04-11T03:52:13.1476068Z 0.15s setup    tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter
2025-04-11T03:52:13.1476375Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32]
2025-04-11T03:52:13.1476647Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0]
2025-04-11T03:52:13.1476917Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7]
2025-04-11T03:52:13.1477227Z 0.15s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32]
2025-04-11T03:52:13.1477482Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32]
2025-04-11T03:52:13.1477734Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7]
2025-04-11T03:52:13.1477982Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7]
2025-04-11T03:52:13.1478245Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4]
2025-04-11T03:52:13.1478547Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4]
2025-04-11T03:52:13.1478814Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7]
2025-04-11T03:52:13.1479075Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device1]
2025-04-11T03:52:13.1479322Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4]
2025-04-11T03:52:13.1479600Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7]
2025-04-11T03:52:13.1479814Z 0.15s setup    tests/test_infer/test_async_engine/test_request_tracer.py::test_request_tracer
2025-04-11T03:52:13.1480077Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32]
2025-04-11T03:52:13.1480323Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7]
2025-04-11T03:52:13.1480591Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device1]
2025-04-11T03:52:13.1480852Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4]
2025-04-11T03:52:13.1481160Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1481337Z 0.14s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False]
2025-04-11T03:52:13.1481673Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32]
2025-04-11T03:52:13.1481863Z 0.14s setup    tests/test_infer/test_continuous_batching.py::test_continuous_batching
2025-04-11T03:52:13.1482039Z 0.14s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False]
2025-04-11T03:52:13.1482329Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1482598Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0]
2025-04-11T03:52:13.1482894Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1483238Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1483518Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention
2025-04-11T03:52:13.1483806Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1484156Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1484436Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1484727Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1485014Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1485311Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1485606Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1485885Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0]
2025-04-11T03:52:13.1486205Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7]
2025-04-11T03:52:13.1486479Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0]
2025-04-11T03:52:13.1486772Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1487024Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32]
2025-04-11T03:52:13.1487298Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2]
2025-04-11T03:52:13.1487551Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32]
2025-04-11T03:52:13.1487777Z 0.14s setup    tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4]
2025-04-11T03:52:13.1488052Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0]
2025-04-11T03:52:13.1488325Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2]
2025-04-11T03:52:13.1488590Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4]
2025-04-11T03:52:13.1488864Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4]
2025-04-11T03:52:13.1489192Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0]
2025-04-11T03:52:13.1489449Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7]
2025-04-11T03:52:13.1489717Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4]
2025-04-11T03:52:13.1489979Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4]
2025-04-11T03:52:13.1490254Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4]
2025-04-11T03:52:13.1490515Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2]
2025-04-11T03:52:13.1490896Z 0.14s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7]
2025-04-11T03:52:13.1491161Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4]
2025-04-11T03:52:13.1491426Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2]
2025-04-11T03:52:13.1491890Z 0.14s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7]
2025-04-11T03:52:13.1492156Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2]
2025-04-11T03:52:13.1492306Z 0.14s setup    tests/test_infer/test_batch_bucket.py::test_bucket
2025-04-11T03:52:13.1492562Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2]
2025-04-11T03:52:13.1492756Z 0.14s setup    tests/test_infer/test_config_and_struct.py::test_config_and_inference
2025-04-11T03:52:13.1493043Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1493360Z 0.14s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32]
2025-04-11T03:52:13.1493646Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1493968Z 0.14s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4]
2025-04-11T03:52:13.1494278Z 0.14s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7]
2025-04-11T03:52:13.1494571Z 0.13s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1494792Z 0.13s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2]
2025-04-11T03:52:13.1495071Z 0.13s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0]
2025-04-11T03:52:13.1495543Z 0.13s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1495830Z 0.13s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1496138Z 0.13s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7]
2025-04-11T03:52:13.1496418Z 0.13s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1496612Z 0.13s setup    tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single
2025-04-11T03:52:13.1496833Z 0.13s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4]
2025-04-11T03:52:13.1497143Z 0.13s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32]
2025-04-11T03:52:13.1497452Z 0.13s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32]
2025-04-11T03:52:13.1497726Z 0.13s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device1]
2025-04-11T03:52:13.1497908Z 0.13s setup    tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0]
2025-04-11T03:52:13.1498073Z 0.13s setup    tests/test_zero/test_gemini/test_search.py::test_search[4]
2025-04-11T03:52:13.1498354Z 0.13s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1498704Z 0.13s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1498881Z 0.13s setup    tests/test_shardformer/test_layer/test_dropout.py::test_dropout
2025-04-11T03:52:13.1499129Z 0.13s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32]
2025-04-11T03:52:13.1499332Z 0.13s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12]
2025-04-11T03:52:13.1499578Z 0.12s setup    tests/test_shardformer/test_flash_attention.py::test_flash_attn_func
2025-04-11T03:52:13.1499810Z 0.12s setup    tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention
2025-04-11T03:52:13.1500060Z 0.12s setup    tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO
2025-04-11T03:52:13.1500282Z 0.12s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8]
2025-04-11T03:52:13.1500477Z 0.12s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4]
2025-04-11T03:52:13.1500734Z 0.12s setup    tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py::test_t5_pipeline_distribution
2025-04-11T03:52:13.1500993Z 0.12s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4]
2025-04-11T03:52:13.1501281Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1501616Z 0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False]
2025-04-11T03:52:13.1501782Z 0.12s setup    tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd
2025-04-11T03:52:13.1501970Z 0.12s setup    tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation
2025-04-11T03:52:13.1502123Z 0.12s setup    tests/test_optimizer/test_dist_lamb.py::test_dist_lamb
2025-04-11T03:52:13.1502316Z 0.12s setup    tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
2025-04-11T03:52:13.1502490Z 0.12s setup    tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2]
2025-04-11T03:52:13.1502659Z 0.12s setup    tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1]
2025-04-11T03:52:13.1502852Z 0.12s setup    tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager
2025-04-11T03:52:13.1503148Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1503324Z 0.12s setup    tests/test_zero/test_gemini/test_inference.py::test_inference[4]
2025-04-11T03:52:13.1503490Z 0.12s setup    tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2
2025-04-11T03:52:13.1503647Z 0.12s setup    tests/test_optimizer/test_dist_came.py::test_dist_came
2025-04-11T03:52:13.1503808Z 0.12s setup    tests/test_optimizer/test_dist_galore.py::test_dist_galore
2025-04-11T03:52:13.1503983Z 0.12s setup    tests/test_shardformer/test_model/test_shard_bert.py::test_bert
2025-04-11T03:52:13.1504181Z 0.12s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12]
2025-04-11T03:52:13.1504416Z 0.12s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4]
2025-04-11T03:52:13.1504637Z 0.12s setup    tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T03:52:13.1504804Z 0.12s setup    tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt
2025-04-11T03:52:13.1505045Z 0.12s setup    tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding
2025-04-11T03:52:13.1505226Z 0.12s setup    tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn
2025-04-11T03:52:13.1505413Z 0.12s setup    tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring
2025-04-11T03:52:13.1505590Z 0.12s setup    tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon
2025-04-11T03:52:13.1505819Z 0.12s setup    tests/test_shardformer/test_layer/test_linear_1d.py::test_linear
2025-04-11T03:52:13.1506016Z 0.12s setup    tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4]
2025-04-11T03:52:13.1506258Z 0.12s setup    tests/test_tensor/test_dtensor/test_dtensor_sharding_spec.py::test_dtensor_sharding_spec
2025-04-11T03:52:13.1506444Z 0.12s setup    tests/test_tensor/test_shape_consistency.py::test_one_step_transform
2025-04-11T03:52:13.1506682Z 0.12s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4]
2025-04-11T03:52:13.1506866Z 0.12s setup    tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2]
2025-04-11T03:52:13.1507021Z 0.12s setup    tests/test_zero/test_gemini/test_search.py::test_search[1]
2025-04-11T03:52:13.1507186Z 0.12s setup    tests/test_zero/test_gemini/test_inference.py::test_inference[1]
2025-04-11T03:52:13.1507352Z 0.12s setup    tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor
2025-04-11T03:52:13.1507527Z 0.12s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1]
2025-04-11T03:52:13.1507705Z 0.12s setup    tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm
2025-04-11T03:52:13.1507876Z 0.12s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2]
2025-04-11T03:52:13.1508168Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1508330Z 0.12s setup    tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2
2025-04-11T03:52:13.1508628Z 0.12s setup    tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation
2025-04-11T03:52:13.1508903Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2]
2025-04-11T03:52:13.1509068Z 0.12s setup    tests/test_tensor/test_sharding_spec.py::test_sharding_spec
2025-04-11T03:52:13.1509342Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2]
2025-04-11T03:52:13.1509525Z 0.12s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4]
2025-04-11T03:52:13.1509692Z 0.12s setup    tests/test_zero/test_gemini/test_optim.py::test_optim[4]
2025-04-11T03:52:13.1509913Z 0.12s setup    tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T03:52:13.1510100Z 0.12s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6]
2025-04-11T03:52:13.1510278Z 0.12s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6]
2025-04-11T03:52:13.1510548Z 0.12s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4]
2025-04-11T03:52:13.1510777Z 0.12s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_get_batch_size
2025-04-11T03:52:13.1510965Z 0.12s setup    tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d
2025-04-11T03:52:13.1511145Z 0.12s setup    tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel
2025-04-11T03:52:13.1511457Z 0.12s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7]
2025-04-11T03:52:13.1511673Z 0.12s setup    tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin
2025-04-11T03:52:13.1511960Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1512252Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1512494Z 0.12s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False]
2025-04-11T03:52:13.1512776Z 0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True]
2025-04-11T03:52:13.1513115Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2]
2025-04-11T03:52:13.1513356Z 0.12s setup    tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4]
2025-04-11T03:52:13.1513519Z 0.12s setup    tests/test_moe/test_kernel.py::test_moe_kernel[data_type0]
2025-04-11T03:52:13.1513807Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1514159Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1514441Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1514727Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1515010Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1515294Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1515574Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1515845Z 0.12s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm
2025-04-11T03:52:13.1516074Z 0.12s setup    tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.1516328Z 0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-False]
2025-04-11T03:52:13.1516619Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1516897Z 0.11s call     tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4]
2025-04-11T03:52:13.1517052Z 0.11s setup    tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all
2025-04-11T03:52:13.1517335Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1517605Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device1]
2025-04-11T03:52:13.1517831Z 0.11s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16]
2025-04-11T03:52:13.1518120Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1518366Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True]
2025-04-11T03:52:13.1518656Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1518830Z 0.11s setup    tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor
2025-04-11T03:52:13.1519155Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device1]
2025-04-11T03:52:13.1519435Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3]
2025-04-11T03:52:13.1519632Z 0.11s setup    tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO
2025-04-11T03:52:13.1519900Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0]
2025-04-11T03:52:13.1520086Z 0.11s setup    tests/test_cluster/test_process_group_mesh.py::test_process_group_mesh
2025-04-11T03:52:13.1520258Z 0.11s setup    tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single
2025-04-11T03:52:13.1520472Z 0.11s setup    tests/test_config/test_load_config.py::test_load_config
2025-04-11T03:52:13.1520635Z 0.11s setup    tests/test_device/test_init_logical_pg.py::test_logical_pg
2025-04-11T03:52:13.1520875Z 0.11s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False]
2025-04-11T03:52:13.1521066Z 0.11s setup    tests/test_cluster/test_device_mesh_manager.py::test_device_mesh_manager
2025-04-11T03:52:13.1521436Z 0.11s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7]
2025-04-11T03:52:13.1521648Z 0.11s setup    tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt
2025-04-11T03:52:13.1521860Z 0.11s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True]
2025-04-11T03:52:13.1522124Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device1]
2025-04-11T03:52:13.1522379Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False]
2025-04-11T03:52:13.1522685Z 0.11s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7]
2025-04-11T03:52:13.1522964Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3]
2025-04-11T03:52:13.1523235Z 0.11s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7]
2025-04-11T03:52:13.1523551Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
2025-04-11T03:52:13.1523833Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4]
2025-04-11T03:52:13.1524104Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3]
2025-04-11T03:52:13.1524341Z 0.11s setup    tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin
2025-04-11T03:52:13.1524615Z 0.11s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32]
2025-04-11T03:52:13.1524835Z 0.11s setup    tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2]
2025-04-11T03:52:13.1525089Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False]
2025-04-11T03:52:13.1525292Z 0.11s setup    tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin
2025-04-11T03:52:13.1525540Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True]
2025-04-11T03:52:13.1525754Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint
2025-04-11T03:52:13.1526049Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1526202Z 0.11s setup    tests/test_device/test_device_mesh.py::test_device_mesh
2025-04-11T03:52:13.1526499Z 0.11s setup    tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin
2025-04-11T03:52:13.1526786Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1527077Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1527288Z 0.11s setup    tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader
2025-04-11T03:52:13.1527600Z 0.11s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32]
2025-04-11T03:52:13.1527804Z 0.11s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True]
2025-04-11T03:52:13.1528118Z 0.11s call     tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention
2025-04-11T03:52:13.1528416Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1528707Z 0.11s call     tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4]
2025-04-11T03:52:13.1529044Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1529351Z 0.11s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32]
2025-04-11T03:52:13.1529567Z 0.10s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False]
2025-04-11T03:52:13.1529786Z 0.10s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8]
2025-04-11T03:52:13.1530063Z 0.10s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4]
2025-04-11T03:52:13.1530224Z 0.10s setup    tests/test_shardformer/test_with_torch_ddp.py::test_gpt2
2025-04-11T03:52:13.1530399Z 0.10s setup    tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2
2025-04-11T03:52:13.1530556Z 0.10s setup    tests/test_booster/test_accelerator.py::test_accelerator
2025-04-11T03:52:13.1530799Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False]
2025-04-11T03:52:13.1531101Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32]
2025-04-11T03:52:13.1531304Z 0.10s setup    tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4]
2025-04-11T03:52:13.1531593Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1531878Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1532107Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16]
2025-04-11T03:52:13.1532418Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7]
2025-04-11T03:52:13.1532659Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False]
2025-04-11T03:52:13.1532948Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4]
2025-04-11T03:52:13.1533138Z 0.10s setup    tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2
2025-04-11T03:52:13.1533394Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4]
2025-04-11T03:52:13.1533639Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True]
2025-04-11T03:52:13.1533881Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True]
2025-04-11T03:52:13.1534235Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1534530Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1534719Z 0.10s setup    tests/test_shardformer/test_model/test_shard_llama.py::test_llama
2025-04-11T03:52:13.1534898Z 0.10s setup    tests/test_shardformer/test_model/test_shard_vit.py::test_vit
2025-04-11T03:52:13.1535197Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1535359Z 0.10s setup    tests/test_tensor/test_comm_spec_apply.py::test_comm_spec
2025-04-11T03:52:13.1535597Z 0.10s setup    tests/test_shardformer/test_model/test_shard_sam.py::test_sam
2025-04-11T03:52:13.1535895Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1536154Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4]
2025-04-11T03:52:13.1536397Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2]
2025-04-11T03:52:13.1536710Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False]
2025-04-11T03:52:13.1536906Z 0.10s setup    tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper
2025-04-11T03:52:13.1537103Z 0.10s setup    tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral
2025-04-11T03:52:13.1537285Z 0.10s setup    tests/test_shardformer/test_model/test_shard_t5.py::test_t5
2025-04-11T03:52:13.1537525Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False]
2025-04-11T03:52:13.1537759Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False]
2025-04-11T03:52:13.1537964Z 0.10s setup    tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4]
2025-04-11T03:52:13.1538192Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T03:52:13.1538366Z 0.10s call     tests/test_moe/test_kernel.py::test_moe_kernel[data_type0]
2025-04-11T03:52:13.1538713Z 0.10s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1538913Z 0.10s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4]
2025-04-11T03:52:13.1539234Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1539510Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4]
2025-04-11T03:52:13.1539756Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False]
2025-04-11T03:52:13.1539933Z 0.10s call     tests/test_shardformer/test_shard_utils.py::test_release_layer
2025-04-11T03:52:13.1540206Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4]
2025-04-11T03:52:13.1540462Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32]
2025-04-11T03:52:13.1540754Z 0.10s setup    tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4]
2025-04-11T03:52:13.1540985Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True]
2025-04-11T03:52:13.1541253Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4]
2025-04-11T03:52:13.1541487Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False]
2025-04-11T03:52:13.1541790Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4]
2025-04-11T03:52:13.1542107Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7]
2025-04-11T03:52:13.1542364Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7]
2025-04-11T03:52:13.1542601Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False]
2025-04-11T03:52:13.1542835Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True]
2025-04-11T03:52:13.1543155Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32]
2025-04-11T03:52:13.1543388Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True]
2025-04-11T03:52:13.1543682Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1543908Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True]
2025-04-11T03:52:13.1544235Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False]
2025-04-11T03:52:13.1544388Z 0.10s call     tests/test_infer/test_batch_bucket.py::test_bucket
2025-04-11T03:52:13.1544639Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32]
2025-04-11T03:52:13.1544882Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T03:52:13.1545012Z 0.10s call     tests/test_lazy/test_ops.py::test_lazy_ops
2025-04-11T03:52:13.1545303Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7]
2025-04-11T03:52:13.1545539Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False]
2025-04-11T03:52:13.1545842Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1546148Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7]
2025-04-11T03:52:13.1546386Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True]
2025-04-11T03:52:13.1546626Z 0.10s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False]
2025-04-11T03:52:13.1546928Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1547123Z 0.10s setup    tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom
2025-04-11T03:52:13.1547378Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4]
2025-04-11T03:52:13.1547688Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32]
2025-04-11T03:52:13.1547864Z 0.10s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False]
2025-04-11T03:52:13.1548137Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4]
2025-04-11T03:52:13.1548377Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4]
2025-04-11T03:52:13.1548604Z 0.10s setup    tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2
2025-04-11T03:52:13.1548838Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True]
2025-04-11T03:52:13.1549062Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True]
2025-04-11T03:52:13.1549415Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32]
2025-04-11T03:52:13.1549685Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4]
2025-04-11T03:52:13.1549926Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True]
2025-04-11T03:52:13.1550172Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32]
2025-04-11T03:52:13.1550427Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7]
2025-04-11T03:52:13.1550750Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4]
2025-04-11T03:52:13.1551036Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32]
2025-04-11T03:52:13.1551285Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7]
2025-04-11T03:52:13.1551528Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7]
2025-04-11T03:52:13.1551852Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7]
2025-04-11T03:52:13.1552081Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True]
2025-04-11T03:52:13.1552337Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4]
2025-04-11T03:52:13.1552655Z 0.10s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7]
2025-04-11T03:52:13.1552836Z 0.10s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False]
2025-04-11T03:52:13.1553120Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7]
2025-04-11T03:52:13.1553405Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7]
2025-04-11T03:52:13.1553767Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1554064Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1554315Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32]
2025-04-11T03:52:13.1554628Z 0.10s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32]
2025-04-11T03:52:13.1554852Z 0.10s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load
2025-04-11T03:52:13.1555142Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1555389Z 0.10s setup    tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4]
2025-04-11T03:52:13.1555683Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1555940Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7]
2025-04-11T03:52:13.1556226Z 0.10s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1556478Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7]
2025-04-11T03:52:13.1556765Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1557114Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1557362Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7]
2025-04-11T03:52:13.1557529Z 0.10s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True]
2025-04-11T03:52:13.1557778Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4]
2025-04-11T03:52:13.1557934Z 0.10s call     tests/test_moe/test_kernel.py::test_moe_kernel[data_type1]
2025-04-11T03:52:13.1558116Z 0.10s call     tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0]
2025-04-11T03:52:13.1574536Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True]
2025-04-11T03:52:13.1574925Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True]
2025-04-11T03:52:13.1575253Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1575522Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7]
2025-04-11T03:52:13.1575904Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32]
2025-04-11T03:52:13.1576211Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32]
2025-04-11T03:52:13.1576467Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7]
2025-04-11T03:52:13.1576729Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7]
2025-04-11T03:52:13.1577029Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1577195Z 0.10s call     tests/test_fp8/test_fp8_cast.py::test_fp8_cast
2025-04-11T03:52:13.1577447Z 0.10s call     tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4]
2025-04-11T03:52:13.1577707Z 0.10s call     tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4]
2025-04-11T03:52:13.1578067Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1578361Z 0.10s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1578566Z 0.10s call     tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm
2025-04-11T03:52:13.1578845Z 0.10s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm
2025-04-11T03:52:13.1579100Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4]
2025-04-11T03:52:13.1579392Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1579684Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1579813Z 0.10s setup    tests/test_lazy/test_ops.py::test_lazy_ops
2025-04-11T03:52:13.1580102Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1580386Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1580674Z 0.10s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4]
2025-04-11T03:52:13.1580982Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T03:52:13.1581284Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1581474Z 0.10s setup    tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p
2025-04-11T03:52:13.1581757Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1582042Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1582236Z 0.10s setup    tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm
2025-04-11T03:52:13.1582587Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T03:52:13.1582869Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1583176Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7]
2025-04-11T03:52:13.1583457Z 0.10s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1583754Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True]
2025-04-11T03:52:13.1583944Z 0.10s setup    tests/test_shardformer/test_model/test_shard_command.py::test_command
2025-04-11T03:52:13.1584179Z 0.10s call     tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4]
2025-04-11T03:52:13.1584435Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32]
2025-04-11T03:52:13.1584716Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1584995Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7]
2025-04-11T03:52:13.1585246Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32]
2025-04-11T03:52:13.1585478Z 0.10s setup    tests/test_shardformer/test_shard_utils.py::test_release_layer
2025-04-11T03:52:13.1585782Z 0.10s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7]
2025-04-11T03:52:13.1586071Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1586350Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1586640Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1586946Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32]
2025-04-11T03:52:13.1587255Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32]
2025-04-11T03:52:13.1587487Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2]
2025-04-11T03:52:13.1587775Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1588059Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1588335Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7]
2025-04-11T03:52:13.1588748Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1589035Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1589320Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1589607Z 0.10s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1589905Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-16-7]
2025-04-11T03:52:13.1590127Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2]
2025-04-11T03:52:13.1590483Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1590767Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1590931Z 0.10s setup    tests/test_moe/test_kernel.py::test_moe_kernel[data_type1]
2025-04-11T03:52:13.1591273Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1591558Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1591840Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1592120Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1592429Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7]
2025-04-11T03:52:13.1592712Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1592997Z 0.10s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1593363Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32]
2025-04-11T03:52:13.1593661Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1593928Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4]
2025-04-11T03:52:13.1594212Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1594429Z 0.10s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True]
2025-04-11T03:52:13.1594729Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7]
2025-04-11T03:52:13.1595016Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1595194Z 0.10s call     tests/test_shardformer/test_model/test_shard_bert.py::test_bert
2025-04-11T03:52:13.1595489Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1595779Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1596071Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1596419Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7]
2025-04-11T03:52:13.1596724Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7]
2025-04-11T03:52:13.1597015Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1597254Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True]
2025-04-11T03:52:13.1597547Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1597784Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False]
2025-04-11T03:52:13.1598134Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1598416Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1598721Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7]
2025-04-11T03:52:13.1599071Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7]
2025-04-11T03:52:13.1599314Z 0.09s call     tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2]
2025-04-11T03:52:13.1599598Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1599889Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1600125Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False]
2025-04-11T03:52:13.1600312Z 0.09s setup    tests/test_tensor/test_shape_consistency.py::test_shape_consistency
2025-04-11T03:52:13.1600556Z 0.09s setup    tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py::test_t5_pipeline_layers
2025-04-11T03:52:13.1600846Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1601197Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1601433Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False]
2025-04-11T03:52:13.1601676Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2]
2025-04-11T03:52:13.1601982Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32]
2025-04-11T03:52:13.1602274Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1602558Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1602854Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1603132Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32]
2025-04-11T03:52:13.1603407Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32]
2025-04-11T03:52:13.1603637Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16]
2025-04-11T03:52:13.1603811Z 0.09s setup    tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec
2025-04-11T03:52:13.1604155Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1604457Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7]
2025-04-11T03:52:13.1604756Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7]
2025-04-11T03:52:13.1605041Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1605263Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4]
2025-04-11T03:52:13.1605610Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1605898Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1606197Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7]
2025-04-11T03:52:13.1606480Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1606767Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2]
2025-04-11T03:52:13.1607056Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1607343Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1607646Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32]
2025-04-11T03:52:13.1607919Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32]
2025-04-11T03:52:13.1608226Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32]
2025-04-11T03:52:13.1608506Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7]
2025-04-11T03:52:13.1608849Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1609157Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32]
2025-04-11T03:52:13.1609378Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8]
2025-04-11T03:52:13.1609611Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True]
2025-04-11T03:52:13.1609836Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4]
2025-04-11T03:52:13.1610135Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7]
2025-04-11T03:52:13.1610362Z 0.09s setup    tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy
2025-04-11T03:52:13.1610581Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8]
2025-04-11T03:52:13.1610870Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1611164Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7]
2025-04-11T03:52:13.1611387Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8]
2025-04-11T03:52:13.1611692Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.0-False]
2025-04-11T03:52:13.1612001Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32]
2025-04-11T03:52:13.1612221Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16]
2025-04-11T03:52:13.1612522Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32]
2025-04-11T03:52:13.1612831Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32]
2025-04-11T03:52:13.1613129Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32]
2025-04-11T03:52:13.1613348Z 0.09s setup    tests/test_lora/test_lora.py::test_torch_ddp_lora
2025-04-11T03:52:13.1613634Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1613904Z 0.09s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm
2025-04-11T03:52:13.1614090Z 0.09s call     tests/test_shardformer/test_flash_attention.py::test_flash_attn_func
2025-04-11T03:52:13.1614451Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7]
2025-04-11T03:52:13.1614749Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7]
2025-04-11T03:52:13.1615047Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32]
2025-04-11T03:52:13.1615355Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32]
2025-04-11T03:52:13.1615652Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7]
2025-04-11T03:52:13.1615944Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1616322Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7]
2025-04-11T03:52:13.1616619Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1616916Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32]
2025-04-11T03:52:13.1617097Z 0.09s setup    tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4]
2025-04-11T03:52:13.1617268Z 0.09s setup    tests/test_tensor/test_shape_consistency_apply.py::test_apply
2025-04-11T03:52:13.1617557Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1617859Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32]
2025-04-11T03:52:13.1618101Z 0.09s call     tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2]
2025-04-11T03:52:13.1618398Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1618616Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4]
2025-04-11T03:52:13.1618893Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7]
2025-04-11T03:52:13.1619184Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1619541Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7]
2025-04-11T03:52:13.1619832Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1620133Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7]
2025-04-11T03:52:13.1620417Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1620705Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1621079Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32]
2025-04-11T03:52:13.1621371Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1621657Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1621990Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32]
2025-04-11T03:52:13.1622281Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1622553Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7]
2025-04-11T03:52:13.1622796Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False]
2025-04-11T03:52:13.1623079Z 0.09s setup    tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py::test_whisper_pipeline_distribution
2025-04-11T03:52:13.1623373Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1623657Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1624004Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1624290Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1624576Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1624762Z 0.09s call     tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon
2025-04-11T03:52:13.1625045Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1625336Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1625638Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7]
2025-04-11T03:52:13.1625932Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1626211Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1626497Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1626779Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1627129Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1627433Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32]
2025-04-11T03:52:13.1627670Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.1-False]
2025-04-11T03:52:13.1627954Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1628236Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1628555Z 0.09s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0]
2025-04-11T03:52:13.1629025Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1629312Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1629598Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1629933Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7]
2025-04-11T03:52:13.1630206Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7]
2025-04-11T03:52:13.1630496Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1630783Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1631075Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1631379Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32]
2025-04-11T03:52:13.1631670Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1631969Z 0.09s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_get_micro_batch
2025-04-11T03:52:13.1632261Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1632531Z 0.09s setup    tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py::test_whisper_pipeline_layers
2025-04-11T03:52:13.1632819Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1633112Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1633401Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1633697Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1633985Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1634224Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True]
2025-04-11T03:52:13.1634524Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7]
2025-04-11T03:52:13.1634800Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32]
2025-04-11T03:52:13.1635097Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True]
2025-04-11T03:52:13.1635391Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1635677Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1635960Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1636265Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32]
2025-04-11T03:52:13.1636610Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1636902Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1637174Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7]
2025-04-11T03:52:13.1637461Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1637826Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7]
2025-04-11T03:52:13.1638115Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1638303Z 0.09s call     tests/test_shardformer/test_model/test_shard_command.py::test_command
2025-04-11T03:52:13.1638580Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32]
2025-04-11T03:52:13.1638885Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32]
2025-04-11T03:52:13.1639189Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32]
2025-04-11T03:52:13.1639483Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1639831Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1640141Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32]
2025-04-11T03:52:13.1640437Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32]
2025-04-11T03:52:13.1640726Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1641028Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7]
2025-04-11T03:52:13.1641335Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32]
2025-04-11T03:52:13.1641636Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32]
2025-04-11T03:52:13.1641938Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7]
2025-04-11T03:52:13.1642214Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7]
2025-04-11T03:52:13.1642500Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1642836Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32]
2025-04-11T03:52:13.1643138Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32]
2025-04-11T03:52:13.1643443Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7]
2025-04-11T03:52:13.1643727Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1644016Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1644249Z 0.09s setup    tests/test_infer/test_models/test_custom_model.py::test_model
2025-04-11T03:52:13.1644536Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1644819Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1645105Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1645455Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1645745Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1646045Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32]
2025-04-11T03:52:13.1646345Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7]
2025-04-11T03:52:13.1646635Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1646922Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1647229Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7]
2025-04-11T03:52:13.1647566Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32]
2025-04-11T03:52:13.1647845Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7]
2025-04-11T03:52:13.1648163Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7]
2025-04-11T03:52:13.1648660Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7]
2025-04-11T03:52:13.1648967Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32]
2025-04-11T03:52:13.1649267Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7]
2025-04-11T03:52:13.1649542Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32]
2025-04-11T03:52:13.1649832Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1650127Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1650428Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32]
2025-04-11T03:52:13.1650782Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1651060Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32]
2025-04-11T03:52:13.1651352Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1651654Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7]
2025-04-11T03:52:13.1651954Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7]
2025-04-11T03:52:13.1652248Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7]
2025-04-11T03:52:13.1652589Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7]
2025-04-11T03:52:13.1652802Z 0.09s setup    tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter
2025-04-11T03:52:13.1653087Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1653449Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7]
2025-04-11T03:52:13.1653744Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7]
2025-04-11T03:52:13.1654029Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1654317Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1654624Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32]
2025-04-11T03:52:13.1654905Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1655199Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1655554Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7]
2025-04-11T03:52:13.1655829Z 0.09s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm
2025-04-11T03:52:13.1656126Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7]
2025-04-11T03:52:13.1656401Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7]
2025-04-11T03:52:13.1656681Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1656982Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7]
2025-04-11T03:52:13.1657276Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1657545Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32]
2025-04-11T03:52:13.1657845Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32]
2025-04-11T03:52:13.1658146Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32]
2025-04-11T03:52:13.1658499Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1658773Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32]
2025-04-11T03:52:13.1659057Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1659338Z 0.09s setup    tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4]
2025-04-11T03:52:13.1659619Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1659901Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1660264Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32]
2025-04-11T03:52:13.1660573Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32]
2025-04-11T03:52:13.1660858Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1661193Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32]
2025-04-11T03:52:13.1661486Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-32-32]
2025-04-11T03:52:13.1661759Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32]
2025-04-11T03:52:13.1662041Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1662248Z 0.09s setup    tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm
2025-04-11T03:52:13.1662516Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7]
2025-04-11T03:52:13.1662818Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7]
2025-04-11T03:52:13.1663164Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32]
2025-04-11T03:52:13.1663464Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7]
2025-04-11T03:52:13.1663763Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32]
2025-04-11T03:52:13.1664060Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7]
2025-04-11T03:52:13.1664338Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32]
2025-04-11T03:52:13.1664635Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32]
2025-04-11T03:52:13.1664926Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1665208Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1665482Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7]
2025-04-11T03:52:13.1665758Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1665998Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False]
2025-04-11T03:52:13.1666349Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1666624Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7]
2025-04-11T03:52:13.1666910Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1667190Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1667463Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7]
2025-04-11T03:52:13.1667709Z 0.09s call     tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel
2025-04-11T03:52:13.1667992Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1668263Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32]
2025-04-11T03:52:13.1668605Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1669024Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32]
2025-04-11T03:52:13.1669264Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False]
2025-04-11T03:52:13.1669551Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1669838Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1670115Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1670414Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32]
2025-04-11T03:52:13.1670692Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32]
2025-04-11T03:52:13.1671038Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1671319Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32]
2025-04-11T03:52:13.1671601Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1671889Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1672189Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7]
2025-04-11T03:52:13.1672501Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32]
2025-04-11T03:52:13.1672791Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1673081Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1673361Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1673669Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32]
2025-04-11T03:52:13.1674019Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1674313Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1674586Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7]
2025-04-11T03:52:13.1674876Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1675179Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-32-32]
2025-04-11T03:52:13.1675463Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1675821Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1676106Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1676382Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32]
2025-04-11T03:52:13.1676722Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1676997Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7]
2025-04-11T03:52:13.1677264Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32]
2025-04-11T03:52:13.1677548Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1677856Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32]
2025-04-11T03:52:13.1678140Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1678442Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-32-7]
2025-04-11T03:52:13.1678792Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1679096Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7]
2025-04-11T03:52:13.1679382Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1679691Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-16-32]
2025-04-11T03:52:13.1679978Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1680267Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1680577Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-16-32]
2025-04-11T03:52:13.1680867Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1681171Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-32-32]
2025-04-11T03:52:13.1681469Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-32-32]
2025-04-11T03:52:13.1681815Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1682116Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-32-32]
2025-04-11T03:52:13.1682393Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7]
2025-04-11T03:52:13.1682566Z 0.09s call     tests/test_shardformer/test_model/test_shard_sam.py::test_sam
2025-04-11T03:52:13.1682850Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1683133Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1683486Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1683767Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1684045Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32]
2025-04-11T03:52:13.1684388Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1684563Z 0.09s call     tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2
2025-04-11T03:52:13.1684864Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-16-32]
2025-04-11T03:52:13.1685150Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1685442Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1685733Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1686018Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1686300Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1686661Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-16-7]
2025-04-11T03:52:13.1686946Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1687253Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32]
2025-04-11T03:52:13.1687543Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1687824Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32]
2025-04-11T03:52:13.1688125Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-16-32]
2025-04-11T03:52:13.1688412Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1688705Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1688990Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1689277Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1689614Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1689902Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1690204Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-16-32]
2025-04-11T03:52:13.1690508Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-32-32]
2025-04-11T03:52:13.1690806Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-16-7]
2025-04-11T03:52:13.1691164Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-16-7]
2025-04-11T03:52:13.1691444Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1691728Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1692027Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-16-32]
2025-04-11T03:52:13.1692402Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-32-7]
2025-04-11T03:52:13.1692683Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7]
2025-04-11T03:52:13.1692989Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32]
2025-04-11T03:52:13.1693291Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-16-7]
2025-04-11T03:52:13.1693574Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1693860Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1694222Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-32-32]
2025-04-11T03:52:13.1694527Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-16-32]
2025-04-11T03:52:13.1694807Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1695112Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-32-7]
2025-04-11T03:52:13.1695396Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1695677Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1695962Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1696249Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1696553Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7]
2025-04-11T03:52:13.1696824Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7]
2025-04-11T03:52:13.1697123Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-16-7]
2025-04-11T03:52:13.1697474Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-32-32]
2025-04-11T03:52:13.1697766Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1698069Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-16-32]
2025-04-11T03:52:13.1698361Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1698592Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.0-True]
2025-04-11T03:52:13.1698976Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32]
2025-04-11T03:52:13.1699271Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1699510Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T03:52:13.1699802Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1700158Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-32-7]
2025-04-11T03:52:13.1700469Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32]
2025-04-11T03:52:13.1700633Z 0.09s call     tests/test_booster/test_accelerator.py::test_accelerator
2025-04-11T03:52:13.1700929Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1701233Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-16-7]
2025-04-11T03:52:13.1701529Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-32-7]
2025-04-11T03:52:13.1701817Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1702171Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-32-7]
2025-04-11T03:52:13.1702471Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-32-7]
2025-04-11T03:52:13.1702692Z 0.09s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_merge_batch
2025-04-11T03:52:13.1702975Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32]
2025-04-11T03:52:13.1703262Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1703449Z 0.09s setup    tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp
2025-04-11T03:52:13.1703750Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-32-7]
2025-04-11T03:52:13.1704025Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7]
2025-04-11T03:52:13.1704304Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7]
2025-04-11T03:52:13.1704605Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-16-7]
2025-04-11T03:52:13.1704890Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1705239Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1705524Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1705794Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32]
2025-04-11T03:52:13.1706078Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1706300Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16]
2025-04-11T03:52:13.1706580Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1706919Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1707207Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1707487Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1707834Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1708027Z 0.09s call     tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm
2025-04-11T03:52:13.1708315Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1708636Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1708870Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.1-True]
2025-04-11T03:52:13.1709110Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.1-False]
2025-04-11T03:52:13.1709389Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1709732Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32]
2025-04-11T03:52:13.1710013Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32]
2025-04-11T03:52:13.1710249Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.0-False]
2025-04-11T03:52:13.1710532Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1710826Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1711061Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T03:52:13.1711352Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1711638Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1711815Z 0.09s call     tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2
2025-04-11T03:52:13.1711997Z 0.09s call     tests/test_shardformer/test_model/test_shard_llama.py::test_llama
2025-04-11T03:52:13.1712291Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32]
2025-04-11T03:52:13.1712521Z 0.09s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False]
2025-04-11T03:52:13.1712754Z 0.09s call     tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom
2025-04-11T03:52:13.1713026Z 0.09s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm
2025-04-11T03:52:13.1713308Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1713572Z 0.09s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm
2025-04-11T03:52:13.1713877Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7]
2025-04-11T03:52:13.1714160Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1714501Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7]
2025-04-11T03:52:13.1714772Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7]
2025-04-11T03:52:13.1715059Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7]
2025-04-11T03:52:13.1715323Z 0.09s call     tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral
2025-04-11T03:52:13.1715605Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7]
2025-04-11T03:52:13.1715871Z 0.09s call     tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention
2025-04-11T03:52:13.1716162Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1716448Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1716736Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1717001Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7]
2025-04-11T03:52:13.1717336Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1717623Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1717907Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1718196Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1718478Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1718769Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1718990Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16]
2025-04-11T03:52:13.1719298Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7]
2025-04-11T03:52:13.1719581Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1719869Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1720105Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.1-True]
2025-04-11T03:52:13.1720464Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1720751Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1720940Z 0.09s call     tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper
2025-04-11T03:52:13.1721231Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1721510Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1721803Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1722133Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7]
2025-04-11T03:52:13.1722426Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1722709Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1723039Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7]
2025-04-11T03:52:13.1723322Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1723610Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1723890Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1724175Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1724418Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.0-True]
2025-04-11T03:52:13.1724651Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.0-False]
2025-04-11T03:52:13.1725004Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1725287Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1725577Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1725881Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7]
2025-04-11T03:52:13.1726174Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1726457Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1726731Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32]
2025-04-11T03:52:13.1727004Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32]
2025-04-11T03:52:13.1727280Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1727574Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1727862Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1728202Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7]
2025-04-11T03:52:13.1728490Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1728781Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1729063Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1729298Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.1-True]
2025-04-11T03:52:13.1729469Z 0.09s call     tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2
2025-04-11T03:52:13.1729806Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7]
2025-04-11T03:52:13.1729974Z 0.09s call     tests/test_shardformer/test_model/test_shard_vit.py::test_vit
2025-04-11T03:52:13.1730254Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1730524Z 0.09s call     tests/test_shardformer/test_model/test_shard_t5.py::test_t5
2025-04-11T03:52:13.1730825Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32]
2025-04-11T03:52:13.1731117Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1731381Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32]
2025-04-11T03:52:13.1731601Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4]
2025-04-11T03:52:13.1731884Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1732167Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1732433Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7]
2025-04-11T03:52:13.1732764Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1733049Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1733326Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1733611Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1733896Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1734188Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1734469Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1734755Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1735034Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1735324Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1735676Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7]
2025-04-11T03:52:13.1735978Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7]
2025-04-11T03:52:13.1736260Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1736544Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1736808Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7]
2025-04-11T03:52:13.1737109Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7]
2025-04-11T03:52:13.1737457Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1737734Z 0.09s setup    tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2]
2025-04-11T03:52:13.1738018Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1738356Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1738636Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1738905Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32]
2025-04-11T03:52:13.1739193Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1739461Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7]
2025-04-11T03:52:13.1739748Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1740030Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1740364Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1740637Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7]
2025-04-11T03:52:13.1740935Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32]
2025-04-11T03:52:13.1741237Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32]
2025-04-11T03:52:13.1741519Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1741804Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1742087Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1742361Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32]
2025-04-11T03:52:13.1742645Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1742945Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32]
2025-04-11T03:52:13.1743224Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1743559Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7]
2025-04-11T03:52:13.1743841Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1744112Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7]
2025-04-11T03:52:13.1744397Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1744674Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1745013Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1745294Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1745597Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7]
2025-04-11T03:52:13.1745921Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32]
2025-04-11T03:52:13.1746205Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1746485Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1746765Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1747040Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1747316Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1747598Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1747913Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32]
2025-04-11T03:52:13.1748190Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32]
2025-04-11T03:52:13.1748465Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4]
2025-04-11T03:52:13.1748782Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7]
2025-04-11T03:52:13.1749267Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32]
2025-04-11T03:52:13.1749664Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1749951Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1750227Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32]
2025-04-11T03:52:13.1750497Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32]
2025-04-11T03:52:13.1750809Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32]
2025-04-11T03:52:13.1751078Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32]
2025-04-11T03:52:13.1751508Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32]
2025-04-11T03:52:13.1751798Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1752095Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1752403Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32]
2025-04-11T03:52:13.1752669Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7]
2025-04-11T03:52:13.1753025Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1753243Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2]
2025-04-11T03:52:13.1753512Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32]
2025-04-11T03:52:13.1753775Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7]
2025-04-11T03:52:13.1754121Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1754390Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7]
2025-04-11T03:52:13.1754666Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1754934Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32]
2025-04-11T03:52:13.1755219Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1755525Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7]
2025-04-11T03:52:13.1755803Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1756155Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32]
2025-04-11T03:52:13.1756419Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7]
2025-04-11T03:52:13.1756698Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32]
2025-04-11T03:52:13.1756918Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2]
2025-04-11T03:52:13.1757190Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32]
2025-04-11T03:52:13.1757471Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1757756Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1758059Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7]
2025-04-11T03:52:13.1758361Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32]
2025-04-11T03:52:13.1758551Z 0.09s setup    tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T03:52:13.1758858Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32]
2025-04-11T03:52:13.1759187Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32]
2025-04-11T03:52:13.1759456Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7]
2025-04-11T03:52:13.1759751Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1760049Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7]
2025-04-11T03:52:13.1760330Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1760683Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32]
2025-04-11T03:52:13.1760968Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1761249Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1761467Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4]
2025-04-11T03:52:13.1761834Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32]
2025-04-11T03:52:13.1762115Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1762400Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1762618Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8]
2025-04-11T03:52:13.1762835Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16]
2025-04-11T03:52:13.1763117Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1763417Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7]
2025-04-11T03:52:13.1763762Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1763995Z 0.09s setup    tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO
2025-04-11T03:52:13.1764271Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1764571Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7]
2025-04-11T03:52:13.1764790Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8]
2025-04-11T03:52:13.1765064Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1765352Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1765633Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1765939Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32]
2025-04-11T03:52:13.1766151Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2]
2025-04-11T03:52:13.1766369Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4]
2025-04-11T03:52:13.1766706Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1767010Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7]
2025-04-11T03:52:13.1767302Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7]
2025-04-11T03:52:13.1767596Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7]
2025-04-11T03:52:13.1767880Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1768178Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7]
2025-04-11T03:52:13.1768474Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2]
2025-04-11T03:52:13.1768690Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16]
2025-04-11T03:52:13.1768989Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7]
2025-04-11T03:52:13.1769330Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1769612Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1769913Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32]
2025-04-11T03:52:13.1770202Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1770412Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8]
2025-04-11T03:52:13.1770705Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7]
2025-04-11T03:52:13.1770991Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1771338Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32]
2025-04-11T03:52:13.1771638Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32]
2025-04-11T03:52:13.1771934Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32]
2025-04-11T03:52:13.1772218Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1772501Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1772782Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1773083Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7]
2025-04-11T03:52:13.1773394Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32]
2025-04-11T03:52:13.1773677Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1773979Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7]
2025-04-11T03:52:13.1774326Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7]
2025-04-11T03:52:13.1774575Z 0.09s call     tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO
2025-04-11T03:52:13.1774871Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32]
2025-04-11T03:52:13.1775148Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1775456Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32]
2025-04-11T03:52:13.1775755Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32]
2025-04-11T03:52:13.1776106Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1776412Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7]
2025-04-11T03:52:13.1776691Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1777051Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32]
2025-04-11T03:52:13.1777352Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32]
2025-04-11T03:52:13.1777647Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32]
2025-04-11T03:52:13.1777937Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1778213Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1778495Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1778716Z 0.09s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint
2025-04-11T03:52:13.1779119Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7]
2025-04-11T03:52:13.1779416Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1779697Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1780001Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32]
2025-04-11T03:52:13.1780284Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1780569Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1780850Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1781154Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32]
2025-04-11T03:52:13.1781434Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1781711Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1782012Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7]
2025-04-11T03:52:13.1782346Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1782651Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7]
2025-04-11T03:52:13.1782941Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1783228Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1783535Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32]
2025-04-11T03:52:13.1783899Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32]
2025-04-11T03:52:13.1784183Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1784484Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32]
2025-04-11T03:52:13.1784837Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1785141Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32]
2025-04-11T03:52:13.1785418Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1785700Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1785986Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1786285Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7]
2025-04-11T03:52:13.1786590Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32]
2025-04-11T03:52:13.1786947Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7]
2025-04-11T03:52:13.1787234Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1787531Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7]
2025-04-11T03:52:13.1787832Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7]
2025-04-11T03:52:13.1788113Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1788450Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32]
2025-04-11T03:52:13.1788753Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7]
2025-04-11T03:52:13.1789042Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1789337Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7]
2025-04-11T03:52:13.1789638Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32]
2025-04-11T03:52:13.1789991Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1790277Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1790569Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1790854Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1791153Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32]
2025-04-11T03:52:13.1791506Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1791793Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1792075Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1792379Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32]
2025-04-11T03:52:13.1792732Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1793015Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1793297Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1793596Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32]
2025-04-11T03:52:13.1793880Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1794178Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7]
2025-04-11T03:52:13.1794538Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7]
2025-04-11T03:52:13.1794823Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1795107Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1795402Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7]
2025-04-11T03:52:13.1795700Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32]
2025-04-11T03:52:13.1795985Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1796852Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1797136Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1797415Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1797697Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1797976Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1798315Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1798596Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1798883Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1799182Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32]
2025-04-11T03:52:13.1799472Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1799830Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7]
2025-04-11T03:52:13.1800111Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1800389Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1800688Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7]
2025-04-11T03:52:13.1801024Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1801305Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1801586Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1801884Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7]
2025-04-11T03:52:13.1802185Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32]
2025-04-11T03:52:13.1802470Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1802832Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32]
2025-04-11T03:52:13.1803128Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32]
2025-04-11T03:52:13.1803428Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32]
2025-04-11T03:52:13.1803726Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7]
2025-04-11T03:52:13.1804028Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32]
2025-04-11T03:52:13.1804324Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7]
2025-04-11T03:52:13.1804631Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7]
2025-04-11T03:52:13.1804927Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7]
2025-04-11T03:52:13.1805219Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7]
2025-04-11T03:52:13.1805481Z 0.08s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-True]
2025-04-11T03:52:13.1805662Z 0.08s setup    tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin
2025-04-11T03:52:13.1805907Z 0.08s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load
2025-04-11T03:52:13.1806110Z 0.08s setup    tests/test_device/test_device_mesh.py::test_device_mesh_from_process_group
2025-04-11T03:52:13.1806345Z 0.03s call     tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T03:52:13.1806577Z 0.01s call     tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T03:52:13.1806583Z 
2025-04-11T03:52:13.1806751Z (1095 durations < 0.005s hidden.  Use -vv to show these durations.)
2025-04-11T03:52:13.1806880Z =========================== short test summary info ============================
2025-04-11T03:52:13.1807133Z FAILED tests/test_booster/test_accelerator.py::test_accelerator - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1807488Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1807623Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1807793Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1808120Z FAILED tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1808181Z 
2025-04-11T03:52:13.1808314Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1808418Z Traceback (most recent call last):
2025-04-11T03:52:13.1808727Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1808812Z     fn(i, *args)
2025-04-11T03:52:13.1809072Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 271, in run_dist
2025-04-11T03:52:13.1809185Z     check_3d_plugin(early_stop=early_stop)
2025-04-11T03:52:13.1809438Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1809537Z     partial_func(**kwargs)
2025-04-11T03:52:13.1809807Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 104, in check_3d_plugin
2025-04-11T03:52:13.1809967Z     err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:13.1810184Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1810334Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1810596Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1810701Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1810986Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1811091Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1811199Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1811487Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1811626Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1811792Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1812166Z FAILED tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1812172Z 
2025-04-11T03:52:13.1812300Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1812396Z Traceback (most recent call last):
2025-04-11T03:52:13.1812688Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1812770Z     fn(i, *args)
2025-04-11T03:52:13.1813042Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 89, in run_dist
2025-04-11T03:52:13.1813141Z     check_dataloader_sharding()
2025-04-11T03:52:13.1813501Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 69, in check_dataloader_sharding
2025-04-11T03:52:13.1813619Z     batch = next(iter(train_dataloader))[0].cuda()
2025-04-11T03:52:13.1813718Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1814002Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1814137Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1814298Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1814640Z FAILED tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1814720Z 
2025-04-11T03:52:13.1814839Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1814939Z Traceback (most recent call last):
2025-04-11T03:52:13.1815220Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1815305Z     fn(i, *args)
2025-04-11T03:52:13.1815568Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 167, in run_dist
2025-04-11T03:52:13.1815741Z     check_gemini_plugin(early_stop=early_stop)
2025-04-11T03:52:13.1815994Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1816084Z     partial_func(**kwargs)
2025-04-11T03:52:13.1816334Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1816422Z     partial_func(**kwargs)
2025-04-11T03:52:13.1816668Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1816757Z     partial_func(**kwargs)
2025-04-11T03:52:13.1816863Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.1817154Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 149, in check_gemini_plugin
2025-04-11T03:52:13.1817358Z     err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn, zero_size, tp_size)
2025-04-11T03:52:13.1817571Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1817669Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1817987Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1818085Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1818364Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1818462Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1818566Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1818838Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1818969Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1819132Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1819514Z FAILED tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1819520Z 
2025-04-11T03:52:13.1819643Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1819734Z Traceback (most recent call last):
2025-04-11T03:52:13.1820020Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1820099Z     fn(i, *args)
2025-04-11T03:52:13.1820381Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 135, in run_dist
2025-04-11T03:52:13.1820510Z     check_low_level_zero_plugin(early_stop=early_stop)
2025-04-11T03:52:13.1820755Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1820898Z     partial_func(**kwargs)
2025-04-11T03:52:13.1821222Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 84, in check_low_level_zero_plugin
2025-04-11T03:52:13.1821371Z     err = run_fn(stage, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:13.1821584Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1821685Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1821934Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1822027Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1822355Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1822452Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1822552Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1822825Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1822957Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1823168Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1823531Z FAILED tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1823535Z 
2025-04-11T03:52:13.1823656Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1823747Z Traceback (most recent call last):
2025-04-11T03:52:13.1824033Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1824111Z     fn(i, *args)
2025-04-11T03:52:13.1824380Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 113, in run_dist
2025-04-11T03:52:13.1824473Z     check_torch_ddp_plugin()
2025-04-11T03:52:13.1824778Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 52, in check_torch_ddp_plugin
2025-04-11T03:52:13.1824899Z     run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:13.1825112Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1825266Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1825517Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1825616Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1825881Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1825977Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1826076Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1826349Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1826482Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1826638Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1827004Z FAILED tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1827010Z 
2025-04-11T03:52:13.1827124Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1827220Z Traceback (most recent call last):
2025-04-11T03:52:13.1827495Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1827571Z     fn(i, *args)
2025-04-11T03:52:13.1827844Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 77, in run_dist
2025-04-11T03:52:13.1827935Z     check_torch_fsdp_plugin()
2025-04-11T03:52:13.1828297Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 70, in check_torch_fsdp_plugin
2025-04-11T03:52:13.1828485Z     run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:13.1828702Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1828799Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1829048Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1829149Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1829416Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1829516Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1829686Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1829965Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1830094Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1830252Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1830620Z FAILED tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1830686Z 
2025-04-11T03:52:13.1830804Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1830903Z Traceback (most recent call last):
2025-04-11T03:52:13.1831182Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1831262Z     fn(i, *args)
2025-04-11T03:52:13.1831523Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_checkpoint_io.py", line 212, in run_dist
2025-04-11T03:52:13.1831613Z     exam_state_dict()
2025-04-11T03:52:13.1831825Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1831917Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1832169Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1832263Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1832535Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1832689Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1832791Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1833068Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1833199Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1833366Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1833740Z FAILED tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1833744Z 
2025-04-11T03:52:13.1833866Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1833959Z Traceback (most recent call last):
2025-04-11T03:52:13.1834247Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1834325Z     fn(i, *args)
2025-04-11T03:52:13.1834605Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_torch_compability.py", line 167, in run_dist
2025-04-11T03:52:13.1834706Z     exam_torch_load_from_gemini()
2025-04-11T03:52:13.1834917Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1835020Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1835276Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1835375Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1835701Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1835797Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1835898Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1836169Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1836304Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1836457Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1836774Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1837050Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1837240Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1837390Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1837737Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1838007Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1838186Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1838342Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1838690Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1838964Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1839090Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1839240Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1839588Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1839854Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1839990Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1840191Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1840541Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1840808Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1840938Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1841088Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1841460Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1841732Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1841856Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1842009Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1842376Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1842645Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1842768Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1842923Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1843385Z FAILED tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1843390Z 
2025-04-11T03:52:13.1843512Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1843608Z Traceback (most recent call last):
2025-04-11T03:52:13.1843891Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1843973Z     fn(i, *args)
2025-04-11T03:52:13.1844283Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py", line 148, in run_dist
2025-04-11T03:52:13.1844370Z     exam_state_dict()
2025-04-11T03:52:13.1844621Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1844772Z     partial_func(**kwargs)
2025-04-11T03:52:13.1845021Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1845110Z     partial_func(**kwargs)
2025-04-11T03:52:13.1845364Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1845450Z     partial_func(**kwargs)
2025-04-11T03:52:13.1845611Z   [Previous line repeated 3 more times]
2025-04-11T03:52:13.1845828Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1845925Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1846175Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1846269Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1846542Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1846641Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1846741Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1847018Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1847150Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1847305Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1847710Z FAILED tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1847985Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1848109Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1848264Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1848691Z FAILED tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1848696Z 
2025-04-11T03:52:13.1848815Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1848906Z Traceback (most recent call last):
2025-04-11T03:52:13.1849194Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1849273Z     fn(i, *args)
2025-04-11T03:52:13.1849634Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py", line 72, in run_dist
2025-04-11T03:52:13.1849730Z     exam_from_pretrained()
2025-04-11T03:52:13.1849943Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1850041Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1850289Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1850389Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1850658Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1850806Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1850907Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1851175Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1851304Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1851455Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1851760Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1852029Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1852212Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1852367Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1852666Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1852933Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1853113Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1853270Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1853564Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1853833Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1853956Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1854104Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1854404Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1854670Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1854795Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1854943Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1855275Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1855542Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1855668Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1855815Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1856194Z FAILED tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1856200Z 
2025-04-11T03:52:13.1856320Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1856414Z Traceback (most recent call last):
2025-04-11T03:52:13.1856699Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1856778Z     fn(i, *args)
2025-04-11T03:52:13.1857046Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 76, in run_dist
2025-04-11T03:52:13.1857138Z     check_torch_ddp_checkpointIO()
2025-04-11T03:52:13.1857390Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1857482Z     partial_func(**kwargs)
2025-04-11T03:52:13.1857726Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1857819Z     partial_func(**kwargs)
2025-04-11T03:52:13.1858115Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1858207Z     partial_func(**kwargs)
2025-04-11T03:52:13.1858307Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.1858631Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 26, in check_torch_ddp_checkpointIO
2025-04-11T03:52:13.1858888Z     model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)
2025-04-11T03:52:13.1859095Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T03:52:13.1859290Z     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T03:52:13.1859619Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_ddp_plugin.py", line 283, in configure
2025-04-11T03:52:13.1859727Z     model = model.to(get_current_device())
2025-04-11T03:52:13.1859991Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:13.1860089Z     return self._apply(convert)
2025-04-11T03:52:13.1860358Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.1860503Z     module._apply(fn)
2025-04-11T03:52:13.1860769Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.1860861Z     param_applied = fn(param)
2025-04-11T03:52:13.1861133Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:13.1861347Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:13.1861454Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1861729Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1861860Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1862019Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1862391Z FAILED tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1862397Z 
2025-04-11T03:52:13.1862570Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1862666Z Traceback (most recent call last):
2025-04-11T03:52:13.1862946Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1863024Z     fn(i, *args)
2025-04-11T03:52:13.1863297Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 156, in run_dist
2025-04-11T03:52:13.1863394Z     check_torch_fsdp_ckpt()
2025-04-11T03:52:13.1863639Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1863729Z     partial_func(**kwargs)
2025-04-11T03:52:13.1864035Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 53, in check_torch_fsdp_ckpt
2025-04-11T03:52:13.1864235Z     fsdp_model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion)
2025-04-11T03:52:13.1864437Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T03:52:13.1864628Z     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T03:52:13.1864888Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 533, in configure
2025-04-11T03:52:13.1865112Z     fsdp_model = TorchFSDPModel(model, device_id=torch.cuda.current_device(), **self.fsdp_kwargs)
2025-04-11T03:52:13.1865381Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 438, in __init__
2025-04-11T03:52:13.1865494Z     self.module = FSDP(module, *args, **kwargs)
2025-04-11T03:52:13.1865924Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 503, in __init__
2025-04-11T03:52:13.1866025Z     _init_param_handle_from_module(
2025-04-11T03:52:13.1866404Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 568, in _init_param_handle_from_module
2025-04-11T03:52:13.1866496Z     _move_module_to_device(
2025-04-11T03:52:13.1866848Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 956, in _move_module_to_device
2025-04-11T03:52:13.1867025Z     _move_states_to_device(params_to_move, bufs_to_move, device_from_device_id)
2025-04-11T03:52:13.1867427Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 986, in _move_states_to_device
2025-04-11T03:52:13.1867546Z     param.data = param.to(device_from_device_id)
2025-04-11T03:52:13.1867645Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1867928Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1868057Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1868276Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1868653Z FAILED tests/test_device/test_init_logical_pg.py::test_logical_pg - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1868658Z 
2025-04-11T03:52:13.1868775Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1868875Z Traceback (most recent call last):
2025-04-11T03:52:13.1869151Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1869233Z     fn(i, *args)
2025-04-11T03:52:13.1869468Z   File "/__w/ColossalAI/ColossalAI/tests/test_device/test_init_logical_pg.py", line 17, in check_layer
2025-04-11T03:52:13.1869592Z     tensor_to_check = torch.tensor([2, 2, 2, 2]).cuda()
2025-04-11T03:52:13.1869689Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1869957Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1870091Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1870420Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1870738Z FAILED tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1870741Z 
2025-04-11T03:52:13.1870854Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1870950Z Traceback (most recent call last):
2025-04-11T03:52:13.1871230Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1871310Z     fn(i, *args)
2025-04-11T03:52:13.1871541Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_all_to_all_single.py", line 67, in run_dist
2025-04-11T03:52:13.1871625Z     check_all2all()
2025-04-11T03:52:13.1871845Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1871944Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1872204Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1872300Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1872569Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1872665Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1872762Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1873040Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1873167Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1873382Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1873671Z FAILED tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1873677Z 
2025-04-11T03:52:13.1873796Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1873889Z Traceback (most recent call last):
2025-04-11T03:52:13.1874167Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1874251Z     fn(i, *args)
2025-04-11T03:52:13.1874473Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all.py", line 31, in run_dist
2025-04-11T03:52:13.1874559Z     check_4gpu()
2025-04-11T03:52:13.1874841Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1874939Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1875192Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1875286Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1875556Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1875711Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1875808Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1876086Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1876220Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1876378Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1876705Z FAILED tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1876712Z 
2025-04-11T03:52:13.1876824Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1876916Z Traceback (most recent call last):
2025-04-11T03:52:13.1877206Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1877281Z     fn(i, *args)
2025-04-11T03:52:13.1877522Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all_single.py", line 29, in run_dist
2025-04-11T03:52:13.1877656Z     check_4gpu()
2025-04-11T03:52:13.1877869Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1877965Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1878213Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1878311Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1878574Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1878671Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1878765Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1879038Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1879166Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1879322Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1879619Z FAILED tests/test_fp8/test_fp8_allgather.py::test_all_gather - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1879623Z 
2025-04-11T03:52:13.1879733Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1879827Z Traceback (most recent call last):
2025-04-11T03:52:13.1880101Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1880182Z     fn(i, *args)
2025-04-11T03:52:13.1880397Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allgather.py", line 37, in run_dist
2025-04-11T03:52:13.1880475Z     check_4gpu()
2025-04-11T03:52:13.1880737Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1880832Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1881088Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1881183Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1881451Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1881547Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1881642Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1881923Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1882113Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1882269Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1882571Z FAILED tests/test_fp8/test_fp8_allreduce.py::test_all_reduce - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1882575Z 
2025-04-11T03:52:13.1882690Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1882839Z Traceback (most recent call last):
2025-04-11T03:52:13.1883121Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1883198Z     fn(i, *args)
2025-04-11T03:52:13.1883414Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allreduce.py", line 47, in run_dist
2025-04-11T03:52:13.1883497Z     check_4gpu()
2025-04-11T03:52:13.1883742Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1883834Z     partial_func(**kwargs)
2025-04-11T03:52:13.1884045Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1884136Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1884391Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1884484Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1884754Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1884903Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1885001Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1885273Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1885401Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1885560Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1885782Z FAILED tests/test_fp8/test_fp8_cast.py::test_fp8_cast - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1886059Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1886184Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1886338Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1886626Z FAILED tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1886631Z 
2025-04-11T03:52:13.1886746Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1886837Z Traceback (most recent call last):
2025-04-11T03:52:13.1887114Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1887197Z     fn(i, *args)
2025-04-11T03:52:13.1887432Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_fsdp_comm_hook.py", line 95, in demo_basic
2025-04-11T03:52:13.1887510Z     run_model()
2025-04-11T03:52:13.1887723Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1887871Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1888124Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1888220Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1888503Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1888602Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1888708Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1888981Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1889115Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1889326Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1889545Z FAILED tests/test_fp8/test_fp8_hook.py::test_fp8_hook - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1889821Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1889949Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1890164Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1890423Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1890697Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1890820Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1890968Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1891229Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1891497Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1891631Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1891781Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1892045Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1892364Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1892496Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1892645Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1892899Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1893171Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1893294Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1893449Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1893770Z FAILED tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1893775Z 
2025-04-11T03:52:13.1893901Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1893995Z Traceback (most recent call last):
2025-04-11T03:52:13.1894309Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1894396Z     fn(i, *args)
2025-04-11T03:52:13.1894627Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_reduce_scatter.py", line 36, in run_dist
2025-04-11T03:52:13.1894713Z     check_4gpu()
2025-04-11T03:52:13.1894924Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1895023Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1895324Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1895425Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1895696Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1895794Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1895898Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1896172Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1896308Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1896461Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1896763Z FAILED tests/test_infer/test_batch_bucket.py::test_bucket - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1897032Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1897157Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1897312Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1897729Z FAILED tests/test_infer/test_continuous_batching.py::test_continuous_batching - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1897733Z 
2025-04-11T03:52:13.1897853Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1897944Z Traceback (most recent call last):
2025-04-11T03:52:13.1898227Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1898307Z     fn(i, *args)
2025-04-11T03:52:13.1898541Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 61, in run_dist
2025-04-11T03:52:13.1898636Z     check_inference_engine()
2025-04-11T03:52:13.1898886Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1898979Z     partial_func(**kwargs)
2025-04-11T03:52:13.1899225Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1899314Z     partial_func(**kwargs)
2025-04-11T03:52:13.1899607Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1899693Z     partial_func(**kwargs)
2025-04-11T03:52:13.1899799Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.1900084Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 39, in check_inference_engine
2025-04-11T03:52:13.1900251Z     model = LlamaForCausalLM(LlamaConfig(num_hidden_layers=2)).cuda()
2025-04-11T03:52:13.1900538Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.1900644Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.1900912Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.1901033Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.1901305Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.1901391Z     module._apply(fn)
2025-04-11T03:52:13.1901661Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.1901745Z     module._apply(fn)
2025-04-11T03:52:13.1902014Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.1902110Z     param_applied = fn(param)
2025-04-11T03:52:13.1902381Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.1902494Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.1902648Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1902932Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1903060Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1903219Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1903461Z FAILED tests/test_infer/test_drafter.py::test_drafter[5] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1903734Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1903861Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1904074Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1904302Z FAILED tests/test_infer/test_drafter.py::test_spec_dec - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1904571Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1904700Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1904849Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1905228Z FAILED tests/test_infer/test_kvcache_manager.py::test_cache_manager - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1905232Z 
2025-04-11T03:52:13.1905347Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1905443Z Traceback (most recent call last):
2025-04-11T03:52:13.1905720Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1905799Z     fn(i, *args)
2025-04-11T03:52:13.1906033Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 168, in run_dist
2025-04-11T03:52:13.1906120Z     check_cache_manager()
2025-04-11T03:52:13.1906371Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1906454Z     partial_func(**kwargs)
2025-04-11T03:52:13.1906708Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 89, in check_cache_manager
2025-04-11T03:52:13.1906865Z     cache_manager = KVCacheManager(inference_config, model_config)
2025-04-11T03:52:13.1907178Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T03:52:13.1907348Z     self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T03:52:13.1907639Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T03:52:13.1907860Z     k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T03:52:13.1907961Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1908238Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1908365Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1908571Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1908948Z FAILED tests/test_infer/test_request_handler.py::test_running_list_and_request_handler - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1908952Z 
2025-04-11T03:52:13.1909068Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1909167Z Traceback (most recent call last):
2025-04-11T03:52:13.1909457Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1909542Z     fn(i, *args)
2025-04-11T03:52:13.1909771Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 95, in run_dist
2025-04-11T03:52:13.1909862Z     check_request_handler()
2025-04-11T03:52:13.1910191Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 70, in check_request_handler
2025-04-11T03:52:13.1910353Z     request_handler = RequestHandler(inference_config, model_config)
2025-04-11T03:52:13.1910615Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 160, in __init__
2025-04-11T03:52:13.1910707Z     self._init_cache(model_config)
2025-04-11T03:52:13.1910974Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 222, in _init_cache
2025-04-11T03:52:13.1911152Z     self.cache_manager = KVCacheManager(self.inference_config, model_config)
2025-04-11T03:52:13.1911415Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T03:52:13.1911642Z     self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T03:52:13.1911936Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T03:52:13.1912156Z     k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T03:52:13.1912257Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1912533Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1912724Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1912883Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1913180Z FAILED tests/test_infer/test_streamingllm.py::test_engine - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1913184Z 
2025-04-11T03:52:13.1913305Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1913400Z Traceback (most recent call last):
2025-04-11T03:52:13.1913681Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1913763Z     fn(i, *args)
2025-04-11T03:52:13.1913988Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 107, in run_dist
2025-04-11T03:52:13.1914087Z     ret[rank] = func_to_run(**kwargs)
2025-04-11T03:52:13.1914337Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 39, in check_streamingllm
2025-04-11T03:52:13.1914416Z     ).cuda()
2025-04-11T03:52:13.1914766Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.1914866Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.1915128Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.1915236Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.1915502Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.1915584Z     module._apply(fn)
2025-04-11T03:52:13.1915847Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.1915929Z     module._apply(fn)
2025-04-11T03:52:13.1916183Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.1916283Z     param_applied = fn(param)
2025-04-11T03:52:13.1916549Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.1916661Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.1916759Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1917036Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1917164Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1917316Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1917745Z FAILED tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1918020Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1918154Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1918310Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1918685Z FAILED tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1918958Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1919154Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1919308Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1919645Z FAILED tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1919919Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1920107Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1920261Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1920584Z FAILED tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1920854Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1920981Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1921136Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1921474Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1921737Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1921864Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1922070Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1922409Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1922680Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1922806Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1922956Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1923298Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1923567Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1923690Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1923847Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1924176Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1924449Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1924573Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1924728Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1925115Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1925386Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1925510Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1925657Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1925996Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1926263Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1926466Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1926614Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1926954Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1927222Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1927406Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1927553Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1927885Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1928156Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1928279Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1928433Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1928769Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1929038Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1929160Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1929312Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1929699Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1929965Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1930091Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1930240Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1930576Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1930837Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1930960Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1931107Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1931450Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1931716Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1931839Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1931993Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1932321Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1932641Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1932768Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1932918Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1933249Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1933521Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1933644Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1933858Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1934201Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1934473Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1934599Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1934802Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1935142Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1935410Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1935535Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1935683Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1936017Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1936288Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1936410Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1936563Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1936952Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1937224Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1937346Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1937503Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1937838Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1938104Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1938231Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1938381Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1938720Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1938985Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1939110Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1939259Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1939598Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1939915Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1940038Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1940193Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1940527Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1940799Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1940919Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1941131Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1941463Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1941734Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1941856Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1942063Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1942408Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1942676Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1942801Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1942950Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1943285Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1943554Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1943678Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1943828Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1944211Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1944483Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1944605Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1944757Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1945094Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1945365Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1945487Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1945633Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1945974Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1946239Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1946363Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1946509Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1946845Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1947167Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1947296Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1947443Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1947789Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1948061Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1948180Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1948333Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1948768Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1949039Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1949161Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1949312Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1949720Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1950011Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1950162Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1950311Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1950659Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1950926Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1951051Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1951200Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1951603Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1951873Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1951998Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1952150Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1952486Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1952760Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1952883Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1953036Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1953374Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1953645Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1953766Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1953914Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1954229Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1954565Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1954697Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1954846Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1955168Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1955436Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1955563Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1955713Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1956086Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1956359Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1956483Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1956633Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1956943Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1957269Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1957390Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1957542Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1957849Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1958118Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1958246Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1958394Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1958705Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1959025Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1959157Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1959305Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1959617Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1959889Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1960011Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1960166Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1960481Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1960753Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1960873Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1961025Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1961331Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1961602Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1961724Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1961928Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1962236Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1962502Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1962631Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1962778Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1963091Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1963418Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1963541Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1963693Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1964001Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1964327Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1964450Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1964600Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1964906Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1965174Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1965298Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1965445Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1965756Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1966020Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1966149Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1966350Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1966663Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1966933Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1967062Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1967212Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1967525Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1967791Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1967917Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1968068Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1968435Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1968705Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1968829Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1968978Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1969395Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1969659Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1969785Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1969933Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1970288Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1970551Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1970743Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1970891Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1971244Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1971508Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1971689Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1971840Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1972170Z FAILED tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1972441Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1972565Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1972714Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1973040Z FAILED tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1973311Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1973435Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1973635Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1974049Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1974320Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1974447Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1974596Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1975004Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1975270Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1975395Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1975544Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1975942Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1976217Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1976340Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1976498Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1976949Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1977221Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1977343Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1977496Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1977892Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1978158Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1978350Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1978498Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1978909Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1979174Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1979365Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1979513Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1979913Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1980182Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1980313Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1980461Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1980862Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1981135Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1981322Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1981473Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1981864Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1982136Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1982257Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1982410Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1982800Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1983065Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1983193Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1983342Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1983733Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1984000Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1984127Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1984331Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1984732Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1985005Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1985129Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1985279Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1985677Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1986008Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1986130Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1986284Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1986675Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1987108Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1987231Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1987378Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1987771Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1988038Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1988167Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1988315Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1988748Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1989080Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1989209Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1989357Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1989748Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1990022Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1990145Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1990296Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1990693Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1990966Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1991087Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1991238Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1991636Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1991903Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1992095Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1992246Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1992652Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1992921Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1993048Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1993196Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1993666Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1993939Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1994064Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1994211Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1994703Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1994982Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1995105Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1995256Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1995660Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1995935Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1996058Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1996213Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1996687Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1996958Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1997087Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1997235Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1997637Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1997908Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1998035Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1998182Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1998586Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1998854Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1998980Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1999131Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1999533Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1999862Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1999988Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2000140Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2000543Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2000817Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2000939Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2001147Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2001548Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2001817Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2001942Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2002146Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2002552Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2002819Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2002943Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2003091Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2003488Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2003762Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2003885Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2004036Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2004496Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2004769Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2004895Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2005045Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2005440Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2005707Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2005836Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2005989Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2006389Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2006657Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2006783Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2006930Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2007375Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2007644Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2007768Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2007920Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2008314Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2008587Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2008773Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2008925Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2009318Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2009592Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2009786Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2009936Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2010332Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2010599Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2010726Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2010871Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2011269Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2011538Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2011665Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2011863Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2012263Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2012529Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2012654Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2012806Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2013200Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2013469Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2013596Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2013746Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2014135Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2014407Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2014531Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2014679Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2015131Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2015403Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2015531Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2015679Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2016077Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2016409Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2016534Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2016684Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2017074Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2017396Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2017520Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2017673Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2018068Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2018337Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2018459Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2018610Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2019007Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2019328Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2019456Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2019602Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2019998Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2020267Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2020391Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2020540Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2020938Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2021209Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2021332Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2021483Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2021880Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2022153Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2022334Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2022487Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2022881Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2023155Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2023275Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2023425Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2023823Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2024147Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2024275Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2024423Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2024821Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2025148Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2025274Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2025422Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2025826Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2026094Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2026219Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2026370Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2026765Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2027142Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2027266Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2027414Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2027811Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2028088Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2028209Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2028355Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2028808Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2029076Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2029202Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2029351Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2029754Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2030095Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2030225Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2030374Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2030771Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2031039Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2031161Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2031309Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2031771Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2032046Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2032166Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2032320Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2032781Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2033048Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2033178Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2033326Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2033731Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2034004Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2034128Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2034277Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2034758Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2035027Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2035147Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2035299Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2035695Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2035971Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2036092Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2036243Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2036635Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2036903Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2037024Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2037172Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2037634Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2037905Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2038030Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2038180Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2038580Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2038847Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2038974Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2039197Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2039599Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2039868Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2040048Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2040201Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2040596Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2040870Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2040994Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2041144Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2041544Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2041814Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2041939Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2042142Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2042544Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2042814Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2042945Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2043094Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2043498Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2043763Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2043888Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2044037Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2044436Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2044711Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2044836Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2044989Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2045439Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2045712Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2045836Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2045989Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2046382Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2046649Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2046839Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2046990Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2047396Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2047668Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2047854Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2048005Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2048405Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2048673Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2048801Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2048954Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2049359Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2049632Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2049809Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2049964Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2050367Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2050694Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2050821Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2050972Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2051375Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2051647Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2051773Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2051922Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2052319Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2052588Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2052715Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2052919Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2053323Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2053596Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2053718Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2053871Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2054269Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2054602Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2054722Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2054875Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2055274Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2055598Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2055719Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2055866Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2056267Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2056539Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2056668Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2056814Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2057221Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2057543Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2057672Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2057820Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2058217Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2058491Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2058613Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2058768Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2059171Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2059445Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2059567Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2059717Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2060117Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2060438Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2060568Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2060714Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2061118Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2061388Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2061512Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2061657Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2062132Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2062404Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2062528Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2062681Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2063135Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2063407Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2063530Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2063679Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2064081Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2064354Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2064476Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2064623Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2065085Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2065358Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2065483Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2065629Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2066032Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2066300Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2066426Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2066574Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2066985Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2067251Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2067372Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2067524Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2067912Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2068255Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2068379Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2068577Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2068961Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2069233Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2069354Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2069580Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2069967Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2070237Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2070365Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2070574Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2070962Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2071227Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2071353Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2071504Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2071886Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2072158Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2072280Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2072434Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2072878Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2073151Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2073273Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2073429Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2073807Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2074074Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2074197Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2074346Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2074736Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2074996Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2075123Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2075271Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2075716Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2075987Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2076112Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2076265Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2076646Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2076916Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2077101Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2077252Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2077630Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2077903Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2078080Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2078228Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2078611Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2078878Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2079005Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2079150Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2079534Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2079801Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2079928Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2080130Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2080520Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2080794Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2080920Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2081069Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2081447Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2081719Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2081846Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2082000Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2082382Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2082653Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2082784Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2082930Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2083374Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2083643Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2083771Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2083922Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2084309Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2084584Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2084770Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2084919Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2085301Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2085575Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2085749Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2085902Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2086286Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2086557Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2086681Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2086829Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2087214Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2087479Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2087658Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2087809Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2088209Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2088480Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2088607Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2088758Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2089148Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2089418Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2089543Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2089699Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2090090Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2090365Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2090488Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2090640Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2091080Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2091353Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2091475Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2091626Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2092012Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2092335Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2092458Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2092606Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2092989Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2093310Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2093437Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2093586Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2093971Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2094245Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2094367Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2094523Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2094903Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2095174Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2095359Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2095512Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2095898Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2096168Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2096292Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2096440Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2096826Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2097096Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2097220Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2097368Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2097759Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2098023Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2098143Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2098349Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2098734Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2099010Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2099131Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2099279Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2099656Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2099991Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2100113Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2100263Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2100644Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2100966Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2101093Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2101239Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2101617Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2101887Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2102013Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2102162Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2102541Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2102870Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2102995Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2103147Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2103526Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2103797Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2103921Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2104073Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2104448Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2104719Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2104845Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2104992Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2105374Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2105643Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2105824Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2105973Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2106361Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2106632Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2106755Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2106909Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2107290Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2107711Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2107836Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2107987Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2108363Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2108728Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2108850Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2109001Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2109382Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2109652Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2109780Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2109929Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2110309Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2110639Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2110772Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2110920Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2111301Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2111573Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2111698Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2111850Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2112223Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2112499Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2112619Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2112769Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2113150Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2113481Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2113605Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2113751Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2114141Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2114410Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2114536Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2114683Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2115137Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2115405Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2115530Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2115680Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2116126Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2116400Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2116520Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2116671Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2117056Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2117327Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2117449Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2117601Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2118039Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2118309Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2118437Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2118585Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2118984Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2119255Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2119383Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2119529Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2119917Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2120185Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2120307Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2120459Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2120846Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2121172Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2121297Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2121445Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2121825Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2122096Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2122217Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2122362Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2122813Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2123079Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2123206Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2123421Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2123809Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2124078Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2124206Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2124359Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2124744Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2125018Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2125141Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2125294Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2125736Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2126008Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2126132Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2126284Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2126671Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2126938Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2127063Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2127210Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2127600Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2127866Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2127992Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2128143Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2128531Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2128854Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2128978Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2129129Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2129511Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2129783Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2129905Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2130122Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2130507Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2130779Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2130903Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2131107Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2131492Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2131760Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2131886Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2132034Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2132413Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2132679Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2132810Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2132956Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2133391Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2133664Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2133792Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2133943Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2134326Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2134593Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2134714Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2134866Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2135239Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2135505Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2135629Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2135775Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2136235Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2136501Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2136629Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2136782Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2137165Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2137437Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2137643Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2137790Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2138163Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2138433Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2138611Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2138765Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2139143Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2139416Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2139541Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2139692Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2140074Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2140344Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2140473Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2140675Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2141058Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2141329Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2141457Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2141604Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2141991Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2142262Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2142386Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2142541Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2142919Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2143194Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2143317Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2143467Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2143902Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2144175Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2144299Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2144449Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2144831Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2145096Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2145284Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2145434Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2145821Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2146095Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2146276Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2146426Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2146802Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2147070Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2147196Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2147347Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2147729Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2148000Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2148174Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2148330Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2148760Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2149024Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2149153Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2149299Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2149692Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2149958Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2150083Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2150230Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2150609Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2150878Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2150998Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2151242Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2151643Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2151916Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2152039Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2152189Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2152567Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2152922Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2153048Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2153199Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2153589Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2153917Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2154044Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2154191Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2154575Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2154844Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2154969Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2155121Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2155507Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2155841Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2155969Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2156122Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2156508Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2156786Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2156906Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2157060Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2157447Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2157721Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2157842Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2157993Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2158376Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2158647Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2158773Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2158977Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2159368Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2159641Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2159770Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2159919Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2160301Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2160635Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2160756Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2160907Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2161279Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2161607Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2161730Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2161880Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2162262Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2162532Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2162661Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2162808Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2163187Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2163505Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2163633Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2163779Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2164158Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2164423Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2164548Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2164698Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2165075Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2165347Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2165470Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2165624Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2165998Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2166267Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2166445Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2166595Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2166978Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2167247Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2167373Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2167519Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2167897Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2168225Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2168351Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2168499Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2168872Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2169201Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2169323Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2169477Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2169852Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2170128Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2170251Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2170401Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2170775Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2171097Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2171225Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2171372Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2171758Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2172028Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2172153Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2172300Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2172688Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2172957Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2173077Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2173229Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2173605Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2173932Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2174055Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2174203Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2174583Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2174853Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2174974Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2175119Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2175563Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2175835Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2175961Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2176108Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2176553Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2176821Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2176945Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2177093Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2177473Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2177746Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2177867Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2178018Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2178452Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2178720Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2178840Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2178992Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2179372Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2179645Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2179767Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2179915Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2180305Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2180567Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2180692Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2180837Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2181217Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2181554Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2181683Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2181832Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2182218Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2182489Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2182610Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2182820Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2183198Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2183473Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2183595Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2183802Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2184183Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2184449Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2184574Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2184725Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2185106Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2185375Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2185500Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2185649Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2186081Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2186349Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2186469Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2186625Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2187003Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2187273Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2187396Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2187547Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2187929Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2188198Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2188320Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2188505Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2188950Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2189217Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2189342Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2189491Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2189878Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2190144Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2190338Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2190486Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2190869Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2191140Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2191322Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2191474Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2191857Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2192129Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2192252Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2192404Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2192785Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2193051Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2193179Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2193384Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2193775Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2194045Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2194172Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2194318Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2194711Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2194980Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2195106Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2195259Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2195644Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2195916Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2196040Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2196191Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2196630Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2196899Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2197023Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2197170Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2197554Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2197820Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2198010Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2198157Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2198545Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2198812Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2198993Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2199144Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2199525Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2199797Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2199920Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2200069Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2200450Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2200723Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2200845Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2201052Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2201440Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2201717Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2201843Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2201992Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2202386Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2202657Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2202786Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2202937Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2203329Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2203597Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2203724Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2203872Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2204305Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2204572Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2204697Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2204850Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2205235Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2205504Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2205686Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2205838Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2206221Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2206593Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2206725Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2206872Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2207258Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2207527Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2207652Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2207801Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2208188Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2208453Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2208654Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2208812Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2209201Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2209474Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2209596Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2209747Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2210136Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2210411Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2210534Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2210680Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2211081Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2211349Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2211474Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2211675Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2212069Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2212341Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2212470Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2212619Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2213014Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2213347Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2213469Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2213620Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2214001Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2214328Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2214449Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2214600Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2214986Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2215258Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2215386Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2215535Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2215924Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2216248Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2216375Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2216523Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2216917Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2217186Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2217314Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2217462Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2217850Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2218124Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2218246Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2218401Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2218793Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2219063Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2219240Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2219389Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2219775Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2220043Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2220170Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2220316Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2220707Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2221035Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2221161Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2221313Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2221701Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2222024Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2222147Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2222300Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2222681Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2222954Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2223075Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2223229Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2223607Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2223934Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2224057Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2224203Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2224595Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2224861Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2224987Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2225138Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2225527Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2225796Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2225926Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2226077Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2226466Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2226883Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2227008Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2227157Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2227542Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2227813Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2227935Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2228087Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2228569Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2228837Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2228964Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2229111Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2229563Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2229835Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2229961Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2230109Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2230497Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2230766Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2230888Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2231041Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2231484Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2231757Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2231880Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2232032Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2232416Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2232690Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2232813Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2232964Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2233352Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2233618Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2233746Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2233896Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2234287Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2234614Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2234742Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2234894Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2235280Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2235553Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2235673Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2235895Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2236276Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2236549Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2236668Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2236887Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2237273Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2237546Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2237670Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2237822Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2238210Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2238478Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2238602Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2238751Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2239197Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2239463Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2239588Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2239736Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2240124Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2240393Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2240515Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2240673Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2241056Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2241324Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2241446Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2241599Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2242044Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2242315Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2242444Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2242592Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2242982Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2243251Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2243439Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2243586Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2243987Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2244257Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2244438Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2244594Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2244985Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2245264Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2245390Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2245543Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2245932Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2246206Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2246330Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2246529Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2246925Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2247195Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2247323Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2247470Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2247859Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2248126Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2248252Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2248401Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2248788Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2249058Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2249183Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2249336Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2249788Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2250059Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2250184Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2250336Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2250713Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2250973Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2251158Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2251307Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2251701Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2252023Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2252207Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2252355Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2252723Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2252992Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2253116Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2253267Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2253629Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2253900Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2254023Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2254226Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2254589Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2254858Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2254981Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2255129Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2255494Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2255762Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2255894Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2256043Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2256404Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2256673Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2256803Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2256954Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2257357Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2257629Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2257754Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2257905Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2258262Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2258531Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2258716Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2258867Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2259229Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2259499Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2259679Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2259829Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2260193Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2260462Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2260589Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2260736Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2261103Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2261371Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2261551Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2261704Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2262057Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2262326Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2262450Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2262600Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2262957Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2263223Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2263346Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2263492Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2263846Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2264106Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2264234Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2264378Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2264790Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2265057Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2265187Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2265336Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2265689Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2265961Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2266154Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2266306Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2266662Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2266990Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2267113Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2267263Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2267615Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2267889Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2268012Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2268162Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2268564Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2268835Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2269029Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2269182Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2269540Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2269808Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2269932Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2270080Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2270440Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2270710Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2270834Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2270984Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2271340Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2271608Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2271728Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2271942Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2272310Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2272579Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2272709Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2272859Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2273219Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2273560Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2273687Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2273839Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2274207Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2274539Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2274662Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2274815Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2275169Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2275441Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2275566Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2275720Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2276078Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2276351Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2276524Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2276674Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2277036Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2277303Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2277427Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2277577Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2277942Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2278208Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2278336Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2278484Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2278837Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2279109Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2279229Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2279439Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2279802Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2280077Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2280202Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2280356Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2280717Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2281051Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2281180Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2281330Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2281698Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2282033Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2282159Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2282306Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2282668Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2282937Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2283061Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2283216Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2283579Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2283910Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2284034Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2284184Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2284542Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2284814Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2284938Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2285087Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2285457Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2285731Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2285856Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2286005Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2286368Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2286639Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2286766Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2286969Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2287330Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2287607Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2287730Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2287884Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2288239Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2288567Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2288688Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2288840Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2289199Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2289521Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2289647Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2289792Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2290148Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2290414Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2290542Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2290691Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2291050Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2291371Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2291494Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2291648Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2292009Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2292282Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2292405Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2292563Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2292927Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2293204Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2293326Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2293478Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2293848Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2294115Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2294240Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2294454Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2294822Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2295095Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2295221Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2295370Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2295733Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2296063Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2296186Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2296338Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2296700Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2297025Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2297146Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2297296Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2297595Z FAILED tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2297861Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2297989Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2298138Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2298516Z FAILED tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2298837Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2298969Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2299117Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2299497Z FAILED tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2299765Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2299887Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2300042Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2300373Z FAILED tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2300645Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2300766Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2300916Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2301188Z FAILED tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2301459Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2301582Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2301786Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2302002Z FAILED tests/test_lazy/test_ops.py::test_lazy_ops - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2302266Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2302399Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2302547Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2302848Z FAILED tests/test_lora/test_lora.py::test_torch_ddp_lora - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2302853Z 
2025-04-11T03:52:13.2302971Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2303138Z Traceback (most recent call last):
2025-04-11T03:52:13.2303440Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2303523Z     fn(i, *args)
2025-04-11T03:52:13.2303738Z   File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 103, in run_dist
2025-04-11T03:52:13.2303822Z     run_lora_test()
2025-04-11T03:52:13.2304036Z   File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 98, in run_lora_test
2025-04-11T03:52:13.2304273Z     check_fwd_bwd(model_fn, data_gen_fn, output_transform_fn, loss_fn, task_type)
2025-04-11T03:52:13.2304492Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.2304591Z     get_accelerator().synchronize()
2025-04-11T03:52:13.2304846Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.2304948Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.2305223Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.2305328Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.2305428Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2305703Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2305831Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2305986Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2306294Z FAILED tests/test_moe/test_kernel.py::test_moe_kernel[data_type0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2306565Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2306696Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2306847Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2307096Z FAILED tests/test_moe/test_kernel.py::test_moe_kernel[data_type1] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2307368Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2307492Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2307647Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2307975Z FAILED tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2307980Z 
2025-04-11T03:52:13.2308102Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2308196Z Traceback (most recent call last):
2025-04-11T03:52:13.2308527Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2308607Z     fn(i, *args)
2025-04-11T03:52:13.2308836Z   File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 165, in run_dist
2025-04-11T03:52:13.2308927Z     check_moe_checkpoint()
2025-04-11T03:52:13.2309246Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2309341Z     partial_func(**kwargs)
2025-04-11T03:52:13.2309594Z   File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 101, in check_moe_checkpoint
2025-04-11T03:52:13.2309736Z     dist.broadcast_object_list(broadcast_objects, src=0)
2025-04-11T03:52:13.2310028Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:13.2310122Z     return func(*args, **kwargs)
2025-04-11T03:52:13.2310477Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in broadcast_object_list
2025-04-11T03:52:13.2310758Z     tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T03:52:13.2311090Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in <listcomp>
2025-04-11T03:52:13.2311296Z     tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T03:52:13.2311641Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2115, in _object_to_tensor
2025-04-11T03:52:13.2311841Z     byte_tensor = torch.ByteTensor(byte_storage).to(device)
2025-04-11T03:52:13.2311949Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2312226Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2312358Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2312512Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2312858Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2313135Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2313257Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2313412Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2313813Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2314089Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2314210Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2314358Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2314695Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2314965Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2315091Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2315237Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2315573Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2315844Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2315968Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2316114Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2316441Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2316713Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2316895Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2317049Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2317370Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2317640Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2317762Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2317913Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2318236Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2318617Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2318752Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2318904Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2319236Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2319561Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2319687Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2319833Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2320160Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2320429Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2320551Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2320705Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2321025Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2321362Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2321491Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2321646Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2321978Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2322261Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2322388Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2322540Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2322868Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2323145Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2323278Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2323427Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2323763Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2324040Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2324173Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2324384Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2324710Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2324987Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2325111Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2325263Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2325592Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2325926Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2326047Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2326198Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2326523Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2326849Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2326976Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2327123Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2327452Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2327718Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2327845Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2327994Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2328318Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2328642Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2328767Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2328918Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2329244Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2329520Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2329643Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2329798Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2330128Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2330406Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2330532Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2330680Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2331072Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2331341Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2331468Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2331673Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2332052Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2332322Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2332450Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2332600Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2332981Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2333313Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2333434Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2333588Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2333964Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2334299Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2334425Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2334581Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2334958Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2335237Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2335365Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2335515Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2335893Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2336225Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2336359Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2336509Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2336883Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2337153Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2337276Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2337430Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2337804Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2338078Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2338202Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2338353Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2338726Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2339000Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2339122Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2339325Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2339703Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2339974Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2340102Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2340250Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2340632Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2341019Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2341144Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2341294Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2341664Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2341995Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2342120Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2342271Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2342641Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2342917Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2343042Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2343193Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2343559Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2343955Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2344082Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2344228Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2344601Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2344870Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2344998Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2345147Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2345520Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2345790Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2345912Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2346063Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2346442Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2346714Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2346837Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2347045Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2347412Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2347686Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2347810Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2347957Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2348338Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2348714Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2348842Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2348990Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2349369Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2349714Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2349840Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2349987Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2350355Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2350626Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2350745Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2350898Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2351258Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2351589Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2351714Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2351866Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2352235Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2352563Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2352692Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2352842Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2353216Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2353487Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2353612Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2353761Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2354095Z FAILED tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2354102Z 
2025-04-11T03:52:13.2354222Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2354321Z Traceback (most recent call last):
2025-04-11T03:52:13.2354673Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2354757Z     fn(i, *args)
2025-04-11T03:52:13.2355006Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 459, in run_dist
2025-04-11T03:52:13.2355102Z     exam_dist_adafactor_base()
2025-04-11T03:52:13.2355361Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2355450Z     partial_func(**kwargs)
2025-04-11T03:52:13.2355699Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2355792Z     partial_func(**kwargs)
2025-04-11T03:52:13.2356075Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 111, in exam_dist_adafactor_base
2025-04-11T03:52:13.2356319Z     model_col = nn.Linear(H, W).to(local_rank)  # Col parallel weight
2025-04-11T03:52:13.2356585Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:13.2356683Z     return self._apply(convert)
2025-04-11T03:52:13.2356953Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2357103Z     param_applied = fn(param)
2025-04-11T03:52:13.2357375Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:13.2357588Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:13.2357694Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2357969Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2358102Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2358258Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2358568Z FAILED tests/test_optimizer/test_dist_came.py::test_dist_came - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2358573Z 
2025-04-11T03:52:13.2358693Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2358790Z Traceback (most recent call last):
2025-04-11T03:52:13.2359127Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2359209Z     fn(i, *args)
2025-04-11T03:52:13.2359445Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 349, in run_dist
2025-04-11T03:52:13.2359588Z     exam_bert_test_on_lowlevelzero_plugin()  # err in TODO layer
2025-04-11T03:52:13.2359843Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2359932Z     partial_func(**kwargs)
2025-04-11T03:52:13.2360227Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 206, in exam_bert_test_on_lowlevelzero_plugin
2025-04-11T03:52:13.2360457Z     ) = build_model_from_low_level_zero_plugin(model_fn, loss_fn, test_config, CAME, DistributedCAME)
2025-04-11T03:52:13.2360769Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 188, in build_model_from_low_level_zero_plugin
2025-04-11T03:52:13.2360869Z     org_model = org_model.cuda()
2025-04-11T03:52:13.2361150Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2361252Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2361513Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2361634Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2361900Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2361986Z     module._apply(fn)
2025-04-11T03:52:13.2362309Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2362395Z     module._apply(fn)
2025-04-11T03:52:13.2362660Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2362756Z     param_applied = fn(param)
2025-04-11T03:52:13.2363027Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2363139Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2363241Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2363522Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2363714Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2363872Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2364191Z FAILED tests/test_optimizer/test_dist_galore.py::test_dist_galore - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2364196Z 
2025-04-11T03:52:13.2364317Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2364466Z Traceback (most recent call last):
2025-04-11T03:52:13.2364749Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2364827Z     fn(i, *args)
2025-04-11T03:52:13.2365078Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_galore.py", line 291, in check_dist_galore
2025-04-11T03:52:13.2365165Z     dist.barrier()
2025-04-11T03:52:13.2365449Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:13.2365545Z     return func(*args, **kwargs)
2025-04-11T03:52:13.2365851Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T03:52:13.2365955Z     work = default_pg.barrier(opts=opts)
2025-04-11T03:52:13.2366054Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2366331Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2366465Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2366675Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2366980Z FAILED tests/test_optimizer/test_dist_lamb.py::test_dist_lamb - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2366985Z 
2025-04-11T03:52:13.2367101Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2367201Z Traceback (most recent call last):
2025-04-11T03:52:13.2367480Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2367557Z     fn(i, *args)
2025-04-11T03:52:13.2367809Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_lamb.py", line 263, in check_dist_lamb
2025-04-11T03:52:13.2367894Z     run_dist_lamb_basic()
2025-04-11T03:52:13.2368154Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2368242Z     partial_func(**kwargs)
2025-04-11T03:52:13.2368499Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2368585Z     partial_func(**kwargs)
2025-04-11T03:52:13.2368827Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2368917Z     partial_func(**kwargs)
2025-04-11T03:52:13.2369133Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.2369234Z     get_accelerator().synchronize()
2025-04-11T03:52:13.2369487Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.2369638Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.2369911Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.2370009Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.2370115Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2370389Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2370523Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2370677Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2371013Z FAILED tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2371082Z 
2025-04-11T03:52:13.2371197Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2371292Z Traceback (most recent call last):
2025-04-11T03:52:13.2371573Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2371654Z     fn(i, *args)
2025-04-11T03:52:13.2371899Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 73, in run_dist
2025-04-11T03:52:13.2372048Z     check_p2p_communication()
2025-04-11T03:52:13.2372340Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 21, in check_p2p_communication
2025-04-11T03:52:13.2372506Z     tensor = torch.ones(1, device=get_accelerator().get_current_device())
2025-04-11T03:52:13.2372609Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2372881Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2373010Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2373166Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2373512Z FAILED tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2373517Z 
2025-04-11T03:52:13.2373635Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2373730Z Traceback (most recent call last):
2025-04-11T03:52:13.2374066Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2374145Z     fn(i, *args)
2025-04-11T03:52:13.2374375Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 68, in run_dist
2025-04-11T03:52:13.2374467Z     check_stage_manager()
2025-04-11T03:52:13.2374718Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 56, in check_stage_manager
2025-04-11T03:52:13.2374816Z     dist.barrier(group=group)
2025-04-11T03:52:13.2375107Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:13.2375202Z     return func(*args, **kwargs)
2025-04-11T03:52:13.2375510Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3441, in barrier
2025-04-11T03:52:13.2375610Z     work = group.barrier(opts=opts)
2025-04-11T03:52:13.2375714Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2375988Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2376119Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2376272Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2376616Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2376622Z 
2025-04-11T03:52:13.2376737Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2376833Z Traceback (most recent call last):
2025-04-11T03:52:13.2377162Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2377241Z     fn(i, *args)
2025-04-11T03:52:13.2377504Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:13.2377601Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2377864Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2377976Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2378240Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2378405Z     module._apply(fn)
2025-04-11T03:52:13.2378667Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2378754Z     module._apply(fn)
2025-04-11T03:52:13.2379017Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2379110Z     param_applied = fn(param)
2025-04-11T03:52:13.2379381Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2379551Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2379652Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2379926Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2380059Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2380211Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2380561Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2380566Z 
2025-04-11T03:52:13.2380677Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2380774Z Traceback (most recent call last):
2025-04-11T03:52:13.2381046Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2381124Z     fn(i, *args)
2025-04-11T03:52:13.2381381Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:13.2381527Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2381791Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2381898Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2382159Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2382243Z     module._apply(fn)
2025-04-11T03:52:13.2382501Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2382585Z     module._apply(fn)
2025-04-11T03:52:13.2382842Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2382937Z     param_applied = fn(param)
2025-04-11T03:52:13.2383202Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2383315Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2383412Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2383689Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2383816Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2383975Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2384322Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2384326Z 
2025-04-11T03:52:13.2384493Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2384594Z Traceback (most recent call last):
2025-04-11T03:52:13.2384869Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2384953Z     fn(i, *args)
2025-04-11T03:52:13.2385211Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:13.2385302Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2385569Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2385673Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2386003Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2386083Z     module._apply(fn)
2025-04-11T03:52:13.2386348Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2386428Z     module._apply(fn)
2025-04-11T03:52:13.2386685Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2386838Z     param_applied = fn(param)
2025-04-11T03:52:13.2387106Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2387216Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2387314Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2387595Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2387727Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2387882Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2388229Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2388233Z 
2025-04-11T03:52:13.2388347Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2388494Z Traceback (most recent call last):
2025-04-11T03:52:13.2388775Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2388921Z     fn(i, *args)
2025-04-11T03:52:13.2389177Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:13.2389270Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2389524Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2389629Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2389892Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2389972Z     module._apply(fn)
2025-04-11T03:52:13.2390236Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2390318Z     module._apply(fn)
2025-04-11T03:52:13.2390580Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2390672Z     param_applied = fn(param)
2025-04-11T03:52:13.2390937Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2391047Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2391144Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2391417Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2391549Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2391705Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2392096Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2392101Z 
2025-04-11T03:52:13.2392221Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2392320Z Traceback (most recent call last):
2025-04-11T03:52:13.2392598Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2392680Z     fn(i, *args)
2025-04-11T03:52:13.2392941Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:13.2393048Z     examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:13.2393378Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:13.2393474Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2393738Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2393844Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2394107Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2394254Z     module._apply(fn)
2025-04-11T03:52:13.2394518Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2394599Z     module._apply(fn)
2025-04-11T03:52:13.2394859Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2394949Z     param_applied = fn(param)
2025-04-11T03:52:13.2395210Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2395321Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2395419Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2395695Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2395821Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2395977Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2396354Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2396360Z 
2025-04-11T03:52:13.2396477Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2396578Z Traceback (most recent call last):
2025-04-11T03:52:13.2396851Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2396935Z     fn(i, *args)
2025-04-11T03:52:13.2397187Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:13.2397294Z     examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:13.2397554Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:13.2397648Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2397913Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2398021Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2398288Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2398373Z     module._apply(fn)
2025-04-11T03:52:13.2398637Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2398723Z     module._apply(fn)
2025-04-11T03:52:13.2398980Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2399075Z     param_applied = fn(param)
2025-04-11T03:52:13.2399392Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2399503Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2399602Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2399882Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2400009Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2400167Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2400499Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2400566Z 
2025-04-11T03:52:13.2400682Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2400781Z Traceback (most recent call last):
2025-04-11T03:52:13.2401058Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2401138Z     fn(i, *args)
2025-04-11T03:52:13.2401393Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:13.2401561Z     examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:13.2401821Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:13.2401915Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2402177Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2402285Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2402553Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2402638Z     module._apply(fn)
2025-04-11T03:52:13.2402905Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2402989Z     module._apply(fn)
2025-04-11T03:52:13.2403252Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2403351Z     param_applied = fn(param)
2025-04-11T03:52:13.2403673Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2403785Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2403883Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2404161Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2404290Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2404449Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2404783Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2404787Z 
2025-04-11T03:52:13.2404901Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2405004Z Traceback (most recent call last):
2025-04-11T03:52:13.2405282Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2405366Z     fn(i, *args)
2025-04-11T03:52:13.2405620Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:13.2405725Z     examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:13.2405981Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:13.2406074Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2406336Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2406438Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2406761Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2406845Z     module._apply(fn)
2025-04-11T03:52:13.2407112Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2407195Z     module._apply(fn)
2025-04-11T03:52:13.2407461Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2407556Z     param_applied = fn(param)
2025-04-11T03:52:13.2407820Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2408052Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2408150Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2408432Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2408564Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2408719Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2409059Z FAILED tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2409118Z 
2025-04-11T03:52:13.2409236Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2409336Z Traceback (most recent call last):
2025-04-11T03:52:13.2409614Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2409693Z     fn(i, *args)
2025-04-11T03:52:13.2409966Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 1070, in run_dist
2025-04-11T03:52:13.2410070Z     run_with_booster_moehybridplugin()
2025-04-11T03:52:13.2410324Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2410412Z     partial_func(**kwargs)
2025-04-11T03:52:13.2410757Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 788, in run_with_booster_moehybridplugin
2025-04-11T03:52:13.2410885Z     torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T03:52:13.2411225Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2411324Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2411585Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2411693Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2411956Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2412044Z     module._apply(fn)
2025-04-11T03:52:13.2412308Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2412400Z     param_applied = fn(param)
2025-04-11T03:52:13.2412666Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2412776Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2412873Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2413145Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2413277Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2413432Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2413726Z FAILED tests/test_shardformer/test_flash_attention.py::test_flash_attn_func - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2413999Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2414188Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2414340Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2414597Z FAILED tests/test_shardformer/test_shard_utils.py::test_release_layer - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2414871Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2414996Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2415149Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2415394Z FAILED tests/test_shardformer/test_with_torch_ddp.py::test_gpt2 - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2415725Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2415848Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2416001Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2416361Z FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2416688Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2416814Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2416962Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2417329Z FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2417596Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2417722Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2417871Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2418230Z FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2418549Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2418673Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2418824Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2419202Z FAILED tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2419209Z 
2025-04-11T03:52:13.2419332Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2419428Z Traceback (most recent call last):
2025-04-11T03:52:13.2419712Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2419790Z     fn(i, *args)
2025-04-11T03:52:13.2420116Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dist_crossentropy.py", line 20, in check_dist_crossentropy
2025-04-11T03:52:13.2420252Z     pred = torch.randn(2, 4, 8, requires_grad=True).cuda()
2025-04-11T03:52:13.2420350Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2420625Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2420749Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2420904Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2421229Z FAILED tests/test_shardformer/test_layer/test_dropout.py::test_dropout - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2421233Z 
2025-04-11T03:52:13.2421352Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2421504Z Traceback (most recent call last):
2025-04-11T03:52:13.2421786Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2421869Z     fn(i, *args)
2025-04-11T03:52:13.2422124Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 60, in run_dist
2025-04-11T03:52:13.2422224Z     check_dropout_parallel_input()
2025-04-11T03:52:13.2422529Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 12, in check_dropout_parallel_input
2025-04-11T03:52:13.2422753Z     dropout_1d = DropoutForParallelInput.from_native_module(dropout, process_group=None)
2025-04-11T03:52:13.2423087Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 42, in from_native_module
2025-04-11T03:52:13.2423307Z     return DropoutForParallelInput(p=p, inplace=inplace, process_group=process_group)
2025-04-11T03:52:13.2423551Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 31, in __init__
2025-04-11T03:52:13.2423753Z     self.randomizer = create_randomizer_with_offset(seed, process_group=process_group)
2025-04-11T03:52:13.2424114Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 318, in create_randomizer_with_offset
2025-04-11T03:52:13.2424303Z     is_synchronized = Randomizer.is_randomizer_index_synchronized(process_group)
2025-04-11T03:52:13.2424609Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 258, in is_randomizer_index_synchronized
2025-04-11T03:52:13.2424844Z     index_tensor = torch.tensor(index, dtype=torch.int32, device=get_accelerator().get_current_device())
2025-04-11T03:52:13.2424949Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2425224Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2425356Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2425513Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2425867Z FAILED tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2425873Z 
2025-04-11T03:52:13.2425997Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2426142Z Traceback (most recent call last):
2025-04-11T03:52:13.2426432Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2426508Z     fn(i, *args)
2025-04-11T03:52:13.2426767Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 47, in run_dist
2025-04-11T03:52:13.2426857Z     check_embedding_1d()
2025-04-11T03:52:13.2427106Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2427199Z     partial_func(**kwargs)
2025-04-11T03:52:13.2427479Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 18, in check_embedding_1d
2025-04-11T03:52:13.2427590Z     embedding = nn.Embedding(32, 128).cuda()
2025-04-11T03:52:13.2427854Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2427965Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2428229Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2428322Z     param_applied = fn(param)
2025-04-11T03:52:13.2428650Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2428760Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2428867Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2429222Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2429361Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2429518Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2429896Z FAILED tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2429905Z 
2025-04-11T03:52:13.2430020Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2430113Z Traceback (most recent call last):
2025-04-11T03:52:13.2430395Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2430541Z     fn(i, *args)
2025-04-11T03:52:13.2430848Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 204, in run_dist
2025-04-11T03:52:13.2430943Z     check_gpt2_qkv_fused_linear_1d()
2025-04-11T03:52:13.2431201Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2431288Z     partial_func(**kwargs)
2025-04-11T03:52:13.2431539Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2431692Z     partial_func(**kwargs)
2025-04-11T03:52:13.2432050Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 194, in check_gpt2_qkv_fused_linear_1d
2025-04-11T03:52:13.2432186Z     check_linear_conv_1d_col(lazy_init, seq_parallel_mode)
2025-04-11T03:52:13.2432518Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 47, in check_linear_conv_1d_col
2025-04-11T03:52:13.2432617Z     linear = Conv1D(192, 48).cuda()
2025-04-11T03:52:13.2432877Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2432985Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2433258Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2433348Z     param_applied = fn(param)
2025-04-11T03:52:13.2433628Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2433806Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2433914Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2434190Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2434319Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2434480Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2434822Z FAILED tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2434826Z 
2025-04-11T03:52:13.2434947Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2435041Z Traceback (most recent call last):
2025-04-11T03:52:13.2435322Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2435402Z     fn(i, *args)
2025-04-11T03:52:13.2435658Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 45, in run_dist
2025-04-11T03:52:13.2435740Z     check_layernorm()
2025-04-11T03:52:13.2435988Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2436078Z     partial_func(**kwargs)
2025-04-11T03:52:13.2436353Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 17, in check_layernorm
2025-04-11T03:52:13.2436459Z     norm = nn.LayerNorm(128, 0.00001).cuda()
2025-04-11T03:52:13.2436776Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2436889Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2437155Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2437248Z     param_applied = fn(param)
2025-04-11T03:52:13.2437524Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2437630Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2437731Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2438004Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2438199Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2438357Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2438691Z FAILED tests/test_shardformer/test_layer/test_linear_1d.py::test_linear - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2438699Z 
2025-04-11T03:52:13.2438813Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2438908Z Traceback (most recent call last):
2025-04-11T03:52:13.2439250Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2439329Z     fn(i, *args)
2025-04-11T03:52:13.2439616Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 279, in check_dist_linear
2025-04-11T03:52:13.2439700Z     run_dist_linear_test()
2025-04-11T03:52:13.2439951Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2440037Z     partial_func(**kwargs)
2025-04-11T03:52:13.2440281Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2440373Z     partial_func(**kwargs)
2025-04-11T03:52:13.2440613Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2440702Z     partial_func(**kwargs)
2025-04-11T03:52:13.2440992Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 270, in run_dist_linear_test
2025-04-11T03:52:13.2441203Z     check_linear_1d_col(lazy_init, seq_parallel_mode, overlap)
2025-04-11T03:52:13.2441491Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T03:52:13.2441585Z     linear = nn.Linear(32, 128).cuda()
2025-04-11T03:52:13.2441859Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2441969Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2442240Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2442331Z     param_applied = fn(param)
2025-04-11T03:52:13.2442604Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2442710Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2442811Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2443095Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2443222Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2443385Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2443752Z FAILED tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2443759Z 
2025-04-11T03:52:13.2443885Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2443979Z Traceback (most recent call last):
2025-04-11T03:52:13.2444313Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2444392Z     fn(i, *args)
2025-04-11T03:52:13.2444675Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 158, in run_dist
2025-04-11T03:52:13.2444770Z     check_linear_1d_col()
2025-04-11T03:52:13.2445020Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2445111Z     partial_func(**kwargs)
2025-04-11T03:52:13.2445420Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T03:52:13.2445578Z     linear = nn.Linear(8, 80).cuda()
2025-04-11T03:52:13.2445837Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2445943Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2446206Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2446296Z     param_applied = fn(param)
2025-04-11T03:52:13.2446564Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2446725Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2446829Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2447106Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2447234Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2447396Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2447728Z FAILED tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2447732Z 
2025-04-11T03:52:13.2447854Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2447948Z Traceback (most recent call last):
2025-04-11T03:52:13.2448224Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2448304Z     fn(i, *args)
2025-04-11T03:52:13.2448643Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 169, in launch_single_ring
2025-04-11T03:52:13.2448736Z     check_packed_seq()
2025-04-11T03:52:13.2448987Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2449078Z     partial_func(**kwargs)
2025-04-11T03:52:13.2449325Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2449415Z     partial_func(**kwargs)
2025-04-11T03:52:13.2449658Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2449741Z     partial_func(**kwargs)
2025-04-11T03:52:13.2449846Z   [Previous line repeated 2 more times]
2025-04-11T03:52:13.2450124Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 94, in check_packed_seq
2025-04-11T03:52:13.2450300Z     padding_mask = torch.ones((bs, seqlen), dtype=torch.int, device=device)
2025-04-11T03:52:13.2450398Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2450674Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2450801Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2450953Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2451298Z FAILED tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2451302Z 
2025-04-11T03:52:13.2451417Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2451570Z Traceback (most recent call last):
2025-04-11T03:52:13.2451849Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2451934Z     fn(i, *args)
2025-04-11T03:52:13.2452219Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 175, in launch_double_ring
2025-04-11T03:52:13.2452320Z     check_ring_attn(inner_ring_size=2)
2025-04-11T03:52:13.2452568Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2452653Z     partial_func(**kwargs)
2025-04-11T03:52:13.2452906Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2453083Z     partial_func(**kwargs)
2025-04-11T03:52:13.2453331Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2453414Z     partial_func(**kwargs)
2025-04-11T03:52:13.2453520Z   [Previous line repeated 2 more times]
2025-04-11T03:52:13.2453793Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 36, in check_ring_attn
2025-04-11T03:52:13.2454057Z     qkv = torch.randn(bs, seq_len, 3, nheads, d, device=device, dtype=dtype, requires_grad=True)
2025-04-11T03:52:13.2454160Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2454437Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2454573Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2454730Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2455125Z FAILED tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2455130Z 
2025-04-11T03:52:13.2455243Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2455344Z Traceback (most recent call last):
2025-04-11T03:52:13.2455635Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2455714Z     fn(i, *args)
2025-04-11T03:52:13.2456085Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 169, in check_all2all_attn
2025-04-11T03:52:13.2456177Z     run_seq_parallel_attn()
2025-04-11T03:52:13.2456424Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2456507Z     partial_func(**kwargs)
2025-04-11T03:52:13.2456750Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2456835Z     partial_func(**kwargs)
2025-04-11T03:52:13.2457075Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2457163Z     partial_func(**kwargs)
2025-04-11T03:52:13.2457262Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.2457585Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 164, in run_seq_parallel_attn
2025-04-11T03:52:13.2457735Z     seq_parallel_attn(seq_len, hidden_dim, head_num, batch_size)
2025-04-11T03:52:13.2458040Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 101, in seq_parallel_attn
2025-04-11T03:52:13.2458174Z     x = torch.randn(batch_size, seq_len, hidden_dim).cuda()
2025-04-11T03:52:13.2458271Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2458542Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2458672Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2458829Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2459270Z FAILED tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2459275Z 
2025-04-11T03:52:13.2459397Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2459491Z Traceback (most recent call last):
2025-04-11T03:52:13.2459773Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2459856Z     fn(i, *args)
2025-04-11T03:52:13.2460160Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 49, in run_dist
2025-04-11T03:52:13.2460263Z     check_vocab_embedding_1d()
2025-04-11T03:52:13.2460570Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2460664Z     partial_func(**kwargs)
2025-04-11T03:52:13.2461009Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 18, in check_vocab_embedding_1d
2025-04-11T03:52:13.2461123Z     embedding = nn.Embedding(128, 32).to("cuda")
2025-04-11T03:52:13.2461381Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:13.2461612Z     return self._apply(convert)
2025-04-11T03:52:13.2461884Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2461977Z     param_applied = fn(param)
2025-04-11T03:52:13.2462247Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:13.2462459Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:13.2462560Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2462841Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2462972Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2463132Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2463397Z FAILED tests/test_shardformer/test_model/test_shard_bert.py::test_bert - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2463730Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2463859Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2464019Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2464285Z FAILED tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2 - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2464559Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2464685Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2464837Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2465107Z FAILED tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2465379Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2465510Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2465660Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2465943Z FAILED tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2466214Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2466340Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2466545Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2466824Z FAILED tests/test_shardformer/test_model/test_shard_command.py::test_command - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2467096Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2467221Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2467375Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2467725Z FAILED tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2467730Z 
2025-04-11T03:52:13.2467850Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2467999Z Traceback (most recent call last):
2025-04-11T03:52:13.2468278Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2468360Z     fn(i, *args)
2025-04-11T03:52:13.2468683Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 216, in check_deepseek
2025-04-11T03:52:13.2468775Z     run_deepseek_test()
2025-04-11T03:52:13.2469092Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2469180Z     partial_func(**kwargs)
2025-04-11T03:52:13.2469479Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 187, in run_deepseek_test
2025-04-11T03:52:13.2469569Z     run_deepseek_commom(config)
2025-04-11T03:52:13.2469870Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 77, in run_deepseek_commom
2025-04-11T03:52:13.2470070Z     torch_model = AutoModel.from_config(config, trust_remote_code=True).cuda().to(dtype)
2025-04-11T03:52:13.2470352Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2470451Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2470714Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2470826Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2471156Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2471244Z     module._apply(fn)
2025-04-11T03:52:13.2471505Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2471602Z     param_applied = fn(param)
2025-04-11T03:52:13.2471865Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2471977Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2472075Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2472357Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2472486Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2472639Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2473017Z FAILED tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2473021Z 
2025-04-11T03:52:13.2473138Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2473234Z Traceback (most recent call last):
2025-04-11T03:52:13.2473516Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2473599Z     fn(i, *args)
2025-04-11T03:52:13.2473911Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 93, in check_deepseek_v3
2025-04-11T03:52:13.2473998Z     run_deepseek_v3_test()
2025-04-11T03:52:13.2474310Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2474398Z     partial_func(**kwargs)
2025-04-11T03:52:13.2474715Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 82, in run_deepseek_v3_test
2025-04-11T03:52:13.2474810Z     check_forward_backward(
2025-04-11T03:52:13.2475125Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 29, in check_forward_backward
2025-04-11T03:52:13.2475393Z     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
2025-04-11T03:52:13.2475695Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 138, in build_model_from_hybrid_plugin
2025-04-11T03:52:13.2475864Z     org_model = org_model.cuda()
2025-04-11T03:52:13.2476149Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2476254Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2476515Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2476683Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2476951Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2477043Z     module._apply(fn)
2025-04-11T03:52:13.2477305Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2477389Z     module._apply(fn)
2025-04-11T03:52:13.2477654Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2477751Z     param_applied = fn(param)
2025-04-11T03:52:13.2478020Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2478128Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2478229Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2478503Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2478686Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2478848Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2479125Z FAILED tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2479398Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2479525Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2479684Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2479945Z FAILED tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2 - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2480216Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2480344Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2480493Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2480761Z FAILED tests/test_shardformer/test_model/test_shard_llama.py::test_llama - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2481025Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2481154Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2481302Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2481583Z FAILED tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2481896Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2482023Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2482176Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2482517Z FAILED tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2482521Z 
2025-04-11T03:52:13.2482639Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2482733Z Traceback (most recent call last):
2025-04-11T03:52:13.2483010Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2483144Z     fn(i, *args)
2025-04-11T03:52:13.2483435Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 210, in check_mixtral
2025-04-11T03:52:13.2483520Z     run_mixtral_test()
2025-04-11T03:52:13.2483772Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2483862Z     partial_func(**kwargs)
2025-04-11T03:52:13.2484207Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 180, in run_mixtral_test
2025-04-11T03:52:13.2484305Z     run_mixtral_commom(config)
2025-04-11T03:52:13.2484593Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 70, in run_mixtral_commom
2025-04-11T03:52:13.2484722Z     torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T03:52:13.2484999Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2485097Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2485357Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2485467Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2485729Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2485812Z     module._apply(fn)
2025-04-11T03:52:13.2486123Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2486216Z     param_applied = fn(param)
2025-04-11T03:52:13.2486480Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2486589Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2486687Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2486966Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2487094Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2487254Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2487524Z FAILED tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2487798Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2487925Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2488075Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2488343Z FAILED tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2 - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2488608Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2488736Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2488884Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2489189Z FAILED tests/test_shardformer/test_model/test_shard_sam.py::test_sam - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2489458Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2489584Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2489740Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2489989Z FAILED tests/test_shardformer/test_model/test_shard_t5.py::test_t5 - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2490258Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2490456Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2490609Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2490871Z FAILED tests/test_shardformer/test_model/test_shard_vit.py::test_vit - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2491144Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2491323Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2491473Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2491761Z FAILED tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2492027Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2492154Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2492304Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2492611Z FAILED tests/test_tensor/test_comm_spec_apply.py::test_comm_spec - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2492616Z 
2025-04-11T03:52:13.2492732Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2492832Z Traceback (most recent call last):
2025-04-11T03:52:13.2493110Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2493190Z     fn(i, *args)
2025-04-11T03:52:13.2493487Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 191, in check_comm
2025-04-11T03:52:13.2493588Z     check_all_gather(device_mesh, rank)
2025-04-11T03:52:13.2493843Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 16, in check_all_gather
2025-04-11T03:52:13.2493963Z     sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T03:52:13.2494066Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2494337Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2494463Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2494619Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2494936Z FAILED tests/test_tensor/test_padded_tensor.py::test_padded_tensor - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2494942Z 
2025-04-11T03:52:13.2495063Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2495158Z Traceback (most recent call last):
2025-04-11T03:52:13.2495441Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2495519Z     fn(i, *args)
2025-04-11T03:52:13.2495768Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_padded_tensor.py", line 14, in check_padded_tensor
2025-04-11T03:52:13.2495889Z     original_tensor = torch.rand(32, 64).to("cuda")
2025-04-11T03:52:13.2495985Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2496316Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2496444Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2496600Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2496926Z FAILED tests/test_tensor/test_shape_consistency_apply.py::test_apply - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2496930Z 
2025-04-11T03:52:13.2497053Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2497147Z Traceback (most recent call last):
2025-04-11T03:52:13.2497426Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2497631Z     fn(i, *args)
2025-04-11T03:52:13.2497894Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_shape_consistency_apply.py", line 41, in check_apply
2025-04-11T03:52:13.2498071Z     tensor_to_comm = torch.cat((sharded_tensor_0, sharded_tensor_1), 1).cuda()
2025-04-11T03:52:13.2498171Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2498451Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2498628Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2498779Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2499104Z FAILED tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2499109Z 
2025-04-11T03:52:13.2499222Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2499320Z Traceback (most recent call last):
2025-04-11T03:52:13.2499597Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2499677Z     fn(i, *args)
2025-04-11T03:52:13.2499933Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 140, in check_comm
2025-04-11T03:52:13.2500045Z     check_all_gather(process_group_dict, rank)
2025-04-11T03:52:13.2500311Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 15, in check_all_gather
2025-04-11T03:52:13.2500429Z     sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T03:52:13.2500533Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2500874Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2501008Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2501160Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2501489Z FAILED tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2501494Z 
2025-04-11T03:52:13.2501607Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2501700Z Traceback (most recent call last):
2025-04-11T03:52:13.2501985Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2502062Z     fn(i, *args)
2025-04-11T03:52:13.2502320Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py", line 25, in check_dtensor
2025-04-11T03:52:13.2502422Z     test_model = TestModel(8, 8).to("cuda")
2025-04-11T03:52:13.2502684Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:13.2502778Z     return self._apply(convert)
2025-04-11T03:52:13.2503038Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2503130Z     module._apply(fn)
2025-04-11T03:52:13.2503390Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2503487Z     param_applied = fn(param)
2025-04-11T03:52:13.2503809Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:13.2504025Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:13.2504124Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2504398Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2504529Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2504684Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2505055Z FAILED tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2505125Z 
2025-04-11T03:52:13.2505240Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2505339Z Traceback (most recent call last):
2025-04-11T03:52:13.2505617Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2505700Z     fn(i, *args)
2025-04-11T03:52:13.2506029Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_layout_converter.py", line 162, in check_layout_converting_apply
2025-04-11T03:52:13.2506211Z     original_tensor = torch.rand(global_shape).cuda()
2025-04-11T03:52:13.2506311Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2506584Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2506715Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2506866Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2507205Z FAILED tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2507209Z 
2025-04-11T03:52:13.2507324Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2507418Z Traceback (most recent call last):
2025-04-11T03:52:13.2507697Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2507777Z     fn(i, *args)
2025-04-11T03:52:13.2508076Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 53, in run_dist
2025-04-11T03:52:13.2508166Z     exam_chunk_memory()
2025-04-11T03:52:13.2508468Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2508556Z     partial_func(**kwargs)
2025-04-11T03:52:13.2508805Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2508901Z     partial_func(**kwargs)
2025-04-11T03:52:13.2509168Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 21, in exam_chunk_memory
2025-04-11T03:52:13.2509278Z     chunk_manager = ChunkManager(config)
2025-04-11T03:52:13.2509517Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T03:52:13.2509764Z     self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:13.2509863Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2510136Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2510263Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2510419Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2510753Z FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2510758Z 
2025-04-11T03:52:13.2510871Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2510966Z Traceback (most recent call last):
2025-04-11T03:52:13.2511317Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2511404Z     fn(i, *args)
2025-04-11T03:52:13.2511642Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T03:52:13.2511727Z     exam_chunk_basic()
2025-04-11T03:52:13.2511980Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2512066Z     partial_func(**kwargs)
2025-04-11T03:52:13.2512317Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2512464Z     partial_func(**kwargs)
2025-04-11T03:52:13.2512715Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2512799Z     partial_func(**kwargs)
2025-04-11T03:52:13.2512899Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.2513162Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T03:52:13.2513245Z     my_chunk = Chunk(
2025-04-11T03:52:13.2513482Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T03:52:13.2513743Z     self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T03:52:13.2513849Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2514122Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2514252Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2514415Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2514745Z FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2514750Z 
2025-04-11T03:52:13.2514873Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2514966Z Traceback (most recent call last):
2025-04-11T03:52:13.2515248Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2515329Z     fn(i, *args)
2025-04-11T03:52:13.2515631Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T03:52:13.2515717Z     exam_chunk_basic()
2025-04-11T03:52:13.2515962Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2516052Z     partial_func(**kwargs)
2025-04-11T03:52:13.2516293Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2516383Z     partial_func(**kwargs)
2025-04-11T03:52:13.2516618Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2516702Z     partial_func(**kwargs)
2025-04-11T03:52:13.2516804Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.2517054Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T03:52:13.2517145Z     my_chunk = Chunk(
2025-04-11T03:52:13.2517375Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T03:52:13.2517575Z     self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T03:52:13.2517673Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2517947Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2518076Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2518230Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2518628Z FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2518633Z 
2025-04-11T03:52:13.2518750Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2518851Z Traceback (most recent call last):
2025-04-11T03:52:13.2519135Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2519217Z     fn(i, *args)
2025-04-11T03:52:13.2519451Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T03:52:13.2519534Z     exam_chunk_basic()
2025-04-11T03:52:13.2519778Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2519925Z     partial_func(**kwargs)
2025-04-11T03:52:13.2520166Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2520251Z     partial_func(**kwargs)
2025-04-11T03:52:13.2520496Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2520582Z     partial_func(**kwargs)
2025-04-11T03:52:13.2520679Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.2520997Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T03:52:13.2521077Z     my_chunk = Chunk(
2025-04-11T03:52:13.2521311Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T03:52:13.2521507Z     self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T03:52:13.2521608Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2521887Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2522016Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2522180Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2522524Z FAILED tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2522529Z 
2025-04-11T03:52:13.2522647Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2522795Z Traceback (most recent call last):
2025-04-11T03:52:13.2523084Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2523162Z     fn(i, *args)
2025-04-11T03:52:13.2523404Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 152, in run_dist
2025-04-11T03:52:13.2523492Z     exam_gemini_grad_acc()
2025-04-11T03:52:13.2523731Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2523818Z     partial_func(**kwargs)
2025-04-11T03:52:13.2524057Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2524145Z     partial_func(**kwargs)
2025-04-11T03:52:13.2524383Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2524468Z     partial_func(**kwargs)
2025-04-11T03:52:13.2524570Z   [Previous line repeated 4 more times]
2025-04-11T03:52:13.2524843Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 71, in exam_gemini_grad_acc
2025-04-11T03:52:13.2524944Z     torch_model = model_builder().cuda()
2025-04-11T03:52:13.2525219Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2525322Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2525581Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2525695Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2526013Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2526102Z     module._apply(fn)
2025-04-11T03:52:13.2526378Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2526460Z     module._apply(fn)
2025-04-11T03:52:13.2526726Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2526819Z     param_applied = fn(param)
2025-04-11T03:52:13.2527093Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2527263Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2527360Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2527640Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2527771Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2527932Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2528249Z FAILED tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2528307Z 
2025-04-11T03:52:13.2528436Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2528530Z Traceback (most recent call last):
2025-04-11T03:52:13.2528802Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2528885Z     fn(i, *args)
2025-04-11T03:52:13.2529122Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T03:52:13.2529218Z     exam_grad_clipping()
2025-04-11T03:52:13.2529460Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2529549Z     partial_func(**kwargs)
2025-04-11T03:52:13.2529789Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2529871Z     partial_func(**kwargs)
2025-04-11T03:52:13.2530116Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2530254Z     partial_func(**kwargs)
2025-04-11T03:52:13.2530354Z   [Previous line repeated 2 more times]
2025-04-11T03:52:13.2530618Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T03:52:13.2530716Z     torch_model = model_builder().cuda()
2025-04-11T03:52:13.2530994Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2531089Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2531355Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2531461Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2531727Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2531809Z     module._apply(fn)
2025-04-11T03:52:13.2532074Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2532155Z     module._apply(fn)
2025-04-11T03:52:13.2532413Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2532511Z     param_applied = fn(param)
2025-04-11T03:52:13.2532777Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2532887Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2532986Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2533319Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2533449Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2533604Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2533924Z FAILED tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2533928Z 
2025-04-11T03:52:13.2534045Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2534145Z Traceback (most recent call last):
2025-04-11T03:52:13.2534419Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2534563Z     fn(i, *args)
2025-04-11T03:52:13.2534808Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T03:52:13.2534901Z     exam_grad_clipping()
2025-04-11T03:52:13.2535146Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2535229Z     partial_func(**kwargs)
2025-04-11T03:52:13.2535478Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2535622Z     partial_func(**kwargs)
2025-04-11T03:52:13.2535871Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2535956Z     partial_func(**kwargs)
2025-04-11T03:52:13.2536058Z   [Previous line repeated 2 more times]
2025-04-11T03:52:13.2536315Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T03:52:13.2536410Z     torch_model = model_builder().cuda()
2025-04-11T03:52:13.2536687Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2536781Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2537040Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2537145Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2537419Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2537552Z     module._apply(fn)
2025-04-11T03:52:13.2537816Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2537904Z     module._apply(fn)
2025-04-11T03:52:13.2538161Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2538257Z     param_applied = fn(param)
2025-04-11T03:52:13.2538518Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2538627Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2538728Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2539003Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2539141Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2539298Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2539622Z FAILED tests/test_zero/test_gemini/test_inference.py::test_inference[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2539626Z 
2025-04-11T03:52:13.2539739Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2539838Z Traceback (most recent call last):
2025-04-11T03:52:13.2540117Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2540195Z     fn(i, *args)
2025-04-11T03:52:13.2540441Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T03:52:13.2540575Z     exam_inference()
2025-04-11T03:52:13.2540825Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2540913Z     partial_func(**kwargs)
2025-04-11T03:52:13.2541162Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2541252Z     partial_func(**kwargs)
2025-04-11T03:52:13.2541493Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2541584Z     partial_func(**kwargs)
2025-04-11T03:52:13.2541839Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T03:52:13.2541995Z     torch_model = model_builder().cuda()
2025-04-11T03:52:13.2542271Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2542369Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2542630Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2542738Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2543063Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2543148Z     module._apply(fn)
2025-04-11T03:52:13.2543413Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2543496Z     module._apply(fn)
2025-04-11T03:52:13.2543764Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2543861Z     param_applied = fn(param)
2025-04-11T03:52:13.2544139Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2544246Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2544347Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2544633Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2544769Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2545003Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2545324Z FAILED tests/test_zero/test_gemini/test_inference.py::test_inference[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2545328Z 
2025-04-11T03:52:13.2545450Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2545546Z Traceback (most recent call last):
2025-04-11T03:52:13.2545823Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2545908Z     fn(i, *args)
2025-04-11T03:52:13.2546149Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T03:52:13.2546235Z     exam_inference()
2025-04-11T03:52:13.2546479Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2546572Z     partial_func(**kwargs)
2025-04-11T03:52:13.2546815Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2546899Z     partial_func(**kwargs)
2025-04-11T03:52:13.2547141Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2547224Z     partial_func(**kwargs)
2025-04-11T03:52:13.2547477Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T03:52:13.2547574Z     torch_model = model_builder().cuda()
2025-04-11T03:52:13.2547849Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2547998Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2548257Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2548367Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2548673Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2548759Z     module._apply(fn)
2025-04-11T03:52:13.2549014Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2549098Z     module._apply(fn)
2025-04-11T03:52:13.2549355Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2549515Z     param_applied = fn(param)
2025-04-11T03:52:13.2549783Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2549889Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2549994Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2550271Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2550467Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2550626Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2550933Z FAILED tests/test_zero/test_gemini/test_optim.py::test_optim[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2550938Z 
2025-04-11T03:52:13.2551054Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2551149Z Traceback (most recent call last):
2025-04-11T03:52:13.2551432Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2551509Z     fn(i, *args)
2025-04-11T03:52:13.2551750Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 185, in run_dist
2025-04-11T03:52:13.2551832Z     exam_model_step()
2025-04-11T03:52:13.2552085Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2552173Z     partial_func(**kwargs)
2025-04-11T03:52:13.2552475Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2552566Z     partial_func(**kwargs)
2025-04-11T03:52:13.2552807Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2552898Z     partial_func(**kwargs)
2025-04-11T03:52:13.2552994Z   [Previous line repeated 2 more times]
2025-04-11T03:52:13.2553243Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 75, in exam_model_step
2025-04-11T03:52:13.2553339Z     torch_model = model_builder().cuda()
2025-04-11T03:52:13.2553666Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2553767Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2554023Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2554133Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2554397Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2554481Z     module._apply(fn)
2025-04-11T03:52:13.2554743Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2554822Z     module._apply(fn)
2025-04-11T03:52:13.2555085Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2555176Z     param_applied = fn(param)
2025-04-11T03:52:13.2555512Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2555621Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2555723Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2556002Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2556133Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2556296Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2556605Z FAILED tests/test_zero/test_gemini/test_search.py::test_search[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2556609Z 
2025-04-11T03:52:13.2556728Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2556887Z Traceback (most recent call last):
2025-04-11T03:52:13.2557167Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2557244Z     fn(i, *args)
2025-04-11T03:52:13.2557480Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T03:52:13.2557572Z     exam_chunk_manager()
2025-04-11T03:52:13.2557886Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T03:52:13.2557986Z     chunk_manager = init_chunk_manager(
2025-04-11T03:52:13.2558250Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T03:52:13.2558337Z     dist.barrier()
2025-04-11T03:52:13.2558627Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:13.2558721Z     return func(*args, **kwargs)
2025-04-11T03:52:13.2559038Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T03:52:13.2559135Z     work = default_pg.barrier(opts=opts)
2025-04-11T03:52:13.2559239Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2559512Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2559650Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2559804Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2560169Z FAILED tests/test_zero/test_gemini/test_search.py::test_search[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2560175Z 
2025-04-11T03:52:13.2560291Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2560384Z Traceback (most recent call last):
2025-04-11T03:52:13.2560670Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2560747Z     fn(i, *args)
2025-04-11T03:52:13.2560982Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T03:52:13.2561070Z     exam_chunk_manager()
2025-04-11T03:52:13.2561331Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T03:52:13.2561427Z     chunk_manager = init_chunk_manager(
2025-04-11T03:52:13.2561688Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T03:52:13.2561774Z     dist.barrier()
2025-04-11T03:52:13.2562065Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:13.2562160Z     return func(*args, **kwargs)
2025-04-11T03:52:13.2562468Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T03:52:13.2562569Z     work = default_pg.barrier(opts=opts)
2025-04-11T03:52:13.2562666Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2562991Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2563127Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2563285Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2563631Z FAILED tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2563636Z 
2025-04-11T03:52:13.2563749Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2563851Z Traceback (most recent call last):
2025-04-11T03:52:13.2564134Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2564284Z     fn(i, *args)
2025-04-11T03:52:13.2564545Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 78, in run_dist
2025-04-11T03:52:13.2564628Z     exam_state_dict()
2025-04-11T03:52:13.2564885Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2564971Z     partial_func(**kwargs)
2025-04-11T03:52:13.2565221Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2565366Z     partial_func(**kwargs)
2025-04-11T03:52:13.2565613Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2565696Z     partial_func(**kwargs)
2025-04-11T03:52:13.2565795Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.2566078Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 45, in exam_state_dict
2025-04-11T03:52:13.2566326Z     model = GeminiDDP(model, config_dict, **placement_config, pin_memory=True, master_weights=master_weights)
2025-04-11T03:52:13.2566561Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 109, in __init__
2025-04-11T03:52:13.2566660Z     self.chunk_manager = ChunkManager(
2025-04-11T03:52:13.2566905Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T03:52:13.2567152Z     self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:13.2567252Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2567592Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2567722Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2567879Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2568182Z FAILED tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2568187Z 
2025-04-11T03:52:13.2568306Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2568401Z Traceback (most recent call last):
2025-04-11T03:52:13.2568682Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2568765Z     fn(i, *args)
2025-04-11T03:52:13.2569004Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 32, in run_dist
2025-04-11T03:52:13.2569096Z     check_all_gather_2d()
2025-04-11T03:52:13.2569360Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 16, in check_all_gather_2d
2025-04-11T03:52:13.2569492Z     tensor = torch.rand(128, device=get_current_device())
2025-04-11T03:52:13.2569590Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2569862Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2569991Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2570145Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2570542Z FAILED tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2570547Z 
2025-04-11T03:52:13.2570661Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2570759Z Traceback (most recent call last):
2025-04-11T03:52:13.2571038Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2571122Z     fn(i, *args)
2025-04-11T03:52:13.2571364Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 139, in run_dist
2025-04-11T03:52:13.2571457Z     exam_zero_1_grad_acc(sync=True)
2025-04-11T03:52:13.2571792Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 86, in exam_zero_1_grad_acc
2025-04-11T03:52:13.2571888Z     zero_model = zero_model.to(device)
2025-04-11T03:52:13.2572151Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:13.2572246Z     return self._apply(convert)
2025-04-11T03:52:13.2572513Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2572681Z     module._apply(fn)
2025-04-11T03:52:13.2572944Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2573039Z     param_applied = fn(param)
2025-04-11T03:52:13.2573308Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:13.2573523Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:13.2573624Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2573908Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2574042Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2574197Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2574526Z FAILED tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2 - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2574532Z 
2025-04-11T03:52:13.2574703Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2574803Z Traceback (most recent call last):
2025-04-11T03:52:13.2575079Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2575162Z     fn(i, *args)
2025-04-11T03:52:13.2575410Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 51, in run_dist
2025-04-11T03:52:13.2575517Z     exam_mem_leak(world_size=world_size)
2025-04-11T03:52:13.2575775Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 36, in exam_mem_leak
2025-04-11T03:52:13.2575868Z     zero_model = MlpModel().cuda()
2025-04-11T03:52:13.2576132Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2576240Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2576510Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2576594Z     module._apply(fn)
2025-04-11T03:52:13.2576860Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2576950Z     param_applied = fn(param)
2025-04-11T03:52:13.2577212Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2577328Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2577423Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2577758Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2577888Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2578052Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2578371Z FAILED tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2 - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2578377Z 
2025-04-11T03:52:13.2578497Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2578591Z Traceback (most recent call last):
2025-04-11T03:52:13.2578866Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2578947Z     fn(i, *args)
2025-04-11T03:52:13.2579327Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 217, in run_dist
2025-04-11T03:52:13.2579420Z     exam_zero_1_torch_ddp()
2025-04-11T03:52:13.2579670Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2579758Z     partial_func(**kwargs)
2025-04-11T03:52:13.2580001Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2580144Z     partial_func(**kwargs)
2025-04-11T03:52:13.2580395Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2580481Z     partial_func(**kwargs)
2025-04-11T03:52:13.2580759Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 151, in exam_zero_1_torch_ddp
2025-04-11T03:52:13.2580868Z     torch_model = MlpModel().cuda().to(dtype)
2025-04-11T03:52:13.2581132Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2581240Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2581501Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2581589Z     module._apply(fn)
2025-04-11T03:52:13.2581845Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2581939Z     param_applied = fn(param)
2025-04-11T03:52:13.2582261Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2582373Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2582474Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2582749Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2582886Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2583042Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2583366Z FAILED tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2583370Z 
2025-04-11T03:52:13.2583486Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2583582Z Traceback (most recent call last):
2025-04-11T03:52:13.2583858Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2583936Z     fn(i, *args)
2025-04-11T03:52:13.2584187Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 123, in run_dist
2025-04-11T03:52:13.2584282Z     exam_zero_1_torch_ddp_ckpt()
2025-04-11T03:52:13.2584530Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2584619Z     partial_func(**kwargs)
2025-04-11T03:52:13.2584910Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 62, in exam_zero_1_torch_ddp_ckpt
2025-04-11T03:52:13.2585006Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2585319Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2585434Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2585695Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2585784Z     module._apply(fn)
2025-04-11T03:52:13.2586045Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2586139Z     param_applied = fn(param)
2025-04-11T03:52:13.2586401Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2586572Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2586670Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2586949Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2587087Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2587246Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2587445Z = 573 failed, 74 passed, 195 skipped, 23 deselected, 91 warnings in 673.91s (0:11:13) =
2025-04-11T03:52:13.9056314Z ##[error]Process completed with exit code 1.
2025-04-11T03:52:13.9135303Z Post job cleanup.
2025-04-11T03:52:13.9138560Z ##[command]/usr/bin/docker exec  a22674922e87e0d0962e2f956706f403d9456d353f40b8411642994284af24b7 sh -c "cat /etc/*release | grep ^ID"
2025-04-11T03:52:14.1281791Z [command]/usr/bin/git version
2025-04-11T03:52:14.1310646Z git version 2.25.1
2025-04-11T03:52:14.1344191Z Temporarily overriding HOME='/__w/_temp/238e140f-ec1b-458b-bab1-cc077c686f3e' before making global git config changes
2025-04-11T03:52:14.1344790Z Adding repository directory to the temporary git global config as a safe directory
2025-04-11T03:52:14.1347765Z [command]/usr/bin/git config --global --add safe.directory /__w/ColossalAI/ColossalAI
2025-04-11T03:52:14.1370754Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2025-04-11T03:52:14.1392790Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2025-04-11T03:52:14.1560247Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2025-04-11T03:52:14.1573959Z http.https://github.com/.extraheader
2025-04-11T03:52:14.1581404Z [command]/usr/bin/git config --local --unset-all http.https://github.com/.extraheader
2025-04-11T03:52:14.1602074Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2025-04-11T03:52:14.1870481Z Post job cleanup.
2025-04-11T03:52:14.1873732Z ##[command]/usr/bin/docker exec  a22674922e87e0d0962e2f956706f403d9456d353f40b8411642994284af24b7 sh -c "cat /etc/*release | grep ^ID"
2025-04-11T03:52:14.4052361Z Stop and remove container: a6f9b4cb19ae4283bedcab830f5b7400_imagecloudluchentechcomhpcaitechpytorchcuda2221210_b76ab0
2025-04-11T03:52:14.4056639Z ##[command]/usr/bin/docker rm --force a22674922e87e0d0962e2f956706f403d9456d353f40b8411642994284af24b7
2025-04-11T03:52:15.4066557Z a22674922e87e0d0962e2f956706f403d9456d353f40b8411642994284af24b7
2025-04-11T03:52:15.4092793Z Remove container network: github_network_b05f10a0d18f44629e5127de4c67c8e5
2025-04-11T03:52:15.4096510Z ##[command]/usr/bin/docker network rm github_network_b05f10a0d18f44629e5127de4c67c8e5
2025-04-11T03:52:15.5454018Z github_network_b05f10a0d18f44629e5127de4c67c8e5
2025-04-11T03:52:15.5508739Z Cleaning up orphan processes
