2025-04-11T03:40:57.9071314Z ##[group]Run CURL_CA_BUNDLE="" PYTHONPATH=$PWD FAST_TEST=1 pytest \
2025-04-11T03:40:57.9071782Z [36;1mCURL_CA_BUNDLE="" PYTHONPATH=$PWD FAST_TEST=1 pytest \[0m
2025-04-11T03:40:57.9072099Z [36;1m-m "not largedist" \[0m
2025-04-11T03:40:57.9072343Z [36;1m--durations=0 \[0m
2025-04-11T03:40:57.9072582Z [36;1m--ignore tests/test_analyzer \[0m
2025-04-11T03:40:57.9072872Z [36;1m--ignore tests/test_auto_parallel \[0m
2025-04-11T03:40:57.9073143Z [36;1m--ignore tests/test_fx \[0m
2025-04-11T03:40:57.9073403Z [36;1m--ignore tests/test_autochunk \[0m
2025-04-11T03:40:57.9073672Z [36;1m--ignore tests/test_gptq \[0m
2025-04-11T03:40:57.9073929Z [36;1m--ignore tests/test_infer_ops \[0m
2025-04-11T03:40:57.9074392Z [36;1m--ignore tests/test_legacy \[0m
2025-04-11T03:40:57.9074662Z [36;1m--ignore tests/test_smoothquant \[0m
2025-04-11T03:40:57.9074918Z [36;1mtests/[0m
2025-04-11T03:40:57.9075298Z shell: bash --noprofile --norc -e -o pipefail {0}
2025-04-11T03:40:57.9075574Z env:
2025-04-11T03:40:57.9075949Z   LD_LIBRARY_PATH: /github/home/.tensornvme/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2025-04-11T03:40:57.9076348Z   LLAMA_PATH: /data/scratch/llama-tiny
2025-04-11T03:40:57.9076614Z   MOE_TENSOR_PATH: /data/scratch/moe_tensors
2025-04-11T03:40:57.9076896Z   HF_ENDPOINT: https://hf-mirror.com
2025-04-11T03:40:57.9077161Z ##[endgroup]
2025-04-11T03:41:07.9561634Z ============================= test session starts ==============================
2025-04-11T03:41:07.9562109Z platform linux -- Python 3.10.14, pytest-7.4.4, pluggy-1.5.0
2025-04-11T03:41:07.9562446Z rootdir: /__w/ColossalAI/ColossalAI
2025-04-11T03:41:07.9562705Z configfile: pytest.ini
2025-04-11T03:41:07.9563024Z plugins: hypothesis-6.131.0, anyio-4.9.0, testmon-2.0.7b1
2025-04-11T03:41:07.9563362Z collected 865 items / 23 deselected / 842 selected
2025-04-11T03:41:07.9563554Z 
2025-04-11T03:41:08.1651220Z tests/test_booster/test_accelerator.py F                                 [  0%]
2025-04-11T03:41:08.1655827Z tests/test_booster/test_mixed_precision/test_fp16_torch.py s             [  0%]
2025-04-11T03:41:16.9176306Z tests/test_booster/test_plugin/test_3d_plugin.py F                       [  0%]
2025-04-11T03:41:22.1080179Z tests/test_booster/test_plugin/test_dp_plugin_base.py F                  [  0%]
2025-04-11T03:41:31.6080959Z tests/test_booster/test_plugin/test_gemini_plugin.py F                   [  0%]
2025-04-11T03:41:38.4531931Z tests/test_booster/test_plugin/test_low_level_zero_plugin.py F           [  0%]
2025-04-11T03:41:46.2271656Z tests/test_booster/test_plugin/test_torch_ddp_plugin.py F                [  0%]
2025-04-11T03:41:53.5966814Z tests/test_booster/test_plugin/test_torch_fsdp_plugin.py F               [  0%]
2025-04-11T03:42:02.1925817Z tests/test_checkpoint_io/test_gemini_checkpoint_io.py F                  [  1%]
2025-04-11T03:42:09.8876762Z tests/test_checkpoint_io/test_gemini_torch_compability.py F              [  1%]
2025-04-11T03:42:14.9195274Z tests/test_checkpoint_io/test_general_checkpoint_io.py F..FFFFFF         [  2%]
2025-04-11T03:42:22.7912909Z tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py F  [  2%]
2025-04-11T03:42:23.0164253Z tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py F          [  2%]
2025-04-11T03:42:29.6316541Z tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py F     [  2%]
2025-04-11T03:42:31.0848577Z tests/test_checkpoint_io/test_safetensors_async_io.py FFFFF              [  3%]
2025-04-11T03:42:35.8861628Z tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py F               [  3%]
2025-04-11T03:42:40.5821337Z tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py F              [  3%]
2025-04-11T03:42:45.3426560Z tests/test_cluster/test_device_mesh_manager.py .                         [  3%]
2025-04-11T03:42:50.4166387Z tests/test_cluster/test_process_group_mesh.py .                          [  3%]
2025-04-11T03:42:50.5296574Z tests/test_config/test_load_config.py .                                  [  3%]
2025-04-11T03:42:50.5302642Z tests/test_device/test_alpha_beta.py s                                   [  3%]
2025-04-11T03:42:55.4350890Z tests/test_device/test_device_mesh.py ..                                 [  4%]
2025-04-11T03:42:55.4356507Z tests/test_device/test_extract_alpha_beta.py s                           [  4%]
2025-04-11T03:43:00.9540740Z tests/test_device/test_init_logical_pg.py F                              [  4%]
2025-04-11T03:43:00.9545301Z tests/test_device/test_search_logical_device_mesh.py s                   [  4%]
2025-04-11T03:43:07.3926835Z tests/test_fp8/test_all_to_all_single.py F                               [  4%]
2025-04-11T03:43:13.9992564Z tests/test_fp8/test_fp8_all_to_all.py F                                  [  4%]
2025-04-11T03:43:19.9205554Z tests/test_fp8/test_fp8_all_to_all_single.py F                           [  4%]
2025-04-11T03:43:25.9966317Z tests/test_fp8/test_fp8_allgather.py F                                   [  4%]
2025-04-11T03:43:32.4139027Z tests/test_fp8/test_fp8_allreduce.py F                                   [  5%]
2025-04-11T03:43:32.7249613Z tests/test_fp8/test_fp8_cast.py F                                        [  5%]
2025-04-11T03:43:39.9960658Z tests/test_fp8/test_fp8_fsdp_comm_hook.py F                              [  5%]
2025-04-11T03:43:40.3493843Z tests/test_fp8/test_fp8_hook.py F                                        [  5%]
2025-04-11T03:43:41.4408832Z tests/test_fp8/test_fp8_linear.py FFFF                                   [  5%]
2025-04-11T03:43:47.2856920Z tests/test_fp8/test_fp8_reduce_scatter.py F                              [  6%]
2025-04-11T03:43:47.5378905Z tests/test_infer/test_batch_bucket.py F                                  [  6%]
2025-04-11T03:43:51.9554765Z tests/test_infer/test_config_and_struct.py .                             [  6%]
2025-04-11T03:44:07.4093276Z tests/test_infer/test_continuous_batching.py F                           [  6%]
2025-04-11T03:44:18.3032125Z tests/test_infer/test_drafter.py FF                                      [  6%]
2025-04-11T03:44:22.6593186Z tests/test_infer/test_kvcache_manager.py .F                              [  6%]
2025-04-11T03:44:26.9987746Z tests/test_infer/test_request_handler.py F                               [  7%]
2025-04-11T03:44:33.5864847Z tests/test_infer/test_streamingllm.py F                                  [  7%]
2025-04-11T03:44:33.7564829Z tests/test_infer/test_async_engine/test_async_engine.py s                [  7%]
2025-04-11T03:44:33.9030026Z tests/test_infer/test_async_engine/test_request_tracer.py .              [  7%]
2025-04-11T03:44:33.9108738Z tests/test_infer/test_kernels/cuda/test_convert_fp8.py sssssssssssssssss [  9%]
2025-04-11T03:44:33.9429410Z ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 17%]
2025-04-11T03:44:33.9515864Z sssssssssssssssssss                                                      [ 20%]
2025-04-11T03:44:34.4716470Z tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py FF   [ 20%]
2025-04-11T03:44:34.9648940Z tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py FF            [ 20%]
2025-04-11T03:44:38.2009401Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py FFFFFFFFFFFFF [ 22%]
2025-04-11T03:44:43.9636542Z FFFFFFFFFFFFFFFFFFFFFFF                                                  [ 24%]
2025-04-11T03:44:47.0162040Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py FFFFFFFFFFFFFFF [ 26%]
2025-04-11T03:44:47.2120253Z F                                                                        [ 26%]
2025-04-11T03:44:48.1292597Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py FFFF    [ 27%]
2025-04-11T03:44:48.5177814Z tests/test_infer/test_kernels/cuda/test_silu_and_mul.py FF               [ 27%]
2025-04-11T03:44:49.2551225Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py ........ [ 28%]
2025-04-11T03:45:00.7607564Z ........................FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 37%]
2025-04-11T03:45:10.7589114Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF                         [ 42%]
2025-04-11T03:45:12.0324394Z tests/test_infer/test_kernels/triton/test_decoding_attn.py sssssssssssss [ 44%]
2025-04-11T03:45:20.8308590Z sssssssssssssssssssssssssssssssssssssssssssssssssssFFFFFFFFFFFFFFFFFFFFF [ 52%]
2025-04-11T03:45:36.6793364Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 61%]
2025-04-11T03:45:51.9281165Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 69%]
2025-04-11T03:45:57.3741424Z FFFFFFFFFFFFFFFFFFFFFFFFFFF                                              [ 73%]
2025-04-11T03:45:57.3747375Z tests/test_infer/test_kernels/triton/test_fused_rotary_embedding.py s    [ 73%]
2025-04-11T03:46:00.1254010Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py FFFFFFFFFFFFFF [ 74%]
2025-04-11T03:46:06.7307040Z FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF                                       [ 78%]
2025-04-11T03:46:06.9237364Z tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py F            [ 79%]
2025-04-11T03:46:07.3434526Z tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py FF    [ 79%]
2025-04-11T03:46:07.5395729Z tests/test_infer/test_kernels/triton/test_xine_copy.py F                 [ 79%]
2025-04-11T03:46:07.5413039Z tests/test_infer/test_models/test_attention.py ssss                      [ 79%]
2025-04-11T03:46:07.6344939Z tests/test_infer/test_models/test_custom_model.py s                      [ 80%]
2025-04-11T03:46:12.1703353Z tests/test_lazy/test_from_pretrained.py .                                [ 80%]
2025-04-11T03:46:29.1368270Z tests/test_lazy/test_models.py .F                                        [ 80%]
2025-04-11T03:46:29.3460780Z tests/test_lazy/test_ops.py F                                            [ 80%]
2025-04-11T03:46:35.9147006Z tests/test_lora/test_lora.py F                                           [ 80%]
2025-04-11T03:46:35.9152386Z tests/test_moe/test_deepseek_layer.py s                                  [ 80%]
2025-04-11T03:46:36.3332752Z tests/test_moe/test_kernel.py FF                                         [ 80%]
2025-04-11T03:46:36.3338210Z tests/test_moe/test_mixtral_layer.py s                                   [ 81%]
2025-04-11T03:46:42.4998964Z tests/test_moe/test_moe_checkpoint.py F                                  [ 81%]
2025-04-11T03:46:42.5004009Z tests/test_moe/test_moe_ep_tp.py s                                       [ 81%]
2025-04-11T03:46:42.5009283Z tests/test_moe/test_moe_ep_zero.py s                                     [ 81%]
2025-04-11T03:46:47.6350416Z tests/test_optimizer/test_adam_kernel.py FFFFFFFFFFFFFFFFFFFF........... [ 85%]
2025-04-11T03:46:47.7269921Z .                                                                        [ 85%]
2025-04-11T03:46:58.9615009Z tests/test_optimizer/test_adam_optim.py F.FFFF.FFFF.FFFF.FFFF.FFFF.FFF   [ 88%]
2025-04-11T03:47:06.3822981Z tests/test_optimizer/test_dist_adafactor.py F                            [ 88%]
2025-04-11T03:47:15.4516361Z tests/test_optimizer/test_dist_came.py F                                 [ 89%]
2025-04-11T03:47:23.2218167Z tests/test_optimizer/test_dist_galore.py F                               [ 89%]
2025-04-11T03:47:31.0024989Z tests/test_optimizer/test_dist_lamb.py F                                 [ 89%]
2025-04-11T03:47:31.1264246Z tests/test_optimizer/test_lr_scheduler.py .                              [ 89%]
2025-04-11T03:47:31.1270420Z tests/test_optimizer/test_nvme.py s                                      [ 89%]
2025-04-11T03:47:35.2045115Z tests/test_pipeline/test_p2p_communication.py F                          [ 89%]
2025-04-11T03:47:40.1825769Z tests/test_pipeline/test_stage_manager.py F                              [ 89%]
2025-04-11T03:47:40.4030931Z tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py ..     [ 90%]
2025-04-11T03:47:40.4973262Z tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py . [ 90%]
2025-04-11T03:47:40.5912032Z .                                                                        [ 90%]
2025-04-11T03:48:01.6506789Z tests/test_pipeline/test_schedule/test_interleaved.py FFFF               [ 90%]
2025-04-11T03:48:22.8671799Z tests/test_pipeline/test_schedule/test_oneF_oneB.py FFFF                 [ 91%]
2025-04-11T03:48:23.1730530Z tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py ...    [ 91%]
2025-04-11T03:48:28.1637976Z tests/test_pipeline/test_schedule/test_zerobubble_pp.py F                [ 91%]
2025-04-11T03:48:28.3960147Z tests/test_shardformer/test_flash_attention.py F                         [ 91%]
2025-04-11T03:48:28.6360726Z tests/test_shardformer/test_shard_utils.py F                             [ 91%]
2025-04-11T03:48:29.0364595Z tests/test_shardformer/test_with_torch_ddp.py F                          [ 92%]
2025-04-11T03:48:29.2567534Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py F [ 92%]
2025-04-11T03:48:29.2569576Z                                                                          [ 92%]
2025-04-11T03:48:29.4576473Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py F [ 92%]
2025-04-11T03:48:29.4576969Z                                                                          [ 92%]
2025-04-11T03:48:29.6580780Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py F [ 92%]
2025-04-11T03:48:29.6581208Z                                                                          [ 92%]
2025-04-11T03:48:34.7174756Z tests/test_shardformer/test_layer/test_dist_crossentropy.py F            [ 92%]
2025-04-11T03:48:38.7384479Z tests/test_shardformer/test_layer/test_dropout.py F                      [ 92%]
2025-04-11T03:48:43.7916084Z tests/test_shardformer/test_layer/test_embedding.py F                    [ 92%]
2025-04-11T03:48:47.9635993Z tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py F     [ 92%]
2025-04-11T03:48:53.1048130Z tests/test_shardformer/test_layer/test_layernorm.py F                    [ 92%]
2025-04-11T03:48:57.2109126Z tests/test_shardformer/test_layer/test_linear_1d.py F                    [ 93%]
2025-04-11T03:49:02.2460836Z tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py F          [ 93%]
2025-04-11T03:49:12.4026825Z tests/test_shardformer/test_layer/test_ring_attn.py FF                   [ 93%]
2025-04-11T03:49:17.4439796Z tests/test_shardformer/test_layer/test_sequence_parallel.py F            [ 93%]
2025-04-11T03:49:21.6465773Z tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py F  [ 93%]
2025-04-11T03:49:21.8781483Z tests/test_shardformer/test_model/test_shard_bert.py F                   [ 93%]
2025-04-11T03:49:22.0810716Z tests/test_shardformer/test_model/test_shard_blip2.py F                  [ 93%]
2025-04-11T03:49:22.2842914Z tests/test_shardformer/test_model/test_shard_bloom.py F                  [ 94%]
2025-04-11T03:49:22.4851888Z tests/test_shardformer/test_model/test_shard_chatglm2.py F               [ 94%]
2025-04-11T03:49:22.6883456Z tests/test_shardformer/test_model/test_shard_command.py F                [ 94%]
2025-04-11T03:49:31.1429808Z tests/test_shardformer/test_model/test_shard_deepseek.py F               [ 94%]
2025-04-11T03:49:42.0005480Z tests/test_shardformer/test_model/test_shard_deepseek_v3.py F            [ 94%]
2025-04-11T03:49:42.2295762Z tests/test_shardformer/test_model/test_shard_falcon.py F                 [ 94%]
2025-04-11T03:49:42.4397038Z tests/test_shardformer/test_model/test_shard_gpt2.py F                   [ 94%]
2025-04-11T03:49:42.4402829Z tests/test_shardformer/test_model/test_shard_gptj.py s                   [ 94%]
2025-04-11T03:49:42.6462011Z tests/test_shardformer/test_model/test_shard_llama.py F                  [ 95%]
2025-04-11T03:49:42.8513312Z tests/test_shardformer/test_model/test_shard_mistral.py F                [ 95%]
2025-04-11T03:49:49.1174958Z tests/test_shardformer/test_model/test_shard_mixtral.py F                [ 95%]
2025-04-11T03:49:49.3433868Z tests/test_shardformer/test_model/test_shard_opt.py F                    [ 95%]
2025-04-11T03:49:49.5493955Z tests/test_shardformer/test_model/test_shard_qwen2.py F                  [ 95%]
2025-04-11T03:49:49.7563961Z tests/test_shardformer/test_model/test_shard_sam.py F                    [ 95%]
2025-04-11T03:49:49.9609892Z tests/test_shardformer/test_model/test_shard_t5.py F                     [ 95%]
2025-04-11T03:49:50.1660363Z tests/test_shardformer/test_model/test_shard_vit.py F                    [ 95%]
2025-04-11T03:49:50.3710038Z tests/test_shardformer/test_model/test_shard_whisper.py F                [ 95%]
2025-04-11T03:49:55.5245989Z tests/test_tensor/test_comm_spec_apply.py F                              [ 96%]
2025-04-11T03:49:55.5251365Z tests/test_tensor/test_mix_gather.py s                                   [ 96%]
2025-04-11T03:49:59.9214533Z tests/test_tensor/test_padded_tensor.py F                                [ 96%]
2025-04-11T03:50:00.1398096Z tests/test_tensor/test_shape_consistency.py ..                           [ 96%]
2025-04-11T03:50:05.0565125Z tests/test_tensor/test_shape_consistency_apply.py F                      [ 96%]
2025-04-11T03:50:05.1786142Z tests/test_tensor/test_sharding_spec.py .                                [ 96%]
2025-04-11T03:50:10.1686090Z tests/test_tensor/test_dtensor/test_comm_spec.py F                       [ 96%]
2025-04-11T03:50:14.1989792Z tests/test_tensor/test_dtensor/test_dtensor.py F                         [ 97%]
2025-04-11T03:50:14.3210135Z tests/test_tensor/test_dtensor/test_dtensor_sharding_spec.py .           [ 97%]
2025-04-11T03:50:27.0356188Z tests/test_tensor/test_dtensor/test_layout_converter.py F                [ 97%]
2025-04-11T03:50:31.2366949Z tests/test_zero/test_gemini/test_chunk_mgrv2.py F                        [ 97%]
2025-04-11T03:50:44.2622978Z tests/test_zero/test_gemini/test_chunkv2.py FFF                          [ 97%]
2025-04-11T03:50:44.2633213Z tests/test_zero/test_gemini/test_gemini_use_rmt.py ss                    [ 97%]
2025-04-11T03:50:50.8158658Z tests/test_zero/test_gemini/test_grad_accum.py F                         [ 98%]
2025-04-11T03:51:04.4952070Z tests/test_zero/test_gemini/test_grad_clip.py FF                         [ 98%]
2025-04-11T03:51:19.4766455Z tests/test_zero/test_gemini/test_inference.py FF                         [ 98%]
2025-04-11T03:51:27.7148293Z tests/test_zero/test_gemini/test_optim.py F                              [ 98%]
2025-04-11T03:51:27.7153560Z tests/test_zero/test_gemini/test_runtime_mem_tracer.py s                 [ 98%]
2025-04-11T03:51:38.0526258Z tests/test_zero/test_gemini/test_search.py FF                            [ 99%]
2025-04-11T03:51:46.2248435Z tests/test_zero/test_gemini/test_zeroddp_state_dict.py F                 [ 99%]
2025-04-11T03:51:46.2258139Z tests/test_zero/test_gemini/test_zerooptim_state_dict.py ss              [ 99%]
2025-04-11T03:51:51.1545387Z tests/test_zero/test_low_level/test_coll_nd.py F                         [ 99%]
2025-04-11T03:51:55.3587249Z tests/test_zero/test_low_level/test_grad_acc.py F                        [ 99%]
2025-04-11T03:52:00.4581844Z tests/test_zero/test_low_level/test_mem_leak.py F                        [ 99%]
2025-04-11T03:52:06.3752708Z tests/test_zero/test_low_level/test_zero1_2.py F                         [ 99%]
2025-04-11T03:52:12.4071496Z tests/test_zero/test_low_level/test_zero_ckpt.py F                       [100%]
2025-04-11T03:52:12.4071921Z 
2025-04-11T03:52:12.4072047Z =================================== FAILURES ===================================
2025-04-11T03:52:12.4072395Z _______________________________ test_accelerator _______________________________
2025-04-11T03:52:12.4072610Z 
2025-04-11T03:52:12.4072709Z args = (), kwargs = {}
2025-04-11T03:52:12.4072870Z 
2025-04-11T03:52:12.4072982Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.4073316Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.4073599Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.4073907Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.4074230Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.4074528Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.4075315Z 
2025-04-11T03:52:12.4075434Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.4075723Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4076085Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.4076431Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.4076717Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4076898Z 
2025-04-11T03:52:12.4076996Z device = None
2025-04-11T03:52:12.4077116Z 
2025-04-11T03:52:12.4077385Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.4078032Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.4078357Z     
2025-04-11T03:52:12.4078552Z         Args:
2025-04-11T03:52:12.4078841Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.4079268Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.4079644Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.4079913Z         """
2025-04-11T03:52:12.4080117Z         _lazy_init()
2025-04-11T03:52:12.4080349Z         with torch.cuda.device(device):
2025-04-11T03:52:12.4080644Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4080950Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4081439Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4081958Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4082341Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4082578Z 
2025-04-11T03:52:12.4082833Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.4083296Z ________________________________ test_3d_plugin ________________________________
2025-04-11T03:52:12.4083520Z 
2025-04-11T03:52:12.4083620Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4084398Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4085099Z 
2025-04-11T03:52:12.4085215Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4085479Z         try_count = 0
2025-04-11T03:52:12.4085713Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4085974Z             max_try, int
2025-04-11T03:52:12.4086254Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4086550Z     
2025-04-11T03:52:12.4086771Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4087036Z             try:
2025-04-11T03:52:12.4087251Z                 try_count += 1
2025-04-11T03:52:12.4087482Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4087733Z                 return ret
2025-04-11T03:52:12.4087972Z             except exception_type as e:
2025-04-11T03:52:12.4088240Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4088602Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4089003Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4089349Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4089730Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4090049Z                     continue
2025-04-11T03:52:12.4090273Z                 else:
2025-04-11T03:52:12.4090613Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4091118Z >                   raise e
2025-04-11T03:52:12.4091264Z 
2025-04-11T03:52:12.4091366Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4091649Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4091970Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4092268Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4092589Z tests/test_booster/test_plugin/test_3d_plugin.py:277: in test_3d_plugin
2025-04-11T03:52:12.4092954Z     spawn(run_dist, 4, early_stop=early_stop)
2025-04-11T03:52:12.4093240Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4093523Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4094080Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4094600Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4095152Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4095614Z     while not context.join():
2025-04-11T03:52:12.4095885Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4096067Z 
2025-04-11T03:52:12.4096281Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0319540>
2025-04-11T03:52:12.4096640Z timeout = None
2025-04-11T03:52:12.4096759Z 
2025-04-11T03:52:12.4096860Z     def join(self, timeout=None):
2025-04-11T03:52:12.4097151Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4097423Z     
2025-04-11T03:52:12.4097674Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4098038Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4098418Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4098762Z         of the first process exiting.
2025-04-11T03:52:12.4099012Z     
2025-04-11T03:52:12.4099270Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4099628Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4099920Z     
2025-04-11T03:52:12.4100098Z         Args:
2025-04-11T03:52:12.4100381Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4100667Z         """
2025-04-11T03:52:12.4100915Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4101215Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4101453Z             return True
2025-04-11T03:52:12.4101658Z     
2025-04-11T03:52:12.4101893Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4102216Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4102497Z             self.sentinels.keys(),
2025-04-11T03:52:12.4102735Z             timeout=timeout,
2025-04-11T03:52:12.4102955Z         )
2025-04-11T03:52:12.4103136Z     
2025-04-11T03:52:12.4103325Z         error_index = None
2025-04-11T03:52:12.4103547Z         for sentinel in ready:
2025-04-11T03:52:12.4103816Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4104146Z             process = self.processes[index]
2025-04-11T03:52:12.4104417Z             process.join()
2025-04-11T03:52:12.4104674Z             if process.exitcode != 0:
2025-04-11T03:52:12.4104929Z                 error_index = index
2025-04-11T03:52:12.4105169Z                 break
2025-04-11T03:52:12.4105372Z     
2025-04-11T03:52:12.4105571Z         # Return if there was no error.
2025-04-11T03:52:12.4105828Z         if error_index is None:
2025-04-11T03:52:12.4106107Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4106437Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4106683Z     
2025-04-11T03:52:12.4106929Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4107377Z         for process in self.processes:
2025-04-11T03:52:12.4107640Z             if process.is_alive():
2025-04-11T03:52:12.4107916Z                 process.terminate()
2025-04-11T03:52:12.4108178Z             process.join()
2025-04-11T03:52:12.4108405Z     
2025-04-11T03:52:12.4108746Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4109072Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4109372Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4109688Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4109986Z             if exitcode < 0:
2025-04-11T03:52:12.4110395Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4110695Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4111021Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4111354Z                     error_index=error_index,
2025-04-11T03:52:12.4111642Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4111924Z                     exit_code=exitcode,
2025-04-11T03:52:12.4112183Z                     signal_name=name,
2025-04-11T03:52:12.4112419Z                 )
2025-04-11T03:52:12.4112626Z             else:
2025-04-11T03:52:12.4112895Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4113238Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4113583Z                     error_index=error_index,
2025-04-11T03:52:12.4113854Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4114126Z                     exit_code=exitcode,
2025-04-11T03:52:12.4114371Z                 )
2025-04-11T03:52:12.4114558Z     
2025-04-11T03:52:12.4114806Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4115192Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4115541Z         msg += original_trace
2025-04-11T03:52:12.4115875Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4116284Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4116613Z E       
2025-04-11T03:52:12.4116864Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4117174Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4117658Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4118139Z E           fn(i, *args)
2025-04-11T03:52:12.4118570Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 271, in run_dist
2025-04-11T03:52:12.4119032Z E           check_3d_plugin(early_stop=early_stop)
2025-04-11T03:52:12.4119482Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4119923Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4120375Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 104, in check_3d_plugin
2025-04-11T03:52:12.4120901Z E           err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:12.4121361Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4121767Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4122209Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4122675Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4123142Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4123597Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4123892Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4124483Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4124981Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4125353Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4125578Z 
2025-04-11T03:52:12.4125893Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4126420Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4126815Z [04/11/25 03:41:16] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4127329Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4127672Z                              :75 launch                                         
2025-04-11T03:52:12.4128007Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4128357Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.4128745Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4129164Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4130545Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4131918Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4133276Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4134667Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4136027Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4137395Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4138762Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4140123Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4141064Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4141904Z   warnings.warn(
2025-04-11T03:52:12.4142703Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4143647Z   warnings.warn(
2025-04-11T03:52:12.4144438Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4145255Z   warnings.warn(
2025-04-11T03:52:12.4146039Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4146983Z   warnings.warn(
2025-04-11T03:52:12.4147937Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4149036Z   warnings.warn(
2025-04-11T03:52:12.4149957Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4150905Z   warnings.warn(
2025-04-11T03:52:12.4151806Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4152754Z   warnings.warn(
2025-04-11T03:52:12.4153658Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4154586Z   warnings.warn(
2025-04-11T03:52:12.4155479Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4156413Z   warnings.warn(
2025-04-11T03:52:12.4157316Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4158259Z   warnings.warn(
2025-04-11T03:52:12.4159149Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4160074Z   warnings.warn(
2025-04-11T03:52:12.4160967Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4161897Z   warnings.warn(
2025-04-11T03:52:12.4162800Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4163870Z   warnings.warn(
2025-04-11T03:52:12.4164770Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4165851Z   warnings.warn(
2025-04-11T03:52:12.4166743Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4167679Z   warnings.warn(
2025-04-11T03:52:12.4168565Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4169497Z   warnings.warn(
2025-04-11T03:52:12.4169913Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:53862 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4170554Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:53862 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4171189Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:53862 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4172090Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4172798Z   warnings.warn(
2025-04-11T03:52:12.4173454Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4174117Z   warnings.warn(
2025-04-11T03:52:12.4174768Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4175459Z   warnings.warn(
2025-04-11T03:52:12.4176100Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4176790Z   warnings.warn(
2025-04-11T03:52:12.4177452Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4178129Z   warnings.warn(
2025-04-11T03:52:12.4178775Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4179449Z   warnings.warn(
2025-04-11T03:52:12.4180094Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4180784Z   warnings.warn(
2025-04-11T03:52:12.4181438Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4182225Z   warnings.warn(
2025-04-11T03:52:12.4182870Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4183539Z   warnings.warn(
2025-04-11T03:52:12.4184191Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4184991Z   warnings.warn(
2025-04-11T03:52:12.4185646Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4186343Z   warnings.warn(
2025-04-11T03:52:12.4186979Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4188278Z   warnings.warn(
2025-04-11T03:52:12.4188595Z __________________________ test_dp_plugin_dataloader ___________________________
2025-04-11T03:52:12.4188835Z 
2025-04-11T03:52:12.4188933Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4189698Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4190356Z 
2025-04-11T03:52:12.4190466Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4190731Z         try_count = 0
2025-04-11T03:52:12.4190970Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4191242Z             max_try, int
2025-04-11T03:52:12.4191523Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4191821Z     
2025-04-11T03:52:12.4192049Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4192317Z             try:
2025-04-11T03:52:12.4192515Z                 try_count += 1
2025-04-11T03:52:12.4192765Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4193023Z                 return ret
2025-04-11T03:52:12.4193263Z             except exception_type as e:
2025-04-11T03:52:12.4193541Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4193908Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4194289Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4194633Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4195019Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4195335Z                     continue
2025-04-11T03:52:12.4195557Z                 else:
2025-04-11T03:52:12.4195897Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4196284Z >                   raise e
2025-04-11T03:52:12.4196429Z 
2025-04-11T03:52:12.4196532Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4196816Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4197140Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4197438Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4197785Z tests/test_booster/test_plugin/test_dp_plugin_base.py:94: in test_dp_plugin_dataloader
2025-04-11T03:52:12.4198150Z     spawn(run_dist, 2)
2025-04-11T03:52:12.4198386Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4198662Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4199287Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4199795Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4200347Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4200803Z     while not context.join():
2025-04-11T03:52:12.4201082Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4201264Z 
2025-04-11T03:52:12.4201483Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6b47c1f940>
2025-04-11T03:52:12.4201956Z timeout = None
2025-04-11T03:52:12.4202073Z 
2025-04-11T03:52:12.4202170Z     def join(self, timeout=None):
2025-04-11T03:52:12.4202464Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4202739Z     
2025-04-11T03:52:12.4202992Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4203356Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4203730Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4204065Z         of the first process exiting.
2025-04-11T03:52:12.4204307Z     
2025-04-11T03:52:12.4204616Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4204975Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4205254Z     
2025-04-11T03:52:12.4205426Z         Args:
2025-04-11T03:52:12.4205685Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4205979Z         """
2025-04-11T03:52:12.4206230Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4206523Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4206775Z             return True
2025-04-11T03:52:12.4206998Z     
2025-04-11T03:52:12.4207236Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4207563Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4207841Z             self.sentinels.keys(),
2025-04-11T03:52:12.4208089Z             timeout=timeout,
2025-04-11T03:52:12.4208306Z         )
2025-04-11T03:52:12.4208486Z     
2025-04-11T03:52:12.4208678Z         error_index = None
2025-04-11T03:52:12.4208903Z         for sentinel in ready:
2025-04-11T03:52:12.4209165Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4209454Z             process = self.processes[index]
2025-04-11T03:52:12.4209723Z             process.join()
2025-04-11T03:52:12.4209970Z             if process.exitcode != 0:
2025-04-11T03:52:12.4210213Z                 error_index = index
2025-04-11T03:52:12.4210455Z                 break
2025-04-11T03:52:12.4210655Z     
2025-04-11T03:52:12.4210848Z         # Return if there was no error.
2025-04-11T03:52:12.4211100Z         if error_index is None:
2025-04-11T03:52:12.4211378Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4211681Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4211925Z     
2025-04-11T03:52:12.4212169Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4212478Z         for process in self.processes:
2025-04-11T03:52:12.4212726Z             if process.is_alive():
2025-04-11T03:52:12.4212980Z                 process.terminate()
2025-04-11T03:52:12.4213222Z             process.join()
2025-04-11T03:52:12.4213437Z     
2025-04-11T03:52:12.4213677Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4214002Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4214300Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4214603Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4214883Z             if exitcode < 0:
2025-04-11T03:52:12.4215294Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4215584Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4215923Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4216259Z                     error_index=error_index,
2025-04-11T03:52:12.4216539Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4216814Z                     exit_code=exitcode,
2025-04-11T03:52:12.4217074Z                     signal_name=name,
2025-04-11T03:52:12.4217307Z                 )
2025-04-11T03:52:12.4217507Z             else:
2025-04-11T03:52:12.4217733Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4218190Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4218529Z                     error_index=error_index,
2025-04-11T03:52:12.4218790Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4219058Z                     exit_code=exitcode,
2025-04-11T03:52:12.4219301Z                 )
2025-04-11T03:52:12.4219499Z     
2025-04-11T03:52:12.4219732Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4220118Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4220464Z         msg += original_trace
2025-04-11T03:52:12.4220802Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4221227Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4221556Z E       
2025-04-11T03:52:12.4221797Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4222104Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4222596Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4223075Z E           fn(i, *args)
2025-04-11T03:52:12.4223488Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 89, in run_dist
2025-04-11T03:52:12.4223966Z E           check_dataloader_sharding()
2025-04-11T03:52:12.4224450Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 69, in check_dataloader_sharding
2025-04-11T03:52:12.4224974Z E           batch = next(iter(train_dataloader))[0].cuda()
2025-04-11T03:52:12.4225277Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4225747Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4226247Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4226615Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4226855Z 
2025-04-11T03:52:12.4227171Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4227725Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4228114Z [04/11/25 03:41:21] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4228512Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4228829Z                              :75 launch                                         
2025-04-11T03:52:12.4229151Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4229483Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4229893Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4230314Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4230835Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:32320 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4231445Z ______________________________ test_gemini_plugin ______________________________
2025-04-11T03:52:12.4231653Z 
2025-04-11T03:52:12.4231760Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4232512Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4233179Z 
2025-04-11T03:52:12.4233293Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4233558Z         try_count = 0
2025-04-11T03:52:12.4233934Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4234197Z             max_try, int
2025-04-11T03:52:12.4234480Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4234764Z     
2025-04-11T03:52:12.4234999Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4235273Z             try:
2025-04-11T03:52:12.4235494Z                 try_count += 1
2025-04-11T03:52:12.4235745Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4235990Z                 return ret
2025-04-11T03:52:12.4236235Z             except exception_type as e:
2025-04-11T03:52:12.4236530Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4236900Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4237281Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4237612Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4237991Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4238314Z                     continue
2025-04-11T03:52:12.4238538Z                 else:
2025-04-11T03:52:12.4238887Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4239271Z >                   raise e
2025-04-11T03:52:12.4239414Z 
2025-04-11T03:52:12.4239512Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4239791Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4240125Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4240420Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4240756Z tests/test_booster/test_plugin/test_gemini_plugin.py:172: in test_gemini_plugin
2025-04-11T03:52:12.4241117Z     spawn(run_dist, 4, early_stop=early_stop)
2025-04-11T03:52:12.4241392Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4241673Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4242107Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4242621Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4243162Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4243606Z     while not context.join():
2025-04-11T03:52:12.4243869Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4244068Z 
2025-04-11T03:52:12.4244266Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0319c90>
2025-04-11T03:52:12.4244618Z timeout = None
2025-04-11T03:52:12.4244733Z 
2025-04-11T03:52:12.4244837Z     def join(self, timeout=None):
2025-04-11T03:52:12.4245128Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4245394Z     
2025-04-11T03:52:12.4245642Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4246007Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4246385Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4246833Z         of the first process exiting.
2025-04-11T03:52:12.4247062Z     
2025-04-11T03:52:12.4247314Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4247672Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4247955Z     
2025-04-11T03:52:12.4248139Z         Args:
2025-04-11T03:52:12.4248391Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4248686Z         """
2025-04-11T03:52:12.4248942Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4249249Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4249611Z             return True
2025-04-11T03:52:12.4249811Z     
2025-04-11T03:52:12.4250048Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4250372Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4250657Z             self.sentinels.keys(),
2025-04-11T03:52:12.4250919Z             timeout=timeout,
2025-04-11T03:52:12.4251143Z         )
2025-04-11T03:52:12.4251341Z     
2025-04-11T03:52:12.4251540Z         error_index = None
2025-04-11T03:52:12.4251770Z         for sentinel in ready:
2025-04-11T03:52:12.4252028Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4252301Z             process = self.processes[index]
2025-04-11T03:52:12.4252563Z             process.join()
2025-04-11T03:52:12.4252801Z             if process.exitcode != 0:
2025-04-11T03:52:12.4253056Z                 error_index = index
2025-04-11T03:52:12.4253293Z                 break
2025-04-11T03:52:12.4253485Z     
2025-04-11T03:52:12.4253679Z         # Return if there was no error.
2025-04-11T03:52:12.4253935Z         if error_index is None:
2025-04-11T03:52:12.4254258Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4254567Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4254801Z     
2025-04-11T03:52:12.4255044Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4255357Z         for process in self.processes:
2025-04-11T03:52:12.4255615Z             if process.is_alive():
2025-04-11T03:52:12.4255862Z                 process.terminate()
2025-04-11T03:52:12.4256096Z             process.join()
2025-04-11T03:52:12.4256311Z     
2025-04-11T03:52:12.4256550Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4256875Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4257174Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4257472Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4257757Z             if exitcode < 0:
2025-04-11T03:52:12.4258013Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4258298Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4258628Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4258944Z                     error_index=error_index,
2025-04-11T03:52:12.4259217Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4259485Z                     exit_code=exitcode,
2025-04-11T03:52:12.4259737Z                     signal_name=name,
2025-04-11T03:52:12.4259974Z                 )
2025-04-11T03:52:12.4260164Z             else:
2025-04-11T03:52:12.4260394Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4260736Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4261074Z                     error_index=error_index,
2025-04-11T03:52:12.4261342Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4261604Z                     exit_code=exitcode,
2025-04-11T03:52:12.4261839Z                 )
2025-04-11T03:52:12.4262031Z     
2025-04-11T03:52:12.4262275Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4262651Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4263131Z         msg += original_trace
2025-04-11T03:52:12.4263466Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4263886Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4264215Z E       
2025-04-11T03:52:12.4264469Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4264761Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4265236Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4265712Z E           fn(i, *args)
2025-04-11T03:52:12.4266267Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 167, in run_dist
2025-04-11T03:52:12.4266739Z E           check_gemini_plugin(early_stop=early_stop)
2025-04-11T03:52:12.4267187Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4267613Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4268023Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4268475Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4268882Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4269289Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4269554Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.4270022Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 149, in check_gemini_plugin
2025-04-11T03:52:12.4270603Z E           err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn, zero_size, tp_size)
2025-04-11T03:52:12.4271099Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4271501Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4271939Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4272380Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4272832Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4273296Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4273583Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4274047Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4274543Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4274912Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4275137Z 
2025-04-11T03:52:12.4275444Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4275981Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4276369Z [04/11/25 03:41:30] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4276731Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4277049Z                              :75 launch                                         
2025-04-11T03:52:12.4277374Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4277708Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.4278109Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4278525Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4279872Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4281344Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4282688Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4284158Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4285494Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4286809Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4288121Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4289441Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4290348Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4291172Z   warnings.warn(
2025-04-11T03:52:12.4291961Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4292801Z   warnings.warn(
2025-04-11T03:52:12.4293586Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4294406Z   warnings.warn(
2025-04-11T03:52:12.4295187Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4295995Z   warnings.warn(
2025-04-11T03:52:12.4296922Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4297879Z   warnings.warn(
2025-04-11T03:52:12.4298776Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4299833Z   warnings.warn(
2025-04-11T03:52:12.4300735Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4301670Z   warnings.warn(
2025-04-11T03:52:12.4302571Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4303626Z   warnings.warn(
2025-04-11T03:52:12.4304532Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4305532Z   warnings.warn(
2025-04-11T03:52:12.4306433Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4307376Z   warnings.warn(
2025-04-11T03:52:12.4308264Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4309250Z   warnings.warn(
2025-04-11T03:52:12.4310159Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4311092Z   warnings.warn(
2025-04-11T03:52:12.4311995Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4312938Z   warnings.warn(
2025-04-11T03:52:12.4313859Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4314805Z   warnings.warn(
2025-04-11T03:52:12.4315720Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4316677Z   warnings.warn(
2025-04-11T03:52:12.4317593Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4318643Z   warnings.warn(
2025-04-11T03:52:12.4319059Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:20466 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4319704Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:20466 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4320600Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4321302Z   warnings.warn(
2025-04-11T03:52:12.4321959Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4322766Z   warnings.warn(
2025-04-11T03:52:12.4323428Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4324120Z   warnings.warn(
2025-04-11T03:52:12.4324762Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4325457Z   warnings.warn(
2025-04-11T03:52:12.4326133Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4326823Z   warnings.warn(
2025-04-11T03:52:12.4327465Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4328143Z   warnings.warn(
2025-04-11T03:52:12.4328798Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4329481Z   warnings.warn(
2025-04-11T03:52:12.4330121Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4330794Z   warnings.warn(
2025-04-11T03:52:12.4331437Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4332113Z   warnings.warn(
2025-04-11T03:52:12.4332751Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4333430Z   warnings.warn(
2025-04-11T03:52:12.4334077Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4334757Z   warnings.warn(
2025-04-11T03:52:12.4335398Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4336078Z   warnings.warn(
2025-04-11T03:52:12.4336339Z __________________________ test_low_level_zero_plugin __________________________
2025-04-11T03:52:12.4336566Z 
2025-04-11T03:52:12.4336664Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4337425Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4338195Z 
2025-04-11T03:52:12.4338303Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4338564Z         try_count = 0
2025-04-11T03:52:12.4338799Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4339054Z             max_try, int
2025-04-11T03:52:12.4339344Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4339641Z     
2025-04-11T03:52:12.4339864Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4340249Z             try:
2025-04-11T03:52:12.4340456Z                 try_count += 1
2025-04-11T03:52:12.4340706Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4340970Z                 return ret
2025-04-11T03:52:12.4341217Z             except exception_type as e:
2025-04-11T03:52:12.4341519Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4341882Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4342278Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4342635Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4343018Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4343339Z                     continue
2025-04-11T03:52:12.4343568Z                 else:
2025-04-11T03:52:12.4343914Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4344302Z >                   raise e
2025-04-11T03:52:12.4344448Z 
2025-04-11T03:52:12.4344549Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4344834Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4345165Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4345456Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4345846Z tests/test_booster/test_plugin/test_low_level_zero_plugin.py:141: in test_low_level_zero_plugin
2025-04-11T03:52:12.4346265Z     spawn(run_dist, 2, early_stop=early_stop)
2025-04-11T03:52:12.4346550Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4346833Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4347269Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4347776Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4348318Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4348826Z     while not context.join():
2025-04-11T03:52:12.4349093Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4349296Z 
2025-04-11T03:52:12.4349512Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f031a590>
2025-04-11T03:52:12.4349867Z timeout = None
2025-04-11T03:52:12.4349982Z 
2025-04-11T03:52:12.4350092Z     def join(self, timeout=None):
2025-04-11T03:52:12.4350383Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4350661Z     
2025-04-11T03:52:12.4350919Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4351280Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4351656Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4351994Z         of the first process exiting.
2025-04-11T03:52:12.4352232Z     
2025-04-11T03:52:12.4352484Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4352841Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4353244Z     
2025-04-11T03:52:12.4353426Z         Args:
2025-04-11T03:52:12.4353691Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4353985Z         """
2025-04-11T03:52:12.4354238Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4354534Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4354785Z             return True
2025-04-11T03:52:12.4355001Z     
2025-04-11T03:52:12.4355242Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4355575Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4355858Z             self.sentinels.keys(),
2025-04-11T03:52:12.4356240Z             timeout=timeout,
2025-04-11T03:52:12.4356468Z         )
2025-04-11T03:52:12.4356651Z     
2025-04-11T03:52:12.4356841Z         error_index = None
2025-04-11T03:52:12.4357063Z         for sentinel in ready:
2025-04-11T03:52:12.4357323Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4357611Z             process = self.processes[index]
2025-04-11T03:52:12.4357879Z             process.join()
2025-04-11T03:52:12.4358120Z             if process.exitcode != 0:
2025-04-11T03:52:12.4358367Z                 error_index = index
2025-04-11T03:52:12.4358610Z                 break
2025-04-11T03:52:12.4358813Z     
2025-04-11T03:52:12.4359008Z         # Return if there was no error.
2025-04-11T03:52:12.4359260Z         if error_index is None:
2025-04-11T03:52:12.4359533Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4359838Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4360082Z     
2025-04-11T03:52:12.4360327Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4360639Z         for process in self.processes:
2025-04-11T03:52:12.4360893Z             if process.is_alive():
2025-04-11T03:52:12.4361143Z                 process.terminate()
2025-04-11T03:52:12.4361395Z             process.join()
2025-04-11T03:52:12.4361625Z     
2025-04-11T03:52:12.4361877Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4362202Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4362503Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4362814Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4363097Z             if exitcode < 0:
2025-04-11T03:52:12.4363350Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4363634Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4363963Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4364287Z                     error_index=error_index,
2025-04-11T03:52:12.4364564Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4364835Z                     exit_code=exitcode,
2025-04-11T03:52:12.4365077Z                     signal_name=name,
2025-04-11T03:52:12.4365339Z                 )
2025-04-11T03:52:12.4365544Z             else:
2025-04-11T03:52:12.4365801Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4366140Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4366468Z                     error_index=error_index,
2025-04-11T03:52:12.4366737Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4367002Z                     exit_code=exitcode,
2025-04-11T03:52:12.4367238Z                 )
2025-04-11T03:52:12.4367431Z     
2025-04-11T03:52:12.4367661Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4368045Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4368387Z         msg += original_trace
2025-04-11T03:52:12.4368709Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4369123Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4369559Z E       
2025-04-11T03:52:12.4369790Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4370086Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4370558Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4371013Z E           fn(i, *args)
2025-04-11T03:52:12.4371448Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 135, in run_dist
2025-04-11T03:52:12.4371934Z E           check_low_level_zero_plugin(early_stop=early_stop)
2025-04-11T03:52:12.4372522Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4372946Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4373437Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 84, in check_low_level_zero_plugin
2025-04-11T03:52:12.4373996Z E           err = run_fn(stage, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:12.4374434Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4374820Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4375255Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4375695Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4376168Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4376637Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4376923Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4377377Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4377874Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4378247Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4378484Z 
2025-04-11T03:52:12.4378793Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4379328Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4379715Z [04/11/25 03:41:37] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4380065Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4380387Z                              :75 launch                                         
2025-04-11T03:52:12.4380709Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4381056Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4381455Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4381874Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4383207Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4384567Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4385907Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4387342Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4388255Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4389129Z   warnings.warn(
2025-04-11T03:52:12.4389932Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4390906Z   warnings.warn(
2025-04-11T03:52:12.4391844Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4392801Z   warnings.warn(
2025-04-11T03:52:12.4393718Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4394682Z   warnings.warn(
2025-04-11T03:52:12.4395577Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4396519Z   warnings.warn(
2025-04-11T03:52:12.4397418Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4398361Z   warnings.warn(
2025-04-11T03:52:12.4399271Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4400213Z   warnings.warn(
2025-04-11T03:52:12.4401121Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4402059Z   warnings.warn(
2025-04-11T03:52:12.4402722Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4403427Z   warnings.warn(
2025-04-11T03:52:12.4404097Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4404792Z   warnings.warn(
2025-04-11T03:52:12.4405448Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4406310Z   warnings.warn(
2025-04-11T03:52:12.4406955Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4407627Z   warnings.warn(
2025-04-11T03:52:12.4407890Z ____________________________ test_torch_ddp_plugin _____________________________
2025-04-11T03:52:12.4408119Z 
2025-04-11T03:52:12.4408219Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4409001Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4409814Z 
2025-04-11T03:52:12.4409924Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4410205Z         try_count = 0
2025-04-11T03:52:12.4410460Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4410727Z             max_try, int
2025-04-11T03:52:12.4411026Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4411338Z     
2025-04-11T03:52:12.4411587Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4411884Z             try:
2025-04-11T03:52:12.4412089Z                 try_count += 1
2025-04-11T03:52:12.4412337Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4412612Z                 return ret
2025-04-11T03:52:12.4412885Z             except exception_type as e:
2025-04-11T03:52:12.4413165Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4413554Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4413955Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4414299Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4414809Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4415191Z                     continue
2025-04-11T03:52:12.4415430Z                 else:
2025-04-11T03:52:12.4415793Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4416177Z >                   raise e
2025-04-11T03:52:12.4416315Z 
2025-04-11T03:52:12.4416429Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4416710Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4417042Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4417357Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4417716Z tests/test_booster/test_plugin/test_torch_ddp_plugin.py:119: in test_torch_ddp_plugin
2025-04-11T03:52:12.4418092Z     spawn(run_dist, 2)
2025-04-11T03:52:12.4418334Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4418615Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4419068Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4419590Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4420139Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4420601Z     while not context.join():
2025-04-11T03:52:12.4420866Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4421063Z 
2025-04-11T03:52:12.4421268Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f031a680>
2025-04-11T03:52:12.4421628Z timeout = None
2025-04-11T03:52:12.4421755Z 
2025-04-11T03:52:12.4421855Z     def join(self, timeout=None):
2025-04-11T03:52:12.4422153Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4422431Z     
2025-04-11T03:52:12.4422796Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4423161Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4423544Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4423873Z         of the first process exiting.
2025-04-11T03:52:12.4424122Z     
2025-04-11T03:52:12.4424380Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4424758Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4425043Z     
2025-04-11T03:52:12.4425224Z         Args:
2025-04-11T03:52:12.4425598Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4425882Z         """
2025-04-11T03:52:12.4426133Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4426438Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4426690Z             return True
2025-04-11T03:52:12.4426903Z     
2025-04-11T03:52:12.4427129Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4427461Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4427754Z             self.sentinels.keys(),
2025-04-11T03:52:12.4428011Z             timeout=timeout,
2025-04-11T03:52:12.4428234Z         )
2025-04-11T03:52:12.4428405Z     
2025-04-11T03:52:12.4428640Z         error_index = None
2025-04-11T03:52:12.4428876Z         for sentinel in ready:
2025-04-11T03:52:12.4429134Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4429415Z             process = self.processes[index]
2025-04-11T03:52:12.4429675Z             process.join()
2025-04-11T03:52:12.4429915Z             if process.exitcode != 0:
2025-04-11T03:52:12.4430171Z                 error_index = index
2025-04-11T03:52:12.4430416Z                 break
2025-04-11T03:52:12.4430622Z     
2025-04-11T03:52:12.4430814Z         # Return if there was no error.
2025-04-11T03:52:12.4431079Z         if error_index is None:
2025-04-11T03:52:12.4431363Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4431670Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4431916Z     
2025-04-11T03:52:12.4432150Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4432469Z         for process in self.processes:
2025-04-11T03:52:12.4432732Z             if process.is_alive():
2025-04-11T03:52:12.4432983Z                 process.terminate()
2025-04-11T03:52:12.4433237Z             process.join()
2025-04-11T03:52:12.4433440Z     
2025-04-11T03:52:12.4433690Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4434034Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4434340Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4434646Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4434926Z             if exitcode < 0:
2025-04-11T03:52:12.4435182Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4435475Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4435801Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4436124Z                     error_index=error_index,
2025-04-11T03:52:12.4436386Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4436653Z                     exit_code=exitcode,
2025-04-11T03:52:12.4436903Z                     signal_name=name,
2025-04-11T03:52:12.4437134Z                 )
2025-04-11T03:52:12.4437330Z             else:
2025-04-11T03:52:12.4437547Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4437882Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4438210Z                     error_index=error_index,
2025-04-11T03:52:12.4438476Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4438873Z                     exit_code=exitcode,
2025-04-11T03:52:12.4439101Z                 )
2025-04-11T03:52:12.4439291Z     
2025-04-11T03:52:12.4439530Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4439910Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4440247Z         msg += original_trace
2025-04-11T03:52:12.4440564Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4440981Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4441306Z E       
2025-04-11T03:52:12.4441709Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4442010Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4442496Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4442975Z E           fn(i, *args)
2025-04-11T03:52:12.4443411Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 113, in run_dist
2025-04-11T03:52:12.4443888Z E           check_torch_ddp_plugin()
2025-04-11T03:52:12.4444383Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 52, in check_torch_ddp_plugin
2025-04-11T03:52:12.4444928Z E           run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:12.4445363Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4445763Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4446212Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4446670Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4447135Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4447601Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4447881Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4448353Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4448866Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4449239Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4449466Z 
2025-04-11T03:52:12.4449787Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4450331Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4450721Z [04/11/25 03:41:45] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4451074Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4451395Z                              :75 launch                                         
2025-04-11T03:52:12.4451729Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4452090Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4452488Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4452907Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4454262Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4455775Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4457143Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4458499Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4459457Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4460402Z   warnings.warn(
2025-04-11T03:52:12.4461253Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4462082Z   warnings.warn(
2025-04-11T03:52:12.4463014Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4463968Z   warnings.warn(
2025-04-11T03:52:12.4464867Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4465813Z   warnings.warn(
2025-04-11T03:52:12.4466720Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4467652Z   warnings.warn(
2025-04-11T03:52:12.4468595Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4469532Z   warnings.warn(
2025-04-11T03:52:12.4470434Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4471374Z   warnings.warn(
2025-04-11T03:52:12.4472276Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4473213Z   warnings.warn(
2025-04-11T03:52:12.4473628Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56766 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4474566Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4475388Z   warnings.warn(
2025-04-11T03:52:12.4476047Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4476736Z   warnings.warn(
2025-04-11T03:52:12.4477389Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4478073Z   warnings.warn(
2025-04-11T03:52:12.4478701Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4479521Z   warnings.warn(
2025-04-11T03:52:12.4479796Z ____________________________ test_torch_fsdp_plugin ____________________________
2025-04-11T03:52:12.4480029Z 
2025-04-11T03:52:12.4480148Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4480907Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4481558Z 
2025-04-11T03:52:12.4481677Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4481944Z         try_count = 0
2025-04-11T03:52:12.4482178Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4482448Z             max_try, int
2025-04-11T03:52:12.4482738Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4483054Z     
2025-04-11T03:52:12.4483286Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4483559Z             try:
2025-04-11T03:52:12.4483765Z                 try_count += 1
2025-04-11T03:52:12.4484016Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4484280Z                 return ret
2025-04-11T03:52:12.4484529Z             except exception_type as e:
2025-04-11T03:52:12.4484803Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4485164Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4485554Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4485903Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4486289Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4486621Z                     continue
2025-04-11T03:52:12.4486843Z                 else:
2025-04-11T03:52:12.4487193Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4487574Z >                   raise e
2025-04-11T03:52:12.4487718Z 
2025-04-11T03:52:12.4487825Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4488129Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4488456Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4488755Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4489118Z tests/test_booster/test_plugin/test_torch_fsdp_plugin.py:83: in test_torch_fsdp_plugin
2025-04-11T03:52:12.4526551Z     spawn(run_dist, 2)
2025-04-11T03:52:12.4526989Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4527303Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4527770Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4528351Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4528932Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4529417Z     while not context.join():
2025-04-11T03:52:12.4529962Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4530159Z 
2025-04-11T03:52:12.4530388Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0164850>
2025-04-11T03:52:12.4530778Z timeout = None
2025-04-11T03:52:12.4530916Z 
2025-04-11T03:52:12.4531017Z     def join(self, timeout=None):
2025-04-11T03:52:12.4531319Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4531600Z     
2025-04-11T03:52:12.4531868Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4532237Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4532814Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4533173Z         of the first process exiting.
2025-04-11T03:52:12.4533430Z     
2025-04-11T03:52:12.4533693Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4534066Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4534347Z     
2025-04-11T03:52:12.4534535Z         Args:
2025-04-11T03:52:12.4534804Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4535106Z         """
2025-04-11T03:52:12.4535365Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4535661Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4535912Z             return True
2025-04-11T03:52:12.4536122Z     
2025-04-11T03:52:12.4536357Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4536691Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4536972Z             self.sentinels.keys(),
2025-04-11T03:52:12.4537228Z             timeout=timeout,
2025-04-11T03:52:12.4537452Z         )
2025-04-11T03:52:12.4537634Z     
2025-04-11T03:52:12.4537813Z         error_index = None
2025-04-11T03:52:12.4538049Z         for sentinel in ready:
2025-04-11T03:52:12.4538314Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4538597Z             process = self.processes[index]
2025-04-11T03:52:12.4538861Z             process.join()
2025-04-11T03:52:12.4539095Z             if process.exitcode != 0:
2025-04-11T03:52:12.4539350Z                 error_index = index
2025-04-11T03:52:12.4539589Z                 break
2025-04-11T03:52:12.4539794Z     
2025-04-11T03:52:12.4539989Z         # Return if there was no error.
2025-04-11T03:52:12.4540234Z         if error_index is None:
2025-04-11T03:52:12.4540519Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4540830Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4541071Z     
2025-04-11T03:52:12.4541316Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4541624Z         for process in self.processes:
2025-04-11T03:52:12.4541882Z             if process.is_alive():
2025-04-11T03:52:12.4542137Z                 process.terminate()
2025-04-11T03:52:12.4542383Z             process.join()
2025-04-11T03:52:12.4542597Z     
2025-04-11T03:52:12.4542832Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4543160Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4543459Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4543769Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4544049Z             if exitcode < 0:
2025-04-11T03:52:12.4544293Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4544585Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4544918Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4545241Z                     error_index=error_index,
2025-04-11T03:52:12.4545515Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4545782Z                     exit_code=exitcode,
2025-04-11T03:52:12.4546165Z                     signal_name=name,
2025-04-11T03:52:12.4546412Z                 )
2025-04-11T03:52:12.4546617Z             else:
2025-04-11T03:52:12.4546851Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4547203Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4547544Z                     error_index=error_index,
2025-04-11T03:52:12.4547826Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4548147Z                     exit_code=exitcode,
2025-04-11T03:52:12.4548395Z                 )
2025-04-11T03:52:12.4548652Z     
2025-04-11T03:52:12.4549065Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4549458Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4549806Z         msg += original_trace
2025-04-11T03:52:12.4550148Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4550585Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4550900Z E       
2025-04-11T03:52:12.4551154Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4551459Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4551947Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4552410Z E           fn(i, *args)
2025-04-11T03:52:12.4552845Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 77, in run_dist
2025-04-11T03:52:12.4553330Z E           check_torch_fsdp_plugin()
2025-04-11T03:52:12.4553835Z E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 70, in check_torch_fsdp_plugin
2025-04-11T03:52:12.4554365Z E           run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:12.4554789Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4555197Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4555628Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4556062Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4556540Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4557008Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4557293Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4557764Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4558266Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4558628Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4558878Z 
2025-04-11T03:52:12.4559196Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4559733Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4560131Z [04/11/25 03:41:52] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4560496Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4560823Z                              :75 launch                                         
2025-04-11T03:52:12.4561139Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4561489Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4561786Z custom_hanging_param_model
2025-04-11T03:52:12.4562027Z custom_hanging_param_model
2025-04-11T03:52:12.4562371Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4562903Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4564253Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4565607Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4567066Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4568432Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4569365Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4570216Z   warnings.warn(
2025-04-11T03:52:12.4571033Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4571877Z   warnings.warn(
2025-04-11T03:52:12.4572824Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4573790Z   warnings.warn(
2025-04-11T03:52:12.4574702Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4575659Z   warnings.warn(
2025-04-11T03:52:12.4576578Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4577546Z   warnings.warn(
2025-04-11T03:52:12.4578464Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4579406Z   warnings.warn(
2025-04-11T03:52:12.4580308Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4581253Z   warnings.warn(
2025-04-11T03:52:12.4582160Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4583196Z   warnings.warn(
2025-04-11T03:52:12.4583853Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4584549Z   warnings.warn(
2025-04-11T03:52:12.4585195Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4585990Z   warnings.warn(
2025-04-11T03:52:12.4586661Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4587363Z   warnings.warn(
2025-04-11T03:52:12.4587998Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4588732Z   warnings.warn(
2025-04-11T03:52:12.4588992Z ______________________________ test_gemini_ckpIO _______________________________
2025-04-11T03:52:12.4589213Z 
2025-04-11T03:52:12.4589313Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4590065Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4590715Z 
2025-04-11T03:52:12.4590837Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4591095Z         try_count = 0
2025-04-11T03:52:12.4591326Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4591596Z             max_try, int
2025-04-11T03:52:12.4591891Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4592192Z     
2025-04-11T03:52:12.4592420Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4592678Z             try:
2025-04-11T03:52:12.4592893Z                 try_count += 1
2025-04-11T03:52:12.4593137Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4593389Z                 return ret
2025-04-11T03:52:12.4593628Z             except exception_type as e:
2025-04-11T03:52:12.4593890Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4594264Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4594649Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4594993Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4595376Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4595712Z                     continue
2025-04-11T03:52:12.4595932Z                 else:
2025-04-11T03:52:12.4596281Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4596665Z >                   raise e
2025-04-11T03:52:12.4596805Z 
2025-04-11T03:52:12.4596908Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4597186Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4597508Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4597798Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4598139Z tests/test_checkpoint_io/test_gemini_checkpoint_io.py:220: in test_gemini_ckpIO
2025-04-11T03:52:12.4598476Z     spawn(run_dist, 4)
2025-04-11T03:52:12.4598711Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4598986Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4599538Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4600062Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4600608Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4601077Z     while not context.join():
2025-04-11T03:52:12.4601354Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4601539Z 
2025-04-11T03:52:12.4601750Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f01a07c0>
2025-04-11T03:52:12.4602227Z timeout = None
2025-04-11T03:52:12.4602354Z 
2025-04-11T03:52:12.4602454Z     def join(self, timeout=None):
2025-04-11T03:52:12.4602748Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4603023Z     
2025-04-11T03:52:12.4603274Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4603630Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4604011Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4604339Z         of the first process exiting.
2025-04-11T03:52:12.4604581Z     
2025-04-11T03:52:12.4604855Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4605223Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4605499Z     
2025-04-11T03:52:12.4605677Z         Args:
2025-04-11T03:52:12.4605933Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4606226Z         """
2025-04-11T03:52:12.4606465Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4606762Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4607064Z             return True
2025-04-11T03:52:12.4607274Z     
2025-04-11T03:52:12.4607505Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4607829Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4608109Z             self.sentinels.keys(),
2025-04-11T03:52:12.4608357Z             timeout=timeout,
2025-04-11T03:52:12.4608575Z         )
2025-04-11T03:52:12.4608754Z     
2025-04-11T03:52:12.4608928Z         error_index = None
2025-04-11T03:52:12.4609156Z         for sentinel in ready:
2025-04-11T03:52:12.4609413Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4609696Z             process = self.processes[index]
2025-04-11T03:52:12.4609956Z             process.join()
2025-04-11T03:52:12.4610189Z             if process.exitcode != 0:
2025-04-11T03:52:12.4610442Z                 error_index = index
2025-04-11T03:52:12.4610678Z                 break
2025-04-11T03:52:12.4610874Z     
2025-04-11T03:52:12.4611066Z         # Return if there was no error.
2025-04-11T03:52:12.4611311Z         if error_index is None:
2025-04-11T03:52:12.4611600Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4611904Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4612145Z     
2025-04-11T03:52:12.4612381Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4612688Z         for process in self.processes:
2025-04-11T03:52:12.4612944Z             if process.is_alive():
2025-04-11T03:52:12.4613192Z                 process.terminate()
2025-04-11T03:52:12.4613434Z             process.join()
2025-04-11T03:52:12.4613644Z     
2025-04-11T03:52:12.4613889Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4614233Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4614535Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4614843Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4615122Z             if exitcode < 0:
2025-04-11T03:52:12.4615529Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4615820Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4616150Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4616476Z                     error_index=error_index,
2025-04-11T03:52:12.4616744Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4617011Z                     exit_code=exitcode,
2025-04-11T03:52:12.4617252Z                     signal_name=name,
2025-04-11T03:52:12.4617485Z                 )
2025-04-11T03:52:12.4617677Z             else:
2025-04-11T03:52:12.4617900Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4618352Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4618684Z                     error_index=error_index,
2025-04-11T03:52:12.4618949Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4619208Z                     exit_code=exitcode,
2025-04-11T03:52:12.4619446Z                 )
2025-04-11T03:52:12.4619633Z     
2025-04-11T03:52:12.4619858Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4620228Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4620561Z         msg += original_trace
2025-04-11T03:52:12.4620880Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4621286Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4621578Z E       
2025-04-11T03:52:12.4621813Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4622107Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4622582Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4623048Z E           fn(i, *args)
2025-04-11T03:52:12.4623450Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_checkpoint_io.py", line 212, in run_dist
2025-04-11T03:52:12.4623914Z E           exam_state_dict()
2025-04-11T03:52:12.4624275Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4624666Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4625094Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4625530Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4625976Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4626441Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4626728Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4627189Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4627696Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4628060Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4628283Z 
2025-04-11T03:52:12.4628640Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4629200Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4629590Z [04/11/25 03:42:01] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4629948Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4630275Z                              :75 launch                                         
2025-04-11T03:52:12.4630594Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4630922Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.4631434Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4631848Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4633192Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4634660Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4635985Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4637308Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4638628Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4639951Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4641257Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4642566Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4643467Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4644295Z   warnings.warn(
2025-04-11T03:52:12.4645071Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4645887Z   warnings.warn(
2025-04-11T03:52:12.4646665Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4647471Z   warnings.warn(
2025-04-11T03:52:12.4648246Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4649063Z   warnings.warn(
2025-04-11T03:52:12.4649997Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4651060Z   warnings.warn(
2025-04-11T03:52:12.4651956Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4652888Z   warnings.warn(
2025-04-11T03:52:12.4653787Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4654842Z   warnings.warn(
2025-04-11T03:52:12.4655751Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4656680Z   warnings.warn(
2025-04-11T03:52:12.4657617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4658561Z   warnings.warn(
2025-04-11T03:52:12.4659455Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4660380Z   warnings.warn(
2025-04-11T03:52:12.4661262Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4662184Z   warnings.warn(
2025-04-11T03:52:12.4663078Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4664009Z   warnings.warn(
2025-04-11T03:52:12.4664898Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4665824Z   warnings.warn(
2025-04-11T03:52:12.4666721Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4667654Z   warnings.warn(
2025-04-11T03:52:12.4668584Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4669525Z   warnings.warn(
2025-04-11T03:52:12.4670440Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4671485Z   warnings.warn(
2025-04-11T03:52:12.4671904Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:62155 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4672819Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4673665Z   warnings.warn(
2025-04-11T03:52:12.4674314Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4674996Z   warnings.warn(
2025-04-11T03:52:12.4675642Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4676360Z   warnings.warn(
2025-04-11T03:52:12.4677001Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4677676Z   warnings.warn(
2025-04-11T03:52:12.4678331Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4679021Z   warnings.warn(
2025-04-11T03:52:12.4679648Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4680320Z   warnings.warn(
2025-04-11T03:52:12.4680965Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4681640Z   warnings.warn(
2025-04-11T03:52:12.4682285Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4682962Z   warnings.warn(
2025-04-11T03:52:12.4683246Z Exception ignored in: <function GeminiDDP.__del__ at 0x7f8d36345ea0>
2025-04-11T03:52:12.4683571Z Traceback (most recent call last):
2025-04-11T03:52:12.4683978Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T03:52:12.4684384Z     self.remove_hooks()
2025-04-11T03:52:12.4684791Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T03:52:12.4685268Z     for p in self.module.parameters():
2025-04-11T03:52:12.4685752Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T03:52:12.4686278Z Exception ignored in: <function GeminiDDP.__del__ at 0x7f1d6e56df30>
2025-04-11T03:52:12.4686603Z Traceback (most recent call last):
2025-04-11T03:52:12.4686994Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T03:52:12.4687493Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T03:52:12.4687904Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T03:52:12.4688213Z     self.remove_hooks()
2025-04-11T03:52:12.4688578Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T03:52:12.4689116Z     for p in self.module.parameters():
2025-04-11T03:52:12.4689574Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T03:52:12.4690121Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T03:52:12.4690513Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T03:52:12.4690874Z Exception ignored in: <function GeminiDDP.__del__ at 0x7f9b14815ea0>
2025-04-11T03:52:12.4691191Z Traceback (most recent call last):
2025-04-11T03:52:12.4691575Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T03:52:12.4692085Z     self.remove_hooks()
2025-04-11T03:52:12.4692460Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T03:52:12.4692876Z     for p in self.module.parameters():
2025-04-11T03:52:12.4693337Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T03:52:12.4693888Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T03:52:12.4694319Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T03:52:12.4694675Z _____________________________ test_gemini_ckpIO[2] _____________________________
2025-04-11T03:52:12.4694880Z 
2025-04-11T03:52:12.4695007Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T03:52:12.4695791Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4696450Z 
2025-04-11T03:52:12.4696566Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4696816Z         try_count = 0
2025-04-11T03:52:12.4697046Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4697306Z             max_try, int
2025-04-11T03:52:12.4697588Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4697880Z     
2025-04-11T03:52:12.4698088Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4698344Z             try:
2025-04-11T03:52:12.4698544Z                 try_count += 1
2025-04-11T03:52:12.4698783Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4699026Z                 return ret
2025-04-11T03:52:12.4699251Z             except exception_type as e:
2025-04-11T03:52:12.4699517Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4699877Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4700255Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4700591Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4700959Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4701270Z                     continue
2025-04-11T03:52:12.4701487Z                 else:
2025-04-11T03:52:12.4701828Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4702201Z >                   raise e
2025-04-11T03:52:12.4702333Z 
2025-04-11T03:52:12.4702438Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4702706Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4703033Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4703330Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4703709Z tests/test_checkpoint_io/test_gemini_torch_compability.py:175: in test_gemini_ckpIO
2025-04-11T03:52:12.4704075Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.4704322Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4704713Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4705140Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4705644Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4706175Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4706623Z     while not context.join():
2025-04-11T03:52:12.4706872Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4707058Z 
2025-04-11T03:52:12.4707260Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0020f70>
2025-04-11T03:52:12.4707786Z timeout = None
2025-04-11T03:52:12.4707904Z 
2025-04-11T03:52:12.4708006Z     def join(self, timeout=None):
2025-04-11T03:52:12.4708293Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4708590Z     
2025-04-11T03:52:12.4708844Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4709207Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4709598Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4709942Z         of the first process exiting.
2025-04-11T03:52:12.4710180Z     
2025-04-11T03:52:12.4710422Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4710787Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4711069Z     
2025-04-11T03:52:12.4711250Z         Args:
2025-04-11T03:52:12.4711503Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4711790Z         """
2025-04-11T03:52:12.4712035Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4712365Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4712611Z             return True
2025-04-11T03:52:12.4712826Z     
2025-04-11T03:52:12.4713053Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4713373Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4713654Z             self.sentinels.keys(),
2025-04-11T03:52:12.4713899Z             timeout=timeout,
2025-04-11T03:52:12.4714116Z         )
2025-04-11T03:52:12.4714288Z     
2025-04-11T03:52:12.4714472Z         error_index = None
2025-04-11T03:52:12.4714699Z         for sentinel in ready:
2025-04-11T03:52:12.4714960Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4715233Z             process = self.processes[index]
2025-04-11T03:52:12.4715498Z             process.join()
2025-04-11T03:52:12.4715737Z             if process.exitcode != 0:
2025-04-11T03:52:12.4715989Z                 error_index = index
2025-04-11T03:52:12.4716222Z                 break
2025-04-11T03:52:12.4716412Z     
2025-04-11T03:52:12.4716607Z         # Return if there was no error.
2025-04-11T03:52:12.4716863Z         if error_index is None:
2025-04-11T03:52:12.4717144Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4717446Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4717679Z     
2025-04-11T03:52:12.4717920Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4718227Z         for process in self.processes:
2025-04-11T03:52:12.4718483Z             if process.is_alive():
2025-04-11T03:52:12.4718730Z                 process.terminate()
2025-04-11T03:52:12.4718962Z             process.join()
2025-04-11T03:52:12.4719174Z     
2025-04-11T03:52:12.4719416Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4719753Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4720052Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4720351Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4720630Z             if exitcode < 0:
2025-04-11T03:52:12.4721008Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4721296Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4721644Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4721969Z                     error_index=error_index,
2025-04-11T03:52:12.4722240Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4722508Z                     exit_code=exitcode,
2025-04-11T03:52:12.4722762Z                     signal_name=name,
2025-04-11T03:52:12.4722993Z                 )
2025-04-11T03:52:12.4723179Z             else:
2025-04-11T03:52:12.4723531Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4723866Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4724200Z                     error_index=error_index,
2025-04-11T03:52:12.4724463Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4724730Z                     exit_code=exitcode,
2025-04-11T03:52:12.4724961Z                 )
2025-04-11T03:52:12.4725157Z     
2025-04-11T03:52:12.4725395Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4725773Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4726110Z         msg += original_trace
2025-04-11T03:52:12.4726428Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4726843Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4727161Z E       
2025-04-11T03:52:12.4727400Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4727702Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4728167Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4728637Z E           fn(i, *args)
2025-04-11T03:52:12.4729068Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_torch_compability.py", line 167, in run_dist
2025-04-11T03:52:12.4729537Z E           exam_torch_load_from_gemini()
2025-04-11T03:52:12.4729929Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4730326Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4730759Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4731223Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4731678Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4732136Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4732419Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4732874Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4733387Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4733758Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4733990Z 
2025-04-11T03:52:12.4734291Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4734837Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4735227Z [04/11/25 03:42:09] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4735584Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4735892Z                              :75 launch                                         
2025-04-11T03:52:12.4736213Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4736549Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4737061Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4737478Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4738841Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4740340Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4741679Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4743039Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4743955Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4744813Z   warnings.warn(
2025-04-11T03:52:12.4745629Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4746457Z   warnings.warn(
2025-04-11T03:52:12.4747404Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4748352Z   warnings.warn(
2025-04-11T03:52:12.4749313Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4750256Z   warnings.warn(
2025-04-11T03:52:12.4751150Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4752094Z   warnings.warn(
2025-04-11T03:52:12.4752993Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4753927Z   warnings.warn(
2025-04-11T03:52:12.4754828Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4755771Z   warnings.warn(
2025-04-11T03:52:12.4756683Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4757738Z   warnings.warn(
2025-04-11T03:52:12.4758151Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60258 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4759048Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4759856Z   warnings.warn(
2025-04-11T03:52:12.4760499Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4761182Z   warnings.warn(
2025-04-11T03:52:12.4761828Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4762519Z   warnings.warn(
2025-04-11T03:52:12.4763154Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4763825Z   warnings.warn(
2025-04-11T03:52:12.4764099Z Exception ignored in: <function GeminiDDP.__del__ at 0x7fc11587d750>
2025-04-11T03:52:12.4764434Z Traceback (most recent call last):
2025-04-11T03:52:12.4764840Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T03:52:12.4765234Z     self.remove_hooks()
2025-04-11T03:52:12.4765616Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T03:52:12.4766043Z     for p in self.module.parameters():
2025-04-11T03:52:12.4766552Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T03:52:12.4767109Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T03:52:12.4767530Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T03:52:12.4767898Z __________________________ test_unsharded_checkpoint ___________________________
2025-04-11T03:52:12.4768116Z 
2025-04-11T03:52:12.4768213Z args = (), kwargs = {}
2025-04-11T03:52:12.4768344Z 
2025-04-11T03:52:12.4768445Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.4768704Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.4768979Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.4769277Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.4769575Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.4769842Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.4770006Z 
2025-04-11T03:52:12.4770106Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.4770378Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4770729Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.4771058Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.4771332Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4771511Z 
2025-04-11T03:52:12.4771592Z device = None
2025-04-11T03:52:12.4771714Z 
2025-04-11T03:52:12.4771838Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.4772182Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.4772488Z     
2025-04-11T03:52:12.4772666Z         Args:
2025-04-11T03:52:12.4772944Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.4773472Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.4773825Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.4774070Z         """
2025-04-11T03:52:12.4774261Z         _lazy_init()
2025-04-11T03:52:12.4774481Z         with torch.cuda.device(device):
2025-04-11T03:52:12.4774741Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4775023Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4775509Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4776108Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4776479Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4776702Z 
2025-04-11T03:52:12.4776949Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.4777413Z ___________________ test_sharded_model_checkpoint[True-True] ___________________
2025-04-11T03:52:12.4777645Z 
2025-04-11T03:52:12.4777749Z use_safetensors = True, use_async = True
2025-04-11T03:52:12.4777925Z 
2025-04-11T03:52:12.4778076Z     @pytest.mark.parametrize("use_safetensors", [True, False])
2025-04-11T03:52:12.4778418Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T03:52:12.4778784Z     def test_sharded_model_checkpoint(use_safetensors: bool, use_async: bool):
2025-04-11T03:52:12.4779123Z         # create a model and optimizer
2025-04-11T03:52:12.4779369Z         model = resnet18()
2025-04-11T03:52:12.4779631Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T03:52:12.4779913Z         # create test data sample
2025-04-11T03:52:12.4780159Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T03:52:12.4780395Z     
2025-04-11T03:52:12.4780574Z         # run fwd and bwd
2025-04-11T03:52:12.4780794Z         y = model(x)
2025-04-11T03:52:12.4780997Z         loss = y.sum()
2025-04-11T03:52:12.4781208Z         loss.backward()
2025-04-11T03:52:12.4781423Z         optimizer.step()
2025-04-11T03:52:12.4781637Z     
2025-04-11T03:52:12.4781871Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4782215Z         optimizer_ckpt_tempfile = tempfile.NamedTemporaryFile()
2025-04-11T03:52:12.4782497Z     
2025-04-11T03:52:12.4782682Z         # save the model and optimizer
2025-04-11T03:52:12.4782944Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T03:52:12.4783185Z     
2025-04-11T03:52:12.4783372Z >       ckpt_io.save_model(
2025-04-11T03:52:12.4783596Z             model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=use_safetensors, use_async=use_async
2025-04-11T03:52:12.4783682Z         )
2025-04-11T03:52:12.4783687Z 
2025-04-11T03:52:12.4783837Z tests/test_checkpoint_io/test_general_checkpoint_io.py:96: 
2025-04-11T03:52:12.4783959Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4784128Z colossalai/checkpoint_io/checkpoint_io_base.py:190: in save_model
2025-04-11T03:52:12.4784220Z     self.save_sharded_model(
2025-04-11T03:52:12.4784418Z colossalai/checkpoint_io/general_checkpoint_io.py:238: in save_sharded_model
2025-04-11T03:52:12.4784610Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T03:52:12.4784806Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T03:52:12.4784956Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T03:52:12.4785142Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T03:52:12.4785385Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4785629Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T03:52:12.4785908Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4786150Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T03:52:12.4786289Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4786413Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T03:52:12.4786644Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4786760Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4786764Z 
2025-04-11T03:52:12.4786929Z tensor = tensor([[[[ 1.1998e-02, -1.2170e-02,  1.5254e-02,  ..., -4.4274e-02,
2025-04-11T03:52:12.4787134Z            -2.0488e-02,  1.2153e-02],
2025-04-11T03:52:12.4787219Z           [...709e-02],
2025-04-11T03:52:12.4787356Z           [-6.2823e-03,  8.1821e-03, -1.6644e-02,  ..., -2.5185e-02,
2025-04-11T03:52:12.4787450Z            -1.2198e-02,  9.3967e-05]]]])
2025-04-11T03:52:12.4787540Z empty = True
2025-04-11T03:52:12.4787544Z 
2025-04-11T03:52:12.4787725Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4787807Z         if empty:
2025-04-11T03:52:12.4787970Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4788079Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4788371Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4788580Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4788753Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4788758Z 
2025-04-11T03:52:12.4788880Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T03:52:12.4789047Z __________________ test_sharded_model_checkpoint[True-False] ___________________
2025-04-11T03:52:12.4789055Z 
2025-04-11T03:52:12.4789161Z use_safetensors = False, use_async = True
2025-04-11T03:52:12.4789166Z 
2025-04-11T03:52:12.4789313Z     @pytest.mark.parametrize("use_safetensors", [True, False])
2025-04-11T03:52:12.4789453Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T03:52:12.4789630Z     def test_sharded_model_checkpoint(use_safetensors: bool, use_async: bool):
2025-04-11T03:52:12.4789736Z         # create a model and optimizer
2025-04-11T03:52:12.4789826Z         model = resnet18()
2025-04-11T03:52:12.4789956Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T03:52:12.4790052Z         # create test data sample
2025-04-11T03:52:12.4790151Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T03:52:12.4790234Z     
2025-04-11T03:52:12.4790320Z         # run fwd and bwd
2025-04-11T03:52:12.4790409Z         y = model(x)
2025-04-11T03:52:12.4790492Z         loss = y.sum()
2025-04-11T03:52:12.4790576Z         loss.backward()
2025-04-11T03:52:12.4790672Z         optimizer.step()
2025-04-11T03:52:12.4790750Z     
2025-04-11T03:52:12.4790883Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4791029Z         optimizer_ckpt_tempfile = tempfile.NamedTemporaryFile()
2025-04-11T03:52:12.4791106Z     
2025-04-11T03:52:12.4791200Z         # save the model and optimizer
2025-04-11T03:52:12.4791297Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T03:52:12.4791381Z     
2025-04-11T03:52:12.4791470Z >       ckpt_io.save_model(
2025-04-11T03:52:12.4791697Z             model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=use_safetensors, use_async=use_async
2025-04-11T03:52:12.4791776Z         )
2025-04-11T03:52:12.4791788Z 
2025-04-11T03:52:12.4791937Z tests/test_checkpoint_io/test_general_checkpoint_io.py:96: 
2025-04-11T03:52:12.4792057Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4792223Z colossalai/checkpoint_io/checkpoint_io_base.py:190: in save_model
2025-04-11T03:52:12.4792458Z     self.save_sharded_model(
2025-04-11T03:52:12.4792651Z colossalai/checkpoint_io/general_checkpoint_io.py:238: in save_sharded_model
2025-04-11T03:52:12.4792840Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T03:52:12.4793024Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T03:52:12.4793168Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T03:52:12.4793343Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T03:52:12.4793576Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4793964Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T03:52:12.4794103Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4794351Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T03:52:12.4794490Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4794625Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T03:52:12.4794853Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4794970Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4794975Z 
2025-04-11T03:52:12.4795136Z tensor = tensor([[[[ 5.6167e-02,  4.7108e-03,  1.3328e-02,  ...,  1.3376e-02,
2025-04-11T03:52:12.4795229Z             3.0058e-02,  2.0246e-02],
2025-04-11T03:52:12.4795319Z           [...166e-03],
2025-04-11T03:52:12.4795453Z           [ 1.1236e-02, -3.2021e-02, -6.4110e-05,  ..., -7.2526e-03,
2025-04-11T03:52:12.4795550Z             2.9210e-03,  1.7852e-02]]]])
2025-04-11T03:52:12.4795631Z empty = True
2025-04-11T03:52:12.4795636Z 
2025-04-11T03:52:12.4795811Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4795904Z         if empty:
2025-04-11T03:52:12.4796059Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4796177Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4796465Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4796619Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4796780Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4796785Z 
2025-04-11T03:52:12.4796909Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T03:52:12.4797074Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4797241Z [04/11/25 03:42:12] WARNING  colossalai - colossalai - WARNING:                 
2025-04-11T03:52:12.4797379Z                              /__w/ColossalAI/ColossalAI/colossalai/checkpoint_io
2025-04-11T03:52:12.4797507Z                              /checkpoint_io_base.py:183 save_model              
2025-04-11T03:52:12.4797662Z                     WARNING  colossalai - colossalai - WARNING: Async save is   
2025-04-11T03:52:12.4797790Z                              only supported when use_safetensors is set to True.
2025-04-11T03:52:12.4797928Z                              Setting use_safetensors to True for async save.    
2025-04-11T03:52:12.4798084Z ___________________ test_sharded_optimizer_checkpoint[False] ___________________
2025-04-11T03:52:12.4798088Z 
2025-04-11T03:52:12.4798173Z use_async = False
2025-04-11T03:52:12.4798178Z 
2025-04-11T03:52:12.4798326Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T03:52:12.4798460Z     def test_sharded_optimizer_checkpoint(use_async: bool):
2025-04-11T03:52:12.4798562Z         # create a model and optimizer
2025-04-11T03:52:12.4798651Z         model = resnet18()
2025-04-11T03:52:12.4798781Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T03:52:12.4798966Z     
2025-04-11T03:52:12.4799064Z         # create test data sample
2025-04-11T03:52:12.4799167Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T03:52:12.4799242Z     
2025-04-11T03:52:12.4799334Z         # run fwd and bwd
2025-04-11T03:52:12.4799415Z         y = model(x)
2025-04-11T03:52:12.4799499Z         loss = y.sum()
2025-04-11T03:52:12.4799596Z         loss.backward()
2025-04-11T03:52:12.4799686Z         optimizer.step()
2025-04-11T03:52:12.4799765Z     
2025-04-11T03:52:12.4799874Z         # create temp directories for checkpoint
2025-04-11T03:52:12.4800001Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4800234Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4800308Z     
2025-04-11T03:52:12.4800409Z         # save the model and optimizer
2025-04-11T03:52:12.4800508Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T03:52:12.4800586Z     
2025-04-11T03:52:12.4800799Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T03:52:12.4801071Z         ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T03:52:12.4801158Z     
2025-04-11T03:52:12.4801248Z         ckpt_io._sync_d2h()
2025-04-11T03:52:12.4801349Z         ckpt_io._sync_io()
2025-04-11T03:52:12.4801424Z     
2025-04-11T03:52:12.4801507Z         # create new model
2025-04-11T03:52:12.4801603Z         new_model = resnet18()
2025-04-11T03:52:12.4801740Z         new_optimizer = Adam(new_model.parameters(), lr=0.001)
2025-04-11T03:52:12.4801820Z     
2025-04-11T03:52:12.4801979Z         ckpt_io.load_model(new_model, str(model_ckpt_dir.name), strict=True)
2025-04-11T03:52:12.4802151Z >       ckpt_io.load_optimizer(new_optimizer, str(optimizer_ckpt_dir.name))
2025-04-11T03:52:12.4802157Z 
2025-04-11T03:52:12.4802308Z tests/test_checkpoint_io/test_general_checkpoint_io.py:149: 
2025-04-11T03:52:12.4802421Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4802608Z colossalai/checkpoint_io/checkpoint_io_base.py:224: in load_optimizer
2025-04-11T03:52:12.4802706Z     self.load_sharded_optimizer(
2025-04-11T03:52:12.4802909Z colossalai/checkpoint_io/general_checkpoint_io.py:100: in load_sharded_optimizer
2025-04-11T03:52:12.4803051Z     load_states_into_optimizer(optimizer, state_dict, id_map)
2025-04-11T03:52:12.4803229Z colossalai/checkpoint_io/utils.py:830: in load_states_into_optimizer
2025-04-11T03:52:12.4803327Z     get_accelerator().synchronize()
2025-04-11T03:52:12.4803487Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.4803596Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.4803708Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4803712Z 
2025-04-11T03:52:12.4803799Z device = None
2025-04-11T03:52:12.4803803Z 
2025-04-11T03:52:12.4803925Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.4804090Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.4804163Z     
2025-04-11T03:52:12.4804240Z         Args:
2025-04-11T03:52:12.4804420Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.4804592Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.4804713Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.4804790Z         """
2025-04-11T03:52:12.4804882Z         _lazy_init()
2025-04-11T03:52:12.4804979Z         with torch.cuda.device(device):
2025-04-11T03:52:12.4805089Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4805203Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4805491Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4805750Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4805913Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4805918Z 
2025-04-11T03:52:12.4806173Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.4806329Z ___________________ test_sharded_optimizer_checkpoint[True] ____________________
2025-04-11T03:52:12.4806334Z 
2025-04-11T03:52:12.4806418Z use_async = True
2025-04-11T03:52:12.4806428Z 
2025-04-11T03:52:12.4806561Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T03:52:12.4806695Z     def test_sharded_optimizer_checkpoint(use_async: bool):
2025-04-11T03:52:12.4806893Z         # create a model and optimizer
2025-04-11T03:52:12.4806984Z         model = resnet18()
2025-04-11T03:52:12.4807110Z         optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T03:52:12.4807187Z     
2025-04-11T03:52:12.4807285Z         # create test data sample
2025-04-11T03:52:12.4807387Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T03:52:12.4807472Z     
2025-04-11T03:52:12.4807571Z         # run fwd and bwd
2025-04-11T03:52:12.4807653Z         y = model(x)
2025-04-11T03:52:12.4807736Z         loss = y.sum()
2025-04-11T03:52:12.4807830Z         loss.backward()
2025-04-11T03:52:12.4807921Z         optimizer.step()
2025-04-11T03:52:12.4808010Z     
2025-04-11T03:52:12.4808156Z         # create temp directories for checkpoint
2025-04-11T03:52:12.4808290Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4808423Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4808500Z     
2025-04-11T03:52:12.4808603Z         # save the model and optimizer
2025-04-11T03:52:12.4808699Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T03:52:12.4808784Z     
2025-04-11T03:52:12.4808986Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T03:52:12.4809251Z >       ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T03:52:12.4809264Z 
2025-04-11T03:52:12.4809416Z tests/test_checkpoint_io/test_general_checkpoint_io.py:139: 
2025-04-11T03:52:12.4809531Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4809708Z colossalai/checkpoint_io/checkpoint_io_base.py:258: in save_optimizer
2025-04-11T03:52:12.4809807Z     self.save_sharded_optimizer(
2025-04-11T03:52:12.4810006Z colossalai/checkpoint_io/general_checkpoint_io.py:144: in save_sharded_optimizer
2025-04-11T03:52:12.4810202Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T03:52:12.4810395Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T03:52:12.4810549Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T03:52:12.4810712Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T03:52:12.4810964Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4811199Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T03:52:12.4811342Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4811581Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T03:52:12.4811717Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4811839Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T03:52:12.4812064Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4812189Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4812194Z 
2025-04-11T03:52:12.4812293Z tensor = tensor(1.), empty = True
2025-04-11T03:52:12.4812299Z 
2025-04-11T03:52:12.4812582Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4812663Z         if empty:
2025-04-11T03:52:12.4812823Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4812933Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4813221Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4813366Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4813528Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4813625Z 
2025-04-11T03:52:12.4813756Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T03:52:12.4813925Z _____________ test_sharded_optimizer_multiple_param_groups[False] ______________
2025-04-11T03:52:12.4813930Z 
2025-04-11T03:52:12.4814020Z use_async = False
2025-04-11T03:52:12.4814028Z 
2025-04-11T03:52:12.4814162Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T03:52:12.4814333Z     def test_sharded_optimizer_multiple_param_groups(use_async: bool):
2025-04-11T03:52:12.4814431Z         # create a model and optimizer
2025-04-11T03:52:12.4814519Z         model = resnet18()
2025-04-11T03:52:12.4814613Z         optimizer = Adam(
2025-04-11T03:52:12.4814840Z             [{"params": model.layer1.parameters()}, {"params": model.layer2.parameters(), "lr": 0.002}], lr=0.001
2025-04-11T03:52:12.4814925Z         )
2025-04-11T03:52:12.4815000Z     
2025-04-11T03:52:12.4815098Z         # create test data sample
2025-04-11T03:52:12.4815190Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T03:52:12.4815267Z     
2025-04-11T03:52:12.4815361Z         # run fwd and bwd
2025-04-11T03:52:12.4815443Z         y = model(x)
2025-04-11T03:52:12.4815529Z         loss = y.sum()
2025-04-11T03:52:12.4815614Z         loss.backward()
2025-04-11T03:52:12.4815702Z         optimizer.step()
2025-04-11T03:52:12.4815788Z     
2025-04-11T03:52:12.4815895Z         # create temp directories for checkpoint
2025-04-11T03:52:12.4816027Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4816162Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4816235Z     
2025-04-11T03:52:12.4816336Z         # save the model and optimizer
2025-04-11T03:52:12.4816434Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T03:52:12.4816513Z     
2025-04-11T03:52:12.4816715Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T03:52:12.4816978Z         ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T03:52:12.4817056Z     
2025-04-11T03:52:12.4817144Z         ckpt_io._sync_d2h()
2025-04-11T03:52:12.4817236Z         ckpt_io._sync_io()
2025-04-11T03:52:12.4817311Z     
2025-04-11T03:52:12.4817401Z         # create new model
2025-04-11T03:52:12.4817494Z         new_model = resnet18()
2025-04-11T03:52:12.4817586Z         new_optimizer = Adam(
2025-04-11T03:52:12.4817838Z             [{"params": new_model.layer1.parameters()}, {"params": new_model.layer2.parameters(), "lr": 0.002}], lr=0.001
2025-04-11T03:52:12.4817917Z         )
2025-04-11T03:52:12.4817996Z     
2025-04-11T03:52:12.4818154Z         ckpt_io.load_model(new_model, str(model_ckpt_dir.name), strict=True)
2025-04-11T03:52:12.4818321Z >       ckpt_io.load_optimizer(new_optimizer, str(optimizer_ckpt_dir.name))
2025-04-11T03:52:12.4818326Z 
2025-04-11T03:52:12.4818478Z tests/test_checkpoint_io/test_general_checkpoint_io.py:222: 
2025-04-11T03:52:12.4818592Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4818777Z colossalai/checkpoint_io/checkpoint_io_base.py:224: in load_optimizer
2025-04-11T03:52:12.4818875Z     self.load_sharded_optimizer(
2025-04-11T03:52:12.4819085Z colossalai/checkpoint_io/general_checkpoint_io.py:100: in load_sharded_optimizer
2025-04-11T03:52:12.4819362Z     load_states_into_optimizer(optimizer, state_dict, id_map)
2025-04-11T03:52:12.4819539Z colossalai/checkpoint_io/utils.py:830: in load_states_into_optimizer
2025-04-11T03:52:12.4819640Z     get_accelerator().synchronize()
2025-04-11T03:52:12.4819797Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.4819902Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.4820012Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4820017Z 
2025-04-11T03:52:12.4820104Z device = None
2025-04-11T03:52:12.4820109Z 
2025-04-11T03:52:12.4820231Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.4820502Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.4820578Z     
2025-04-11T03:52:12.4820655Z         Args:
2025-04-11T03:52:12.4820839Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.4821015Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.4821135Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.4821211Z         """
2025-04-11T03:52:12.4821300Z         _lazy_init()
2025-04-11T03:52:12.4821400Z         with torch.cuda.device(device):
2025-04-11T03:52:12.4821505Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4821622Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4821911Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4822060Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4822222Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4822226Z 
2025-04-11T03:52:12.4822476Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.4822649Z ______________ test_sharded_optimizer_multiple_param_groups[True] ______________
2025-04-11T03:52:12.4822653Z 
2025-04-11T03:52:12.4822741Z use_async = True
2025-04-11T03:52:12.4822746Z 
2025-04-11T03:52:12.4822878Z     @pytest.mark.parametrize("use_async", [False, True])
2025-04-11T03:52:12.4823039Z     def test_sharded_optimizer_multiple_param_groups(use_async: bool):
2025-04-11T03:52:12.4823140Z         # create a model and optimizer
2025-04-11T03:52:12.4823230Z         model = resnet18()
2025-04-11T03:52:12.4823324Z         optimizer = Adam(
2025-04-11T03:52:12.4823550Z             [{"params": model.layer1.parameters()}, {"params": model.layer2.parameters(), "lr": 0.002}], lr=0.001
2025-04-11T03:52:12.4823631Z         )
2025-04-11T03:52:12.4823715Z     
2025-04-11T03:52:12.4823807Z         # create test data sample
2025-04-11T03:52:12.4823909Z         x = torch.randn(1, 3, 224, 224)
2025-04-11T03:52:12.4823983Z     
2025-04-11T03:52:12.4824073Z         # run fwd and bwd
2025-04-11T03:52:12.4824156Z         y = model(x)
2025-04-11T03:52:12.4824237Z         loss = y.sum()
2025-04-11T03:52:12.4824329Z         loss.backward()
2025-04-11T03:52:12.4824419Z         optimizer.step()
2025-04-11T03:52:12.4824493Z     
2025-04-11T03:52:12.4824600Z         # create temp directories for checkpoint
2025-04-11T03:52:12.4824724Z         model_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4824867Z         optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T03:52:12.4824941Z     
2025-04-11T03:52:12.4825039Z         # save the model and optimizer
2025-04-11T03:52:12.4825137Z         ckpt_io = GeneralCheckpointIO()
2025-04-11T03:52:12.4825212Z     
2025-04-11T03:52:12.4825420Z         ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
2025-04-11T03:52:12.4825675Z >       ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T03:52:12.4825680Z 
2025-04-11T03:52:12.4825957Z tests/test_checkpoint_io/test_general_checkpoint_io.py:210: 
2025-04-11T03:52:12.4826073Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4826256Z colossalai/checkpoint_io/checkpoint_io_base.py:258: in save_optimizer
2025-04-11T03:52:12.4826356Z     self.save_sharded_optimizer(
2025-04-11T03:52:12.4826561Z colossalai/checkpoint_io/general_checkpoint_io.py:144: in save_sharded_optimizer
2025-04-11T03:52:12.4826751Z     total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
2025-04-11T03:52:12.4826940Z colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
2025-04-11T03:52:12.4827185Z     sub_pinned_state_dict = create_pinned_state_dict(state_dict)
2025-04-11T03:52:12.4827347Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T03:52:12.4827587Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4827825Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T03:52:12.4827975Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4828215Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T03:52:12.4828350Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4828534Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T03:52:12.4828775Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4828904Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4828912Z 
2025-04-11T03:52:12.4829009Z tensor = tensor(1.), empty = True
2025-04-11T03:52:12.4829014Z 
2025-04-11T03:52:12.4829208Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4829288Z         if empty:
2025-04-11T03:52:12.4829454Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4829569Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4829853Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4830001Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4830165Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4830169Z 
2025-04-11T03:52:12.4830305Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T03:52:12.4830451Z _____________________________ test_hybrid_ckpIO[4] _____________________________
2025-04-11T03:52:12.4830458Z 
2025-04-11T03:52:12.4830586Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.4831203Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4831213Z 
2025-04-11T03:52:12.4831333Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4831418Z         try_count = 0
2025-04-11T03:52:12.4831526Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4831618Z             max_try, int
2025-04-11T03:52:12.4831768Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4831849Z     
2025-04-11T03:52:12.4831967Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4832051Z             try:
2025-04-11T03:52:12.4832141Z                 try_count += 1
2025-04-11T03:52:12.4832237Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4832328Z                 return ret
2025-04-11T03:52:12.4832425Z             except exception_type as e:
2025-04-11T03:52:12.4832537Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4832850Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4832970Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4833123Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4833275Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4833363Z                     continue
2025-04-11T03:52:12.4833444Z                 else:
2025-04-11T03:52:12.4833676Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4833860Z >                   raise e
2025-04-11T03:52:12.4833865Z 
2025-04-11T03:52:12.4833963Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4834086Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4834221Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4834323Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4834563Z tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py:155: in test_hybrid_ckpIO
2025-04-11T03:52:12.4834665Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.4834768Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4834875Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4835143Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4835321Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4835615Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4835710Z     while not context.join():
2025-04-11T03:52:12.4835827Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4835831Z 
2025-04-11T03:52:12.4836029Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0250430>
2025-04-11T03:52:12.4836113Z timeout = None
2025-04-11T03:52:12.4836124Z 
2025-04-11T03:52:12.4836217Z     def join(self, timeout=None):
2025-04-11T03:52:12.4836348Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4836431Z     
2025-04-11T03:52:12.4836579Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4836730Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4836895Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4836989Z         of the first process exiting.
2025-04-11T03:52:12.4837074Z     
2025-04-11T03:52:12.4837224Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4837368Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4837440Z     
2025-04-11T03:52:12.4837521Z         Args:
2025-04-11T03:52:12.4837663Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4837749Z         """
2025-04-11T03:52:12.4837895Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4837992Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4838098Z             return True
2025-04-11T03:52:12.4838176Z     
2025-04-11T03:52:12.4838309Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4838443Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4838539Z             self.sentinels.keys(),
2025-04-11T03:52:12.4838636Z             timeout=timeout,
2025-04-11T03:52:12.4838712Z         )
2025-04-11T03:52:12.4838804Z     
2025-04-11T03:52:12.4838890Z         error_index = None
2025-04-11T03:52:12.4838975Z         for sentinel in ready:
2025-04-11T03:52:12.4839090Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4839195Z             process = self.processes[index]
2025-04-11T03:52:12.4839288Z             process.join()
2025-04-11T03:52:12.4839492Z             if process.exitcode != 0:
2025-04-11T03:52:12.4839585Z                 error_index = index
2025-04-11T03:52:12.4839674Z                 break
2025-04-11T03:52:12.4839750Z     
2025-04-11T03:52:12.4839850Z         # Return if there was no error.
2025-04-11T03:52:12.4839940Z         if error_index is None:
2025-04-11T03:52:12.4840076Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4840183Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4840261Z     
2025-04-11T03:52:12.4840408Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4840601Z         for process in self.processes:
2025-04-11T03:52:12.4840700Z             if process.is_alive():
2025-04-11T03:52:12.4840798Z                 process.terminate()
2025-04-11T03:52:12.4840884Z             process.join()
2025-04-11T03:52:12.4840967Z     
2025-04-11T03:52:12.4841109Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4841238Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4841351Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4841478Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4841572Z             if exitcode < 0:
2025-04-11T03:52:12.4841680Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4841797Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4841948Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4842057Z                     error_index=error_index,
2025-04-11T03:52:12.4842165Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4842256Z                     exit_code=exitcode,
2025-04-11T03:52:12.4842352Z                     signal_name=name,
2025-04-11T03:52:12.4842428Z                 )
2025-04-11T03:52:12.4842512Z             else:
2025-04-11T03:52:12.4842621Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4842784Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4842886Z                     error_index=error_index,
2025-04-11T03:52:12.4842990Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4843086Z                     exit_code=exitcode,
2025-04-11T03:52:12.4843164Z                 )
2025-04-11T03:52:12.4843243Z     
2025-04-11T03:52:12.4843376Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4843544Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4843643Z         msg += original_trace
2025-04-11T03:52:12.4843814Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4843980Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4844057Z E       
2025-04-11T03:52:12.4844184Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4844295Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4844599Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4844687Z E           fn(i, *args)
2025-04-11T03:52:12.4845008Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py", line 148, in run_dist
2025-04-11T03:52:12.4845102Z E           exam_state_dict()
2025-04-11T03:52:12.4845365Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4845465Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4845717Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4845808Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4846063Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4846276Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4846393Z E         [Previous line repeated 3 more times]
2025-04-11T03:52:12.4846615Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4846724Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4846986Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4847097Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4847396Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4847635Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4847749Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4848035Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4848186Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4848345Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4848350Z 
2025-04-11T03:52:12.4848652Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4848815Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4849002Z [04/11/25 03:42:21] WARNING  colossalai - colossalai.shardformer.modeling.llama 
2025-04-11T03:52:12.4849142Z                              - WARNING: `use_cache=True` is incompatible with   
2025-04-11T03:52:12.4849275Z                              pipeline parallelism. Setting `use_cache=False`... 
2025-04-11T03:52:12.4849431Z [04/11/25 03:42:21] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4849557Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4849676Z                              :75 launch                                         
2025-04-11T03:52:12.4849818Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4849943Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.4850149Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4850302Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4851433Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4851616Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4852715Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4852888Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4853984Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4854256Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4855370Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4855639Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4856333Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4856423Z   warnings.warn(
2025-04-11T03:52:12.4857108Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4857191Z   warnings.warn(
2025-04-11T03:52:12.4857893Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4857978Z   warnings.warn(
2025-04-11T03:52:12.4858662Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4858746Z   warnings.warn(
2025-04-11T03:52:12.4859572Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4859659Z   warnings.warn(
2025-04-11T03:52:12.4860462Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4860547Z   warnings.warn(
2025-04-11T03:52:12.4861349Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4861432Z   warnings.warn(
2025-04-11T03:52:12.4862217Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4862305Z   warnings.warn(
2025-04-11T03:52:12.4863089Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4863284Z   warnings.warn(
2025-04-11T03:52:12.4864076Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4864162Z   warnings.warn(
2025-04-11T03:52:12.4864948Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4865158Z   warnings.warn(
2025-04-11T03:52:12.4865958Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4866055Z   warnings.warn(
2025-04-11T03:52:12.4866852Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4866946Z   warnings.warn(
2025-04-11T03:52:12.4867741Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4867832Z   warnings.warn(
2025-04-11T03:52:12.4868666Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4868760Z   warnings.warn(
2025-04-11T03:52:12.4869540Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4869629Z   warnings.warn(
2025-04-11T03:52:12.4870180Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4870268Z   warnings.warn(
2025-04-11T03:52:12.4870798Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4870887Z   warnings.warn(
2025-04-11T03:52:12.4871423Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4871504Z   warnings.warn(
2025-04-11T03:52:12.4872035Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4872117Z   warnings.warn(
2025-04-11T03:52:12.4872668Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4872869Z   warnings.warn(
2025-04-11T03:52:12.4873404Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4873484Z   warnings.warn(
2025-04-11T03:52:12.4874015Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4874094Z   warnings.warn(
2025-04-11T03:52:12.4874615Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4874858Z   warnings.warn(
2025-04-11T03:52:12.4875014Z _______________________ test_low_level_zero_checkpointIO _______________________
2025-04-11T03:52:12.4875023Z 
2025-04-11T03:52:12.4875129Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4875134Z 
2025-04-11T03:52:12.4875239Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4875333Z         try_count = 0
2025-04-11T03:52:12.4875437Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4875529Z             max_try, int
2025-04-11T03:52:12.4875680Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4875753Z     
2025-04-11T03:52:12.4875877Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4875955Z             try:
2025-04-11T03:52:12.4876053Z                 try_count += 1
2025-04-11T03:52:12.4876151Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.4876155Z 
2025-04-11T03:52:12.4876255Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.4876378Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4876501Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.4876605Z     get_accelerator().synchronize()
2025-04-11T03:52:12.4876763Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.4876869Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.4876981Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4876985Z 
2025-04-11T03:52:12.4877065Z device = None
2025-04-11T03:52:12.4877076Z 
2025-04-11T03:52:12.4877201Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.4877355Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.4877444Z     
2025-04-11T03:52:12.4877520Z         Args:
2025-04-11T03:52:12.4877697Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.4877872Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.4877991Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.4878076Z         """
2025-04-11T03:52:12.4878159Z         _lazy_init()
2025-04-11T03:52:12.4878264Z         with torch.cuda.device(device):
2025-04-11T03:52:12.4878371Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4878487Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4878779Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4878920Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4879088Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4879096Z 
2025-04-11T03:52:12.4879333Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.4879489Z ______________________ test_huggingface_compatibility[2] _______________________
2025-04-11T03:52:12.4879599Z 
2025-04-11T03:52:12.4879726Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T03:52:12.4880339Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4880346Z 
2025-04-11T03:52:12.4880454Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4880547Z         try_count = 0
2025-04-11T03:52:12.4880649Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4880731Z             max_try, int
2025-04-11T03:52:12.4881013Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4881091Z     
2025-04-11T03:52:12.4881213Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4881292Z             try:
2025-04-11T03:52:12.4881385Z                 try_count += 1
2025-04-11T03:52:12.4881484Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4881570Z                 return ret
2025-04-11T03:52:12.4881675Z             except exception_type as e:
2025-04-11T03:52:12.4881795Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4882003Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4882123Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4882271Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4882439Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4882528Z                     continue
2025-04-11T03:52:12.4882624Z                 else:
2025-04-11T03:52:12.4882847Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4882936Z >                   raise e
2025-04-11T03:52:12.4882943Z 
2025-04-11T03:52:12.4883041Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4883156Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4883297Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4883388Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4883658Z tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py:79: in test_huggingface_compatibility
2025-04-11T03:52:12.4883752Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.4883864Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4883971Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4884233Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4884421Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4884709Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4884813Z     while not context.join():
2025-04-11T03:52:12.4884925Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4884930Z 
2025-04-11T03:52:12.4885135Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0251de0>
2025-04-11T03:52:12.4885218Z timeout = None
2025-04-11T03:52:12.4885222Z 
2025-04-11T03:52:12.4885327Z     def join(self, timeout=None):
2025-04-11T03:52:12.4885456Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4885530Z     
2025-04-11T03:52:12.4885686Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4885838Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4886011Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4886108Z         of the first process exiting.
2025-04-11T03:52:12.4886182Z     
2025-04-11T03:52:12.4886336Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4886583Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4886667Z     
2025-04-11T03:52:12.4886742Z         Args:
2025-04-11T03:52:12.4886891Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4886969Z         """
2025-04-11T03:52:12.4887110Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4887212Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4887297Z             return True
2025-04-11T03:52:12.4887379Z     
2025-04-11T03:52:12.4887513Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4887721Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4887825Z             self.sentinels.keys(),
2025-04-11T03:52:12.4887912Z             timeout=timeout,
2025-04-11T03:52:12.4888000Z         )
2025-04-11T03:52:12.4888074Z     
2025-04-11T03:52:12.4888171Z         error_index = None
2025-04-11T03:52:12.4888262Z         for sentinel in ready:
2025-04-11T03:52:12.4888370Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4888479Z             process = self.processes[index]
2025-04-11T03:52:12.4888570Z             process.join()
2025-04-11T03:52:12.4888673Z             if process.exitcode != 0:
2025-04-11T03:52:12.4888764Z                 error_index = index
2025-04-11T03:52:12.4888843Z                 break
2025-04-11T03:52:12.4888923Z     
2025-04-11T03:52:12.4889016Z         # Return if there was no error.
2025-04-11T03:52:12.4889105Z         if error_index is None:
2025-04-11T03:52:12.4889240Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4889341Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4889419Z     
2025-04-11T03:52:12.4889560Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4889667Z         for process in self.processes:
2025-04-11T03:52:12.4889762Z             if process.is_alive():
2025-04-11T03:52:12.4889864Z                 process.terminate()
2025-04-11T03:52:12.4889952Z             process.join()
2025-04-11T03:52:12.4890027Z     
2025-04-11T03:52:12.4890176Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4890294Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4890414Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4890540Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4890626Z             if exitcode < 0:
2025-04-11T03:52:12.4890742Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4890851Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4891014Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4891114Z                     error_index=error_index,
2025-04-11T03:52:12.4891237Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4891332Z                     exit_code=exitcode,
2025-04-11T03:52:12.4891422Z                     signal_name=name,
2025-04-11T03:52:12.4891512Z                 )
2025-04-11T03:52:12.4891590Z             else:
2025-04-11T03:52:12.4891706Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4891872Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4891975Z                     error_index=error_index,
2025-04-11T03:52:12.4892079Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4892170Z                     exit_code=exitcode,
2025-04-11T03:52:12.4892255Z                 )
2025-04-11T03:52:12.4892330Z     
2025-04-11T03:52:12.4892468Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4892644Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4892733Z         msg += original_trace
2025-04-11T03:52:12.4893023Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4893186Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4893274Z E       
2025-04-11T03:52:12.4893405Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4893514Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4893819Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4893904Z E           fn(i, *args)
2025-04-11T03:52:12.4894230Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py", line 72, in run_dist
2025-04-11T03:52:12.4894443Z E           exam_from_pretrained()
2025-04-11T03:52:12.4894674Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.4894775Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.4895045Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.4895150Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.4895430Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.4895549Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4895660Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4895957Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4896097Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4896273Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4896278Z 
2025-04-11T03:52:12.4896585Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4896753Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4896915Z [04/11/25 03:42:28] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4897048Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4897164Z                              :75 launch                                         
2025-04-11T03:52:12.4897304Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4897437Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4897632Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4897792Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4898962Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4899149Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4900253Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.4900446Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.4901133Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4901334Z   warnings.warn(
2025-04-11T03:52:12.4901999Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.4902085Z   warnings.warn(
2025-04-11T03:52:12.4902912Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4903123Z   warnings.warn(
2025-04-11T03:52:12.4903910Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4903991Z   warnings.warn(
2025-04-11T03:52:12.4904787Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4904872Z   warnings.warn(
2025-04-11T03:52:12.4905669Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4905753Z   warnings.warn(
2025-04-11T03:52:12.4906550Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4906630Z   warnings.warn(
2025-04-11T03:52:12.4907426Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.4907507Z   warnings.warn(
2025-04-11T03:52:12.4908060Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4908144Z   warnings.warn(
2025-04-11T03:52:12.4908787Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4908876Z   warnings.warn(
2025-04-11T03:52:12.4909416Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4909529Z   warnings.warn(
2025-04-11T03:52:12.4910064Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.4910275Z   warnings.warn(
2025-04-11T03:52:12.4910422Z ___________________________ test_create_pin[1-True] ____________________________
2025-04-11T03:52:12.4910426Z 
2025-04-11T03:52:12.4910531Z empty = True, num_threads = 1
2025-04-11T03:52:12.4910535Z 
2025-04-11T03:52:12.4910661Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T03:52:12.4910791Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T03:52:12.4910922Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T03:52:12.4911028Z         model_state_dict = gen_model_state_dict()
2025-04-11T03:52:12.4911285Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T03:52:12.4911411Z 
2025-04-11T03:52:12.4911564Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T03:52:12.4911690Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4911860Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T03:52:12.4912114Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4912356Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T03:52:12.4912506Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4912749Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T03:52:12.4912889Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4913022Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T03:52:12.4913254Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4913374Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4913378Z 
2025-04-11T03:52:12.4913522Z tensor = tensor([[0.7278, 0.6708, 0.5495,  ..., 0.8589, 0.0268, 0.0457],
2025-04-11T03:52:12.4913701Z         [0.2799, 0.6305, 0.0349,  ..., 0.2965, 0.9488,...0.5473, 0.9859, 0.3709,  ..., 0.4942, 0.6802, 0.4158],
2025-04-11T03:52:12.4913815Z         [0.6189, 0.1026, 0.3877,  ..., 0.9755, 0.7854, 0.8188]])
2025-04-11T03:52:12.4913904Z empty = True
2025-04-11T03:52:12.4913909Z 
2025-04-11T03:52:12.4914096Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4914178Z         if empty:
2025-04-11T03:52:12.4914343Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4914454Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4914759Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4914904Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4915080Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4915087Z 
2025-04-11T03:52:12.4915217Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T03:52:12.4915361Z ___________________________ test_create_pin[1-False] ___________________________
2025-04-11T03:52:12.4915372Z 
2025-04-11T03:52:12.4915467Z empty = False, num_threads = 1
2025-04-11T03:52:12.4915471Z 
2025-04-11T03:52:12.4915598Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T03:52:12.4915722Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T03:52:12.4915851Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T03:52:12.4915964Z         model_state_dict = gen_model_state_dict()
2025-04-11T03:52:12.4916218Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T03:52:12.4916223Z 
2025-04-11T03:52:12.4916374Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T03:52:12.4916485Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4916760Z colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
2025-04-11T03:52:12.4917002Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4917241Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
2025-04-11T03:52:12.4917381Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4917617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
2025-04-11T03:52:12.4917763Z     return tree_unflatten([func(i) for i in flat_args], spec)
2025-04-11T03:52:12.4918002Z colossalai/checkpoint_io/utils.py:977: in <lambda>
2025-04-11T03:52:12.4918226Z     return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
2025-04-11T03:52:12.4918343Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4918350Z 
2025-04-11T03:52:12.4918491Z tensor = tensor([[0.9313, 0.0163, 0.9832,  ..., 0.2312, 0.5103, 0.1652],
2025-04-11T03:52:12.4918672Z         [0.5121, 0.5898, 0.1334,  ..., 0.6431, 0.9040,...0.2168, 0.6617, 0.6909,  ..., 0.6668, 0.3573, 0.0393],
2025-04-11T03:52:12.4918787Z         [0.9993, 0.1955, 0.7855,  ..., 0.2109, 0.8531, 0.2361]])
2025-04-11T03:52:12.4918878Z empty = False
2025-04-11T03:52:12.4918883Z 
2025-04-11T03:52:12.4919060Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4919144Z         if empty:
2025-04-11T03:52:12.4919301Z             return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4919402Z >       return tensor.pin_memory()
2025-04-11T03:52:12.4919520Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4919809Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4919958Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4920120Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4920124Z 
2025-04-11T03:52:12.4920256Z colossalai/checkpoint_io/utils.py:968: RuntimeError
2025-04-11T03:52:12.4920398Z ___________________________ test_create_pin[4-True] ____________________________
2025-04-11T03:52:12.4920403Z 
2025-04-11T03:52:12.4920504Z empty = True, num_threads = 4
2025-04-11T03:52:12.4920508Z 
2025-04-11T03:52:12.4920632Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T03:52:12.4920749Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T03:52:12.4920884Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T03:52:12.4920989Z         model_state_dict = gen_model_state_dict()
2025-04-11T03:52:12.4921233Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T03:52:12.4921238Z 
2025-04-11T03:52:12.4921387Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T03:52:12.4921504Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4921669Z colossalai/checkpoint_io/utils.py:987: in create_pinned_state_dict
2025-04-11T03:52:12.4921765Z     elems[idx] = future.result()
2025-04-11T03:52:12.4921979Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:451: in result
2025-04-11T03:52:12.4922072Z     return self.__get_result()
2025-04-11T03:52:12.4922302Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
2025-04-11T03:52:12.4922393Z     raise self._exception
2025-04-11T03:52:12.4922596Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py:58: in run
2025-04-11T03:52:12.4922707Z     result = self.fn(*self.args, **self.kwargs)
2025-04-11T03:52:12.4922816Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4922826Z 
2025-04-11T03:52:12.4923072Z tensor = tensor([[0.9931, 0.0592, 0.6090,  ..., 0.0731, 0.8302, 0.3647],
2025-04-11T03:52:12.4923244Z         [0.8529, 0.9077, 0.4732,  ..., 0.0980, 0.4233,...0.5983, 0.8300, 0.4153,  ..., 0.0877, 0.5103, 0.6271],
2025-04-11T03:52:12.4923365Z         [0.7461, 0.1834, 0.2279,  ..., 0.9305, 0.2178, 0.5575]])
2025-04-11T03:52:12.4923446Z empty = True
2025-04-11T03:52:12.4923451Z 
2025-04-11T03:52:12.4923629Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4923710Z         if empty:
2025-04-11T03:52:12.4923874Z >           return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4923981Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4924383Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4924532Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4924691Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4924698Z 
2025-04-11T03:52:12.4924828Z colossalai/checkpoint_io/utils.py:967: RuntimeError
2025-04-11T03:52:12.4924971Z ___________________________ test_create_pin[4-False] ___________________________
2025-04-11T03:52:12.4924977Z 
2025-04-11T03:52:12.4925077Z empty = False, num_threads = 4
2025-04-11T03:52:12.4925082Z 
2025-04-11T03:52:12.4925204Z     @pytest.mark.parametrize("empty", [True, False])
2025-04-11T03:52:12.4925322Z     @pytest.mark.parametrize("num_threads", [1, 4])
2025-04-11T03:52:12.4925451Z     def test_create_pin(empty: bool, num_threads: int):
2025-04-11T03:52:12.4925563Z         model_state_dict = gen_model_state_dict()
2025-04-11T03:52:12.4925813Z >       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T03:52:12.4925818Z 
2025-04-11T03:52:12.4925971Z tests/test_checkpoint_io/test_safetensors_async_io.py:120: 
2025-04-11T03:52:12.4926105Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4926270Z colossalai/checkpoint_io/utils.py:987: in create_pinned_state_dict
2025-04-11T03:52:12.4926391Z     elems[idx] = future.result()
2025-04-11T03:52:12.4926597Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:451: in result
2025-04-11T03:52:12.4926699Z     return self.__get_result()
2025-04-11T03:52:12.4926925Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
2025-04-11T03:52:12.4927018Z     raise self._exception
2025-04-11T03:52:12.4927218Z /opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py:58: in run
2025-04-11T03:52:12.4927330Z     result = self.fn(*self.args, **self.kwargs)
2025-04-11T03:52:12.4927446Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4927450Z 
2025-04-11T03:52:12.4927604Z tensor = tensor([[0.8519, 0.4295, 0.4258,  ..., 0.7715, 0.7338, 0.9187],
2025-04-11T03:52:12.4927778Z         [0.1316, 0.1529, 0.8565,  ..., 0.6041, 0.0071,...0.2519, 0.7662, 0.7709,  ..., 0.9714, 0.1224, 0.5552],
2025-04-11T03:52:12.4927907Z         [0.6948, 0.7876, 0.6498,  ..., 0.6190, 0.9752, 0.7557]])
2025-04-11T03:52:12.4927991Z empty = False
2025-04-11T03:52:12.4927995Z 
2025-04-11T03:52:12.4928179Z     def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
2025-04-11T03:52:12.4928261Z         if empty:
2025-04-11T03:52:12.4928426Z             return torch.empty_like(tensor, pin_memory=True, device="cpu")
2025-04-11T03:52:12.4928526Z >       return tensor.pin_memory()
2025-04-11T03:52:12.4928635Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4928935Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4929075Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4929247Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4929381Z 
2025-04-11T03:52:12.4929505Z colossalai/checkpoint_io/utils.py:968: RuntimeError
2025-04-11T03:52:12.4929653Z ________________________________ test_save_load ________________________________
2025-04-11T03:52:12.4929657Z 
2025-04-11T03:52:12.4929741Z args = (), kwargs = {}
2025-04-11T03:52:12.4929746Z 
2025-04-11T03:52:12.4929854Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.4929952Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.4930065Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.4930194Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.4930401Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.4930512Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.4930517Z 
2025-04-11T03:52:12.4930622Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.4930747Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4930915Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.4931019Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.4931145Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4931150Z 
2025-04-11T03:52:12.4931236Z device = None
2025-04-11T03:52:12.4931240Z 
2025-04-11T03:52:12.4931382Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.4931546Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.4931635Z     
2025-04-11T03:52:12.4931719Z         Args:
2025-04-11T03:52:12.4931895Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.4932084Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.4932201Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.4932292Z         """
2025-04-11T03:52:12.4932380Z         _lazy_init()
2025-04-11T03:52:12.4932491Z         with torch.cuda.device(device):
2025-04-11T03:52:12.4932599Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.4932712Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4933015Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4933162Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4933337Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4933342Z 
2025-04-11T03:52:12.4933587Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.4933749Z _________________________ test_torch_ddp_checkpointIO __________________________
2025-04-11T03:52:12.4933753Z 
2025-04-11T03:52:12.4933856Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4934468Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4934486Z 
2025-04-11T03:52:12.4934596Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4934684Z         try_count = 0
2025-04-11T03:52:12.4934803Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4934901Z             max_try, int
2025-04-11T03:52:12.4935077Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4935157Z     
2025-04-11T03:52:12.4935291Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4935378Z             try:
2025-04-11T03:52:12.4935470Z                 try_count += 1
2025-04-11T03:52:12.4935581Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4935669Z                 return ret
2025-04-11T03:52:12.4935778Z             except exception_type as e:
2025-04-11T03:52:12.4936006Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4936204Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4936338Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4936494Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4936665Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4936754Z                     continue
2025-04-11T03:52:12.4936848Z                 else:
2025-04-11T03:52:12.4937076Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4937279Z >                   raise e
2025-04-11T03:52:12.4937294Z 
2025-04-11T03:52:12.4937396Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4937512Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4937663Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4937757Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4937999Z tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py:81: in test_torch_ddp_checkpointIO
2025-04-11T03:52:12.4938089Z     spawn(run_dist, 2)
2025-04-11T03:52:12.4938201Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4938318Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4938580Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4938770Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4939064Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4939167Z     while not context.join():
2025-04-11T03:52:12.4939280Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4939287Z 
2025-04-11T03:52:12.4939490Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0164af0>
2025-04-11T03:52:12.4939584Z timeout = None
2025-04-11T03:52:12.4939588Z 
2025-04-11T03:52:12.4939684Z     def join(self, timeout=None):
2025-04-11T03:52:12.4939824Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4939901Z     
2025-04-11T03:52:12.4940067Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4940221Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4940392Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4940512Z         of the first process exiting.
2025-04-11T03:52:12.4940591Z     
2025-04-11T03:52:12.4940753Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4940897Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4940986Z     
2025-04-11T03:52:12.4941067Z         Args:
2025-04-11T03:52:12.4941211Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4941302Z         """
2025-04-11T03:52:12.4941444Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4941551Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4941635Z             return True
2025-04-11T03:52:12.4941709Z     
2025-04-11T03:52:12.4941855Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4941978Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4942084Z             self.sentinels.keys(),
2025-04-11T03:52:12.4942172Z             timeout=timeout,
2025-04-11T03:52:12.4942255Z         )
2025-04-11T03:52:12.4942329Z     
2025-04-11T03:52:12.4942416Z         error_index = None
2025-04-11T03:52:12.4942514Z         for sentinel in ready:
2025-04-11T03:52:12.4942626Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4942833Z             process = self.processes[index]
2025-04-11T03:52:12.4942925Z             process.join()
2025-04-11T03:52:12.4943022Z             if process.exitcode != 0:
2025-04-11T03:52:12.4943121Z                 error_index = index
2025-04-11T03:52:12.4943200Z                 break
2025-04-11T03:52:12.4943278Z     
2025-04-11T03:52:12.4943373Z         # Return if there was no error.
2025-04-11T03:52:12.4943464Z         if error_index is None:
2025-04-11T03:52:12.4943607Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4943707Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4943785Z     
2025-04-11T03:52:12.4944022Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4944148Z         for process in self.processes:
2025-04-11T03:52:12.4944245Z             if process.is_alive():
2025-04-11T03:52:12.4944336Z                 process.terminate()
2025-04-11T03:52:12.4944438Z             process.join()
2025-04-11T03:52:12.4944514Z     
2025-04-11T03:52:12.4944663Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4944786Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4944904Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4945041Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4945129Z             if exitcode < 0:
2025-04-11T03:52:12.4945251Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4945365Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4945525Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4945628Z                     error_index=error_index,
2025-04-11T03:52:12.4945732Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4945830Z                     exit_code=exitcode,
2025-04-11T03:52:12.4945919Z                     signal_name=name,
2025-04-11T03:52:12.4946004Z                 )
2025-04-11T03:52:12.4946082Z             else:
2025-04-11T03:52:12.4946190Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4946363Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4946459Z                     error_index=error_index,
2025-04-11T03:52:12.4946567Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4946655Z                     exit_code=exitcode,
2025-04-11T03:52:12.4946739Z                 )
2025-04-11T03:52:12.4946812Z     
2025-04-11T03:52:12.4946947Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4947132Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4947220Z         msg += original_trace
2025-04-11T03:52:12.4947399Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4947562Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4947651Z E       
2025-04-11T03:52:12.4947781Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4947886Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4948194Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4948281Z E           fn(i, *args)
2025-04-11T03:52:12.4948618Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 76, in run_dist
2025-04-11T03:52:12.4948724Z E           check_torch_ddp_checkpointIO()
2025-04-11T03:52:12.4948990Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4949086Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4949350Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4949451Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4949832Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4949933Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4950040Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.4950373Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 26, in check_torch_ddp_checkpointIO
2025-04-11T03:52:12.4950617Z E           model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)
2025-04-11T03:52:12.4950827Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T03:52:12.4951270Z E           model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T03:52:12.4951538Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_ddp_plugin.py", line 283, in configure
2025-04-11T03:52:12.4951661Z E           model = model.to(get_current_device())
2025-04-11T03:52:12.4951927Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:12.4952035Z E           return self._apply(convert)
2025-04-11T03:52:12.4952308Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.4952413Z E           module._apply(fn)
2025-04-11T03:52:12.4952684Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.4952782Z E           param_applied = fn(param)
2025-04-11T03:52:12.4953073Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:12.4953289Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.4953424Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4953727Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4953887Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4954058Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4954063Z 
2025-04-11T03:52:12.4954370Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4954531Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4954695Z [04/11/25 03:42:35] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4954837Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4954948Z                              :75 launch                                         
2025-04-11T03:52:12.4955093Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4955223Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4955422Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4955562Z _____________________________ test_torch_fsdp_ckpt _____________________________
2025-04-11T03:52:12.4955566Z 
2025-04-11T03:52:12.4955660Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4956269Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4956278Z 
2025-04-11T03:52:12.4956389Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4956479Z         try_count = 0
2025-04-11T03:52:12.4956582Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4956796Z             max_try, int
2025-04-11T03:52:12.4956948Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4957031Z     
2025-04-11T03:52:12.4957148Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4957228Z             try:
2025-04-11T03:52:12.4957324Z                 try_count += 1
2025-04-11T03:52:12.4957420Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4957509Z                 return ret
2025-04-11T03:52:12.4957606Z             except exception_type as e:
2025-04-11T03:52:12.4957713Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4957907Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4958149Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4958305Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4958460Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4958554Z                     continue
2025-04-11T03:52:12.4958634Z                 else:
2025-04-11T03:52:12.4958854Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4958941Z >                   raise e
2025-04-11T03:52:12.4958946Z 
2025-04-11T03:52:12.4959043Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4959164Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4959298Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4959400Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4959612Z tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py:162: in test_torch_fsdp_ckpt
2025-04-11T03:52:12.4959697Z     spawn(run_dist, 2)
2025-04-11T03:52:12.4959812Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4959916Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4960180Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4960356Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4960646Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4960738Z     while not context.join():
2025-04-11T03:52:12.4960850Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4960854Z 
2025-04-11T03:52:12.4961063Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0318730>
2025-04-11T03:52:12.4961148Z timeout = None
2025-04-11T03:52:12.4961153Z 
2025-04-11T03:52:12.4961253Z     def join(self, timeout=None):
2025-04-11T03:52:12.4961382Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4961462Z     
2025-04-11T03:52:12.4961608Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4961756Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4961928Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4962026Z         of the first process exiting.
2025-04-11T03:52:12.4962109Z     
2025-04-11T03:52:12.4962256Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4962404Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4962479Z     
2025-04-11T03:52:12.4962556Z         Args:
2025-04-11T03:52:12.4962705Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4962786Z         """
2025-04-11T03:52:12.4962945Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4963047Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4963132Z             return True
2025-04-11T03:52:12.4963214Z     
2025-04-11T03:52:12.4963471Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4963598Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4963694Z             self.sentinels.keys(),
2025-04-11T03:52:12.4963786Z             timeout=timeout,
2025-04-11T03:52:12.4963862Z         )
2025-04-11T03:52:12.4963935Z     
2025-04-11T03:52:12.4964029Z         error_index = None
2025-04-11T03:52:12.4964116Z         for sentinel in ready:
2025-04-11T03:52:12.4964237Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4964338Z             process = self.processes[index]
2025-04-11T03:52:12.4964428Z             process.join()
2025-04-11T03:52:12.4964652Z             if process.exitcode != 0:
2025-04-11T03:52:12.4964746Z                 error_index = index
2025-04-11T03:52:12.4964832Z                 break
2025-04-11T03:52:12.4964908Z     
2025-04-11T03:52:12.4965006Z         # Return if there was no error.
2025-04-11T03:52:12.4965116Z         if error_index is None:
2025-04-11T03:52:12.4965256Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4965366Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4965438Z     
2025-04-11T03:52:12.4965589Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4965687Z         for process in self.processes:
2025-04-11T03:52:12.4965779Z             if process.is_alive():
2025-04-11T03:52:12.4965881Z                 process.terminate()
2025-04-11T03:52:12.4965969Z             process.join()
2025-04-11T03:52:12.4966049Z     
2025-04-11T03:52:12.4966194Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4966316Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4966436Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4966558Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4966651Z             if exitcode < 0:
2025-04-11T03:52:12.4966761Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4966875Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4967026Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4967124Z                     error_index=error_index,
2025-04-11T03:52:12.4967234Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4967328Z                     exit_code=exitcode,
2025-04-11T03:52:12.4967423Z                     signal_name=name,
2025-04-11T03:52:12.4967500Z                 )
2025-04-11T03:52:12.4967576Z             else:
2025-04-11T03:52:12.4967683Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4967851Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4967955Z                     error_index=error_index,
2025-04-11T03:52:12.4968056Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4968153Z                     exit_code=exitcode,
2025-04-11T03:52:12.4968230Z                 )
2025-04-11T03:52:12.4968304Z     
2025-04-11T03:52:12.4968442Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4968616Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4968707Z         msg += original_trace
2025-04-11T03:52:12.4968884Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4969043Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4969124Z E       
2025-04-11T03:52:12.4969252Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4969364Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4969662Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4969753Z E           fn(i, *args)
2025-04-11T03:52:12.4970041Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 156, in run_dist
2025-04-11T03:52:12.4970246Z E           check_torch_fsdp_ckpt()
2025-04-11T03:52:12.4970516Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.4970608Z E           partial_func(**kwargs)
2025-04-11T03:52:12.4970935Z E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 53, in check_torch_fsdp_ckpt
2025-04-11T03:52:12.4971140Z E           fsdp_model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion)
2025-04-11T03:52:12.4971356Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T03:52:12.4971662Z E           model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T03:52:12.4971939Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 533, in configure
2025-04-11T03:52:12.4972174Z E           fsdp_model = TorchFSDPModel(model, device_id=torch.cuda.current_device(), **self.fsdp_kwargs)
2025-04-11T03:52:12.4972444Z E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 438, in __init__
2025-04-11T03:52:12.4972575Z E           self.module = FSDP(module, *args, **kwargs)
2025-04-11T03:52:12.4972946Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 503, in __init__
2025-04-11T03:52:12.4973056Z E           _init_param_handle_from_module(
2025-04-11T03:52:12.4973423Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 568, in _init_param_handle_from_module
2025-04-11T03:52:12.4973532Z E           _move_module_to_device(
2025-04-11T03:52:12.4973877Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 956, in _move_module_to_device
2025-04-11T03:52:12.4974072Z E           _move_states_to_device(params_to_move, bufs_to_move, device_from_device_id)
2025-04-11T03:52:12.4974418Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 986, in _move_states_to_device
2025-04-11T03:52:12.4974542Z E           param.data = param.to(device_from_device_id)
2025-04-11T03:52:12.4974660Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4974948Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4975094Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4975258Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4975263Z 
2025-04-11T03:52:12.4975574Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4975732Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4975899Z [04/11/25 03:42:39] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4976032Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4976141Z                              :75 launch                                         
2025-04-11T03:52:12.4976285Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4976411Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.4976617Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4976758Z _______________________________ test_logical_pg ________________________________
2025-04-11T03:52:12.4976762Z 
2025-04-11T03:52:12.4976865Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4977451Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4977555Z 
2025-04-11T03:52:12.4977666Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4977753Z         try_count = 0
2025-04-11T03:52:12.4977857Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4977973Z             max_try, int
2025-04-11T03:52:12.4978128Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4978217Z     
2025-04-11T03:52:12.4978335Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4978544Z             try:
2025-04-11T03:52:12.4978636Z                 try_count += 1
2025-04-11T03:52:12.4978733Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4978832Z                 return ret
2025-04-11T03:52:12.4978928Z             except exception_type as e:
2025-04-11T03:52:12.4979044Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4979233Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4979354Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4979511Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4979671Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4979766Z                     continue
2025-04-11T03:52:12.4979856Z                 else:
2025-04-11T03:52:12.4980097Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4980186Z >                   raise e
2025-04-11T03:52:12.4980190Z 
2025-04-11T03:52:12.4980299Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4980423Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4980563Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4980671Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4980834Z tests/test_device/test_init_logical_pg.py:33: in test_logical_pg
2025-04-11T03:52:12.4980935Z     spawn(check_layer, 4)
2025-04-11T03:52:12.4981040Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4981143Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4981410Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4981587Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4981886Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4981978Z     while not context.join():
2025-04-11T03:52:12.4982098Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4982103Z 
2025-04-11T03:52:12.4982307Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f024f0a0>
2025-04-11T03:52:12.4982399Z timeout = None
2025-04-11T03:52:12.4982403Z 
2025-04-11T03:52:12.4982497Z     def join(self, timeout=None):
2025-04-11T03:52:12.4982625Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.4982709Z     
2025-04-11T03:52:12.4982859Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.4983013Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.4983177Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.4983277Z         of the first process exiting.
2025-04-11T03:52:12.4983362Z     
2025-04-11T03:52:12.4983512Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.4983661Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.4983736Z     
2025-04-11T03:52:12.4983922Z         Args:
2025-04-11T03:52:12.4984064Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.4984139Z         """
2025-04-11T03:52:12.4984291Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.4984388Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.4984480Z             return True
2025-04-11T03:52:12.4984557Z     
2025-04-11T03:52:12.4984692Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.4984823Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.4984920Z             self.sentinels.keys(),
2025-04-11T03:52:12.4985017Z             timeout=timeout,
2025-04-11T03:52:12.4985212Z         )
2025-04-11T03:52:12.4985301Z     
2025-04-11T03:52:12.4985390Z         error_index = None
2025-04-11T03:52:12.4985480Z         for sentinel in ready:
2025-04-11T03:52:12.4985602Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.4985707Z             process = self.processes[index]
2025-04-11T03:52:12.4985808Z             process.join()
2025-04-11T03:52:12.4985910Z             if process.exitcode != 0:
2025-04-11T03:52:12.4986003Z                 error_index = index
2025-04-11T03:52:12.4986095Z                 break
2025-04-11T03:52:12.4986169Z     
2025-04-11T03:52:12.4986277Z         # Return if there was no error.
2025-04-11T03:52:12.4986371Z         if error_index is None:
2025-04-11T03:52:12.4986510Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.4986621Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.4986696Z     
2025-04-11T03:52:12.4986850Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.4986960Z         for process in self.processes:
2025-04-11T03:52:12.4987071Z             if process.is_alive():
2025-04-11T03:52:12.4987170Z                 process.terminate()
2025-04-11T03:52:12.4987261Z             process.join()
2025-04-11T03:52:12.4987352Z     
2025-04-11T03:52:12.4987504Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.4987634Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.4987748Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.4987876Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.4987976Z             if exitcode < 0:
2025-04-11T03:52:12.4988089Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.4988207Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4988364Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.4988527Z                     error_index=error_index,
2025-04-11T03:52:12.4988635Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4988730Z                     exit_code=exitcode,
2025-04-11T03:52:12.4988833Z                     signal_name=name,
2025-04-11T03:52:12.4988913Z                 )
2025-04-11T03:52:12.4989026Z             else:
2025-04-11T03:52:12.4989134Z                 raise ProcessExitedException(
2025-04-11T03:52:12.4989313Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.4989438Z                     error_index=error_index,
2025-04-11T03:52:12.4989554Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.4989664Z                     exit_code=exitcode,
2025-04-11T03:52:12.4989746Z                 )
2025-04-11T03:52:12.4989834Z     
2025-04-11T03:52:12.4989970Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.4990143Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.4990248Z         msg += original_trace
2025-04-11T03:52:12.4990426Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.4990604Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.4990682Z E       
2025-04-11T03:52:12.4990986Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.4991093Z E       Traceback (most recent call last):
2025-04-11T03:52:12.4991394Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.4991491Z E           fn(i, *args)
2025-04-11T03:52:12.4991743Z E         File "/__w/ColossalAI/ColossalAI/tests/test_device/test_init_logical_pg.py", line 17, in check_layer
2025-04-11T03:52:12.4991880Z E           tensor_to_check = torch.tensor([2, 2, 2, 2]).cuda()
2025-04-11T03:52:12.4991993Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.4992428Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.4992567Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.4992732Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.4992740Z 
2025-04-11T03:52:12.4993060Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.4993213Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.4993383Z [04/11/25 03:43:00] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.4993516Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.4993638Z                              :75 launch                                         
2025-04-11T03:52:12.4993779Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.4993918Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.4994117Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.4994266Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.4994578Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43505 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.4994719Z ____________________________ test_all_to_all_single ____________________________
2025-04-11T03:52:12.4994723Z 
2025-04-11T03:52:12.4994834Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.4995436Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.4995446Z 
2025-04-11T03:52:12.4995563Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.4995648Z         try_count = 0
2025-04-11T03:52:12.4995762Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.4995848Z             max_try, int
2025-04-11T03:52:12.4996000Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.4996087Z     
2025-04-11T03:52:12.4996204Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.4996291Z             try:
2025-04-11T03:52:12.4996379Z                 try_count += 1
2025-04-11T03:52:12.4996475Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.4996568Z                 return ret
2025-04-11T03:52:12.4996669Z             except exception_type as e:
2025-04-11T03:52:12.4996782Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.4996972Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.4997105Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.4997253Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.4997412Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.4997508Z                     continue
2025-04-11T03:52:12.4997697Z                 else:
2025-04-11T03:52:12.4997936Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.4998039Z >                   raise e
2025-04-11T03:52:12.4998044Z 
2025-04-11T03:52:12.4998160Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.4998281Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.4998416Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.4998523Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.4998691Z tests/test_fp8/test_all_to_all_single.py:73: in test_all_to_all_single
2025-04-11T03:52:12.4998882Z     spawn(run_dist, 4)
2025-04-11T03:52:12.4998991Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.4999109Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.4999374Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.4999561Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.4999863Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.4999964Z     while not context.join():
2025-04-11T03:52:12.5000092Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5000097Z 
2025-04-11T03:52:12.5000299Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0319960>
2025-04-11T03:52:12.5000392Z timeout = None
2025-04-11T03:52:12.5000397Z 
2025-04-11T03:52:12.5000493Z     def join(self, timeout=None):
2025-04-11T03:52:12.5000625Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5000711Z     
2025-04-11T03:52:12.5000860Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5001015Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5001186Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5001288Z         of the first process exiting.
2025-04-11T03:52:12.5001367Z     
2025-04-11T03:52:12.5001523Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5001675Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5001754Z     
2025-04-11T03:52:12.5001845Z         Args:
2025-04-11T03:52:12.5001990Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5002079Z         """
2025-04-11T03:52:12.5002221Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5002321Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5002415Z             return True
2025-04-11T03:52:12.5002490Z     
2025-04-11T03:52:12.5002637Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5002762Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5002862Z             self.sentinels.keys(),
2025-04-11T03:52:12.5002961Z             timeout=timeout,
2025-04-11T03:52:12.5003038Z         )
2025-04-11T03:52:12.5003122Z     
2025-04-11T03:52:12.5003210Z         error_index = None
2025-04-11T03:52:12.5003299Z         for sentinel in ready:
2025-04-11T03:52:12.5003421Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5003523Z             process = self.processes[index]
2025-04-11T03:52:12.5003619Z             process.join()
2025-04-11T03:52:12.5003717Z             if process.exitcode != 0:
2025-04-11T03:52:12.5003818Z                 error_index = index
2025-04-11T03:52:12.5003900Z                 break
2025-04-11T03:52:12.5003975Z     
2025-04-11T03:52:12.5004081Z         # Return if there was no error.
2025-04-11T03:52:12.5004171Z         if error_index is None:
2025-04-11T03:52:12.5004315Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5004521Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5004599Z     
2025-04-11T03:52:12.5004753Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5004856Z         for process in self.processes:
2025-04-11T03:52:12.5004957Z             if process.is_alive():
2025-04-11T03:52:12.5005052Z                 process.terminate()
2025-04-11T03:52:12.5005139Z             process.join()
2025-04-11T03:52:12.5005223Z     
2025-04-11T03:52:12.5005366Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5005494Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5005605Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5005865Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5005954Z             if exitcode < 0:
2025-04-11T03:52:12.5006062Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5006181Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5006338Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5006445Z                     error_index=error_index,
2025-04-11T03:52:12.5006551Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5006652Z                     exit_code=exitcode,
2025-04-11T03:52:12.5006743Z                     signal_name=name,
2025-04-11T03:52:12.5006820Z                 )
2025-04-11T03:52:12.5006908Z             else:
2025-04-11T03:52:12.5007013Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5007193Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5007296Z                     error_index=error_index,
2025-04-11T03:52:12.5007401Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5007512Z                     exit_code=exitcode,
2025-04-11T03:52:12.5007591Z                 )
2025-04-11T03:52:12.5007679Z     
2025-04-11T03:52:12.5007823Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5008017Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5008107Z         msg += original_trace
2025-04-11T03:52:12.5008285Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5008458Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5008536Z E       
2025-04-11T03:52:12.5008674Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5008778Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5009086Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5009188Z E           fn(i, *args)
2025-04-11T03:52:12.5009484Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_all_to_all_single.py", line 67, in run_dist
2025-04-11T03:52:12.5009585Z E           check_all2all()
2025-04-11T03:52:12.5009814Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5009929Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5010190Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5010299Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5010589Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5010699Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5010818Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5011108Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5011257Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5011427Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5011535Z 
2025-04-11T03:52:12.5011856Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5012014Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5012173Z [04/11/25 03:43:06] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5012315Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5012429Z                              :75 launch                                         
2025-04-11T03:52:12.5012672Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5012800Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.5013007Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5013160Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5013457Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:48660 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5013754Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:48660 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5014319Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5014423Z   warnings.warn(
2025-04-11T03:52:12.5014966Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5015070Z   warnings.warn(
2025-04-11T03:52:12.5015619Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5015716Z   warnings.warn(
2025-04-11T03:52:12.5016247Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5016344Z   warnings.warn(
2025-04-11T03:52:12.5016880Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5016981Z   warnings.warn(
2025-04-11T03:52:12.5017514Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5017601Z   warnings.warn(
2025-04-11T03:52:12.5018139Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5018222Z   warnings.warn(
2025-04-11T03:52:12.5018754Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5018835Z   warnings.warn(
2025-04-11T03:52:12.5018983Z _______________________________ test_all_to_all ________________________________
2025-04-11T03:52:12.5018987Z 
2025-04-11T03:52:12.5019084Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5019692Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5019807Z 
2025-04-11T03:52:12.5019915Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5020008Z         try_count = 0
2025-04-11T03:52:12.5020114Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5020199Z             max_try, int
2025-04-11T03:52:12.5020359Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5020434Z     
2025-04-11T03:52:12.5020559Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5020638Z             try:
2025-04-11T03:52:12.5020815Z                 try_count += 1
2025-04-11T03:52:12.5020919Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5021005Z                 return ret
2025-04-11T03:52:12.5021113Z             except exception_type as e:
2025-04-11T03:52:12.5021219Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5021419Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5021543Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5021690Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5021857Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5021941Z                     continue
2025-04-11T03:52:12.5022029Z                 else:
2025-04-11T03:52:12.5022251Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5022346Z >                   raise e
2025-04-11T03:52:12.5022351Z 
2025-04-11T03:52:12.5022449Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5022563Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5022711Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5022804Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5022965Z tests/test_fp8/test_fp8_all_to_all.py:36: in test_all_to_all
2025-04-11T03:52:12.5023052Z     spawn(run_dist, 4)
2025-04-11T03:52:12.5023162Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5023273Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5023532Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5023722Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5024009Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5024114Z     while not context.join():
2025-04-11T03:52:12.5024226Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5024231Z 
2025-04-11T03:52:12.5024437Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0165c00>
2025-04-11T03:52:12.5024522Z timeout = None
2025-04-11T03:52:12.5024527Z 
2025-04-11T03:52:12.5024620Z     def join(self, timeout=None):
2025-04-11T03:52:12.5024757Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5024833Z     
2025-04-11T03:52:12.5024988Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5025135Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5025311Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5025411Z         of the first process exiting.
2025-04-11T03:52:12.5025501Z     
2025-04-11T03:52:12.5025664Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5025805Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5025905Z     
2025-04-11T03:52:12.5025984Z         Args:
2025-04-11T03:52:12.5026127Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5026327Z         """
2025-04-11T03:52:12.5026473Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5026580Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5026666Z             return True
2025-04-11T03:52:12.5026751Z     
2025-04-11T03:52:12.5026888Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5027012Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5027118Z             self.sentinels.keys(),
2025-04-11T03:52:12.5027208Z             timeout=timeout,
2025-04-11T03:52:12.5027294Z         )
2025-04-11T03:52:12.5027466Z     
2025-04-11T03:52:12.5027557Z         error_index = None
2025-04-11T03:52:12.5027671Z         for sentinel in ready:
2025-04-11T03:52:12.5027786Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5027902Z             process = self.processes[index]
2025-04-11T03:52:12.5027993Z             process.join()
2025-04-11T03:52:12.5028096Z             if process.exitcode != 0:
2025-04-11T03:52:12.5028200Z                 error_index = index
2025-04-11T03:52:12.5028281Z                 break
2025-04-11T03:52:12.5028368Z     
2025-04-11T03:52:12.5028508Z         # Return if there was no error.
2025-04-11T03:52:12.5028611Z         if error_index is None:
2025-04-11T03:52:12.5028749Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5028848Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5028932Z     
2025-04-11T03:52:12.5029077Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5029186Z         for process in self.processes:
2025-04-11T03:52:12.5029284Z             if process.is_alive():
2025-04-11T03:52:12.5029383Z                 process.terminate()
2025-04-11T03:52:12.5029482Z             process.join()
2025-04-11T03:52:12.5029557Z     
2025-04-11T03:52:12.5029708Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5029832Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5029955Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5030080Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5030170Z             if exitcode < 0:
2025-04-11T03:52:12.5030289Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5030402Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5030564Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5030666Z                     error_index=error_index,
2025-04-11T03:52:12.5030779Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5030881Z                     exit_code=exitcode,
2025-04-11T03:52:12.5030973Z                     signal_name=name,
2025-04-11T03:52:12.5031062Z                 )
2025-04-11T03:52:12.5031142Z             else:
2025-04-11T03:52:12.5031261Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5031432Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5031530Z                     error_index=error_index,
2025-04-11T03:52:12.5031644Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5031736Z                     exit_code=exitcode,
2025-04-11T03:52:12.5031821Z                 )
2025-04-11T03:52:12.5031897Z     
2025-04-11T03:52:12.5032035Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5032216Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5032306Z         msg += original_trace
2025-04-11T03:52:12.5032500Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5032666Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5032783Z E       
2025-04-11T03:52:12.5032919Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5033160Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5033469Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5033557Z E           fn(i, *args)
2025-04-11T03:52:12.5033796Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all.py", line 31, in run_dist
2025-04-11T03:52:12.5033888Z E           check_4gpu()
2025-04-11T03:52:12.5034126Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5034230Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5034626Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5034746Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5035030Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5035151Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5035260Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5035555Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5035694Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5035858Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5035871Z 
2025-04-11T03:52:12.5036197Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5036357Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5036522Z [04/11/25 03:43:13] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5036655Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5036780Z                              :75 launch                                         
2025-04-11T03:52:12.5036922Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5037058Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.5037257Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5037406Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5037714Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:48337 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5038277Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5038381Z   warnings.warn(
2025-04-11T03:52:12.5038929Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5039028Z   warnings.warn(
2025-04-11T03:52:12.5039571Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5039662Z   warnings.warn(
2025-04-11T03:52:12.5040197Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5040293Z   warnings.warn(
2025-04-11T03:52:12.5040837Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5041026Z   warnings.warn(
2025-04-11T03:52:12.5041559Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5041642Z   warnings.warn(
2025-04-11T03:52:12.5042193Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5042275Z   warnings.warn(
2025-04-11T03:52:12.5042936Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5043020Z   warnings.warn(
2025-04-11T03:52:12.5043173Z ____________________________ test_all_to_all_single ____________________________
2025-04-11T03:52:12.5043181Z 
2025-04-11T03:52:12.5043281Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5043879Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5043895Z 
2025-04-11T03:52:12.5044005Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5044092Z         try_count = 0
2025-04-11T03:52:12.5044215Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5044310Z             max_try, int
2025-04-11T03:52:12.5044472Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5044548Z     
2025-04-11T03:52:12.5044674Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5044751Z             try:
2025-04-11T03:52:12.5044842Z                 try_count += 1
2025-04-11T03:52:12.5044948Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5045033Z                 return ret
2025-04-11T03:52:12.5045141Z             except exception_type as e:
2025-04-11T03:52:12.5045246Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5045438Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5045569Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5045719Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5045887Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5045976Z                     continue
2025-04-11T03:52:12.5046067Z                 else:
2025-04-11T03:52:12.5046293Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5046381Z >                   raise e
2025-04-11T03:52:12.5046387Z 
2025-04-11T03:52:12.5046494Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5046611Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5046755Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5046846Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5047033Z tests/test_fp8/test_fp8_all_to_all_single.py:34: in test_all_to_all_single
2025-04-11T03:52:12.5047121Z     spawn(run_dist, 4)
2025-04-11T03:52:12.5047226Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5047343Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5047604Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5047792Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5048077Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5048294Z     while not context.join():
2025-04-11T03:52:12.5048410Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5048415Z 
2025-04-11T03:52:12.5048612Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f031bca0>
2025-04-11T03:52:12.5048706Z timeout = None
2025-04-11T03:52:12.5048711Z 
2025-04-11T03:52:12.5048805Z     def join(self, timeout=None):
2025-04-11T03:52:12.5048946Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5049021Z     
2025-04-11T03:52:12.5049179Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5049419Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5049586Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5049694Z         of the first process exiting.
2025-04-11T03:52:12.5049770Z     
2025-04-11T03:52:12.5049932Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5050072Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5050157Z     
2025-04-11T03:52:12.5050236Z         Args:
2025-04-11T03:52:12.5050379Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5050469Z         """
2025-04-11T03:52:12.5050614Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5050721Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5050806Z             return True
2025-04-11T03:52:12.5050881Z     
2025-04-11T03:52:12.5051031Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5051162Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5051265Z             self.sentinels.keys(),
2025-04-11T03:52:12.5051354Z             timeout=timeout,
2025-04-11T03:52:12.5051440Z         )
2025-04-11T03:52:12.5051522Z     
2025-04-11T03:52:12.5051613Z         error_index = None
2025-04-11T03:52:12.5051713Z         for sentinel in ready:
2025-04-11T03:52:12.5051822Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5051934Z             process = self.processes[index]
2025-04-11T03:52:12.5052023Z             process.join()
2025-04-11T03:52:12.5052122Z             if process.exitcode != 0:
2025-04-11T03:52:12.5052223Z                 error_index = index
2025-04-11T03:52:12.5052305Z                 break
2025-04-11T03:52:12.5052392Z     
2025-04-11T03:52:12.5052488Z         # Return if there was no error.
2025-04-11T03:52:12.5052578Z         if error_index is None:
2025-04-11T03:52:12.5052727Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5052826Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5052910Z     
2025-04-11T03:52:12.5053054Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5053166Z         for process in self.processes:
2025-04-11T03:52:12.5053262Z             if process.is_alive():
2025-04-11T03:52:12.5053359Z                 process.terminate()
2025-04-11T03:52:12.5053460Z             process.join()
2025-04-11T03:52:12.5053536Z     
2025-04-11T03:52:12.5053692Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5053813Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5053924Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5054063Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5054151Z             if exitcode < 0:
2025-04-11T03:52:12.5054275Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5054388Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5054549Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5054651Z                     error_index=error_index,
2025-04-11T03:52:12.5054757Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5054967Z                     exit_code=exitcode,
2025-04-11T03:52:12.5055060Z                     signal_name=name,
2025-04-11T03:52:12.5055146Z                 )
2025-04-11T03:52:12.5055227Z             else:
2025-04-11T03:52:12.5055333Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5055508Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5055608Z                     error_index=error_index,
2025-04-11T03:52:12.5055719Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5055809Z                     exit_code=exitcode,
2025-04-11T03:52:12.5055995Z                 )
2025-04-11T03:52:12.5056071Z     
2025-04-11T03:52:12.5056210Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5056394Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5056486Z         msg += original_trace
2025-04-11T03:52:12.5056675Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5056840Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5056919Z E       
2025-04-11T03:52:12.5057057Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5057161Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5057472Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5057557Z E           fn(i, *args)
2025-04-11T03:52:12.5057816Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all_single.py", line 29, in run_dist
2025-04-11T03:52:12.5057908Z E           check_4gpu()
2025-04-11T03:52:12.5058132Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5058246Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5058514Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5058629Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5058914Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5059033Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5059145Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5059430Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5059580Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5059747Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5059752Z 
2025-04-11T03:52:12.5060066Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5060225Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5060405Z [04/11/25 03:43:19] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5060542Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5060663Z                              :75 launch                                         
2025-04-11T03:52:12.5060806Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5060934Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.5061138Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5061291Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5061849Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5062176Z   warnings.warn(
2025-04-11T03:52:12.5062722Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5062809Z   warnings.warn(
2025-04-11T03:52:12.5063361Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5063562Z   warnings.warn(
2025-04-11T03:52:12.5064108Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5064192Z   warnings.warn(
2025-04-11T03:52:12.5064735Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5064831Z   warnings.warn(
2025-04-11T03:52:12.5065358Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5065453Z   warnings.warn(
2025-04-11T03:52:12.5065990Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5066090Z   warnings.warn(
2025-04-11T03:52:12.5066620Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5066715Z   warnings.warn(
2025-04-11T03:52:12.5066856Z _______________________________ test_all_gather ________________________________
2025-04-11T03:52:12.5066860Z 
2025-04-11T03:52:12.5066968Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5067563Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5067569Z 
2025-04-11T03:52:12.5067679Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5067778Z         try_count = 0
2025-04-11T03:52:12.5067885Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5067981Z             max_try, int
2025-04-11T03:52:12.5068135Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5068220Z     
2025-04-11T03:52:12.5068340Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5068463Z             try:
2025-04-11T03:52:12.5068569Z                 try_count += 1
2025-04-11T03:52:12.5068666Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5068760Z                 return ret
2025-04-11T03:52:12.5068859Z             except exception_type as e:
2025-04-11T03:52:12.5068964Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5069166Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5069291Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5069459Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5069623Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5069733Z                     continue
2025-04-11T03:52:12.5069816Z                 else:
2025-04-11T03:52:12.5070178Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5070274Z >                   raise e
2025-04-11T03:52:12.5070279Z 
2025-04-11T03:52:12.5070377Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5070500Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5070635Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5070737Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5070887Z tests/test_fp8/test_fp8_allgather.py:42: in test_all_gather
2025-04-11T03:52:12.5070975Z     spawn(run_dist, 4)
2025-04-11T03:52:12.5071199Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5071306Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5071580Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5071760Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5072070Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5072165Z     while not context.join():
2025-04-11T03:52:12.5072281Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5072297Z 
2025-04-11T03:52:12.5072503Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0164100>
2025-04-11T03:52:12.5072587Z timeout = None
2025-04-11T03:52:12.5072593Z 
2025-04-11T03:52:12.5072695Z     def join(self, timeout=None):
2025-04-11T03:52:12.5072828Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5072914Z     
2025-04-11T03:52:12.5073066Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5073217Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5073392Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5073494Z         of the first process exiting.
2025-04-11T03:52:12.5073581Z     
2025-04-11T03:52:12.5073732Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5073884Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5076980Z     
2025-04-11T03:52:12.5077059Z         Args:
2025-04-11T03:52:12.5077212Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5077289Z         """
2025-04-11T03:52:12.5077441Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5077539Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5077634Z             return True
2025-04-11T03:52:12.5077709Z     
2025-04-11T03:52:12.5077848Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5077981Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5078079Z             self.sentinels.keys(),
2025-04-11T03:52:12.5078177Z             timeout=timeout,
2025-04-11T03:52:12.5078255Z         )
2025-04-11T03:52:12.5078330Z     
2025-04-11T03:52:12.5078425Z         error_index = None
2025-04-11T03:52:12.5078517Z         for sentinel in ready:
2025-04-11T03:52:12.5078636Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5078739Z             process = self.processes[index]
2025-04-11T03:52:12.5078827Z             process.join()
2025-04-11T03:52:12.5078939Z             if process.exitcode != 0:
2025-04-11T03:52:12.5079031Z                 error_index = index
2025-04-11T03:52:12.5079119Z                 break
2025-04-11T03:52:12.5079218Z     
2025-04-11T03:52:12.5079326Z         # Return if there was no error.
2025-04-11T03:52:12.5079416Z         if error_index is None:
2025-04-11T03:52:12.5079560Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5079670Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5079752Z     
2025-04-11T03:52:12.5080020Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5080122Z         for process in self.processes:
2025-04-11T03:52:12.5080216Z             if process.is_alive():
2025-04-11T03:52:12.5080324Z                 process.terminate()
2025-04-11T03:52:12.5080414Z             process.join()
2025-04-11T03:52:12.5080502Z     
2025-04-11T03:52:12.5080648Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5080785Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5080899Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5081031Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5081258Z             if exitcode < 0:
2025-04-11T03:52:12.5081369Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5081490Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5081645Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5081758Z                     error_index=error_index,
2025-04-11T03:52:12.5081873Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5081967Z                     exit_code=exitcode,
2025-04-11T03:52:12.5082070Z                     signal_name=name,
2025-04-11T03:52:12.5082150Z                 )
2025-04-11T03:52:12.5082245Z             else:
2025-04-11T03:52:12.5082356Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5082528Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5082642Z                     error_index=error_index,
2025-04-11T03:52:12.5082752Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5082861Z                     exit_code=exitcode,
2025-04-11T03:52:12.5082942Z                 )
2025-04-11T03:52:12.5083022Z     
2025-04-11T03:52:12.5083177Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5083353Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5083457Z         msg += original_trace
2025-04-11T03:52:12.5083632Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5083803Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5083881Z E       
2025-04-11T03:52:12.5084013Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5084127Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5084427Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5084525Z E           fn(i, *args)
2025-04-11T03:52:12.5084755Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allgather.py", line 37, in run_dist
2025-04-11T03:52:12.5084851Z E           check_4gpu()
2025-04-11T03:52:12.5085073Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5085180Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5085450Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5085557Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5085863Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5085974Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5086106Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5086393Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5086535Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5086710Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5086715Z 
2025-04-11T03:52:12.5087129Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5087297Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5087460Z [04/11/25 03:43:25] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5087600Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5087708Z                              :75 launch                                         
2025-04-11T03:52:12.5087858Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5088098Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.5088299Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5088457Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5088763Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56858 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5089067Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56858 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5089625Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5089720Z   warnings.warn(
2025-04-11T03:52:12.5090249Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5090348Z   warnings.warn(
2025-04-11T03:52:12.5090886Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5090970Z   warnings.warn(
2025-04-11T03:52:12.5091508Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5091590Z   warnings.warn(
2025-04-11T03:52:12.5092147Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5092231Z   warnings.warn(
2025-04-11T03:52:12.5092783Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5092865Z   warnings.warn(
2025-04-11T03:52:12.5093426Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5093507Z   warnings.warn(
2025-04-11T03:52:12.5094056Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5094139Z   warnings.warn(
2025-04-11T03:52:12.5094279Z _______________________________ test_all_reduce ________________________________
2025-04-11T03:52:12.5094294Z 
2025-04-11T03:52:12.5094390Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5094998Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5095107Z 
2025-04-11T03:52:12.5095240Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5095325Z         try_count = 0
2025-04-11T03:52:12.5095447Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5095535Z             max_try, int
2025-04-11T03:52:12.5095705Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5095784Z     
2025-04-11T03:52:12.5095904Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5095993Z             try:
2025-04-11T03:52:12.5096083Z                 try_count += 1
2025-04-11T03:52:12.5096190Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5096378Z                 return ret
2025-04-11T03:52:12.5096484Z             except exception_type as e:
2025-04-11T03:52:12.5096602Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5096795Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5096930Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5097079Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5097244Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5097330Z                     continue
2025-04-11T03:52:12.5097413Z                 else:
2025-04-11T03:52:12.5097648Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5097735Z >                   raise e
2025-04-11T03:52:12.5097743Z 
2025-04-11T03:52:12.5097853Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5097973Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5098120Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5098213Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5098373Z tests/test_fp8/test_fp8_allreduce.py:52: in test_all_reduce
2025-04-11T03:52:12.5098473Z     spawn(run_dist, 4)
2025-04-11T03:52:12.5098582Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5098699Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5098957Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5099148Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5099431Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5099525Z     while not context.join():
2025-04-11T03:52:12.5099652Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5099657Z 
2025-04-11T03:52:12.5099856Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0318a30>
2025-04-11T03:52:12.5099950Z timeout = None
2025-04-11T03:52:12.5099955Z 
2025-04-11T03:52:12.5100056Z     def join(self, timeout=None):
2025-04-11T03:52:12.5100195Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5100272Z     
2025-04-11T03:52:12.5100423Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5100578Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5100745Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5100853Z         of the first process exiting.
2025-04-11T03:52:12.5100929Z     
2025-04-11T03:52:12.5101088Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5101230Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5101307Z     
2025-04-11T03:52:12.5101395Z         Args:
2025-04-11T03:52:12.5101538Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5101623Z         """
2025-04-11T03:52:12.5101871Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5101970Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5102063Z             return True
2025-04-11T03:52:12.5102139Z     
2025-04-11T03:52:12.5102283Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5102407Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5102515Z             self.sentinels.keys(),
2025-04-11T03:52:12.5102605Z             timeout=timeout,
2025-04-11T03:52:12.5102683Z         )
2025-04-11T03:52:12.5102767Z     
2025-04-11T03:52:12.5102853Z         error_index = None
2025-04-11T03:52:12.5103061Z         for sentinel in ready:
2025-04-11T03:52:12.5103174Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5103278Z             process = self.processes[index]
2025-04-11T03:52:12.5103377Z             process.join()
2025-04-11T03:52:12.5103474Z             if process.exitcode != 0:
2025-04-11T03:52:12.5103576Z                 error_index = index
2025-04-11T03:52:12.5103657Z                 break
2025-04-11T03:52:12.5103730Z     
2025-04-11T03:52:12.5103836Z         # Return if there was no error.
2025-04-11T03:52:12.5103926Z         if error_index is None:
2025-04-11T03:52:12.5104072Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5104172Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5104255Z     
2025-04-11T03:52:12.5104405Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5104505Z         for process in self.processes:
2025-04-11T03:52:12.5104618Z             if process.is_alive():
2025-04-11T03:52:12.5104719Z                 process.terminate()
2025-04-11T03:52:12.5104827Z             process.join()
2025-04-11T03:52:12.5104905Z     
2025-04-11T03:52:12.5105049Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5105180Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5105296Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5105429Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5105517Z             if exitcode < 0:
2025-04-11T03:52:12.5105635Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5105745Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5105899Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5106007Z                     error_index=error_index,
2025-04-11T03:52:12.5106110Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5106212Z                     exit_code=exitcode,
2025-04-11T03:52:12.5106307Z                     signal_name=name,
2025-04-11T03:52:12.5106386Z                 )
2025-04-11T03:52:12.5106476Z             else:
2025-04-11T03:52:12.5106581Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5106757Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5106858Z                     error_index=error_index,
2025-04-11T03:52:12.5106970Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5107062Z                     exit_code=exitcode,
2025-04-11T03:52:12.5107139Z                 )
2025-04-11T03:52:12.5107225Z     
2025-04-11T03:52:12.5107362Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5107541Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5107631Z         msg += original_trace
2025-04-11T03:52:12.5107806Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5107978Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5108059Z E       
2025-04-11T03:52:12.5108200Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5108304Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5108749Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5108838Z E           fn(i, *args)
2025-04-11T03:52:12.5109075Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allreduce.py", line 47, in run_dist
2025-04-11T03:52:12.5109182Z E           check_4gpu()
2025-04-11T03:52:12.5109446Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.5109559Z E           partial_func(**kwargs)
2025-04-11T03:52:12.5109786Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5110077Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5110340Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5110450Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5110748Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5110863Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5110986Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5111272Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5111424Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5111589Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5111595Z 
2025-04-11T03:52:12.5111908Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5112070Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5112232Z [04/11/25 03:43:31] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5112379Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5112490Z                              :75 launch                                         
2025-04-11T03:52:12.5112641Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5112770Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.5112977Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5113126Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5113424Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30330 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5113739Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30330 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5114014Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30330 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5114579Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5114666Z   warnings.warn(
2025-04-11T03:52:12.5115200Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5115286Z   warnings.warn(
2025-04-11T03:52:12.5115830Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5115911Z   warnings.warn(
2025-04-11T03:52:12.5116445Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5116646Z   warnings.warn(
2025-04-11T03:52:12.5117181Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5117263Z   warnings.warn(
2025-04-11T03:52:12.5117784Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5118007Z   warnings.warn(
2025-04-11T03:52:12.5118554Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5118653Z   warnings.warn(
2025-04-11T03:52:12.5119175Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5119270Z   warnings.warn(
2025-04-11T03:52:12.5119408Z ________________________________ test_fp8_cast _________________________________
2025-04-11T03:52:12.5119412Z 
2025-04-11T03:52:12.5119505Z args = (), kwargs = {}
2025-04-11T03:52:12.5119509Z 
2025-04-11T03:52:12.5119610Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.5119714Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.5119840Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.5119960Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.5120083Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.5120182Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.5120189Z 
2025-04-11T03:52:12.5120296Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.5120412Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5120575Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.5120689Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.5120804Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5120808Z 
2025-04-11T03:52:12.5120904Z device = None
2025-04-11T03:52:12.5120908Z 
2025-04-11T03:52:12.5121036Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5121214Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5121292Z     
2025-04-11T03:52:12.5121370Z         Args:
2025-04-11T03:52:12.5121555Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5121731Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5121859Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5121937Z         """
2025-04-11T03:52:12.5122030Z         _lazy_init()
2025-04-11T03:52:12.5122132Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5122240Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5122362Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5122662Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5122840Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5123008Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5123012Z 
2025-04-11T03:52:12.5123260Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5123504Z __________________________________ test_fsdp ___________________________________
2025-04-11T03:52:12.5123509Z 
2025-04-11T03:52:12.5123618Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5124213Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5124219Z 
2025-04-11T03:52:12.5124334Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5124420Z         try_count = 0
2025-04-11T03:52:12.5124527Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5124748Z             max_try, int
2025-04-11T03:52:12.5124901Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5124988Z     
2025-04-11T03:52:12.5125105Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5125186Z             try:
2025-04-11T03:52:12.5125288Z                 try_count += 1
2025-04-11T03:52:12.5125384Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5125479Z                 return ret
2025-04-11T03:52:12.5125577Z             except exception_type as e:
2025-04-11T03:52:12.5125684Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5125887Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5126013Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5126176Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5126335Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5126429Z                     continue
2025-04-11T03:52:12.5126509Z                 else:
2025-04-11T03:52:12.5126735Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5126828Z >                   raise e
2025-04-11T03:52:12.5126833Z 
2025-04-11T03:52:12.5126927Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5127049Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5127183Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5127281Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5127431Z tests/test_fp8/test_fp8_fsdp_comm_hook.py:104: in test_fsdp
2025-04-11T03:52:12.5127524Z     spawn(demo_basic, n_gpus)
2025-04-11T03:52:12.5127638Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5127742Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5128017Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5128197Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5128496Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5128592Z     while not context.join():
2025-04-11T03:52:12.5128706Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5128719Z 
2025-04-11T03:52:12.5128921Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f031a590>
2025-04-11T03:52:12.5129004Z timeout = None
2025-04-11T03:52:12.5129008Z 
2025-04-11T03:52:12.5129110Z     def join(self, timeout=None):
2025-04-11T03:52:12.5129239Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5129324Z     
2025-04-11T03:52:12.5129476Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5129627Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5129798Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5129895Z         of the first process exiting.
2025-04-11T03:52:12.5129979Z     
2025-04-11T03:52:12.5130227Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5130376Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5130453Z     
2025-04-11T03:52:12.5130531Z         Args:
2025-04-11T03:52:12.5130681Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5130758Z         """
2025-04-11T03:52:12.5130909Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5131005Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5131097Z             return True
2025-04-11T03:52:12.5131174Z     
2025-04-11T03:52:12.5131422Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5131554Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5131649Z             self.sentinels.keys(),
2025-04-11T03:52:12.5131758Z             timeout=timeout,
2025-04-11T03:52:12.5131835Z         )
2025-04-11T03:52:12.5131920Z     
2025-04-11T03:52:12.5132015Z         error_index = None
2025-04-11T03:52:12.5132112Z         for sentinel in ready:
2025-04-11T03:52:12.5132230Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5132332Z             process = self.processes[index]
2025-04-11T03:52:12.5132421Z             process.join()
2025-04-11T03:52:12.5132528Z             if process.exitcode != 0:
2025-04-11T03:52:12.5132619Z                 error_index = index
2025-04-11T03:52:12.5132701Z                 break
2025-04-11T03:52:12.5132776Z     
2025-04-11T03:52:12.5132875Z         # Return if there was no error.
2025-04-11T03:52:12.5132979Z         if error_index is None:
2025-04-11T03:52:12.5133119Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5133232Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5133309Z     
2025-04-11T03:52:12.5133461Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5133561Z         for process in self.processes:
2025-04-11T03:52:12.5133663Z             if process.is_alive():
2025-04-11T03:52:12.5133768Z                 process.terminate()
2025-04-11T03:52:12.5133858Z             process.join()
2025-04-11T03:52:12.5133942Z     
2025-04-11T03:52:12.5134086Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5134206Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5134326Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5134451Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5134550Z             if exitcode < 0:
2025-04-11T03:52:12.5134667Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5134785Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5134939Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5135037Z                     error_index=error_index,
2025-04-11T03:52:12.5135161Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5135254Z                     exit_code=exitcode,
2025-04-11T03:52:12.5135356Z                     signal_name=name,
2025-04-11T03:52:12.5135438Z                 )
2025-04-11T03:52:12.5135529Z             else:
2025-04-11T03:52:12.5135636Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5135805Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5135912Z                     error_index=error_index,
2025-04-11T03:52:12.5136019Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5136120Z                     exit_code=exitcode,
2025-04-11T03:52:12.5136204Z                 )
2025-04-11T03:52:12.5136280Z     
2025-04-11T03:52:12.5136426Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5136596Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5136697Z         msg += original_trace
2025-04-11T03:52:12.5136978Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5137156Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5137235Z E       
2025-04-11T03:52:12.5137368Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5137482Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5137787Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5137880Z E           fn(i, *args)
2025-04-11T03:52:12.5138130Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_fsdp_comm_hook.py", line 95, in demo_basic
2025-04-11T03:52:12.5138335Z E           run_model()
2025-04-11T03:52:12.5138561Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5138665Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5138941Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5139046Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5139333Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5139442Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5139561Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5139847Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5139993Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5140169Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5140174Z 
2025-04-11T03:52:12.5140478Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5140647Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5140759Z Running basic FSDP example on rank 4.
2025-04-11T03:52:12.5140874Z Running basic FSDP example on rank 5.
2025-04-11T03:52:12.5140977Z Running basic FSDP example on rank 1.
2025-04-11T03:52:12.5141074Z Running basic FSDP example on rank 6.
2025-04-11T03:52:12.5141185Z Running basic FSDP example on rank 3.
2025-04-11T03:52:12.5141281Z Running basic FSDP example on rank 0.
2025-04-11T03:52:12.5141448Z [04/11/25 03:43:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5141582Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5141703Z                              :75 launch                                         
2025-04-11T03:52:12.5141843Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5141971Z                              environment is initialized, world size: 8          
2025-04-11T03:52:12.5142079Z Running basic FSDP example on rank 7.
2025-04-11T03:52:12.5142173Z Running basic FSDP example on rank 2.
2025-04-11T03:52:12.5142380Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5142528Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5142827Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43732 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5143111Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43732 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.5143672Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5143759Z   warnings.warn(
2025-04-11T03:52:12.5144402Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5144497Z   warnings.warn(
2025-04-11T03:52:12.5145037Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5145130Z   warnings.warn(
2025-04-11T03:52:12.5145656Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5145867Z   warnings.warn(
2025-04-11T03:52:12.5146394Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5146478Z   warnings.warn(
2025-04-11T03:52:12.5146996Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5147079Z   warnings.warn(
2025-04-11T03:52:12.5147603Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5147685Z   warnings.warn(
2025-04-11T03:52:12.5148226Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5148312Z   warnings.warn(
2025-04-11T03:52:12.5148898Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5148987Z   warnings.warn(
2025-04-11T03:52:12.5149527Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5149611Z   warnings.warn(
2025-04-11T03:52:12.5150163Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5150249Z   warnings.warn(
2025-04-11T03:52:12.5150774Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5150872Z   warnings.warn(
2025-04-11T03:52:12.5151411Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5151507Z   warnings.warn(
2025-04-11T03:52:12.5152032Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5152129Z   warnings.warn(
2025-04-11T03:52:12.5152665Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5152762Z   warnings.warn(
2025-04-11T03:52:12.5153287Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5153507Z   warnings.warn(
2025-04-11T03:52:12.5154252Z [rank3]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T03:52:12.5154989Z [rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T03:52:12.5155831Z [rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T03:52:12.5156571Z [rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T03:52:12.5157294Z [rank4]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T03:52:12.5158037Z [rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
2025-04-11T03:52:12.5158184Z ________________________________ test_fp8_hook _________________________________
2025-04-11T03:52:12.5158189Z 
2025-04-11T03:52:12.5158485Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T03:52:12.5158579Z     def test_fp8_hook():
2025-04-11T03:52:12.5158674Z         # create tensors
2025-04-11T03:52:12.5158873Z >       w = nn.Parameter(torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE))
2025-04-11T03:52:12.5158988Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5159286Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5159429Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5159603Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5159608Z 
2025-04-11T03:52:12.5159729Z tests/test_fp8/test_fp8_hook.py:41: RuntimeError
2025-04-11T03:52:12.5159889Z __________________________ test_fp8_linear[True-True] __________________________
2025-04-11T03:52:12.5159893Z 
2025-04-11T03:52:12.5159989Z use_bias = True, use_batch = True
2025-04-11T03:52:12.5159993Z 
2025-04-11T03:52:12.5160272Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T03:52:12.5160408Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T03:52:12.5160548Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T03:52:12.5160679Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T03:52:12.5160764Z         # create tensors
2025-04-11T03:52:12.5161091Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T03:52:12.5161203Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5161499Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5161636Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5161803Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5161808Z 
2025-04-11T03:52:12.5161932Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T03:52:12.5162079Z _________________________ test_fp8_linear[True-False] __________________________
2025-04-11T03:52:12.5162202Z 
2025-04-11T03:52:12.5162315Z use_bias = False, use_batch = True
2025-04-11T03:52:12.5162320Z 
2025-04-11T03:52:12.5162589Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T03:52:12.5162734Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T03:52:12.5162866Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T03:52:12.5163005Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T03:52:12.5163091Z         # create tensors
2025-04-11T03:52:12.5163285Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T03:52:12.5163407Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5163688Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5163840Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5164004Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5164008Z 
2025-04-11T03:52:12.5164142Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T03:52:12.5164290Z _________________________ test_fp8_linear[False-True] __________________________
2025-04-11T03:52:12.5164297Z 
2025-04-11T03:52:12.5164403Z use_bias = True, use_batch = False
2025-04-11T03:52:12.5164407Z 
2025-04-11T03:52:12.5164673Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T03:52:12.5164808Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T03:52:12.5164936Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T03:52:12.5165062Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T03:52:12.5165159Z         # create tensors
2025-04-11T03:52:12.5165353Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T03:52:12.5165470Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5165750Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5165894Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5166054Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5166059Z 
2025-04-11T03:52:12.5166180Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T03:52:12.5166353Z _________________________ test_fp8_linear[False-False] _________________________
2025-04-11T03:52:12.5166358Z 
2025-04-11T03:52:12.5166458Z use_bias = False, use_batch = False
2025-04-11T03:52:12.5166462Z 
2025-04-11T03:52:12.5166743Z     @pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
2025-04-11T03:52:12.5166876Z     @pytest.mark.parametrize("use_bias", [True, False])
2025-04-11T03:52:12.5167011Z     @pytest.mark.parametrize("use_batch", [True, False])
2025-04-11T03:52:12.5167138Z     def test_fp8_linear(use_bias: bool, use_batch: bool):
2025-04-11T03:52:12.5167232Z         # create tensors
2025-04-11T03:52:12.5167531Z >       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
2025-04-11T03:52:12.5167641Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5167930Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5168064Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5168229Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5168234Z 
2025-04-11T03:52:12.5168355Z tests/test_fp8/test_fp8_linear.py:20: RuntimeError
2025-04-11T03:52:12.5168508Z _____________________________ test_reduce_scatter ______________________________
2025-04-11T03:52:12.5168622Z 
2025-04-11T03:52:12.5168722Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5169331Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5169349Z 
2025-04-11T03:52:12.5169455Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5169539Z         try_count = 0
2025-04-11T03:52:12.5169654Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5169738Z             max_try, int
2025-04-11T03:52:12.5169899Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5169976Z     
2025-04-11T03:52:12.5170104Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5170183Z             try:
2025-04-11T03:52:12.5170274Z                 try_count += 1
2025-04-11T03:52:12.5170383Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5170466Z                 return ret
2025-04-11T03:52:12.5170576Z             except exception_type as e:
2025-04-11T03:52:12.5170681Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5170874Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5171012Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5171162Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5171333Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5171421Z                     continue
2025-04-11T03:52:12.5171511Z                 else:
2025-04-11T03:52:12.5171737Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5171826Z >                   raise e
2025-04-11T03:52:12.5171830Z 
2025-04-11T03:52:12.5171939Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5172054Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5172197Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5172292Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5172469Z tests/test_fp8/test_fp8_reduce_scatter.py:41: in test_reduce_scatter
2025-04-11T03:52:12.5172558Z     spawn(run_dist, 4)
2025-04-11T03:52:12.5172664Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5172776Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5173032Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5173221Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5173512Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5173618Z     while not context.join():
2025-04-11T03:52:12.5173734Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5173737Z 
2025-04-11T03:52:12.5173938Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f031b430>
2025-04-11T03:52:12.5174144Z timeout = None
2025-04-11T03:52:12.5174149Z 
2025-04-11T03:52:12.5174245Z     def join(self, timeout=None):
2025-04-11T03:52:12.5174384Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5174460Z     
2025-04-11T03:52:12.5174618Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5174766Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5174934Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5175039Z         of the first process exiting.
2025-04-11T03:52:12.5175115Z     
2025-04-11T03:52:12.5175367Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5175511Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5175636Z     
2025-04-11T03:52:12.5175717Z         Args:
2025-04-11T03:52:12.5175859Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5175957Z         """
2025-04-11T03:52:12.5176101Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5176209Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5176301Z             return True
2025-04-11T03:52:12.5176375Z     
2025-04-11T03:52:12.5176521Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5176644Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5176749Z             self.sentinels.keys(),
2025-04-11T03:52:12.5176838Z             timeout=timeout,
2025-04-11T03:52:12.5176925Z         )
2025-04-11T03:52:12.5177001Z     
2025-04-11T03:52:12.5177090Z         error_index = None
2025-04-11T03:52:12.5177189Z         for sentinel in ready:
2025-04-11T03:52:12.5177298Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5177409Z             process = self.processes[index]
2025-04-11T03:52:12.5177499Z             process.join()
2025-04-11T03:52:12.5177599Z             if process.exitcode != 0:
2025-04-11T03:52:12.5177700Z                 error_index = index
2025-04-11T03:52:12.5177782Z                 break
2025-04-11T03:52:12.5177865Z     
2025-04-11T03:52:12.5177962Z         # Return if there was no error.
2025-04-11T03:52:12.5178053Z         if error_index is None:
2025-04-11T03:52:12.5178200Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5178303Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5178387Z     
2025-04-11T03:52:12.5178532Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5178641Z         for process in self.processes:
2025-04-11T03:52:12.5178740Z             if process.is_alive():
2025-04-11T03:52:12.5178838Z                 process.terminate()
2025-04-11T03:52:12.5178937Z             process.join()
2025-04-11T03:52:12.5179013Z     
2025-04-11T03:52:12.5179168Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5179291Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5179404Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5179540Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5179628Z             if exitcode < 0:
2025-04-11T03:52:12.5179749Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5179861Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5180026Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5180129Z                     error_index=error_index,
2025-04-11T03:52:12.5180236Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5180344Z                     exit_code=exitcode,
2025-04-11T03:52:12.5180437Z                     signal_name=name,
2025-04-11T03:52:12.5180524Z                 )
2025-04-11T03:52:12.5180604Z             else:
2025-04-11T03:52:12.5180713Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5181094Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5181196Z                     error_index=error_index,
2025-04-11T03:52:12.5181313Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5181411Z                     exit_code=exitcode,
2025-04-11T03:52:12.5181512Z                 )
2025-04-11T03:52:12.5181594Z     
2025-04-11T03:52:12.5181734Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5181927Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5182021Z         msg += original_trace
2025-04-11T03:52:12.5182317Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5182485Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5182568Z E       
2025-04-11T03:52:12.5182715Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5182822Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5183131Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5183216Z E           fn(i, *args)
2025-04-11T03:52:12.5183466Z E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_reduce_scatter.py", line 36, in run_dist
2025-04-11T03:52:12.5183551Z E           check_4gpu()
2025-04-11T03:52:12.5183775Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.5183889Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.5184155Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.5184272Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.5184550Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.5184681Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5184826Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5185116Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5185262Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5185423Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5185428Z 
2025-04-11T03:52:12.5185742Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5185899Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5186064Z [04/11/25 03:43:46] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5186196Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5186316Z                              :75 launch                                         
2025-04-11T03:52:12.5186456Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5186585Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.5186789Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5186938Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5187502Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5187590Z   warnings.warn(
2025-04-11T03:52:12.5188129Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5188315Z   warnings.warn(
2025-04-11T03:52:12.5188913Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5188997Z   warnings.warn(
2025-04-11T03:52:12.5189526Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5189619Z   warnings.warn(
2025-04-11T03:52:12.5190295Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5190389Z   warnings.warn(
2025-04-11T03:52:12.5190919Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5191017Z   warnings.warn(
2025-04-11T03:52:12.5191559Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5191653Z   warnings.warn(
2025-04-11T03:52:12.5192182Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.5192276Z   warnings.warn(
2025-04-11T03:52:12.5192410Z _________________________________ test_bucket __________________________________
2025-04-11T03:52:12.5192414Z 
2025-04-11T03:52:12.5192492Z kwargs = {}
2025-04-11T03:52:12.5192703Z val = {'block_size': 4, 'dtype': torch.float16, 'max_batch_size': 4, 'max_input_len': 32, ...}
2025-04-11T03:52:12.5192943Z arg_map = {'test_config': {'block_size': 4, 'dtype': torch.float16, 'max_batch_size': 4, 'max_input_len': 32, ...}}
2025-04-11T03:52:12.5193409Z partial_func = functools.partial(<function test_bucket at 0x7f68f1d997e0>, test_config={'block_size': 4, 'max_batch_size': 4, 'max_input_len': 32, 'max_output_len': 8, 'dtype': torch.float16, 'tp_size': 1})
2025-04-11T03:52:12.5193415Z 
2025-04-11T03:52:12.5193527Z     def _execute_function_by_param(**kwargs):
2025-04-11T03:52:12.5193624Z         for val in values:
2025-04-11T03:52:12.5193722Z             arg_map = {argument: val}
2025-04-11T03:52:12.5193852Z             partial_func = partial(func, **arg_map)
2025-04-11T03:52:12.5193950Z >           partial_func(**kwargs)
2025-04-11T03:52:12.5193954Z 
2025-04-11T03:52:12.5194063Z colossalai/testing/utils.py:64: 
2025-04-11T03:52:12.5194203Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5194348Z tests/test_infer/test_batch_bucket.py:42: in test_bucket
2025-04-11T03:52:12.5194523Z     cache_manager = KVCacheManager(inference_config, model_config)
2025-04-11T03:52:12.5194693Z colossalai/inference/kv_cache/kvcache_manager.py:105: in __init__
2025-04-11T03:52:12.5194864Z     self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T03:52:12.5194980Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5194984Z 
2025-04-11T03:52:12.5195221Z self = <colossalai.inference.kv_cache.kvcache_manager.KVCacheManager object at 0x7f68f0228fa0>
2025-04-11T03:52:12.5195370Z kalloc_shape = (40, 4, 4, 32), valloc_shape = (40, 4, 4, 32)
2025-04-11T03:52:12.5195378Z 
2025-04-11T03:52:12.5195471Z     def _init_device_caches(
2025-04-11T03:52:12.5195643Z         self, kalloc_shape: Tuple[int, ...], valloc_shape: Tuple[int, ...]
2025-04-11T03:52:12.5195754Z     ) -> Tuple[torch.Tensor, torch.Tensor]:
2025-04-11T03:52:12.5196006Z         """Initialize the physical cache on the device.
2025-04-11T03:52:12.5196083Z     
2025-04-11T03:52:12.5196281Z         For each layer of the model, we allocate two tensors for key and value respectively,
2025-04-11T03:52:12.5196448Z         with shape of [num_blocks, num_kv_heads, block_size, head_size]
2025-04-11T03:52:12.5196527Z         """
2025-04-11T03:52:12.5196638Z         k_cache: List[torch.Tensor] = []
2025-04-11T03:52:12.5196735Z         v_cache: List[torch.Tensor] = []
2025-04-11T03:52:12.5196842Z         for _ in range(self.num_layers):
2025-04-11T03:52:12.5197062Z >           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T03:52:12.5197297Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5197599Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5197751Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5197936Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5197941Z 
2025-04-11T03:52:12.5198113Z colossalai/inference/kv_cache/kvcache_manager.py:519: RuntimeError
2025-04-11T03:52:12.5198274Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5198422Z [04/11/25 03:43:47] INFO     colossalai -                                       
2025-04-11T03:52:12.5198570Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T03:52:12.5198678Z                              INFO:                                              
2025-04-11T03:52:12.5198809Z                              /__w/ColossalAI/ColossalAI/colossalai/inference/kv_
2025-04-11T03:52:12.5198941Z                              cache/kvcache_manager.py:104 __init__              
2025-04-11T03:52:12.5199066Z                     INFO     colossalai -                                       
2025-04-11T03:52:12.5199211Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T03:52:12.5199334Z                              INFO: Allocating KV cache with shape: (40, 4, 4,   
2025-04-11T03:52:12.5199460Z                              32) consisting of 40 blocks.                       
2025-04-11T03:52:12.5199603Z ___________________________ test_continuous_batching ___________________________
2025-04-11T03:52:12.5199608Z 
2025-04-11T03:52:12.5199713Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5200309Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5200318Z 
2025-04-11T03:52:12.5200425Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5200516Z         try_count = 0
2025-04-11T03:52:12.5200621Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5200721Z             max_try, int
2025-04-11T03:52:12.5200868Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5200955Z     
2025-04-11T03:52:12.5201082Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5201170Z             try:
2025-04-11T03:52:12.5201268Z                 try_count += 1
2025-04-11T03:52:12.5201366Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5201465Z                 return ret
2025-04-11T03:52:12.5201567Z             except exception_type as e:
2025-04-11T03:52:12.5201677Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5201877Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5202001Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5202160Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5202427Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5202521Z                     continue
2025-04-11T03:52:12.5202602Z                 else:
2025-04-11T03:52:12.5202827Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5202920Z >                   raise e
2025-04-11T03:52:12.5202924Z 
2025-04-11T03:52:12.5203024Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5203149Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5203284Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5203495Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5203686Z tests/test_infer/test_continuous_batching.py:67: in test_continuous_batching
2025-04-11T03:52:12.5203773Z     spawn(run_dist, 1)
2025-04-11T03:52:12.5203890Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5203996Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5204269Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5204448Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5204746Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5204841Z     while not context.join():
2025-04-11T03:52:12.5204954Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5204966Z 
2025-04-11T03:52:12.5205162Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f202a470>
2025-04-11T03:52:12.5205248Z timeout = None
2025-04-11T03:52:12.5205252Z 
2025-04-11T03:52:12.5205356Z     def join(self, timeout=None):
2025-04-11T03:52:12.5205484Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5205569Z     
2025-04-11T03:52:12.5205719Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5205871Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5206047Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5206148Z         of the first process exiting.
2025-04-11T03:52:12.5206235Z     
2025-04-11T03:52:12.5206384Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5206532Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5206608Z     
2025-04-11T03:52:12.5206687Z         Args:
2025-04-11T03:52:12.5206837Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5206918Z         """
2025-04-11T03:52:12.5207069Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5207165Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5207249Z             return True
2025-04-11T03:52:12.5207338Z     
2025-04-11T03:52:12.5207474Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5207609Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5207705Z             self.sentinels.keys(),
2025-04-11T03:52:12.5207803Z             timeout=timeout,
2025-04-11T03:52:12.5207881Z         )
2025-04-11T03:52:12.5207957Z     
2025-04-11T03:52:12.5208054Z         error_index = None
2025-04-11T03:52:12.5208145Z         for sentinel in ready:
2025-04-11T03:52:12.5208262Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5208364Z             process = self.processes[index]
2025-04-11T03:52:12.5208454Z             process.join()
2025-04-11T03:52:12.5208566Z             if process.exitcode != 0:
2025-04-11T03:52:12.5208659Z                 error_index = index
2025-04-11T03:52:12.5208746Z                 break
2025-04-11T03:52:12.5208821Z     
2025-04-11T03:52:12.5208918Z         # Return if there was no error.
2025-04-11T03:52:12.5209020Z         if error_index is None:
2025-04-11T03:52:12.5209261Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5209375Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5209456Z     
2025-04-11T03:52:12.5209615Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5209722Z         for process in self.processes:
2025-04-11T03:52:12.5209819Z             if process.is_alive():
2025-04-11T03:52:12.5209928Z                 process.terminate()
2025-04-11T03:52:12.5210020Z             process.join()
2025-04-11T03:52:12.5210108Z     
2025-04-11T03:52:12.5210268Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5210548Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5210681Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5210810Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5210911Z             if exitcode < 0:
2025-04-11T03:52:12.5211027Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5211151Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5211307Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5211410Z                     error_index=error_index,
2025-04-11T03:52:12.5211527Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5211623Z                     exit_code=exitcode,
2025-04-11T03:52:12.5211727Z                     signal_name=name,
2025-04-11T03:52:12.5211807Z                 )
2025-04-11T03:52:12.5211897Z             else:
2025-04-11T03:52:12.5212003Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5212172Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5212281Z                     error_index=error_index,
2025-04-11T03:52:12.5212387Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5212489Z                     exit_code=exitcode,
2025-04-11T03:52:12.5212570Z                 )
2025-04-11T03:52:12.5212645Z     
2025-04-11T03:52:12.5212790Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5212964Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5213063Z         msg += original_trace
2025-04-11T03:52:12.5213241Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5213410Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5213488Z E       
2025-04-11T03:52:12.5213617Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5213732Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5214033Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5214124Z E           fn(i, *args)
2025-04-11T03:52:12.5214379Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 61, in run_dist
2025-04-11T03:52:12.5214484Z E           check_inference_engine()
2025-04-11T03:52:12.5214745Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.5214839Z E           partial_func(**kwargs)
2025-04-11T03:52:12.5215104Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.5215196Z E           partial_func(**kwargs)
2025-04-11T03:52:12.5215456Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.5215549Z E           partial_func(**kwargs)
2025-04-11T03:52:12.5215668Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.5215954Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 39, in check_inference_engine
2025-04-11T03:52:12.5216238Z E           model = LlamaForCausalLM(LlamaConfig(num_hidden_layers=2)).cuda()
2025-04-11T03:52:12.5216538Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.5216646Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.5216922Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.5217043Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.5217323Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.5217528Z E           module._apply(fn)
2025-04-11T03:52:12.5217799Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.5217899Z E           module._apply(fn)
2025-04-11T03:52:12.5218165Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.5218279Z E           param_applied = fn(param)
2025-04-11T03:52:12.5218558Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.5218690Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.5218802Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5219102Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5219243Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5219411Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5219416Z 
2025-04-11T03:52:12.5219733Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5219893Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5220061Z [04/11/25 03:43:55] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5220201Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5220324Z                              :75 launch                                         
2025-04-11T03:52:12.5220472Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5220612Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.5220810Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5220963Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5222104Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.5222287Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.5222987Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.5223080Z   warnings.warn(
2025-04-11T03:52:12.5223918Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5224111Z   warnings.warn(
2025-04-11T03:52:12.5224932Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5225020Z   warnings.warn(
2025-04-11T03:52:12.5225842Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5226055Z   warnings.warn(
2025-04-11T03:52:12.5226861Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5226949Z   warnings.warn(
2025-04-11T03:52:12.5227762Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5227846Z   warnings.warn(
2025-04-11T03:52:12.5228696Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5228783Z   warnings.warn(
2025-04-11T03:52:12.5229596Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5229678Z   warnings.warn(
2025-04-11T03:52:12.5230465Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5230561Z   warnings.warn(
2025-04-11T03:52:12.5231349Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5231446Z   warnings.warn(
2025-04-11T03:52:12.5231581Z _______________________________ test_drafter[5] ________________________________
2025-04-11T03:52:12.5231586Z 
2025-04-11T03:52:12.5231966Z tokenizer = LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fas... special=True),
2025-04-11T03:52:12.5232198Z 	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
2025-04-11T03:52:12.5232295Z }
2025-04-11T03:52:12.5232380Z spec_num = 5
2025-04-11T03:52:12.5232385Z 
2025-04-11T03:52:12.5232525Z     @pytest.mark.parametrize("spec_num", [SPEC_NUM])
2025-04-11T03:52:12.5232643Z     def test_drafter(tokenizer, spec_num: int):
2025-04-11T03:52:12.5232737Z         torch.manual_seed(123)
2025-04-11T03:52:12.5232825Z     
2025-04-11T03:52:12.5232923Z         device = get_current_device()
2025-04-11T03:52:12.5233066Z         toy_config = LlamaConfig(num_hidden_layers=NUM_LAYERS)
2025-04-11T03:52:12.5233312Z         toy_config.pad_token_id = tokenizer.eos_token_id
2025-04-11T03:52:12.5233439Z         drafter_model = LlamaForCausalLM(toy_config)
2025-04-11T03:52:12.5233553Z >       drafter_model = drafter_model.eval().cuda()
2025-04-11T03:52:12.5233558Z 
2025-04-11T03:52:12.5233660Z tests/test_infer/test_drafter.py:27: 
2025-04-11T03:52:12.5233784Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5234036Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2548: in cuda
2025-04-11T03:52:12.5234146Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.5234484Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: in cuda
2025-04-11T03:52:12.5234606Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.5234846Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.5234942Z     module._apply(fn)
2025-04-11T03:52:12.5235191Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.5235279Z     module._apply(fn)
2025-04-11T03:52:12.5235523Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.5235621Z     param_applied = fn(param)
2025-04-11T03:52:12.5235746Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5235750Z 
2025-04-11T03:52:12.5235845Z t = Parameter containing:
2025-04-11T03:52:12.5235988Z tensor([[-0.0259,  0.0026,  0.0006,  ...,  0.0104,  0.0194,  0.0062],
2025-04-11T03:52:12.5236100Z         [-0.0076,  0.0020,...5,  0.0329,  0.0046],
2025-04-11T03:52:12.5236230Z         [-0.0124,  0.0230, -0.0264,  ..., -0.0224, -0.0274, -0.0157]],
2025-04-11T03:52:12.5236329Z        requires_grad=True)
2025-04-11T03:52:12.5236334Z 
2025-04-11T03:52:12.5236447Z >   return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.5236567Z E   RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5236860Z E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5236997Z E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5237171Z E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5237176Z 
2025-04-11T03:52:12.5237427Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: RuntimeError
2025-04-11T03:52:12.5237573Z ________________________________ test_spec_dec _________________________________
2025-04-11T03:52:12.5237580Z 
2025-04-11T03:52:12.5237950Z tokenizer = LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fas... special=True),
2025-04-11T03:52:12.5238190Z 	2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
2025-04-11T03:52:12.5238280Z }
2025-04-11T03:52:12.5238285Z 
2025-04-11T03:52:12.5238391Z     def test_spec_dec(tokenizer):
2025-04-11T03:52:12.5238487Z         spec_num = SPEC_NUM
2025-04-11T03:52:12.5238585Z         device = get_current_device()
2025-04-11T03:52:12.5238713Z         tokenizer.pad_token = tokenizer.eos_token
2025-04-11T03:52:12.5238789Z     
2025-04-11T03:52:12.5238901Z         # Dummy config for Glide Model
2025-04-11T03:52:12.5239004Z         glide_config = GlideLlamaConfig(
2025-04-11T03:52:12.5239105Z             intermediate_size=8192,
2025-04-11T03:52:12.5239197Z             large_hidden_size=4096,
2025-04-11T03:52:12.5239297Z             large_num_attention_heads=32,
2025-04-11T03:52:12.5239408Z             num_hidden_layers=NUM_LAYERS,
2025-04-11T03:52:12.5239488Z         )
2025-04-11T03:52:12.5239636Z         drafter_model = GlideLlamaForCausalLM(glide_config)
2025-04-11T03:52:12.5239712Z     
2025-04-11T03:52:12.5239821Z         assert hasattr(drafter_model, "model")
2025-04-11T03:52:12.5240062Z         assert hasattr(drafter_model.model, "layers")
2025-04-11T03:52:12.5240201Z         for _, layer in enumerate(drafter_model.model.layers):
2025-04-11T03:52:12.5240317Z             assert hasattr(layer, "cross_attn")
2025-04-11T03:52:12.5240394Z     
2025-04-11T03:52:12.5240544Z         # Init the Drafter by providing the sharded drafter model
2025-04-11T03:52:12.5240742Z >       drafter = Drafter(drafter_model, tokenizer, device=device, dtype=torch.float16)
2025-04-11T03:52:12.5240748Z 
2025-04-11T03:52:12.5240850Z tests/test_infer/test_drafter.py:65: 
2025-04-11T03:52:12.5240976Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5241199Z colossalai/inference/spec/drafter.py:31: in __init__
2025-04-11T03:52:12.5241322Z     self._drafter_model = model.to(self._device)
2025-04-11T03:52:12.5241571Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.5241685Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.5241923Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.5242027Z     return self._apply(convert)
2025-04-11T03:52:12.5242283Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.5242371Z     module._apply(fn)
2025-04-11T03:52:12.5242619Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.5242706Z     module._apply(fn)
2025-04-11T03:52:12.5242950Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.5243049Z     param_applied = fn(param)
2025-04-11T03:52:12.5243166Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5243170Z 
2025-04-11T03:52:12.5243273Z t = Parameter containing:
2025-04-11T03:52:12.5243418Z tensor([[-0.0389,  0.0039, -0.0004,  ...,  0.0133,  0.0029, -0.0177],
2025-04-11T03:52:12.5243532Z         [-0.0144,  0.0054,...4,  0.0227,  0.0264],
2025-04-11T03:52:12.5243660Z         [ 0.0320, -0.0080,  0.0294,  ...,  0.0173,  0.0005, -0.0045]],
2025-04-11T03:52:12.5243756Z        requires_grad=True)
2025-04-11T03:52:12.5243761Z 
2025-04-11T03:52:12.5243845Z     def convert(t):
2025-04-11T03:52:12.5243981Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.5244177Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.5244301Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.5244521Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.5244637Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5244939Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5245082Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5245250Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5245264Z 
2025-04-11T03:52:12.5245525Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.5245666Z ______________________________ test_cache_manager ______________________________
2025-04-11T03:52:12.5245670Z 
2025-04-11T03:52:12.5245774Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5246384Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5246393Z 
2025-04-11T03:52:12.5246511Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5246594Z         try_count = 0
2025-04-11T03:52:12.5246706Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5246891Z             max_try, int
2025-04-11T03:52:12.5247044Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5247129Z     
2025-04-11T03:52:12.5247244Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5247333Z             try:
2025-04-11T03:52:12.5247421Z                 try_count += 1
2025-04-11T03:52:12.5247537Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5247634Z                 return ret
2025-04-11T03:52:12.5247733Z             except exception_type as e:
2025-04-11T03:52:12.5247871Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5248158Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5248293Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5248444Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5248616Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5248704Z                     continue
2025-04-11T03:52:12.5248785Z                 else:
2025-04-11T03:52:12.5249017Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5249103Z >                   raise e
2025-04-11T03:52:12.5249107Z 
2025-04-11T03:52:12.5249218Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5249334Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5249478Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5249572Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5249739Z tests/test_infer/test_kvcache_manager.py:174: in test_cache_manager
2025-04-11T03:52:12.5249837Z     spawn(run_dist, 1)
2025-04-11T03:52:12.5249942Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5250060Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5250317Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5250503Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5250789Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5250882Z     while not context.join():
2025-04-11T03:52:12.5251004Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5251008Z 
2025-04-11T03:52:12.5251206Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f202a500>
2025-04-11T03:52:12.5251304Z timeout = None
2025-04-11T03:52:12.5251309Z 
2025-04-11T03:52:12.5251403Z     def join(self, timeout=None):
2025-04-11T03:52:12.5251541Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5251618Z     
2025-04-11T03:52:12.5251770Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5251931Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5252097Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5252208Z         of the first process exiting.
2025-04-11T03:52:12.5252284Z     
2025-04-11T03:52:12.5252443Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5252584Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5252660Z     
2025-04-11T03:52:12.5252750Z         Args:
2025-04-11T03:52:12.5252893Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5252985Z         """
2025-04-11T03:52:12.5253129Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5253227Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5253322Z             return True
2025-04-11T03:52:12.5253524Z     
2025-04-11T03:52:12.5253669Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5253792Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5253891Z             self.sentinels.keys(),
2025-04-11T03:52:12.5253989Z             timeout=timeout,
2025-04-11T03:52:12.5254066Z         )
2025-04-11T03:52:12.5254165Z     
2025-04-11T03:52:12.5254257Z         error_index = None
2025-04-11T03:52:12.5254369Z         for sentinel in ready:
2025-04-11T03:52:12.5254486Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5254590Z             process = self.processes[index]
2025-04-11T03:52:12.5254700Z             process.join()
2025-04-11T03:52:12.5254917Z             if process.exitcode != 0:
2025-04-11T03:52:12.5255020Z                 error_index = index
2025-04-11T03:52:12.5255102Z                 break
2025-04-11T03:52:12.5255178Z     
2025-04-11T03:52:12.5255286Z         # Return if there was no error.
2025-04-11T03:52:12.5255377Z         if error_index is None:
2025-04-11T03:52:12.5255526Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5255628Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5255711Z     
2025-04-11T03:52:12.5255854Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5255957Z         for process in self.processes:
2025-04-11T03:52:12.5256063Z             if process.is_alive():
2025-04-11T03:52:12.5256161Z                 process.terminate()
2025-04-11T03:52:12.5256257Z             process.join()
2025-04-11T03:52:12.5256334Z     
2025-04-11T03:52:12.5256481Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5256613Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5256735Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5256872Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5256969Z             if exitcode < 0:
2025-04-11T03:52:12.5257085Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5257209Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5257365Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5257474Z                     error_index=error_index,
2025-04-11T03:52:12.5257582Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5257685Z                     exit_code=exitcode,
2025-04-11T03:52:12.5257776Z                     signal_name=name,
2025-04-11T03:52:12.5257855Z                 )
2025-04-11T03:52:12.5257943Z             else:
2025-04-11T03:52:12.5258053Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5258236Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5258334Z                     error_index=error_index,
2025-04-11T03:52:12.5258449Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5258546Z                     exit_code=exitcode,
2025-04-11T03:52:12.5258624Z                 )
2025-04-11T03:52:12.5258710Z     
2025-04-11T03:52:12.5258847Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5259029Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5259118Z         msg += original_trace
2025-04-11T03:52:12.5259297Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5259471Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5259548Z E       
2025-04-11T03:52:12.5259688Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5259794Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5260110Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5260196Z E           fn(i, *args)
2025-04-11T03:52:12.5260442Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 168, in run_dist
2025-04-11T03:52:12.5260648Z E           check_cache_manager()
2025-04-11T03:52:12.5260909Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.5261013Z E           partial_func(**kwargs)
2025-04-11T03:52:12.5261275Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 89, in check_cache_manager
2025-04-11T03:52:12.5261443Z E           cache_manager = KVCacheManager(inference_config, model_config)
2025-04-11T03:52:12.5261709Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T03:52:12.5261986Z E           self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T03:52:12.5262291Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T03:52:12.5262515Z E           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T03:52:12.5262638Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5262920Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5263069Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5263235Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5263240Z 
2025-04-11T03:52:12.5263558Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5263719Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5263879Z [04/11/25 03:44:22] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5264030Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5264147Z                              :75 launch                                         
2025-04-11T03:52:12.5264304Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5264444Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.5264607Z [04/11/25 03:44:22] INFO     colossalai -                                       
2025-04-11T03:52:12.5264745Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T03:52:12.5264859Z                              INFO:                                              
2025-04-11T03:52:12.5264992Z                              /__w/ColossalAI/ColossalAI/colossalai/inference/kv_
2025-04-11T03:52:12.5265115Z                              cache/kvcache_manager.py:104 __init__              
2025-04-11T03:52:12.5265249Z                     INFO     colossalai -                                       
2025-04-11T03:52:12.5265390Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T03:52:12.5265521Z                              INFO: Allocating KV cache with shape: (80, 16, 8,  
2025-04-11T03:52:12.5265640Z                              32) consisting of 80 blocks.                       
2025-04-11T03:52:12.5265846Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5265997Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5267127Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.5267434Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.5267598Z ____________________ test_running_list_and_request_handler _____________________
2025-04-11T03:52:12.5267603Z 
2025-04-11T03:52:12.5267702Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5268304Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5268321Z 
2025-04-11T03:52:12.5268467Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5268694Z         try_count = 0
2025-04-11T03:52:12.5268816Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5268906Z             max_try, int
2025-04-11T03:52:12.5269075Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5269154Z     
2025-04-11T03:52:12.5269273Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5269368Z             try:
2025-04-11T03:52:12.5269456Z                 try_count += 1
2025-04-11T03:52:12.5269567Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5269651Z                 return ret
2025-04-11T03:52:12.5269762Z             except exception_type as e:
2025-04-11T03:52:12.5269868Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5270054Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5270186Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5270339Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5270505Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5270592Z                     continue
2025-04-11T03:52:12.5270680Z                 else:
2025-04-11T03:52:12.5270907Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5270993Z >                   raise e
2025-04-11T03:52:12.5270997Z 
2025-04-11T03:52:12.5271105Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5271222Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5271365Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5271455Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5271659Z tests/test_infer/test_request_handler.py:101: in test_running_list_and_request_handler
2025-04-11T03:52:12.5271753Z     spawn(run_dist, 1)
2025-04-11T03:52:12.5271860Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5271971Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5272227Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5272415Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5272703Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5272804Z     while not context.join():
2025-04-11T03:52:12.5272915Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5272919Z 
2025-04-11T03:52:12.5273116Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be79fc70>
2025-04-11T03:52:12.5273225Z timeout = None
2025-04-11T03:52:12.5273230Z 
2025-04-11T03:52:12.5273323Z     def join(self, timeout=None):
2025-04-11T03:52:12.5273466Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5273545Z     
2025-04-11T03:52:12.5273699Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5273851Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5274016Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5274266Z         of the first process exiting.
2025-04-11T03:52:12.5274343Z     
2025-04-11T03:52:12.5274505Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5274646Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5274722Z     
2025-04-11T03:52:12.5274811Z         Args:
2025-04-11T03:52:12.5274953Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5275040Z         """
2025-04-11T03:52:12.5275184Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5275293Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5275488Z             return True
2025-04-11T03:52:12.5275568Z     
2025-04-11T03:52:12.5275717Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5275840Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5275949Z             self.sentinels.keys(),
2025-04-11T03:52:12.5276042Z             timeout=timeout,
2025-04-11T03:52:12.5276118Z         )
2025-04-11T03:52:12.5276204Z     
2025-04-11T03:52:12.5276289Z         error_index = None
2025-04-11T03:52:12.5276387Z         for sentinel in ready:
2025-04-11T03:52:12.5276498Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5276599Z             process = self.processes[index]
2025-04-11T03:52:12.5276697Z             process.join()
2025-04-11T03:52:12.5276796Z             if process.exitcode != 0:
2025-04-11T03:52:12.5276900Z                 error_index = index
2025-04-11T03:52:12.5276982Z                 break
2025-04-11T03:52:12.5277067Z     
2025-04-11T03:52:12.5277167Z         # Return if there was no error.
2025-04-11T03:52:12.5277257Z         if error_index is None:
2025-04-11T03:52:12.5277404Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5277504Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5277587Z     
2025-04-11T03:52:12.5277735Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5277839Z         for process in self.processes:
2025-04-11T03:52:12.5277942Z             if process.is_alive():
2025-04-11T03:52:12.5278038Z                 process.terminate()
2025-04-11T03:52:12.5278137Z             process.join()
2025-04-11T03:52:12.5278214Z     
2025-04-11T03:52:12.5278367Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5278489Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5278600Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5278738Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5278829Z             if exitcode < 0:
2025-04-11T03:52:12.5278949Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5279060Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5279212Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5279325Z                     error_index=error_index,
2025-04-11T03:52:12.5279429Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5279532Z                     exit_code=exitcode,
2025-04-11T03:52:12.5279627Z                     signal_name=name,
2025-04-11T03:52:12.5279715Z                 )
2025-04-11T03:52:12.5279794Z             else:
2025-04-11T03:52:12.5279899Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5280074Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5280170Z                     error_index=error_index,
2025-04-11T03:52:12.5280284Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5280373Z                     exit_code=exitcode,
2025-04-11T03:52:12.5280450Z                 )
2025-04-11T03:52:12.5280534Z     
2025-04-11T03:52:12.5280669Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5280981Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5281074Z         msg += original_trace
2025-04-11T03:52:12.5281258Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5281420Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5281499Z E       
2025-04-11T03:52:12.5281638Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5281741Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5282051Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5282249Z E           fn(i, *args)
2025-04-11T03:52:12.5282504Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 95, in run_dist
2025-04-11T03:52:12.5282601Z E           check_request_handler()
2025-04-11T03:52:12.5282876Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 70, in check_request_handler
2025-04-11T03:52:12.5283050Z E           request_handler = RequestHandler(inference_config, model_config)
2025-04-11T03:52:12.5283311Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 160, in __init__
2025-04-11T03:52:12.5283425Z E           self._init_cache(model_config)
2025-04-11T03:52:12.5283688Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 222, in _init_cache
2025-04-11T03:52:12.5283893Z E           self.cache_manager = KVCacheManager(self.inference_config, model_config)
2025-04-11T03:52:12.5284156Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T03:52:12.5284332Z E           self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T03:52:12.5284626Z E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T03:52:12.5284846Z E           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T03:52:12.5284968Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5285253Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5285399Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5285562Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5285566Z 
2025-04-11T03:52:12.5285878Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5286036Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5286203Z [04/11/25 03:44:26] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5286333Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5286448Z                              :75 launch                                         
2025-04-11T03:52:12.5286599Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5286725Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.5286872Z [04/11/25 03:44:26] INFO     colossalai -                                       
2025-04-11T03:52:12.5287008Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T03:52:12.5287120Z                              INFO:                                              
2025-04-11T03:52:12.5287250Z                              /__w/ColossalAI/ColossalAI/colossalai/inference/kv_
2025-04-11T03:52:12.5287375Z                              cache/kvcache_manager.py:104 __init__              
2025-04-11T03:52:12.5287507Z                     INFO     colossalai -                                       
2025-04-11T03:52:12.5287755Z                              colossalai.inference.kv_cache.kvcache_manager -    
2025-04-11T03:52:12.5287888Z                              INFO: Allocating KV cache with shape: (24, 4, 8, 8)
2025-04-11T03:52:12.5288009Z                              consisting of 24 blocks.                           
2025-04-11T03:52:12.5288215Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5288366Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5289496Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.5289878Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.5290028Z _________________________________ test_engine __________________________________
2025-04-11T03:52:12.5290032Z 
2025-04-11T03:52:12.5290133Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.5290729Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.5290745Z 
2025-04-11T03:52:12.5290858Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.5290953Z         try_count = 0
2025-04-11T03:52:12.5291077Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.5291170Z             max_try, int
2025-04-11T03:52:12.5291337Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.5291428Z     
2025-04-11T03:52:12.5291558Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.5291666Z             try:
2025-04-11T03:52:12.5291762Z                 try_count += 1
2025-04-11T03:52:12.5291878Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.5291969Z                 return ret
2025-04-11T03:52:12.5292084Z             except exception_type as e:
2025-04-11T03:52:12.5292197Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.5292392Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.5292536Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.5292694Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.5292867Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.5292960Z                     continue
2025-04-11T03:52:12.5293053Z                 else:
2025-04-11T03:52:12.5293284Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.5293374Z >                   raise e
2025-04-11T03:52:12.5293379Z 
2025-04-11T03:52:12.5293493Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.5293612Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5293759Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.5293856Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.5294013Z tests/test_infer/test_streamingllm.py:117: in test_engine
2025-04-11T03:52:12.5294179Z     spawn(run_dist, 1, func_to_run=check_streamingllm, ret=result_list)
2025-04-11T03:52:12.5294292Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.5294414Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.5294674Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.5294973Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.5295262Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.5295364Z     while not context.join():
2025-04-11T03:52:12.5295479Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5295483Z 
2025-04-11T03:52:12.5295685Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f007f100>
2025-04-11T03:52:12.5295776Z timeout = None
2025-04-11T03:52:12.5295781Z 
2025-04-11T03:52:12.5295875Z     def join(self, timeout=None):
2025-04-11T03:52:12.5296115Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.5296192Z     
2025-04-11T03:52:12.5296352Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.5296501Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.5296671Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.5296780Z         of the first process exiting.
2025-04-11T03:52:12.5296857Z     
2025-04-11T03:52:12.5297017Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.5297158Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.5297244Z     
2025-04-11T03:52:12.5297324Z         Args:
2025-04-11T03:52:12.5297469Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.5297556Z         """
2025-04-11T03:52:12.5297699Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.5297807Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.5297891Z             return True
2025-04-11T03:52:12.5297966Z     
2025-04-11T03:52:12.5298114Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.5298238Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.5298349Z             self.sentinels.keys(),
2025-04-11T03:52:12.5298437Z             timeout=timeout,
2025-04-11T03:52:12.5298514Z         )
2025-04-11T03:52:12.5298599Z     
2025-04-11T03:52:12.5298686Z         error_index = None
2025-04-11T03:52:12.5298784Z         for sentinel in ready:
2025-04-11T03:52:12.5298896Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.5299008Z             process = self.processes[index]
2025-04-11T03:52:12.5299099Z             process.join()
2025-04-11T03:52:12.5299198Z             if process.exitcode != 0:
2025-04-11T03:52:12.5299299Z                 error_index = index
2025-04-11T03:52:12.5299384Z                 break
2025-04-11T03:52:12.5299467Z     
2025-04-11T03:52:12.5299565Z         # Return if there was no error.
2025-04-11T03:52:12.5299655Z         if error_index is None:
2025-04-11T03:52:12.5299800Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.5299900Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.5299989Z     
2025-04-11T03:52:12.5300134Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.5300236Z         for process in self.processes:
2025-04-11T03:52:12.5300342Z             if process.is_alive():
2025-04-11T03:52:12.5300439Z                 process.terminate()
2025-04-11T03:52:12.5300538Z             process.join()
2025-04-11T03:52:12.5300613Z     
2025-04-11T03:52:12.5300774Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.5300903Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.5301014Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.5301160Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.5301248Z             if exitcode < 0:
2025-04-11T03:52:12.5301377Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.5301485Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5301750Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.5301864Z                     error_index=error_index,
2025-04-11T03:52:12.5301973Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5302081Z                     exit_code=exitcode,
2025-04-11T03:52:12.5302180Z                     signal_name=name,
2025-04-11T03:52:12.5302271Z                 )
2025-04-11T03:52:12.5302355Z             else:
2025-04-11T03:52:12.5302467Z                 raise ProcessExitedException(
2025-04-11T03:52:12.5302649Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.5302855Z                     error_index=error_index,
2025-04-11T03:52:12.5302972Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.5303068Z                     exit_code=exitcode,
2025-04-11T03:52:12.5303162Z                 )
2025-04-11T03:52:12.5303243Z     
2025-04-11T03:52:12.5303383Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.5303583Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.5303673Z         msg += original_trace
2025-04-11T03:52:12.5303856Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.5304019Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.5304095Z E       
2025-04-11T03:52:12.5304233Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.5304334Z E       Traceback (most recent call last):
2025-04-11T03:52:12.5304642Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.5304731Z E           fn(i, *args)
2025-04-11T03:52:12.5304974Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 107, in run_dist
2025-04-11T03:52:12.5305076Z E           ret[rank] = func_to_run(**kwargs)
2025-04-11T03:52:12.5305334Z E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 39, in check_streamingllm
2025-04-11T03:52:12.5305420Z E           ).cuda()
2025-04-11T03:52:12.5305708Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.5305820Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.5306086Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.5306213Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.5306480Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.5306575Z E           module._apply(fn)
2025-04-11T03:52:12.5306851Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.5306945Z E           module._apply(fn)
2025-04-11T03:52:12.5307217Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.5307315Z E           param_applied = fn(param)
2025-04-11T03:52:12.5307592Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.5307710Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.5307824Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5308108Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5308251Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5308453Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5308458Z 
2025-04-11T03:52:12.5308760Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.5309048Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.5309207Z [04/11/25 03:44:30] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.5309348Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.5309458Z                              :75 launch                                         
2025-04-11T03:52:12.5309599Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.5309731Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.5310056Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.5310236Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.5311437Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.5311626Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.5312322Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.5312418Z   warnings.warn(
2025-04-11T03:52:12.5313228Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.5352471Z   warnings.warn(
2025-04-11T03:52:12.5352713Z ________________________ test_flash_decoding_attention _________________________
2025-04-11T03:52:12.5352722Z 
2025-04-11T03:52:12.5352833Z args = (), kwargs = {}
2025-04-11T03:52:12.5352839Z 
2025-04-11T03:52:12.5352950Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.5353078Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.5353200Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.5353325Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.5353467Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.5353574Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.5353579Z 
2025-04-11T03:52:12.5353692Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.5353817Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5354007Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.5354111Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.5354230Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5354235Z 
2025-04-11T03:52:12.5354322Z device = None
2025-04-11T03:52:12.5354328Z 
2025-04-11T03:52:12.5354458Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5354639Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5354716Z     
2025-04-11T03:52:12.5354803Z         Args:
2025-04-11T03:52:12.5355006Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5355199Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5355331Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5355640Z         """
2025-04-11T03:52:12.5355778Z         _lazy_init()
2025-04-11T03:52:12.5355880Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5355998Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5356118Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5356437Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5356606Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5356786Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5357025Z 
2025-04-11T03:52:12.5357290Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5357448Z ______________________ test_vllm_flash_decoding_attention ______________________
2025-04-11T03:52:12.5357454Z 
2025-04-11T03:52:12.5357547Z args = (), kwargs = {}
2025-04-11T03:52:12.5357556Z 
2025-04-11T03:52:12.5357654Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.5357768Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.5357881Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.5357999Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.5358116Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.5358211Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.5358216Z 
2025-04-11T03:52:12.5358323Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.5358438Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5358612Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.5358713Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.5358827Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5358832Z 
2025-04-11T03:52:12.5358921Z device = None
2025-04-11T03:52:12.5358931Z 
2025-04-11T03:52:12.5359051Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5359217Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5359292Z     
2025-04-11T03:52:12.5359378Z         Args:
2025-04-11T03:52:12.5359554Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5359722Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5359842Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5359918Z         """
2025-04-11T03:52:12.5360008Z         _lazy_init()
2025-04-11T03:52:12.5360108Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5360213Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5360330Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5360622Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5360773Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5360938Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5360942Z 
2025-04-11T03:52:12.5361191Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5361342Z _____________________ test_get_cos_and_sin[dtype0-64-64-4] _____________________
2025-04-11T03:52:12.5361346Z 
2025-04-11T03:52:12.5361509Z BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float16
2025-04-11T03:52:12.5361514Z 
2025-04-11T03:52:12.5361634Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.5361756Z     @pytest.mark.parametrize("MAX_SEQ_LEN", [64])
2025-04-11T03:52:12.5361877Z     @pytest.mark.parametrize("HEAD_DIM", [64])
2025-04-11T03:52:12.5362044Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T03:52:12.5362344Z     def test_get_cos_and_sin(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
2025-04-11T03:52:12.5362452Z         MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
2025-04-11T03:52:12.5362653Z >       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
2025-04-11T03:52:12.5362768Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5363064Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5363212Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5363375Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5363587Z 
2025-04-11T03:52:12.5363788Z tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py:24: RuntimeError
2025-04-11T03:52:12.5363942Z _____________________ test_get_cos_and_sin[dtype1-64-64-4] _____________________
2025-04-11T03:52:12.5363946Z 
2025-04-11T03:52:12.5364108Z BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float32
2025-04-11T03:52:12.5364116Z 
2025-04-11T03:52:12.5364227Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.5364349Z     @pytest.mark.parametrize("MAX_SEQ_LEN", [64])
2025-04-11T03:52:12.5364461Z     @pytest.mark.parametrize("HEAD_DIM", [64])
2025-04-11T03:52:12.5364626Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T03:52:12.5364787Z     def test_get_cos_and_sin(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
2025-04-11T03:52:12.5364893Z         MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
2025-04-11T03:52:12.5365085Z >       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
2025-04-11T03:52:12.5365195Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5365491Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5365630Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5365800Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5365805Z 
2025-04-11T03:52:12.5365985Z tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py:24: RuntimeError
2025-04-11T03:52:12.5366136Z ____________________ test_kv_cache_memcopy[True-16-8-16-4] _____________________
2025-04-11T03:52:12.5366149Z 
2025-04-11T03:52:12.5366304Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5366393Z same_context_len = True
2025-04-11T03:52:12.5366397Z 
2025-04-11T03:52:12.5366514Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5366652Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5366808Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5366924Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5367068Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5367179Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5367261Z         bsz: int,
2025-04-11T03:52:12.5367358Z         block_size: int,
2025-04-11T03:52:12.5367454Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5367547Z         num_kv_heads: int,
2025-04-11T03:52:12.5367640Z         same_context_len: bool,
2025-04-11T03:52:12.5367717Z     ):
2025-04-11T03:52:12.5367954Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5367960Z 
2025-04-11T03:52:12.5368116Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5368238Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5368245Z 
2025-04-11T03:52:12.5368392Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5368487Z same_context_len = True
2025-04-11T03:52:12.5368491Z 
2025-04-11T03:52:12.5368588Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5368779Z         bsz: int,
2025-04-11T03:52:12.5368877Z         block_size: int,
2025-04-11T03:52:12.5368972Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5369063Z         num_kv_heads: int,
2025-04-11T03:52:12.5369154Z         same_context_len: bool,
2025-04-11T03:52:12.5369238Z     ):
2025-04-11T03:52:12.5369327Z         torch.manual_seed(123)
2025-04-11T03:52:12.5369404Z     
2025-04-11T03:52:12.5369616Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5369739Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5369840Z         dtype = torch.float16
2025-04-11T03:52:12.5370205Z         device = get_current_device()
2025-04-11T03:52:12.5370278Z     
2025-04-11T03:52:12.5370373Z         if same_context_len:
2025-04-11T03:52:12.5370608Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5370732Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5371017Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5371166Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5371330Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5371334Z 
2025-04-11T03:52:12.5371515Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5371672Z ____________________ test_kv_cache_memcopy[True-16-8-16-7] _____________________
2025-04-11T03:52:12.5371679Z 
2025-04-11T03:52:12.5371828Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5371924Z same_context_len = True
2025-04-11T03:52:12.5371929Z 
2025-04-11T03:52:12.5372036Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5372173Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5372319Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5372442Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5372584Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5372677Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5372768Z         bsz: int,
2025-04-11T03:52:12.5372855Z         block_size: int,
2025-04-11T03:52:12.5372955Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5373040Z         num_kv_heads: int,
2025-04-11T03:52:12.5373132Z         same_context_len: bool,
2025-04-11T03:52:12.5373213Z     ):
2025-04-11T03:52:12.5373442Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5373447Z 
2025-04-11T03:52:12.5373606Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5373719Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5373727Z 
2025-04-11T03:52:12.5373878Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5373964Z same_context_len = True
2025-04-11T03:52:12.5373968Z 
2025-04-11T03:52:12.5374078Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5374161Z         bsz: int,
2025-04-11T03:52:12.5374262Z         block_size: int,
2025-04-11T03:52:12.5374361Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5374445Z         num_kv_heads: int,
2025-04-11T03:52:12.5374549Z         same_context_len: bool,
2025-04-11T03:52:12.5374622Z     ):
2025-04-11T03:52:12.5374708Z         torch.manual_seed(123)
2025-04-11T03:52:12.5374795Z     
2025-04-11T03:52:12.5374996Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5375128Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5375218Z         dtype = torch.float16
2025-04-11T03:52:12.5375314Z         device = get_current_device()
2025-04-11T03:52:12.5375513Z     
2025-04-11T03:52:12.5375602Z         if same_context_len:
2025-04-11T03:52:12.5375839Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5375947Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5376234Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5376376Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5376533Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5376644Z 
2025-04-11T03:52:12.5376827Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5376976Z ____________________ test_kv_cache_memcopy[True-16-8-16-32] ____________________
2025-04-11T03:52:12.5376981Z 
2025-04-11T03:52:12.5377137Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5377226Z same_context_len = True
2025-04-11T03:52:12.5377231Z 
2025-04-11T03:52:12.5377348Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5377477Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5377625Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5377741Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5377881Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5377982Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5378059Z         bsz: int,
2025-04-11T03:52:12.5378152Z         block_size: int,
2025-04-11T03:52:12.5378243Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5378327Z         num_kv_heads: int,
2025-04-11T03:52:12.5378420Z         same_context_len: bool,
2025-04-11T03:52:12.5378495Z     ):
2025-04-11T03:52:12.5378726Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5378734Z 
2025-04-11T03:52:12.5378886Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5379007Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5379012Z 
2025-04-11T03:52:12.5379156Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5379245Z same_context_len = True
2025-04-11T03:52:12.5379250Z 
2025-04-11T03:52:12.5379346Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5379424Z         bsz: int,
2025-04-11T03:52:12.5379515Z         block_size: int,
2025-04-11T03:52:12.5379609Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5379699Z         num_kv_heads: int,
2025-04-11T03:52:12.5379785Z         same_context_len: bool,
2025-04-11T03:52:12.5379859Z     ):
2025-04-11T03:52:12.5379952Z         torch.manual_seed(123)
2025-04-11T03:52:12.5380026Z     
2025-04-11T03:52:12.5380229Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5380352Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5380446Z         dtype = torch.float16
2025-04-11T03:52:12.5380538Z         device = get_current_device()
2025-04-11T03:52:12.5380611Z     
2025-04-11T03:52:12.5380707Z         if same_context_len:
2025-04-11T03:52:12.5380928Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5381043Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5381323Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5381463Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5381626Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5381631Z 
2025-04-11T03:52:12.5381940Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5382094Z ____________________ test_kv_cache_memcopy[True-16-8-32-4] _____________________
2025-04-11T03:52:12.5382098Z 
2025-04-11T03:52:12.5382242Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5382333Z same_context_len = True
2025-04-11T03:52:12.5382337Z 
2025-04-11T03:52:12.5382451Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5382585Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5382731Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5382937Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5383085Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5383174Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5383258Z         bsz: int,
2025-04-11T03:52:12.5383342Z         block_size: int,
2025-04-11T03:52:12.5383440Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5383525Z         num_kv_heads: int,
2025-04-11T03:52:12.5383610Z         same_context_len: bool,
2025-04-11T03:52:12.5383690Z     ):
2025-04-11T03:52:12.5383911Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5383915Z 
2025-04-11T03:52:12.5384073Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5384186Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5384191Z 
2025-04-11T03:52:12.5384337Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5384424Z same_context_len = True
2025-04-11T03:52:12.5384428Z 
2025-04-11T03:52:12.5384523Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5384606Z         bsz: int,
2025-04-11T03:52:12.5384690Z         block_size: int,
2025-04-11T03:52:12.5384787Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5384874Z         num_kv_heads: int,
2025-04-11T03:52:12.5384968Z         same_context_len: bool,
2025-04-11T03:52:12.5385044Z     ):
2025-04-11T03:52:12.5385132Z         torch.manual_seed(123)
2025-04-11T03:52:12.5385210Z     
2025-04-11T03:52:12.5385409Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5385546Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5385633Z         dtype = torch.float16
2025-04-11T03:52:12.5385731Z         device = get_current_device()
2025-04-11T03:52:12.5385810Z     
2025-04-11T03:52:12.5385897Z         if same_context_len:
2025-04-11T03:52:12.5386132Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5386239Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5386528Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5386670Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5386828Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5386832Z 
2025-04-11T03:52:12.5387017Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5387161Z ____________________ test_kv_cache_memcopy[True-16-8-32-7] _____________________
2025-04-11T03:52:12.5387165Z 
2025-04-11T03:52:12.5387315Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5387399Z same_context_len = True
2025-04-11T03:52:12.5387405Z 
2025-04-11T03:52:12.5387519Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5387645Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5387796Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5387909Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5388160Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5388256Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5388332Z         bsz: int,
2025-04-11T03:52:12.5389004Z         block_size: int,
2025-04-11T03:52:12.5389098Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5389181Z         num_kv_heads: int,
2025-04-11T03:52:12.5389276Z         same_context_len: bool,
2025-04-11T03:52:12.5389351Z     ):
2025-04-11T03:52:12.5389580Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5389585Z 
2025-04-11T03:52:12.5389875Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5389993Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5389998Z 
2025-04-11T03:52:12.5390144Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5390233Z same_context_len = True
2025-04-11T03:52:12.5390245Z 
2025-04-11T03:52:12.5390339Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5390416Z         bsz: int,
2025-04-11T03:52:12.5390505Z         block_size: int,
2025-04-11T03:52:12.5390597Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5390685Z         num_kv_heads: int,
2025-04-11T03:52:12.5390773Z         same_context_len: bool,
2025-04-11T03:52:12.5390846Z     ):
2025-04-11T03:52:12.5390943Z         torch.manual_seed(123)
2025-04-11T03:52:12.5391016Z     
2025-04-11T03:52:12.5391224Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5391346Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5391434Z         dtype = torch.float16
2025-04-11T03:52:12.5391533Z         device = get_current_device()
2025-04-11T03:52:12.5391607Z     
2025-04-11T03:52:12.5391700Z         if same_context_len:
2025-04-11T03:52:12.5391923Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5392040Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5392319Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5392455Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5392618Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5392622Z 
2025-04-11T03:52:12.5392800Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5392959Z ____________________ test_kv_cache_memcopy[True-16-8-32-32] ____________________
2025-04-11T03:52:12.5392963Z 
2025-04-11T03:52:12.5393112Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5393202Z same_context_len = True
2025-04-11T03:52:12.5393206Z 
2025-04-11T03:52:12.5393314Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5393447Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5393588Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5393702Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5393846Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5393935Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5394015Z         bsz: int,
2025-04-11T03:52:12.5394096Z         block_size: int,
2025-04-11T03:52:12.5394185Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5394274Z         num_kv_heads: int,
2025-04-11T03:52:12.5394364Z         same_context_len: bool,
2025-04-11T03:52:12.5394445Z     ):
2025-04-11T03:52:12.5394663Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5394667Z 
2025-04-11T03:52:12.5394823Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5395062Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5395066Z 
2025-04-11T03:52:12.5395215Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5395302Z same_context_len = True
2025-04-11T03:52:12.5395306Z 
2025-04-11T03:52:12.5395399Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5395484Z         bsz: int,
2025-04-11T03:52:12.5395568Z         block_size: int,
2025-04-11T03:52:12.5395663Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5395748Z         num_kv_heads: int,
2025-04-11T03:52:12.5395836Z         same_context_len: bool,
2025-04-11T03:52:12.5396011Z     ):
2025-04-11T03:52:12.5396099Z         torch.manual_seed(123)
2025-04-11T03:52:12.5396179Z     
2025-04-11T03:52:12.5396376Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5396507Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5396604Z         dtype = torch.float16
2025-04-11T03:52:12.5396702Z         device = get_current_device()
2025-04-11T03:52:12.5396788Z     
2025-04-11T03:52:12.5396871Z         if same_context_len:
2025-04-11T03:52:12.5397101Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5397210Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5397490Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5397646Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5397810Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5397815Z 
2025-04-11T03:52:12.5397998Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5398144Z ____________________ test_kv_cache_memcopy[True-16-8-64-4] _____________________
2025-04-11T03:52:12.5398151Z 
2025-04-11T03:52:12.5398303Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5398389Z same_context_len = True
2025-04-11T03:52:12.5398393Z 
2025-04-11T03:52:12.5398507Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5398634Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5398775Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5398901Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5399041Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5399142Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5399219Z         bsz: int,
2025-04-11T03:52:12.5399310Z         block_size: int,
2025-04-11T03:52:12.5399403Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5399489Z         num_kv_heads: int,
2025-04-11T03:52:12.5399586Z         same_context_len: bool,
2025-04-11T03:52:12.5399665Z     ):
2025-04-11T03:52:12.5399895Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5399900Z 
2025-04-11T03:52:12.5400053Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5400175Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5400179Z 
2025-04-11T03:52:12.5400324Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5400409Z same_context_len = True
2025-04-11T03:52:12.5400419Z 
2025-04-11T03:52:12.5400513Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5400596Z         bsz: int,
2025-04-11T03:52:12.5400690Z         block_size: int,
2025-04-11T03:52:12.5400783Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5400874Z         num_kv_heads: int,
2025-04-11T03:52:12.5400961Z         same_context_len: bool,
2025-04-11T03:52:12.5401034Z     ):
2025-04-11T03:52:12.5401240Z         torch.manual_seed(123)
2025-04-11T03:52:12.5401316Z     
2025-04-11T03:52:12.5401521Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5401642Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5401731Z         dtype = torch.float16
2025-04-11T03:52:12.5401835Z         device = get_current_device()
2025-04-11T03:52:12.5401910Z     
2025-04-11T03:52:12.5402004Z         if same_context_len:
2025-04-11T03:52:12.5402230Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5402347Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5402744Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5402881Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5403050Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5403057Z 
2025-04-11T03:52:12.5403236Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5403393Z ____________________ test_kv_cache_memcopy[True-16-8-64-7] _____________________
2025-04-11T03:52:12.5403397Z 
2025-04-11T03:52:12.5403540Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5403633Z same_context_len = True
2025-04-11T03:52:12.5403637Z 
2025-04-11T03:52:12.5403746Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5403878Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5404024Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5404141Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5404293Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5404383Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5404476Z         bsz: int,
2025-04-11T03:52:12.5404558Z         block_size: int,
2025-04-11T03:52:12.5404649Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5404742Z         num_kv_heads: int,
2025-04-11T03:52:12.5404832Z         same_context_len: bool,
2025-04-11T03:52:12.5404915Z     ):
2025-04-11T03:52:12.5405139Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5405143Z 
2025-04-11T03:52:12.5405301Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5405412Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5405419Z 
2025-04-11T03:52:12.5405566Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5405660Z same_context_len = True
2025-04-11T03:52:12.5405664Z 
2025-04-11T03:52:12.5405758Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5405846Z         bsz: int,
2025-04-11T03:52:12.5405936Z         block_size: int,
2025-04-11T03:52:12.5406035Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5406119Z         num_kv_heads: int,
2025-04-11T03:52:12.5406207Z         same_context_len: bool,
2025-04-11T03:52:12.5406292Z     ):
2025-04-11T03:52:12.5406382Z         torch.manual_seed(123)
2025-04-11T03:52:12.5406464Z     
2025-04-11T03:52:12.5406664Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5406785Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5406883Z         dtype = torch.float16
2025-04-11T03:52:12.5406978Z         device = get_current_device()
2025-04-11T03:52:12.5407068Z     
2025-04-11T03:52:12.5407155Z         if same_context_len:
2025-04-11T03:52:12.5407391Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5407499Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5407900Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5408049Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5408209Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5408213Z 
2025-04-11T03:52:12.5408399Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5408545Z ____________________ test_kv_cache_memcopy[True-16-8-64-32] ____________________
2025-04-11T03:52:12.5408549Z 
2025-04-11T03:52:12.5408703Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5408895Z same_context_len = True
2025-04-11T03:52:12.5408900Z 
2025-04-11T03:52:12.5409018Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5409150Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5409296Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5409438Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5409580Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5409678Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5409756Z         bsz: int,
2025-04-11T03:52:12.5409848Z         block_size: int,
2025-04-11T03:52:12.5409939Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5410023Z         num_kv_heads: int,
2025-04-11T03:52:12.5410121Z         same_context_len: bool,
2025-04-11T03:52:12.5410195Z     ):
2025-04-11T03:52:12.5410425Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5410433Z 
2025-04-11T03:52:12.5410586Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5410699Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5410711Z 
2025-04-11T03:52:12.5410861Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5410948Z same_context_len = True
2025-04-11T03:52:12.5410952Z 
2025-04-11T03:52:12.5411056Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5411134Z         bsz: int,
2025-04-11T03:52:12.5411227Z         block_size: int,
2025-04-11T03:52:12.5411318Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5411401Z         num_kv_heads: int,
2025-04-11T03:52:12.5411497Z         same_context_len: bool,
2025-04-11T03:52:12.5411576Z     ):
2025-04-11T03:52:12.5411709Z         torch.manual_seed(123)
2025-04-11T03:52:12.5411799Z     
2025-04-11T03:52:12.5412009Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5412132Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5412221Z         dtype = torch.float16
2025-04-11T03:52:12.5412322Z         device = get_current_device()
2025-04-11T03:52:12.5412396Z     
2025-04-11T03:52:12.5412493Z         if same_context_len:
2025-04-11T03:52:12.5412715Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5412824Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5413117Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5413258Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5413423Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5413427Z 
2025-04-11T03:52:12.5413609Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5413763Z ____________________ test_kv_cache_memcopy[True-16-32-16-4] ____________________
2025-04-11T03:52:12.5413767Z 
2025-04-11T03:52:12.5413915Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5414122Z same_context_len = True
2025-04-11T03:52:12.5414126Z 
2025-04-11T03:52:12.5414236Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5414364Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5414517Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5414635Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5414782Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5414870Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5414954Z         bsz: int,
2025-04-11T03:52:12.5415039Z         block_size: int,
2025-04-11T03:52:12.5415235Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5415329Z         num_kv_heads: int,
2025-04-11T03:52:12.5415418Z         same_context_len: bool,
2025-04-11T03:52:12.5415499Z     ):
2025-04-11T03:52:12.5415722Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5415730Z 
2025-04-11T03:52:12.5415888Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5416003Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5416007Z 
2025-04-11T03:52:12.5416151Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5416245Z same_context_len = True
2025-04-11T03:52:12.5416249Z 
2025-04-11T03:52:12.5416346Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5416431Z         bsz: int,
2025-04-11T03:52:12.5416514Z         block_size: int,
2025-04-11T03:52:12.5416609Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5416697Z         num_kv_heads: int,
2025-04-11T03:52:12.5416785Z         same_context_len: bool,
2025-04-11T03:52:12.5416868Z     ):
2025-04-11T03:52:12.5416957Z         torch.manual_seed(123)
2025-04-11T03:52:12.5417038Z     
2025-04-11T03:52:12.5417237Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5417358Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5417457Z         dtype = torch.float16
2025-04-11T03:52:12.5417553Z         device = get_current_device()
2025-04-11T03:52:12.5417632Z     
2025-04-11T03:52:12.5417717Z         if same_context_len:
2025-04-11T03:52:12.5417944Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5418059Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5418345Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5418499Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5418658Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5418662Z 
2025-04-11T03:52:12.5418855Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5419012Z ____________________ test_kv_cache_memcopy[True-16-32-16-7] ____________________
2025-04-11T03:52:12.5419016Z 
2025-04-11T03:52:12.5419192Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5419277Z same_context_len = True
2025-04-11T03:52:12.5419281Z 
2025-04-11T03:52:12.5419394Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5419520Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5419661Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5419787Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5419928Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5420021Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5420098Z         bsz: int,
2025-04-11T03:52:12.5420182Z         block_size: int,
2025-04-11T03:52:12.5420280Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5420481Z         num_kv_heads: int,
2025-04-11T03:52:12.5420581Z         same_context_len: bool,
2025-04-11T03:52:12.5420655Z     ):
2025-04-11T03:52:12.5420884Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5420889Z 
2025-04-11T03:52:12.5421043Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5421158Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5421168Z 
2025-04-11T03:52:12.5421312Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5421494Z same_context_len = True
2025-04-11T03:52:12.5421498Z 
2025-04-11T03:52:12.5421607Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5421685Z         bsz: int,
2025-04-11T03:52:12.5421776Z         block_size: int,
2025-04-11T03:52:12.5421867Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5421951Z         num_kv_heads: int,
2025-04-11T03:52:12.5422051Z         same_context_len: bool,
2025-04-11T03:52:12.5422127Z     ):
2025-04-11T03:52:12.5422223Z         torch.manual_seed(123)
2025-04-11T03:52:12.5422301Z     
2025-04-11T03:52:12.5422500Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5422627Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5422716Z         dtype = torch.float16
2025-04-11T03:52:12.5422817Z         device = get_current_device()
2025-04-11T03:52:12.5422890Z     
2025-04-11T03:52:12.5422981Z         if same_context_len:
2025-04-11T03:52:12.5423206Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5423317Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5423609Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5423751Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5423919Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5423923Z 
2025-04-11T03:52:12.5424101Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5424257Z ___________________ test_kv_cache_memcopy[True-16-32-16-32] ____________________
2025-04-11T03:52:12.5424261Z 
2025-04-11T03:52:12.5424411Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5424504Z same_context_len = True
2025-04-11T03:52:12.5424508Z 
2025-04-11T03:52:12.5424620Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5424748Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5424901Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5425015Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5425167Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5425256Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5425342Z         bsz: int,
2025-04-11T03:52:12.5425426Z         block_size: int,
2025-04-11T03:52:12.5425517Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5425609Z         num_kv_heads: int,
2025-04-11T03:52:12.5425696Z         same_context_len: bool,
2025-04-11T03:52:12.5425776Z     ):
2025-04-11T03:52:12.5425996Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5426001Z 
2025-04-11T03:52:12.5426152Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5426278Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5426283Z 
2025-04-11T03:52:12.5426430Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5426525Z same_context_len = True
2025-04-11T03:52:12.5426643Z 
2025-04-11T03:52:12.5426740Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5426825Z         bsz: int,
2025-04-11T03:52:12.5426910Z         block_size: int,
2025-04-11T03:52:12.5427001Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5427093Z         num_kv_heads: int,
2025-04-11T03:52:12.5427178Z         same_context_len: bool,
2025-04-11T03:52:12.5427262Z     ):
2025-04-11T03:52:12.5427349Z         torch.manual_seed(123)
2025-04-11T03:52:12.5427430Z     
2025-04-11T03:52:12.5427630Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5427749Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5427945Z         dtype = torch.float16
2025-04-11T03:52:12.5428039Z         device = get_current_device()
2025-04-11T03:52:12.5428132Z     
2025-04-11T03:52:12.5428245Z         if same_context_len:
2025-04-11T03:52:12.5428528Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5428652Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5428938Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5429085Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5429245Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5429249Z 
2025-04-11T03:52:12.5429433Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5429582Z ____________________ test_kv_cache_memcopy[True-16-32-32-4] ____________________
2025-04-11T03:52:12.5429589Z 
2025-04-11T03:52:12.5429746Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5429834Z same_context_len = True
2025-04-11T03:52:12.5429838Z 
2025-04-11T03:52:12.5429949Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5430091Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5430234Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5430361Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5430500Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5430602Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5430681Z         bsz: int,
2025-04-11T03:52:12.5430768Z         block_size: int,
2025-04-11T03:52:12.5430874Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5430961Z         num_kv_heads: int,
2025-04-11T03:52:12.5431054Z         same_context_len: bool,
2025-04-11T03:52:12.5431134Z     ):
2025-04-11T03:52:12.5431354Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5431365Z 
2025-04-11T03:52:12.5431517Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5431629Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5431636Z 
2025-04-11T03:52:12.5431789Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5431875Z same_context_len = True
2025-04-11T03:52:12.5431879Z 
2025-04-11T03:52:12.5431982Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5432061Z         bsz: int,
2025-04-11T03:52:12.5432150Z         block_size: int,
2025-04-11T03:52:12.5432243Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5432327Z         num_kv_heads: int,
2025-04-11T03:52:12.5432423Z         same_context_len: bool,
2025-04-11T03:52:12.5432498Z     ):
2025-04-11T03:52:12.5432595Z         torch.manual_seed(123)
2025-04-11T03:52:12.5432668Z     
2025-04-11T03:52:12.5432870Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5432996Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5433085Z         dtype = torch.float16
2025-04-11T03:52:12.5433304Z         device = get_current_device()
2025-04-11T03:52:12.5433378Z     
2025-04-11T03:52:12.5433469Z         if same_context_len:
2025-04-11T03:52:12.5433695Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5433807Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5434103Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5434240Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5434612Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5434616Z 
2025-04-11T03:52:12.5434794Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5434948Z ____________________ test_kv_cache_memcopy[True-16-32-32-7] ____________________
2025-04-11T03:52:12.5434955Z 
2025-04-11T03:52:12.5435100Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5435186Z same_context_len = True
2025-04-11T03:52:12.5435197Z 
2025-04-11T03:52:12.5435304Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5435431Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5435581Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5435699Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5435845Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5435934Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5436015Z         bsz: int,
2025-04-11T03:52:12.5436108Z         block_size: int,
2025-04-11T03:52:12.5436198Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5436289Z         num_kv_heads: int,
2025-04-11T03:52:12.5436378Z         same_context_len: bool,
2025-04-11T03:52:12.5436461Z     ):
2025-04-11T03:52:12.5436684Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5436688Z 
2025-04-11T03:52:12.5436844Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5436968Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5436972Z 
2025-04-11T03:52:12.5437114Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5437210Z same_context_len = True
2025-04-11T03:52:12.5437214Z 
2025-04-11T03:52:12.5437308Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5437393Z         bsz: int,
2025-04-11T03:52:12.5437479Z         block_size: int,
2025-04-11T03:52:12.5437570Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5437663Z         num_kv_heads: int,
2025-04-11T03:52:12.5437751Z         same_context_len: bool,
2025-04-11T03:52:12.5437832Z     ):
2025-04-11T03:52:12.5437920Z         torch.manual_seed(123)
2025-04-11T03:52:12.5437995Z     
2025-04-11T03:52:12.5438203Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5438320Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5438414Z         dtype = torch.float16
2025-04-11T03:52:12.5438508Z         device = get_current_device()
2025-04-11T03:52:12.5438588Z     
2025-04-11T03:52:12.5438676Z         if same_context_len:
2025-04-11T03:52:12.5438902Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5439020Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5439313Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5439462Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5439627Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5439738Z 
2025-04-11T03:52:12.5439931Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5440082Z ___________________ test_kv_cache_memcopy[True-16-32-32-32] ____________________
2025-04-11T03:52:12.5440086Z 
2025-04-11T03:52:12.5440245Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5440333Z same_context_len = True
2025-04-11T03:52:12.5440337Z 
2025-04-11T03:52:12.5440447Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5440588Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5440870Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5440996Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5441137Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5441233Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5441314Z         bsz: int,
2025-04-11T03:52:12.5441402Z         block_size: int,
2025-04-11T03:52:12.5441500Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5441586Z         num_kv_heads: int,
2025-04-11T03:52:12.5441678Z         same_context_len: bool,
2025-04-11T03:52:12.5441752Z     ):
2025-04-11T03:52:12.5441984Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5442002Z 
2025-04-11T03:52:12.5442158Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5442276Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5442280Z 
2025-04-11T03:52:12.5442450Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5442540Z same_context_len = True
2025-04-11T03:52:12.5442544Z 
2025-04-11T03:52:12.5442651Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5442727Z         bsz: int,
2025-04-11T03:52:12.5442823Z         block_size: int,
2025-04-11T03:52:12.5442922Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5443009Z         num_kv_heads: int,
2025-04-11T03:52:12.5443108Z         same_context_len: bool,
2025-04-11T03:52:12.5443183Z     ):
2025-04-11T03:52:12.5443282Z         torch.manual_seed(123)
2025-04-11T03:52:12.5443359Z     
2025-04-11T03:52:12.5443563Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5443688Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5443776Z         dtype = torch.float16
2025-04-11T03:52:12.5443878Z         device = get_current_device()
2025-04-11T03:52:12.5443956Z     
2025-04-11T03:52:12.5444044Z         if same_context_len:
2025-04-11T03:52:12.5444275Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5444385Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5444674Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5444813Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5444980Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5444984Z 
2025-04-11T03:52:12.5445160Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5445318Z ____________________ test_kv_cache_memcopy[True-16-32-64-4] ____________________
2025-04-11T03:52:12.5445321Z 
2025-04-11T03:52:12.5445471Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5445564Z same_context_len = True
2025-04-11T03:52:12.5445568Z 
2025-04-11T03:52:12.5445686Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5445815Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5445965Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5446188Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5446335Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5446426Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5446505Z         bsz: int,
2025-04-11T03:52:12.5446599Z         block_size: int,
2025-04-11T03:52:12.5446693Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5446786Z         num_kv_heads: int,
2025-04-11T03:52:12.5446876Z         same_context_len: bool,
2025-04-11T03:52:12.5446951Z     ):
2025-04-11T03:52:12.5447183Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5447282Z 
2025-04-11T03:52:12.5447434Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5447558Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5447562Z 
2025-04-11T03:52:12.5447704Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5447802Z same_context_len = True
2025-04-11T03:52:12.5447806Z 
2025-04-11T03:52:12.5447900Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5447986Z         bsz: int,
2025-04-11T03:52:12.5448070Z         block_size: int,
2025-04-11T03:52:12.5448161Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5448256Z         num_kv_heads: int,
2025-04-11T03:52:12.5448342Z         same_context_len: bool,
2025-04-11T03:52:12.5448423Z     ):
2025-04-11T03:52:12.5448509Z         torch.manual_seed(123)
2025-04-11T03:52:12.5448583Z     
2025-04-11T03:52:12.5448786Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5448909Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5449007Z         dtype = torch.float16
2025-04-11T03:52:12.5449101Z         device = get_current_device()
2025-04-11T03:52:12.5449183Z     
2025-04-11T03:52:12.5449271Z         if same_context_len:
2025-04-11T03:52:12.5449501Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5449618Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5449899Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5450034Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5450189Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5450194Z 
2025-04-11T03:52:12.5450371Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5450520Z ____________________ test_kv_cache_memcopy[True-16-32-64-7] ____________________
2025-04-11T03:52:12.5450524Z 
2025-04-11T03:52:12.5450671Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5450775Z same_context_len = True
2025-04-11T03:52:12.5450782Z 
2025-04-11T03:52:12.5450894Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5451039Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5451184Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5451315Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5451455Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5451546Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5451631Z         bsz: int,
2025-04-11T03:52:12.5451714Z         block_size: int,
2025-04-11T03:52:12.5451813Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5451900Z         num_kv_heads: int,
2025-04-11T03:52:12.5451993Z         same_context_len: bool,
2025-04-11T03:52:12.5452069Z     ):
2025-04-11T03:52:12.5452289Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5452294Z 
2025-04-11T03:52:12.5452573Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5452688Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5452692Z 
2025-04-11T03:52:12.5452842Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5452932Z same_context_len = True
2025-04-11T03:52:12.5452936Z 
2025-04-11T03:52:12.5453050Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5453126Z         bsz: int,
2025-04-11T03:52:12.5453210Z         block_size: int,
2025-04-11T03:52:12.5453306Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5453391Z         num_kv_heads: int,
2025-04-11T03:52:12.5453569Z         same_context_len: bool,
2025-04-11T03:52:12.5453645Z     ):
2025-04-11T03:52:12.5453728Z         torch.manual_seed(123)
2025-04-11T03:52:12.5453812Z     
2025-04-11T03:52:12.5454010Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5454146Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5454238Z         dtype = torch.float16
2025-04-11T03:52:12.5454343Z         device = get_current_device()
2025-04-11T03:52:12.5454417Z     
2025-04-11T03:52:12.5454504Z         if same_context_len:
2025-04-11T03:52:12.5454733Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5454842Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5455132Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5455273Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5455438Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5455442Z 
2025-04-11T03:52:12.5455619Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5455770Z ___________________ test_kv_cache_memcopy[True-16-32-64-32] ____________________
2025-04-11T03:52:12.5455781Z 
2025-04-11T03:52:12.5455927Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5456013Z same_context_len = True
2025-04-11T03:52:12.5456018Z 
2025-04-11T03:52:12.5456136Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5456263Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5456413Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5456531Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5456683Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5456771Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5456848Z         bsz: int,
2025-04-11T03:52:12.5456940Z         block_size: int,
2025-04-11T03:52:12.5457030Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5457119Z         num_kv_heads: int,
2025-04-11T03:52:12.5457209Z         same_context_len: bool,
2025-04-11T03:52:12.5457284Z     ):
2025-04-11T03:52:12.5457514Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5457519Z 
2025-04-11T03:52:12.5457670Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5457792Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5457796Z 
2025-04-11T03:52:12.5457938Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5458032Z same_context_len = True
2025-04-11T03:52:12.5458039Z 
2025-04-11T03:52:12.5458136Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5458220Z         bsz: int,
2025-04-11T03:52:12.5458303Z         block_size: int,
2025-04-11T03:52:12.5458394Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5458485Z         num_kv_heads: int,
2025-04-11T03:52:12.5458683Z         same_context_len: bool,
2025-04-11T03:52:12.5458767Z     ):
2025-04-11T03:52:12.5458855Z         torch.manual_seed(123)
2025-04-11T03:52:12.5458929Z     
2025-04-11T03:52:12.5459137Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5459256Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5459351Z         dtype = torch.float16
2025-04-11T03:52:12.5459445Z         device = get_current_device()
2025-04-11T03:52:12.5459520Z     
2025-04-11T03:52:12.5459615Z         if same_context_len:
2025-04-11T03:52:12.5459842Z >           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5460069Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5460357Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5460499Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5460664Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5460668Z 
2025-04-11T03:52:12.5460852Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
2025-04-11T03:52:12.5461000Z ____________________ test_kv_cache_memcopy[False-16-8-16-4] ____________________
2025-04-11T03:52:12.5461004Z 
2025-04-11T03:52:12.5461153Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5461258Z same_context_len = False
2025-04-11T03:52:12.5461263Z 
2025-04-11T03:52:12.5461379Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5461524Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5461666Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5461798Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5461939Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5462031Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5462117Z         bsz: int,
2025-04-11T03:52:12.5462201Z         block_size: int,
2025-04-11T03:52:12.5462303Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5462387Z         num_kv_heads: int,
2025-04-11T03:52:12.5462482Z         same_context_len: bool,
2025-04-11T03:52:12.5462557Z     ):
2025-04-11T03:52:12.5462777Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5462782Z 
2025-04-11T03:52:12.5462940Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5463053Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5463057Z 
2025-04-11T03:52:12.5463213Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5463301Z same_context_len = False
2025-04-11T03:52:12.5463305Z 
2025-04-11T03:52:12.5463408Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5463488Z         bsz: int,
2025-04-11T03:52:12.5463572Z         block_size: int,
2025-04-11T03:52:12.5463671Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5463755Z         num_kv_heads: int,
2025-04-11T03:52:12.5463845Z         same_context_len: bool,
2025-04-11T03:52:12.5463921Z     ):
2025-04-11T03:52:12.5464008Z         torch.manual_seed(123)
2025-04-11T03:52:12.5464089Z     
2025-04-11T03:52:12.5464287Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5464414Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5464508Z         dtype = torch.float16
2025-04-11T03:52:12.5464609Z         device = get_current_device()
2025-04-11T03:52:12.5464683Z     
2025-04-11T03:52:12.5464770Z         if same_context_len:
2025-04-11T03:52:12.5465004Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5465217Z         else:
2025-04-11T03:52:12.5465466Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5465579Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5465869Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5466010Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5466170Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5466175Z 
2025-04-11T03:52:12.5466468Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5466617Z ____________________ test_kv_cache_memcopy[False-16-8-16-7] ____________________
2025-04-11T03:52:12.5466621Z 
2025-04-11T03:52:12.5466779Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5466869Z same_context_len = False
2025-04-11T03:52:12.5466874Z 
2025-04-11T03:52:12.5466993Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5467120Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5467272Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5467387Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5467528Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5467629Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5467707Z         bsz: int,
2025-04-11T03:52:12.5467796Z         block_size: int,
2025-04-11T03:52:12.5467892Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5467977Z         num_kv_heads: int,
2025-04-11T03:52:12.5468073Z         same_context_len: bool,
2025-04-11T03:52:12.5468147Z     ):
2025-04-11T03:52:12.5468373Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5468381Z 
2025-04-11T03:52:12.5468577Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5468699Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5468703Z 
2025-04-11T03:52:12.5468847Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5468935Z same_context_len = False
2025-04-11T03:52:12.5468948Z 
2025-04-11T03:52:12.5469054Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5469140Z         bsz: int,
2025-04-11T03:52:12.5469231Z         block_size: int,
2025-04-11T03:52:12.5469330Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5469425Z         num_kv_heads: int,
2025-04-11T03:52:12.5469517Z         same_context_len: bool,
2025-04-11T03:52:12.5469594Z     ):
2025-04-11T03:52:12.5469691Z         torch.manual_seed(123)
2025-04-11T03:52:12.5469765Z     
2025-04-11T03:52:12.5469969Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5470090Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5470178Z         dtype = torch.float16
2025-04-11T03:52:12.5470280Z         device = get_current_device()
2025-04-11T03:52:12.5470354Z     
2025-04-11T03:52:12.5470450Z         if same_context_len:
2025-04-11T03:52:12.5470676Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5470760Z         else:
2025-04-11T03:52:12.5470994Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5471110Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5471400Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5471539Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5471817Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5471822Z 
2025-04-11T03:52:12.5472001Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5472159Z ___________________ test_kv_cache_memcopy[False-16-8-16-32] ____________________
2025-04-11T03:52:12.5472164Z 
2025-04-11T03:52:12.5472313Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5472408Z same_context_len = False
2025-04-11T03:52:12.5472412Z 
2025-04-11T03:52:12.5472521Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5472651Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5472909Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5473028Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5473179Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5473268Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5473361Z         bsz: int,
2025-04-11T03:52:12.5473445Z         block_size: int,
2025-04-11T03:52:12.5473537Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5473633Z         num_kv_heads: int,
2025-04-11T03:52:12.5473723Z         same_context_len: bool,
2025-04-11T03:52:12.5473806Z     ):
2025-04-11T03:52:12.5474034Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5474039Z 
2025-04-11T03:52:12.5474199Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5474324Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5474331Z 
2025-04-11T03:52:12.5474482Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5474575Z same_context_len = False
2025-04-11T03:52:12.5474580Z 
2025-04-11T03:52:12.5474675Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5474760Z         bsz: int,
2025-04-11T03:52:12.5474843Z         block_size: int,
2025-04-11T03:52:12.5474941Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5475025Z         num_kv_heads: int,
2025-04-11T03:52:12.5475110Z         same_context_len: bool,
2025-04-11T03:52:12.5475195Z     ):
2025-04-11T03:52:12.5475284Z         torch.manual_seed(123)
2025-04-11T03:52:12.5475365Z     
2025-04-11T03:52:12.5475561Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5475681Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5475779Z         dtype = torch.float16
2025-04-11T03:52:12.5475872Z         device = get_current_device()
2025-04-11T03:52:12.5475956Z     
2025-04-11T03:52:12.5476044Z         if same_context_len:
2025-04-11T03:52:12.5476264Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5476351Z         else:
2025-04-11T03:52:12.5476583Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5476705Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5476992Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5477138Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5477302Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5477306Z 
2025-04-11T03:52:12.5477492Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5477646Z ____________________ test_kv_cache_memcopy[False-16-8-32-4] ____________________
2025-04-11T03:52:12.5477651Z 
2025-04-11T03:52:12.5477797Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5477889Z same_context_len = False
2025-04-11T03:52:12.5477894Z 
2025-04-11T03:52:12.5478106Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5478242Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5478384Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5478507Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5478647Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5478737Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5478822Z         bsz: int,
2025-04-11T03:52:12.5478905Z         block_size: int,
2025-04-11T03:52:12.5479002Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5479088Z         num_kv_heads: int,
2025-04-11T03:52:12.5479278Z         same_context_len: bool,
2025-04-11T03:52:12.5479355Z     ):
2025-04-11T03:52:12.5479581Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5479585Z 
2025-04-11T03:52:12.5479746Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5479863Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5479867Z 
2025-04-11T03:52:12.5480021Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5480107Z same_context_len = False
2025-04-11T03:52:12.5480111Z 
2025-04-11T03:52:12.5480217Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5480294Z         bsz: int,
2025-04-11T03:52:12.5480378Z         block_size: int,
2025-04-11T03:52:12.5480479Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5480562Z         num_kv_heads: int,
2025-04-11T03:52:12.5480662Z         same_context_len: bool,
2025-04-11T03:52:12.5480738Z     ):
2025-04-11T03:52:12.5480826Z         torch.manual_seed(123)
2025-04-11T03:52:12.5480912Z     
2025-04-11T03:52:12.5481111Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5481236Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5481335Z         dtype = torch.float16
2025-04-11T03:52:12.5481440Z         device = get_current_device()
2025-04-11T03:52:12.5481527Z     
2025-04-11T03:52:12.5481612Z         if same_context_len:
2025-04-11T03:52:12.5481850Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5481928Z         else:
2025-04-11T03:52:12.5482166Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5482277Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5482569Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5482702Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5482860Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5482868Z 
2025-04-11T03:52:12.5483053Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5483203Z ____________________ test_kv_cache_memcopy[False-16-8-32-7] ____________________
2025-04-11T03:52:12.5483207Z 
2025-04-11T03:52:12.5483364Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5483449Z same_context_len = False
2025-04-11T03:52:12.5483453Z 
2025-04-11T03:52:12.5483572Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5483701Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5483853Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5483971Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5484110Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5484209Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5484288Z         bsz: int,
2025-04-11T03:52:12.5484481Z         block_size: int,
2025-04-11T03:52:12.5484574Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5484658Z         num_kv_heads: int,
2025-04-11T03:52:12.5484753Z         same_context_len: bool,
2025-04-11T03:52:12.5484827Z     ):
2025-04-11T03:52:12.5485058Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5485062Z 
2025-04-11T03:52:12.5485214Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5485337Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5485342Z 
2025-04-11T03:52:12.5485485Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5485664Z same_context_len = False
2025-04-11T03:52:12.5485677Z 
2025-04-11T03:52:12.5485771Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5485848Z         bsz: int,
2025-04-11T03:52:12.5485939Z         block_size: int,
2025-04-11T03:52:12.5486048Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5486139Z         num_kv_heads: int,
2025-04-11T03:52:12.5486227Z         same_context_len: bool,
2025-04-11T03:52:12.5486302Z     ):
2025-04-11T03:52:12.5486397Z         torch.manual_seed(123)
2025-04-11T03:52:12.5486472Z     
2025-04-11T03:52:12.5486681Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5486799Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5486891Z         dtype = torch.float16
2025-04-11T03:52:12.5486993Z         device = get_current_device()
2025-04-11T03:52:12.5487068Z     
2025-04-11T03:52:12.5487169Z         if same_context_len:
2025-04-11T03:52:12.5487393Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5487477Z         else:
2025-04-11T03:52:12.5487708Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5487820Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5488112Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5488248Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5488415Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5488420Z 
2025-04-11T03:52:12.5488600Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5488760Z ___________________ test_kv_cache_memcopy[False-16-8-32-32] ____________________
2025-04-11T03:52:12.5488767Z 
2025-04-11T03:52:12.5488915Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5489009Z same_context_len = False
2025-04-11T03:52:12.5489014Z 
2025-04-11T03:52:12.5489122Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5489253Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5489403Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5489518Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5489665Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5489753Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5489838Z         bsz: int,
2025-04-11T03:52:12.5489920Z         block_size: int,
2025-04-11T03:52:12.5490010Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5490101Z         num_kv_heads: int,
2025-04-11T03:52:12.5490187Z         same_context_len: bool,
2025-04-11T03:52:12.5490270Z     ):
2025-04-11T03:52:12.5490490Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5490494Z 
2025-04-11T03:52:12.5490647Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5490769Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5490899Z 
2025-04-11T03:52:12.5491046Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5491142Z same_context_len = False
2025-04-11T03:52:12.5491146Z 
2025-04-11T03:52:12.5491242Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5491328Z         bsz: int,
2025-04-11T03:52:12.5491411Z         block_size: int,
2025-04-11T03:52:12.5491503Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5491596Z         num_kv_heads: int,
2025-04-11T03:52:12.5491682Z         same_context_len: bool,
2025-04-11T03:52:12.5491765Z     ):
2025-04-11T03:52:12.5491949Z         torch.manual_seed(123)
2025-04-11T03:52:12.5492032Z     
2025-04-11T03:52:12.5492232Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5492366Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5492471Z         dtype = torch.float16
2025-04-11T03:52:12.5492572Z         device = get_current_device()
2025-04-11T03:52:12.5492653Z     
2025-04-11T03:52:12.5492747Z         if same_context_len:
2025-04-11T03:52:12.5492980Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5493066Z         else:
2025-04-11T03:52:12.5493300Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5493417Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5493701Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5493850Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5494012Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5494016Z 
2025-04-11T03:52:12.5494208Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5494358Z ____________________ test_kv_cache_memcopy[False-16-8-64-4] ____________________
2025-04-11T03:52:12.5494362Z 
2025-04-11T03:52:12.5494509Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5494603Z same_context_len = False
2025-04-11T03:52:12.5494607Z 
2025-04-11T03:52:12.5494715Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5494851Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5494992Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5495115Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5495258Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5495347Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5495432Z         bsz: int,
2025-04-11T03:52:12.5495518Z         block_size: int,
2025-04-11T03:52:12.5495616Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5495701Z         num_kv_heads: int,
2025-04-11T03:52:12.5495789Z         same_context_len: bool,
2025-04-11T03:52:12.5495870Z     ):
2025-04-11T03:52:12.5496088Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5496092Z 
2025-04-11T03:52:12.5496254Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5496370Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5496374Z 
2025-04-11T03:52:12.5496528Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5496619Z same_context_len = False
2025-04-11T03:52:12.5496622Z 
2025-04-11T03:52:12.5496724Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5496802Z         bsz: int,
2025-04-11T03:52:12.5496887Z         block_size: int,
2025-04-11T03:52:12.5496987Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5497185Z         num_kv_heads: int,
2025-04-11T03:52:12.5497284Z         same_context_len: bool,
2025-04-11T03:52:12.5497360Z     ):
2025-04-11T03:52:12.5497449Z         torch.manual_seed(123)
2025-04-11T03:52:12.5497531Z     
2025-04-11T03:52:12.5497730Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5497856Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5497947Z         dtype = torch.float16
2025-04-11T03:52:12.5498050Z         device = get_current_device()
2025-04-11T03:52:12.5498124Z     
2025-04-11T03:52:12.5498210Z         if same_context_len:
2025-04-11T03:52:12.5498443Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5498634Z         else:
2025-04-11T03:52:12.5498877Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5498986Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5499276Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5499423Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5499583Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5499588Z 
2025-04-11T03:52:12.5499776Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5499924Z ____________________ test_kv_cache_memcopy[False-16-8-64-7] ____________________
2025-04-11T03:52:12.5499929Z 
2025-04-11T03:52:12.5500085Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5500173Z same_context_len = False
2025-04-11T03:52:12.5500177Z 
2025-04-11T03:52:12.5500294Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5500422Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5500567Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5500691Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5500832Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5500931Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5501010Z         bsz: int,
2025-04-11T03:52:12.5501104Z         block_size: int,
2025-04-11T03:52:12.5501196Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5501282Z         num_kv_heads: int,
2025-04-11T03:52:12.5501380Z         same_context_len: bool,
2025-04-11T03:52:12.5501454Z     ):
2025-04-11T03:52:12.5501683Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5501691Z 
2025-04-11T03:52:12.5501847Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5501971Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5501978Z 
2025-04-11T03:52:12.5502122Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5502207Z same_context_len = False
2025-04-11T03:52:12.5502218Z 
2025-04-11T03:52:12.5502312Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5502390Z         bsz: int,
2025-04-11T03:52:12.5502480Z         block_size: int,
2025-04-11T03:52:12.5502572Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5502665Z         num_kv_heads: int,
2025-04-11T03:52:12.5502752Z         same_context_len: bool,
2025-04-11T03:52:12.5502828Z     ):
2025-04-11T03:52:12.5502922Z         torch.manual_seed(123)
2025-04-11T03:52:12.5503001Z     
2025-04-11T03:52:12.5503209Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5503326Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5503414Z         dtype = torch.float16
2025-04-11T03:52:12.5503516Z         device = get_current_device()
2025-04-11T03:52:12.5503690Z     
2025-04-11T03:52:12.5503787Z         if same_context_len:
2025-04-11T03:52:12.5504020Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5504118Z         else:
2025-04-11T03:52:12.5504349Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5504458Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5504748Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5505037Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5505208Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5505212Z 
2025-04-11T03:52:12.5505387Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5505550Z ___________________ test_kv_cache_memcopy[False-16-8-64-32] ____________________
2025-04-11T03:52:12.5505554Z 
2025-04-11T03:52:12.5505702Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5505797Z same_context_len = False
2025-04-11T03:52:12.5505801Z 
2025-04-11T03:52:12.5505910Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5506038Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5506189Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5506307Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5506454Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5506550Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5506637Z         bsz: int,
2025-04-11T03:52:12.5506721Z         block_size: int,
2025-04-11T03:52:12.5506815Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5506908Z         num_kv_heads: int,
2025-04-11T03:52:12.5506999Z         same_context_len: bool,
2025-04-11T03:52:12.5507085Z     ):
2025-04-11T03:52:12.5507308Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5507312Z 
2025-04-11T03:52:12.5507466Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5507590Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5507593Z 
2025-04-11T03:52:12.5507740Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
2025-04-11T03:52:12.5507835Z same_context_len = False
2025-04-11T03:52:12.5507842Z 
2025-04-11T03:52:12.5507937Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5508022Z         bsz: int,
2025-04-11T03:52:12.5508106Z         block_size: int,
2025-04-11T03:52:12.5508196Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5508288Z         num_kv_heads: int,
2025-04-11T03:52:12.5508376Z         same_context_len: bool,
2025-04-11T03:52:12.5508511Z     ):
2025-04-11T03:52:12.5508599Z         torch.manual_seed(123)
2025-04-11T03:52:12.5508682Z     
2025-04-11T03:52:12.5508881Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5509001Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5509098Z         dtype = torch.float16
2025-04-11T03:52:12.5509191Z         device = get_current_device()
2025-04-11T03:52:12.5509273Z     
2025-04-11T03:52:12.5509359Z         if same_context_len:
2025-04-11T03:52:12.5509580Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5509670Z         else:
2025-04-11T03:52:12.5509899Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5510018Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5510297Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5510563Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5510724Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5510729Z 
2025-04-11T03:52:12.5510917Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5511068Z ___________________ test_kv_cache_memcopy[False-16-32-16-4] ____________________
2025-04-11T03:52:12.5511072Z 
2025-04-11T03:52:12.5511219Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5511415Z same_context_len = False
2025-04-11T03:52:12.5511420Z 
2025-04-11T03:52:12.5511531Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5511670Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5511813Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5511938Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5512081Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5512172Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5512283Z         bsz: int,
2025-04-11T03:52:12.5512390Z         block_size: int,
2025-04-11T03:52:12.5512494Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5512577Z         num_kv_heads: int,
2025-04-11T03:52:12.5512664Z         same_context_len: bool,
2025-04-11T03:52:12.5512744Z     ):
2025-04-11T03:52:12.5512968Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5512977Z 
2025-04-11T03:52:12.5513136Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5513253Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5513257Z 
2025-04-11T03:52:12.5513407Z bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5513497Z same_context_len = False
2025-04-11T03:52:12.5513501Z 
2025-04-11T03:52:12.5513602Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5513679Z         bsz: int,
2025-04-11T03:52:12.5513761Z         block_size: int,
2025-04-11T03:52:12.5513875Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5513967Z         num_kv_heads: int,
2025-04-11T03:52:12.5514059Z         same_context_len: bool,
2025-04-11T03:52:12.5514135Z     ):
2025-04-11T03:52:12.5514225Z         torch.manual_seed(123)
2025-04-11T03:52:12.5514310Z     
2025-04-11T03:52:12.5514511Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5514643Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5514731Z         dtype = torch.float16
2025-04-11T03:52:12.5514832Z         device = get_current_device()
2025-04-11T03:52:12.5514905Z     
2025-04-11T03:52:12.5514994Z         if same_context_len:
2025-04-11T03:52:12.5515225Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5515304Z         else:
2025-04-11T03:52:12.5515540Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5515649Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5515929Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5516075Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5516239Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5516244Z 
2025-04-11T03:52:12.5516430Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5516579Z ___________________ test_kv_cache_memcopy[False-16-32-16-7] ____________________
2025-04-11T03:52:12.5516707Z 
2025-04-11T03:52:12.5516861Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5516949Z same_context_len = False
2025-04-11T03:52:12.5516953Z 
2025-04-11T03:52:12.5517074Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5517200Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5517341Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5517463Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5517606Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5517805Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5517884Z         bsz: int,
2025-04-11T03:52:12.5517979Z         block_size: int,
2025-04-11T03:52:12.5518072Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5518157Z         num_kv_heads: int,
2025-04-11T03:52:12.5518256Z         same_context_len: bool,
2025-04-11T03:52:12.5518335Z     ):
2025-04-11T03:52:12.5518566Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5518570Z 
2025-04-11T03:52:12.5518723Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5518850Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5518854Z 
2025-04-11T03:52:12.5518997Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5519086Z same_context_len = False
2025-04-11T03:52:12.5519090Z 
2025-04-11T03:52:12.5519193Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5519274Z         bsz: int,
2025-04-11T03:52:12.5519363Z         block_size: int,
2025-04-11T03:52:12.5519451Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5519542Z         num_kv_heads: int,
2025-04-11T03:52:12.5519630Z         same_context_len: bool,
2025-04-11T03:52:12.5519708Z     ):
2025-04-11T03:52:12.5519813Z         torch.manual_seed(123)
2025-04-11T03:52:12.5519888Z     
2025-04-11T03:52:12.5520100Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5520221Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5520313Z         dtype = torch.float16
2025-04-11T03:52:12.5520415Z         device = get_current_device()
2025-04-11T03:52:12.5520493Z     
2025-04-11T03:52:12.5520587Z         if same_context_len:
2025-04-11T03:52:12.5520812Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5520895Z         else:
2025-04-11T03:52:12.5521128Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5521238Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5521531Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5521669Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5521835Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5521839Z 
2025-04-11T03:52:12.5522020Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5522177Z ___________________ test_kv_cache_memcopy[False-16-32-16-32] ___________________
2025-04-11T03:52:12.5522181Z 
2025-04-11T03:52:12.5522328Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5522422Z same_context_len = False
2025-04-11T03:52:12.5522429Z 
2025-04-11T03:52:12.5522538Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5522666Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5522817Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5522933Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5523192Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5523282Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5523361Z         bsz: int,
2025-04-11T03:52:12.5523452Z         block_size: int,
2025-04-11T03:52:12.5523544Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5523635Z         num_kv_heads: int,
2025-04-11T03:52:12.5523723Z         same_context_len: bool,
2025-04-11T03:52:12.5523804Z     ):
2025-04-11T03:52:12.5524024Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5524028Z 
2025-04-11T03:52:12.5524272Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5524396Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5524400Z 
2025-04-11T03:52:12.5524550Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5524649Z same_context_len = False
2025-04-11T03:52:12.5524653Z 
2025-04-11T03:52:12.5524749Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5524832Z         bsz: int,
2025-04-11T03:52:12.5524920Z         block_size: int,
2025-04-11T03:52:12.5525010Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5525109Z         num_kv_heads: int,
2025-04-11T03:52:12.5525195Z         same_context_len: bool,
2025-04-11T03:52:12.5525284Z     ):
2025-04-11T03:52:12.5525369Z         torch.manual_seed(123)
2025-04-11T03:52:12.5525455Z     
2025-04-11T03:52:12.5525653Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5525778Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5525877Z         dtype = torch.float16
2025-04-11T03:52:12.5525972Z         device = get_current_device()
2025-04-11T03:52:12.5526053Z     
2025-04-11T03:52:12.5526138Z         if same_context_len:
2025-04-11T03:52:12.5526363Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5526454Z         else:
2025-04-11T03:52:12.5526680Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5526798Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5527083Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5527230Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5527392Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5527400Z 
2025-04-11T03:52:12.5527584Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5527735Z ___________________ test_kv_cache_memcopy[False-16-32-32-4] ____________________
2025-04-11T03:52:12.5527739Z 
2025-04-11T03:52:12.5527889Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5527987Z same_context_len = False
2025-04-11T03:52:12.5527991Z 
2025-04-11T03:52:12.5528101Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5528232Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5528376Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5528503Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5528643Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5528732Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5528822Z         bsz: int,
2025-04-11T03:52:12.5528906Z         block_size: int,
2025-04-11T03:52:12.5529006Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5529091Z         num_kv_heads: int,
2025-04-11T03:52:12.5529178Z         same_context_len: bool,
2025-04-11T03:52:12.5529258Z     ):
2025-04-11T03:52:12.5529479Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5529646Z 
2025-04-11T03:52:12.5529806Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5529921Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5529925Z 
2025-04-11T03:52:12.5530076Z bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5530165Z same_context_len = False
2025-04-11T03:52:12.5530169Z 
2025-04-11T03:52:12.5530270Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5530351Z         bsz: int,
2025-04-11T03:52:12.5530532Z         block_size: int,
2025-04-11T03:52:12.5530632Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5530717Z         num_kv_heads: int,
2025-04-11T03:52:12.5530812Z         same_context_len: bool,
2025-04-11T03:52:12.5530897Z     ):
2025-04-11T03:52:12.5530986Z         torch.manual_seed(123)
2025-04-11T03:52:12.5531071Z     
2025-04-11T03:52:12.5531268Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5531394Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5531483Z         dtype = torch.float16
2025-04-11T03:52:12.5531586Z         device = get_current_device()
2025-04-11T03:52:12.5531663Z     
2025-04-11T03:52:12.5531749Z         if same_context_len:
2025-04-11T03:52:12.5531980Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5532058Z         else:
2025-04-11T03:52:12.5532293Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5532406Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5532689Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5532836Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5532994Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5532998Z 
2025-04-11T03:52:12.5533183Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5533333Z ___________________ test_kv_cache_memcopy[False-16-32-32-7] ____________________
2025-04-11T03:52:12.5533337Z 
2025-04-11T03:52:12.5533488Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5533573Z same_context_len = False
2025-04-11T03:52:12.5533577Z 
2025-04-11T03:52:12.5533692Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5533823Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5533963Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5534085Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5534225Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5534327Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5534404Z         bsz: int,
2025-04-11T03:52:12.5534491Z         block_size: int,
2025-04-11T03:52:12.5534585Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5534668Z         num_kv_heads: int,
2025-04-11T03:52:12.5534764Z         same_context_len: bool,
2025-04-11T03:52:12.5534839Z     ):
2025-04-11T03:52:12.5535067Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5535072Z 
2025-04-11T03:52:12.5535224Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5535348Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5535352Z 
2025-04-11T03:52:12.5535495Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5535583Z same_context_len = False
2025-04-11T03:52:12.5535784Z 
2025-04-11T03:52:12.5535889Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5535969Z         bsz: int,
2025-04-11T03:52:12.5536067Z         block_size: int,
2025-04-11T03:52:12.5536160Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5536272Z         num_kv_heads: int,
2025-04-11T03:52:12.5536358Z         same_context_len: bool,
2025-04-11T03:52:12.5536439Z     ):
2025-04-11T03:52:12.5536533Z         torch.manual_seed(123)
2025-04-11T03:52:12.5536608Z     
2025-04-11T03:52:12.5536819Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5536938Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5537155Z         dtype = torch.float16
2025-04-11T03:52:12.5537254Z         device = get_current_device()
2025-04-11T03:52:12.5537329Z     
2025-04-11T03:52:12.5537422Z         if same_context_len:
2025-04-11T03:52:12.5537645Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5537732Z         else:
2025-04-11T03:52:12.5537963Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5538076Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5538369Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5538504Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5538670Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5538678Z 
2025-04-11T03:52:12.5538859Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5539017Z ___________________ test_kv_cache_memcopy[False-16-32-32-32] ___________________
2025-04-11T03:52:12.5539021Z 
2025-04-11T03:52:12.5539169Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5539269Z same_context_len = False
2025-04-11T03:52:12.5539274Z 
2025-04-11T03:52:12.5539386Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5539513Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5539666Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5539785Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5539933Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5540025Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5540103Z         bsz: int,
2025-04-11T03:52:12.5540196Z         block_size: int,
2025-04-11T03:52:12.5540293Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5540386Z         num_kv_heads: int,
2025-04-11T03:52:12.5540473Z         same_context_len: bool,
2025-04-11T03:52:12.5540551Z     ):
2025-04-11T03:52:12.5540771Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5540778Z 
2025-04-11T03:52:12.5540933Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5541056Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5541060Z 
2025-04-11T03:52:12.5541208Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5541302Z same_context_len = False
2025-04-11T03:52:12.5541306Z 
2025-04-11T03:52:12.5541399Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5541483Z         bsz: int,
2025-04-11T03:52:12.5541566Z         block_size: int,
2025-04-11T03:52:12.5541658Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5541753Z         num_kv_heads: int,
2025-04-11T03:52:12.5541842Z         same_context_len: bool,
2025-04-11T03:52:12.5541924Z     ):
2025-04-11T03:52:12.5542012Z         torch.manual_seed(123)
2025-04-11T03:52:12.5542086Z     
2025-04-11T03:52:12.5542292Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5542521Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5542620Z         dtype = torch.float16
2025-04-11T03:52:12.5542718Z         device = get_current_device()
2025-04-11T03:52:12.5542800Z     
2025-04-11T03:52:12.5542888Z         if same_context_len:
2025-04-11T03:52:12.5543113Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5543202Z         else:
2025-04-11T03:52:12.5543432Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5543646Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5543931Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5544075Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5544239Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5544244Z 
2025-04-11T03:52:12.5544421Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5544581Z ___________________ test_kv_cache_memcopy[False-16-32-64-4] ____________________
2025-04-11T03:52:12.5544585Z 
2025-04-11T03:52:12.5544732Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5544831Z same_context_len = False
2025-04-11T03:52:12.5544835Z 
2025-04-11T03:52:12.5544945Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5545087Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5545229Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5545356Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5545495Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5545597Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5545690Z         bsz: int,
2025-04-11T03:52:12.5545773Z         block_size: int,
2025-04-11T03:52:12.5545878Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5545962Z         num_kv_heads: int,
2025-04-11T03:52:12.5546049Z         same_context_len: bool,
2025-04-11T03:52:12.5546139Z     ):
2025-04-11T03:52:12.5546359Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5546364Z 
2025-04-11T03:52:12.5546524Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5546638Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5546645Z 
2025-04-11T03:52:12.5546797Z bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5546884Z same_context_len = False
2025-04-11T03:52:12.5546888Z 
2025-04-11T03:52:12.5546989Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5547069Z         bsz: int,
2025-04-11T03:52:12.5547149Z         block_size: int,
2025-04-11T03:52:12.5547247Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5547331Z         num_kv_heads: int,
2025-04-11T03:52:12.5547422Z         same_context_len: bool,
2025-04-11T03:52:12.5547495Z     ):
2025-04-11T03:52:12.5547582Z         torch.manual_seed(123)
2025-04-11T03:52:12.5547665Z     
2025-04-11T03:52:12.5547862Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5547989Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5548077Z         dtype = torch.float16
2025-04-11T03:52:12.5548183Z         device = get_current_device()
2025-04-11T03:52:12.5548259Z     
2025-04-11T03:52:12.5548347Z         if same_context_len:
2025-04-11T03:52:12.5548629Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5548707Z         else:
2025-04-11T03:52:12.5549065Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5549174Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5549456Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5549603Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5549762Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5549767Z 
2025-04-11T03:52:12.5549954Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5550212Z ___________________ test_kv_cache_memcopy[False-16-32-64-7] ____________________
2025-04-11T03:52:12.5550216Z 
2025-04-11T03:52:12.5550368Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5550455Z same_context_len = False
2025-04-11T03:52:12.5550463Z 
2025-04-11T03:52:12.5550579Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5550707Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5550849Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5550971Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5551112Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5551209Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5551290Z         bsz: int,
2025-04-11T03:52:12.5551381Z         block_size: int,
2025-04-11T03:52:12.5551473Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5551561Z         num_kv_heads: int,
2025-04-11T03:52:12.5551661Z         same_context_len: bool,
2025-04-11T03:52:12.5551735Z     ):
2025-04-11T03:52:12.5551962Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5551966Z 
2025-04-11T03:52:12.5552121Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5552243Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5552247Z 
2025-04-11T03:52:12.5552392Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5552478Z same_context_len = False
2025-04-11T03:52:12.5552483Z 
2025-04-11T03:52:12.5552588Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5552665Z         bsz: int,
2025-04-11T03:52:12.5552756Z         block_size: int,
2025-04-11T03:52:12.5552846Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5552938Z         num_kv_heads: int,
2025-04-11T03:52:12.5553030Z         same_context_len: bool,
2025-04-11T03:52:12.5553104Z     ):
2025-04-11T03:52:12.5553201Z         torch.manual_seed(123)
2025-04-11T03:52:12.5553274Z     
2025-04-11T03:52:12.5553480Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5553603Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5553692Z         dtype = torch.float16
2025-04-11T03:52:12.5553792Z         device = get_current_device()
2025-04-11T03:52:12.5553866Z     
2025-04-11T03:52:12.5553958Z         if same_context_len:
2025-04-11T03:52:12.5554181Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5554267Z         else:
2025-04-11T03:52:12.5554500Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5554611Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5554903Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5555041Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5555213Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5555330Z 
2025-04-11T03:52:12.5555518Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5555681Z ___________________ test_kv_cache_memcopy[False-16-32-64-32] ___________________
2025-04-11T03:52:12.5555686Z 
2025-04-11T03:52:12.5555837Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5555927Z same_context_len = False
2025-04-11T03:52:12.5555939Z 
2025-04-11T03:52:12.5556049Z     @pytest.mark.parametrize("bsz", [4, 7, 32])
2025-04-11T03:52:12.5556176Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.5556427Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
2025-04-11T03:52:12.5556544Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.5556696Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5556787Z     def test_kv_cache_memcopy(
2025-04-11T03:52:12.5556868Z         bsz: int,
2025-04-11T03:52:12.5556962Z         block_size: int,
2025-04-11T03:52:12.5557052Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5557145Z         num_kv_heads: int,
2025-04-11T03:52:12.5557229Z         same_context_len: bool,
2025-04-11T03:52:12.5557309Z     ):
2025-04-11T03:52:12.5557531Z >       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T03:52:12.5557535Z 
2025-04-11T03:52:12.5557686Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152: 
2025-04-11T03:52:12.5557807Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5557814Z 
2025-04-11T03:52:12.5557960Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
2025-04-11T03:52:12.5558055Z same_context_len = False
2025-04-11T03:52:12.5558059Z 
2025-04-11T03:52:12.5558155Z     def run_context_copy_kv_to_cache(
2025-04-11T03:52:12.5558237Z         bsz: int,
2025-04-11T03:52:12.5558326Z         block_size: int,
2025-04-11T03:52:12.5558417Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5558507Z         num_kv_heads: int,
2025-04-11T03:52:12.5558593Z         same_context_len: bool,
2025-04-11T03:52:12.5558672Z     ):
2025-04-11T03:52:12.5558760Z         torch.manual_seed(123)
2025-04-11T03:52:12.5558833Z     
2025-04-11T03:52:12.5559037Z         assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
2025-04-11T03:52:12.5559154Z         max_seq_len = max_num_blocks_per_seq * block_size
2025-04-11T03:52:12.5559250Z         dtype = torch.float16
2025-04-11T03:52:12.5559342Z         device = get_current_device()
2025-04-11T03:52:12.5559425Z     
2025-04-11T03:52:12.5559510Z         if same_context_len:
2025-04-11T03:52:12.5559733Z             context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
2025-04-11T03:52:12.5559816Z         else:
2025-04-11T03:52:12.5560046Z >           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
2025-04-11T03:52:12.5560166Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5560446Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5560590Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5560749Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5560753Z 
2025-04-11T03:52:12.5560933Z tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
2025-04-11T03:52:12.5561087Z ___________________________ test_rms_layernorm[64-2] ___________________________
2025-04-11T03:52:12.5561090Z 
2025-04-11T03:52:12.5561168Z M = 2, N = 64
2025-04-11T03:52:12.5561173Z 
2025-04-11T03:52:12.5561293Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5561413Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5561631Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5561721Z         torch.manual_seed(123)
2025-04-11T03:52:12.5561827Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5561923Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5561928Z 
2025-04-11T03:52:12.5562081Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5562203Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5562468Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
2025-04-11T03:52:12.5562576Z     with torch.cuda.device(device):
2025-04-11T03:52:12.5562810Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5562815Z 
2025-04-11T03:52:12.5562953Z self = <torch.cuda.device object at 0x7f68f05afd90>
2025-04-11T03:52:12.5562957Z 
2025-04-11T03:52:12.5563042Z     def __enter__(self):
2025-04-11T03:52:12.5563191Z >       self.prev_idx = torch.cuda._exchange_device(self.idx)
2025-04-11T03:52:12.5563315Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5563611Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5563759Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5563919Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5563923Z 
2025-04-11T03:52:12.5564177Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
2025-04-11T03:52:12.5564317Z ___________________________ test_rms_layernorm[64-4] ___________________________
2025-04-11T03:52:12.5564324Z 
2025-04-11T03:52:12.5564409Z M = 4, N = 64
2025-04-11T03:52:12.5564413Z 
2025-04-11T03:52:12.5564525Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5564646Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5564764Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5564854Z         torch.manual_seed(123)
2025-04-11T03:52:12.5564958Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5565056Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5565061Z 
2025-04-11T03:52:12.5565216Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5565328Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5565332Z 
2025-04-11T03:52:12.5565412Z device = None
2025-04-11T03:52:12.5565425Z 
2025-04-11T03:52:12.5565551Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5565710Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5565792Z     
2025-04-11T03:52:12.5565867Z         Args:
2025-04-11T03:52:12.5566051Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5566221Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5566338Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5566421Z         """
2025-04-11T03:52:12.5566507Z         _lazy_init()
2025-04-11T03:52:12.5566614Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5566721Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5566837Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5567120Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5567255Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5567425Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5567429Z 
2025-04-11T03:52:12.5567670Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5567819Z ___________________________ test_rms_layernorm[64-8] ___________________________
2025-04-11T03:52:12.5567956Z 
2025-04-11T03:52:12.5568034Z M = 8, N = 64
2025-04-11T03:52:12.5568038Z 
2025-04-11T03:52:12.5568157Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5568280Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5568389Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5568477Z         torch.manual_seed(123)
2025-04-11T03:52:12.5568570Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5568674Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5568678Z 
2025-04-11T03:52:12.5568827Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5569044Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5569048Z 
2025-04-11T03:52:12.5569133Z device = None
2025-04-11T03:52:12.5569138Z 
2025-04-11T03:52:12.5569271Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5569437Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5569515Z     
2025-04-11T03:52:12.5569605Z         Args:
2025-04-11T03:52:12.5569786Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5569971Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5570088Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5570177Z         """
2025-04-11T03:52:12.5570266Z         _lazy_init()
2025-04-11T03:52:12.5570370Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5570489Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5570610Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5570908Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5571053Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5571221Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5571234Z 
2025-04-11T03:52:12.5571476Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5571622Z __________________________ test_rms_layernorm[64-16] ___________________________
2025-04-11T03:52:12.5571626Z 
2025-04-11T03:52:12.5571715Z M = 16, N = 64
2025-04-11T03:52:12.5571719Z 
2025-04-11T03:52:12.5571831Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5571966Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5572077Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5572188Z         torch.manual_seed(123)
2025-04-11T03:52:12.5572285Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5572391Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5572395Z 
2025-04-11T03:52:12.5572564Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5572687Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5572692Z 
2025-04-11T03:52:12.5572783Z device = None
2025-04-11T03:52:12.5572787Z 
2025-04-11T03:52:12.5572915Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5573079Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5573160Z     
2025-04-11T03:52:12.5573241Z         Args:
2025-04-11T03:52:12.5573422Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5573597Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5573719Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5573798Z         """
2025-04-11T03:52:12.5573893Z         _lazy_init()
2025-04-11T03:52:12.5573996Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5574210Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5574331Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5574616Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5574765Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5574930Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5574934Z 
2025-04-11T03:52:12.5575185Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5575325Z __________________________ test_rms_layernorm[128-2] ___________________________
2025-04-11T03:52:12.5575434Z 
2025-04-11T03:52:12.5575525Z M = 2, N = 128
2025-04-11T03:52:12.5575530Z 
2025-04-11T03:52:12.5575641Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5575766Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5575881Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5575972Z         torch.manual_seed(123)
2025-04-11T03:52:12.5576072Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5576165Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5576169Z 
2025-04-11T03:52:12.5576327Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5576446Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5576450Z 
2025-04-11T03:52:12.5576536Z device = None
2025-04-11T03:52:12.5576541Z 
2025-04-11T03:52:12.5576672Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5576827Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5576913Z     
2025-04-11T03:52:12.5576988Z         Args:
2025-04-11T03:52:12.5577166Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5577332Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5577442Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5577526Z         """
2025-04-11T03:52:12.5577606Z         _lazy_init()
2025-04-11T03:52:12.5577715Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5577820Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5577927Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5578216Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5578352Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5578522Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5578527Z 
2025-04-11T03:52:12.5578763Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5578917Z __________________________ test_rms_layernorm[128-4] ___________________________
2025-04-11T03:52:12.5578921Z 
2025-04-11T03:52:12.5578998Z M = 4, N = 128
2025-04-11T03:52:12.5579002Z 
2025-04-11T03:52:12.5579117Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5579238Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5579339Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5579436Z         torch.manual_seed(123)
2025-04-11T03:52:12.5579529Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5579634Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5579639Z 
2025-04-11T03:52:12.5579785Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5579908Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5579913Z 
2025-04-11T03:52:12.5579991Z device = None
2025-04-11T03:52:12.5579995Z 
2025-04-11T03:52:12.5580122Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5580379Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5580456Z     
2025-04-11T03:52:12.5580540Z         Args:
2025-04-11T03:52:12.5580711Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5580882Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5580991Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5581069Z         """
2025-04-11T03:52:12.5581156Z         _lazy_init()
2025-04-11T03:52:12.5581255Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5581453Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5581561Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5581858Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5582000Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5582184Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5582198Z 
2025-04-11T03:52:12.5582436Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5582580Z __________________________ test_rms_layernorm[128-8] ___________________________
2025-04-11T03:52:12.5582584Z 
2025-04-11T03:52:12.5582670Z M = 8, N = 128
2025-04-11T03:52:12.5582674Z 
2025-04-11T03:52:12.5582783Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5582911Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5583015Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5583111Z         torch.manual_seed(123)
2025-04-11T03:52:12.5583203Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5583297Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5583300Z 
2025-04-11T03:52:12.5583454Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5583569Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5583573Z 
2025-04-11T03:52:12.5583661Z device = None
2025-04-11T03:52:12.5583666Z 
2025-04-11T03:52:12.5583785Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5583943Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5584019Z     
2025-04-11T03:52:12.5584095Z         Args:
2025-04-11T03:52:12.5584275Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5584439Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5584559Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5584635Z         """
2025-04-11T03:52:12.5584723Z         _lazy_init()
2025-04-11T03:52:12.5584819Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5584927Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5585045Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5585327Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5585468Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5585627Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5585632Z 
2025-04-11T03:52:12.5585875Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5586016Z __________________________ test_rms_layernorm[128-16] __________________________
2025-04-11T03:52:12.5586020Z 
2025-04-11T03:52:12.5586097Z M = 16, N = 128
2025-04-11T03:52:12.5586107Z 
2025-04-11T03:52:12.5586215Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5586336Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5586577Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5586673Z         torch.manual_seed(123)
2025-04-11T03:52:12.5586775Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5586872Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5586877Z 
2025-04-11T03:52:12.5587027Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5587146Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5587150Z 
2025-04-11T03:52:12.5587231Z device = None
2025-04-11T03:52:12.5587235Z 
2025-04-11T03:52:12.5587362Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5587609Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5587696Z     
2025-04-11T03:52:12.5587773Z         Args:
2025-04-11T03:52:12.5587944Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5588120Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5588236Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5588331Z         """
2025-04-11T03:52:12.5588463Z         _lazy_init()
2025-04-11T03:52:12.5588575Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5588679Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5588787Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5589084Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5589220Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5589391Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5589395Z 
2025-04-11T03:52:12.5589635Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5589788Z __________________________ test_rms_layernorm[512-2] ___________________________
2025-04-11T03:52:12.5589792Z 
2025-04-11T03:52:12.5589871Z M = 2, N = 512
2025-04-11T03:52:12.5589876Z 
2025-04-11T03:52:12.5589992Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5590113Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5590212Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5590311Z         torch.manual_seed(123)
2025-04-11T03:52:12.5590404Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5590505Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5590509Z 
2025-04-11T03:52:12.5590656Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5590778Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5590782Z 
2025-04-11T03:52:12.5590862Z device = None
2025-04-11T03:52:12.5590865Z 
2025-04-11T03:52:12.5590987Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5591156Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5591240Z     
2025-04-11T03:52:12.5591328Z         Args:
2025-04-11T03:52:12.5591504Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5591684Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5591792Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5591867Z         """
2025-04-11T03:52:12.5591959Z         _lazy_init()
2025-04-11T03:52:12.5592058Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5592171Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5592280Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5592562Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5592704Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5592983Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5592988Z 
2025-04-11T03:52:12.5593235Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5593373Z __________________________ test_rms_layernorm[512-4] ___________________________
2025-04-11T03:52:12.5593378Z 
2025-04-11T03:52:12.5593465Z M = 4, N = 512
2025-04-11T03:52:12.5593470Z 
2025-04-11T03:52:12.5593579Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5593707Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5593915Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5594006Z         torch.manual_seed(123)
2025-04-11T03:52:12.5594110Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5594205Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5594209Z 
2025-04-11T03:52:12.5594369Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5594483Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5594487Z 
2025-04-11T03:52:12.5594573Z device = None
2025-04-11T03:52:12.5594577Z 
2025-04-11T03:52:12.5594699Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5594860Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5594936Z     
2025-04-11T03:52:12.5595013Z         Args:
2025-04-11T03:52:12.5595189Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5595354Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5595474Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5595551Z         """
2025-04-11T03:52:12.5595632Z         _lazy_init()
2025-04-11T03:52:12.5595737Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5595843Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5595959Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5596240Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5596386Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5596547Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5596551Z 
2025-04-11T03:52:12.5596789Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5596935Z __________________________ test_rms_layernorm[512-8] ___________________________
2025-04-11T03:52:12.5596939Z 
2025-04-11T03:52:12.5597018Z M = 8, N = 512
2025-04-11T03:52:12.5597022Z 
2025-04-11T03:52:12.5597137Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5597261Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5597377Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5597464Z         torch.manual_seed(123)
2025-04-11T03:52:12.5597563Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5597654Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5597658Z 
2025-04-11T03:52:12.5597810Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5597931Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5597935Z 
2025-04-11T03:52:12.5598014Z device = None
2025-04-11T03:52:12.5598018Z 
2025-04-11T03:52:12.5598147Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5598303Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5598382Z     
2025-04-11T03:52:12.5598457Z         Args:
2025-04-11T03:52:12.5598625Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5598807Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5599032Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5599126Z         """
2025-04-11T03:52:12.5599211Z         _lazy_init()
2025-04-11T03:52:12.5599319Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5599426Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5599548Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5599838Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5599972Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5600244Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5600248Z 
2025-04-11T03:52:12.5600488Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5600641Z __________________________ test_rms_layernorm[512-16] __________________________
2025-04-11T03:52:12.5600645Z 
2025-04-11T03:52:12.5600722Z M = 16, N = 512
2025-04-11T03:52:12.5600726Z 
2025-04-11T03:52:12.5600840Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5600959Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5601059Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5601157Z         torch.manual_seed(123)
2025-04-11T03:52:12.5601251Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5601352Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5601356Z 
2025-04-11T03:52:12.5601505Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5601624Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5601636Z 
2025-04-11T03:52:12.5601718Z device = None
2025-04-11T03:52:12.5601723Z 
2025-04-11T03:52:12.5601842Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5602005Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5602081Z     
2025-04-11T03:52:12.5602164Z         Args:
2025-04-11T03:52:12.5602337Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5602512Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5602620Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5602694Z         """
2025-04-11T03:52:12.5602786Z         _lazy_init()
2025-04-11T03:52:12.5602882Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5602992Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5603101Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5603382Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5603526Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5603689Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5603693Z 
2025-04-11T03:52:12.5603939Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5604081Z __________________________ test_rms_layernorm[5120-2] __________________________
2025-04-11T03:52:12.5604085Z 
2025-04-11T03:52:12.5604171Z M = 2, N = 5120
2025-04-11T03:52:12.5604176Z 
2025-04-11T03:52:12.5604285Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5604411Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5604516Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5604604Z         torch.manual_seed(123)
2025-04-11T03:52:12.5604705Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5604797Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5604801Z 
2025-04-11T03:52:12.5604960Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5605188Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5605192Z 
2025-04-11T03:52:12.5605282Z device = None
2025-04-11T03:52:12.5605286Z 
2025-04-11T03:52:12.5605406Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5605558Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5605641Z     
2025-04-11T03:52:12.5605719Z         Args:
2025-04-11T03:52:12.5605898Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5606065Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5606275Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5606351Z         """
2025-04-11T03:52:12.5606434Z         _lazy_init()
2025-04-11T03:52:12.5606540Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5606648Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5606763Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5607053Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5607194Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5607351Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5607355Z 
2025-04-11T03:52:12.5607594Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5607745Z __________________________ test_rms_layernorm[5120-4] __________________________
2025-04-11T03:52:12.5607749Z 
2025-04-11T03:52:12.5607826Z M = 4, N = 5120
2025-04-11T03:52:12.5607831Z 
2025-04-11T03:52:12.5607949Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5608070Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5608188Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5608286Z         torch.manual_seed(123)
2025-04-11T03:52:12.5608381Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5608491Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5608495Z 
2025-04-11T03:52:12.5608646Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5608769Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5608773Z 
2025-04-11T03:52:12.5608852Z device = None
2025-04-11T03:52:12.5608857Z 
2025-04-11T03:52:12.5608986Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5609137Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5609219Z     
2025-04-11T03:52:12.5609294Z         Args:
2025-04-11T03:52:12.5609463Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5609637Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5609748Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5609829Z         """
2025-04-11T03:52:12.5609911Z         _lazy_init()
2025-04-11T03:52:12.5610010Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5610120Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5610228Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5610516Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5610651Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5610819Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5610823Z 
2025-04-11T03:52:12.5611057Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5611323Z __________________________ test_rms_layernorm[5120-8] __________________________
2025-04-11T03:52:12.5611327Z 
2025-04-11T03:52:12.5611404Z M = 8, N = 5120
2025-04-11T03:52:12.5611409Z 
2025-04-11T03:52:12.5611518Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5611650Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5611754Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5611850Z         torch.manual_seed(123)
2025-04-11T03:52:12.5611942Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5612040Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5612044Z 
2025-04-11T03:52:12.5612193Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5612410Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5612422Z 
2025-04-11T03:52:12.5612502Z device = None
2025-04-11T03:52:12.5612507Z 
2025-04-11T03:52:12.5612630Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5612795Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5612890Z     
2025-04-11T03:52:12.5612998Z         Args:
2025-04-11T03:52:12.5613170Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5613338Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5613453Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5613529Z         """
2025-04-11T03:52:12.5613617Z         _lazy_init()
2025-04-11T03:52:12.5613716Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5613831Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5613941Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5614226Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5614371Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5614535Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5614539Z 
2025-04-11T03:52:12.5614791Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5614935Z _________________________ test_rms_layernorm[5120-16] __________________________
2025-04-11T03:52:12.5614940Z 
2025-04-11T03:52:12.5615026Z M = 16, N = 5120
2025-04-11T03:52:12.5615030Z 
2025-04-11T03:52:12.5615143Z     @pytest.mark.parametrize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.5615275Z     @pytest.mark.parametrize("N", [64, 128, 512, 5120])
2025-04-11T03:52:12.5615384Z     def test_rms_layernorm(M: int, N: int):
2025-04-11T03:52:12.5615474Z         torch.manual_seed(123)
2025-04-11T03:52:12.5615579Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5615676Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5615681Z 
2025-04-11T03:52:12.5615845Z tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16: 
2025-04-11T03:52:12.5615959Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5615963Z 
2025-04-11T03:52:12.5616050Z device = None
2025-04-11T03:52:12.5616054Z 
2025-04-11T03:52:12.5616174Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5616327Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5616407Z     
2025-04-11T03:52:12.5616483Z         Args:
2025-04-11T03:52:12.5616659Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5616828Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5616942Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5617017Z         """
2025-04-11T03:52:12.5617101Z         _lazy_init()
2025-04-11T03:52:12.5617214Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5617436Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5617553Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5617844Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5617981Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5618156Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5618160Z 
2025-04-11T03:52:12.5618398Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5618655Z ____________________ test_rotary_emb[dtype0-64-16-32-64-4] _____________________
2025-04-11T03:52:12.5618660Z 
2025-04-11T03:52:12.5618814Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 16, D = 64, dtype = torch.float16
2025-04-11T03:52:12.5618818Z 
2025-04-11T03:52:12.5618937Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.5619050Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T03:52:12.5619159Z     @pytest.mark.parametrize("H", [32])
2025-04-11T03:52:12.5619265Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T03:52:12.5619363Z     @pytest.mark.parametrize("D", [64])
2025-04-11T03:52:12.5619545Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T03:52:12.5619684Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T03:52:12.5619785Z         torch.manual_seed(10)
2025-04-11T03:52:12.5619891Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T03:52:12.5620011Z         # our crafted op equals to Transformers
2025-04-11T03:52:12.5620147Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5620282Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5620366Z     
2025-04-11T03:52:12.5620536Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T03:52:12.5620626Z     
2025-04-11T03:52:12.5620729Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T03:52:12.5620809Z     
2025-04-11T03:52:12.5620907Z         cos, sin = emb(x0, position_ids)
2025-04-11T03:52:12.5621031Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T03:52:12.5621144Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5621245Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5621345Z         cos_2 = cos[:, : D // 2]
2025-04-11T03:52:12.5621431Z         sin_2 = sin[:, : D // 2]
2025-04-11T03:52:12.5621554Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5621708Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T03:52:12.5621921Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T03:52:12.5622054Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T03:52:12.5622127Z     
2025-04-11T03:52:12.5622221Z         # create data
2025-04-11T03:52:12.5622312Z         block_size = 32
2025-04-11T03:52:12.5622481Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T03:52:12.5622583Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5622733Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.5622849Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5623140Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5623283Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5623449Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5623453Z 
2025-04-11T03:52:12.5623648Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T03:52:12.5623803Z ____________________ test_rotary_emb[dtype0-64-32-32-64-4] _____________________
2025-04-11T03:52:12.5623916Z 
2025-04-11T03:52:12.5624068Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 32, D = 64, dtype = torch.float16
2025-04-11T03:52:12.5624073Z 
2025-04-11T03:52:12.5624190Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.5624301Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T03:52:12.5624403Z     @pytest.mark.parametrize("H", [32])
2025-04-11T03:52:12.5624504Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T03:52:12.5624601Z     @pytest.mark.parametrize("D", [64])
2025-04-11T03:52:12.5624766Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T03:52:12.5624908Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T03:52:12.5625109Z         torch.manual_seed(10)
2025-04-11T03:52:12.5625205Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T03:52:12.5625323Z         # our crafted op equals to Transformers
2025-04-11T03:52:12.5625459Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5625588Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5625666Z     
2025-04-11T03:52:12.5625837Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T03:52:12.5625917Z     
2025-04-11T03:52:12.5626017Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T03:52:12.5626092Z     
2025-04-11T03:52:12.5626189Z         cos, sin = emb(x0, position_ids)
2025-04-11T03:52:12.5626310Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T03:52:12.5626415Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5626510Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5626602Z         cos_2 = cos[:, : D // 2]
2025-04-11T03:52:12.5626685Z         sin_2 = sin[:, : D // 2]
2025-04-11T03:52:12.5626806Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5626944Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T03:52:12.5627162Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T03:52:12.5627314Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T03:52:12.5627386Z     
2025-04-11T03:52:12.5627470Z         # create data
2025-04-11T03:52:12.5627554Z         block_size = 32
2025-04-11T03:52:12.5627713Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T03:52:12.5627820Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5627958Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.5628074Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5628362Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5628533Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5628700Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5628708Z 
2025-04-11T03:52:12.5628900Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T03:52:12.5629056Z ____________________ test_rotary_emb[dtype1-64-16-32-64-4] _____________________
2025-04-11T03:52:12.5629060Z 
2025-04-11T03:52:12.5629207Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 16, D = 64, dtype = torch.float32
2025-04-11T03:52:12.5629211Z 
2025-04-11T03:52:12.5629325Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.5629430Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T03:52:12.5629533Z     @pytest.mark.parametrize("H", [32])
2025-04-11T03:52:12.5629637Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T03:52:12.5629736Z     @pytest.mark.parametrize("D", [64])
2025-04-11T03:52:12.5629900Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T03:52:12.5630036Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T03:52:12.5630277Z         torch.manual_seed(10)
2025-04-11T03:52:12.5630374Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T03:52:12.5630480Z         # our crafted op equals to Transformers
2025-04-11T03:52:12.5630608Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5630739Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5630825Z     
2025-04-11T03:52:12.5630999Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T03:52:12.5631084Z     
2025-04-11T03:52:12.5631181Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T03:52:12.5631267Z     
2025-04-11T03:52:12.5631361Z         cos, sin = emb(x0, position_ids)
2025-04-11T03:52:12.5631640Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T03:52:12.5631753Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5631861Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5631950Z         cos_2 = cos[:, : D // 2]
2025-04-11T03:52:12.5632038Z         sin_2 = sin[:, : D // 2]
2025-04-11T03:52:12.5632157Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5632326Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T03:52:12.5632542Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T03:52:12.5632677Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T03:52:12.5632750Z     
2025-04-11T03:52:12.5632855Z         # create data
2025-04-11T03:52:12.5632943Z         block_size = 32
2025-04-11T03:52:12.5633103Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T03:52:12.5633205Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5633342Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.5633452Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5633733Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5633872Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5634030Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5634034Z 
2025-04-11T03:52:12.5634224Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T03:52:12.5634371Z ____________________ test_rotary_emb[dtype1-64-32-32-64-4] _____________________
2025-04-11T03:52:12.5634376Z 
2025-04-11T03:52:12.5634522Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 32, D = 64, dtype = torch.float32
2025-04-11T03:52:12.5634530Z 
2025-04-11T03:52:12.5634644Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.5634748Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T03:52:12.5634851Z     @pytest.mark.parametrize("H", [32])
2025-04-11T03:52:12.5634950Z     @pytest.mark.parametrize("K_H", [16, 32])
2025-04-11T03:52:12.5635058Z     @pytest.mark.parametrize("D", [64])
2025-04-11T03:52:12.5635225Z     @pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
2025-04-11T03:52:12.5635365Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
2025-04-11T03:52:12.5635461Z         torch.manual_seed(10)
2025-04-11T03:52:12.5635577Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T03:52:12.5635699Z         # our crafted op equals to Transformers
2025-04-11T03:52:12.5635823Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5635948Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.5636019Z     
2025-04-11T03:52:12.5636186Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T03:52:12.5636266Z     
2025-04-11T03:52:12.5636361Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T03:52:12.5636439Z     
2025-04-11T03:52:12.5636532Z         cos, sin = emb(x0, position_ids)
2025-04-11T03:52:12.5636647Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T03:52:12.5636950Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5637045Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.5637131Z         cos_2 = cos[:, : D // 2]
2025-04-11T03:52:12.5637211Z         sin_2 = sin[:, : D // 2]
2025-04-11T03:52:12.5637332Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5637459Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T03:52:12.5637666Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T03:52:12.5637795Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T03:52:12.5637971Z     
2025-04-11T03:52:12.5638057Z         # create data
2025-04-11T03:52:12.5638140Z         block_size = 32
2025-04-11T03:52:12.5638301Z         max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
2025-04-11T03:52:12.5638399Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.5638535Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.5638645Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5638926Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5639061Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5639217Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5639221Z 
2025-04-11T03:52:12.5639415Z tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
2025-04-11T03:52:12.5639564Z _____________________ test_silu_and_mul[dtype0-11008-64-2] _____________________
2025-04-11T03:52:12.5639567Z 
2025-04-11T03:52:12.5639709Z SHAPE_X = 2, SHAPE_Y = 64, SHAPE_Z = 11008, dtype = torch.float32
2025-04-11T03:52:12.5639714Z 
2025-04-11T03:52:12.5639827Z     @pytest.mark.parametrize("SHAPE_X", [2])
2025-04-11T03:52:12.5639936Z     @pytest.mark.parametrize("SHAPE_Y", [64])
2025-04-11T03:52:12.5640051Z     @pytest.mark.parametrize("SHAPE_Z", [11008])
2025-04-11T03:52:12.5640211Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16])
2025-04-11T03:52:12.5640350Z     def test_silu_and_mul(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype):
2025-04-11T03:52:12.5640438Z         torch.manual_seed(5)
2025-04-11T03:52:12.5640532Z         device = get_current_device()
2025-04-11T03:52:12.5640713Z >       ref_input = torch.randn(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype=dtype, device=device)
2025-04-11T03:52:12.5640816Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5641107Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5641238Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5641398Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5641404Z 
2025-04-11T03:52:12.5641572Z tests/test_infer/test_kernels/cuda/test_silu_and_mul.py:17: RuntimeError
2025-04-11T03:52:12.5641716Z _____________________ test_silu_and_mul[dtype1-11008-64-2] _____________________
2025-04-11T03:52:12.5641723Z 
2025-04-11T03:52:12.5641861Z SHAPE_X = 2, SHAPE_Y = 64, SHAPE_Z = 11008, dtype = torch.float16
2025-04-11T03:52:12.5641866Z 
2025-04-11T03:52:12.5641982Z     @pytest.mark.parametrize("SHAPE_X", [2])
2025-04-11T03:52:12.5642095Z     @pytest.mark.parametrize("SHAPE_Y", [64])
2025-04-11T03:52:12.5642208Z     @pytest.mark.parametrize("SHAPE_Z", [11008])
2025-04-11T03:52:12.5642374Z     @pytest.mark.parametrize("dtype", [torch.float32, torch.float16])
2025-04-11T03:52:12.5642514Z     def test_silu_and_mul(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype):
2025-04-11T03:52:12.5642607Z         torch.manual_seed(5)
2025-04-11T03:52:12.5642703Z         device = get_current_device()
2025-04-11T03:52:12.5642881Z >       ref_input = torch.randn(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype=dtype, device=device)
2025-04-11T03:52:12.5643097Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5643377Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5643527Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5643684Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5643689Z 
2025-04-11T03:52:12.5643865Z tests/test_infer/test_kernels/cuda/test_silu_and_mul.py:17: RuntimeError
2025-04-11T03:52:12.5644035Z _____________ test_context_attention[True-False-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.5644159Z 
2025-04-11T03:52:12.5644320Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5644475Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5644565Z use_new_kcache_layout = True
2025-04-11T03:52:12.5644569Z 
2025-04-11T03:52:12.5644787Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5644892Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5645020Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5645159Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5645285Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5645402Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5645539Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5645678Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5645836Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5645934Z     def test_context_attention(
2025-04-11T03:52:12.5646011Z         bsz: int,
2025-04-11T03:52:12.5646097Z         block_size: int,
2025-04-11T03:52:12.5646196Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5646283Z         num_attn_heads: int,
2025-04-11T03:52:12.5646378Z         kv_group_num: int,
2025-04-11T03:52:12.5646466Z         same_context_len: bool,
2025-04-11T03:52:12.5646557Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5646649Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5646722Z     ):
2025-04-11T03:52:12.5646845Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5647043Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5647232Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5647417Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5647600Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5647678Z             return
2025-04-11T03:52:12.5647758Z     
2025-04-11T03:52:12.5647857Z         torch.manual_seed(123)
2025-04-11T03:52:12.5647955Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5648051Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5648144Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5648149Z 
2025-04-11T03:52:12.5648319Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5648436Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5648683Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
2025-04-11T03:52:12.5648783Z     with torch.cuda.device(device):
2025-04-11T03:52:12.5648896Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5648900Z 
2025-04-11T03:52:12.5649027Z self = <torch.cuda.device object at 0x7f68f0220610>
2025-04-11T03:52:12.5649032Z 
2025-04-11T03:52:12.5649116Z     def __enter__(self):
2025-04-11T03:52:12.5649250Z >       self.prev_idx = torch.cuda._exchange_device(self.idx)
2025-04-11T03:52:12.5649539Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5649819Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5649958Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5650117Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5650121Z 
2025-04-11T03:52:12.5650365Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
2025-04-11T03:52:12.5650532Z _____________ test_context_attention[True-False-True-1-16-8-16-32] _____________
2025-04-11T03:52:12.5650663Z 
2025-04-11T03:52:12.5650826Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5650976Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5651072Z use_new_kcache_layout = True
2025-04-11T03:52:12.5651079Z 
2025-04-11T03:52:12.5651279Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5651387Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5651515Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5651654Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5651777Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5651891Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5652036Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5652171Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5652343Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5652466Z     def test_context_attention(
2025-04-11T03:52:12.5652546Z         bsz: int,
2025-04-11T03:52:12.5652648Z         block_size: int,
2025-04-11T03:52:12.5652746Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5652831Z         num_attn_heads: int,
2025-04-11T03:52:12.5652923Z         kv_group_num: int,
2025-04-11T03:52:12.5653009Z         same_context_len: bool,
2025-04-11T03:52:12.5653097Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5653186Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5653261Z     ):
2025-04-11T03:52:12.5653377Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5653576Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5653765Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5653940Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5654113Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5654191Z             return
2025-04-11T03:52:12.5654268Z     
2025-04-11T03:52:12.5654362Z         torch.manual_seed(123)
2025-04-11T03:52:12.5654462Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5654562Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5654655Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5654659Z 
2025-04-11T03:52:12.5654833Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5654946Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5654950Z 
2025-04-11T03:52:12.5655037Z device = None
2025-04-11T03:52:12.5655041Z 
2025-04-11T03:52:12.5655163Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5655322Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5655404Z     
2025-04-11T03:52:12.5655479Z         Args:
2025-04-11T03:52:12.5655655Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5655943Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5656056Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5656137Z         """
2025-04-11T03:52:12.5656219Z         _lazy_init()
2025-04-11T03:52:12.5656325Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5656430Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5656547Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5656833Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5656971Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5657255Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5657260Z 
2025-04-11T03:52:12.5657505Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5657685Z _____________ test_context_attention[True-False-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.5657690Z 
2025-04-11T03:52:12.5657839Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5657993Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5658082Z use_new_kcache_layout = True
2025-04-11T03:52:12.5658086Z 
2025-04-11T03:52:12.5658292Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5658399Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5658519Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5658665Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5658782Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5658903Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5659041Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5659182Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5659334Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5659427Z     def test_context_attention(
2025-04-11T03:52:12.5659512Z         bsz: int,
2025-04-11T03:52:12.5659594Z         block_size: int,
2025-04-11T03:52:12.5659688Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5659776Z         num_attn_heads: int,
2025-04-11T03:52:12.5659862Z         kv_group_num: int,
2025-04-11T03:52:12.5659957Z         same_context_len: bool,
2025-04-11T03:52:12.5660040Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5660141Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5660215Z     ):
2025-04-11T03:52:12.5660334Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5660524Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5660706Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5660887Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5661049Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5661135Z             return
2025-04-11T03:52:12.5661210Z     
2025-04-11T03:52:12.5661304Z         torch.manual_seed(123)
2025-04-11T03:52:12.5661405Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5661496Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5661597Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5661604Z 
2025-04-11T03:52:12.5661772Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5661890Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5661894Z 
2025-04-11T03:52:12.5661974Z device = None
2025-04-11T03:52:12.5661979Z 
2025-04-11T03:52:12.5662218Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5662375Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5662454Z     
2025-04-11T03:52:12.5662540Z         Args:
2025-04-11T03:52:12.5662710Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5662888Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5662997Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5663081Z         """
2025-04-11T03:52:12.5663162Z         _lazy_init()
2025-04-11T03:52:12.5663387Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5663504Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5663614Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5663909Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5664054Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5664217Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5664229Z 
2025-04-11T03:52:12.5664470Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5664643Z _____________ test_context_attention[True-False-True-1-16-8-32-32] _____________
2025-04-11T03:52:12.5664647Z 
2025-04-11T03:52:12.5664806Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5664953Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5665053Z use_new_kcache_layout = True
2025-04-11T03:52:12.5665057Z 
2025-04-11T03:52:12.5665258Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5665373Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5665497Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5665639Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5665764Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5665879Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5666023Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5666159Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5666316Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5666405Z     def test_context_attention(
2025-04-11T03:52:12.5666486Z         bsz: int,
2025-04-11T03:52:12.5666576Z         block_size: int,
2025-04-11T03:52:12.5666666Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5666755Z         num_attn_heads: int,
2025-04-11T03:52:12.5666840Z         kv_group_num: int,
2025-04-11T03:52:12.5666927Z         same_context_len: bool,
2025-04-11T03:52:12.5667024Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5667118Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5667197Z     ):
2025-04-11T03:52:12.5667308Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5667527Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5667716Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5667888Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5668056Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5668135Z             return
2025-04-11T03:52:12.5668212Z     
2025-04-11T03:52:12.5668298Z         torch.manual_seed(123)
2025-04-11T03:52:12.5668401Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5668539Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5668746Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5668750Z 
2025-04-11T03:52:12.5668923Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5669034Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5669039Z 
2025-04-11T03:52:12.5669123Z device = None
2025-04-11T03:52:12.5669127Z 
2025-04-11T03:52:12.5669244Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5669398Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5669469Z     
2025-04-11T03:52:12.5669543Z         Args:
2025-04-11T03:52:12.5669718Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5669992Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5670105Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5670177Z         """
2025-04-11T03:52:12.5670266Z         _lazy_init()
2025-04-11T03:52:12.5670362Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5670464Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5670575Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5670857Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5671002Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5671159Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5671163Z 
2025-04-11T03:52:12.5671406Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5671573Z _____________ test_context_attention[True-False-True-1-16-16-16-7] _____________
2025-04-11T03:52:12.5671576Z 
2025-04-11T03:52:12.5671726Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5671885Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5671982Z use_new_kcache_layout = True
2025-04-11T03:52:12.5671986Z 
2025-04-11T03:52:12.5672187Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5672294Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5672414Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5672556Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5672675Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5672790Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5672929Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5673068Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5673220Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5673317Z     def test_context_attention(
2025-04-11T03:52:12.5673392Z         bsz: int,
2025-04-11T03:52:12.5673475Z         block_size: int,
2025-04-11T03:52:12.5673571Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5673655Z         num_attn_heads: int,
2025-04-11T03:52:12.5673742Z         kv_group_num: int,
2025-04-11T03:52:12.5673826Z         same_context_len: bool,
2025-04-11T03:52:12.5673911Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5674005Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5674080Z     ):
2025-04-11T03:52:12.5674197Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5674390Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5674575Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5674745Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5675019Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5675101Z             return
2025-04-11T03:52:12.5675176Z     
2025-04-11T03:52:12.5675268Z         torch.manual_seed(123)
2025-04-11T03:52:12.5675369Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5675464Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5675556Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5675560Z 
2025-04-11T03:52:12.5675728Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5675854Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5675954Z 
2025-04-11T03:52:12.5676035Z device = None
2025-04-11T03:52:12.5676040Z 
2025-04-11T03:52:12.5676170Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5676321Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5676396Z     
2025-04-11T03:52:12.5676481Z         Args:
2025-04-11T03:52:12.5676653Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5676834Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5676939Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5677023Z         """
2025-04-11T03:52:12.5677104Z         _lazy_init()
2025-04-11T03:52:12.5677205Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5677309Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5677415Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5677706Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5677847Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5678013Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5678020Z 
2025-04-11T03:52:12.5678258Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5678436Z ____________ test_context_attention[True-False-True-1-16-16-16-32] _____________
2025-04-11T03:52:12.5678440Z 
2025-04-11T03:52:12.5678591Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5678741Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5678832Z use_new_kcache_layout = True
2025-04-11T03:52:12.5678836Z 
2025-04-11T03:52:12.5679035Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5679149Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5679268Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5679411Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5679533Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5679658Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5679795Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5679932Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5680089Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5680179Z     def test_context_attention(
2025-04-11T03:52:12.5680260Z         bsz: int,
2025-04-11T03:52:12.5680341Z         block_size: int,
2025-04-11T03:52:12.5680433Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5680520Z         num_attn_heads: int,
2025-04-11T03:52:12.5680606Z         kv_group_num: int,
2025-04-11T03:52:12.5680696Z         same_context_len: bool,
2025-04-11T03:52:12.5680781Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5680875Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5680949Z     ):
2025-04-11T03:52:12.5681059Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5681397Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5681585Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5681759Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5681923Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5682000Z             return
2025-04-11T03:52:12.5682072Z     
2025-04-11T03:52:12.5682157Z         torch.manual_seed(123)
2025-04-11T03:52:12.5682260Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5682462Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5682557Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5682561Z 
2025-04-11T03:52:12.5682728Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5682841Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5682853Z 
2025-04-11T03:52:12.5682930Z device = None
2025-04-11T03:52:12.5682935Z 
2025-04-11T03:52:12.5683053Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5683208Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5683281Z     
2025-04-11T03:52:12.5683360Z         Args:
2025-04-11T03:52:12.5683527Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5683692Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5683807Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5683885Z         """
2025-04-11T03:52:12.5683969Z         _lazy_init()
2025-04-11T03:52:12.5684065Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5684172Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5684285Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5684571Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5684711Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5684875Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5684879Z 
2025-04-11T03:52:12.5685123Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5685289Z _____________ test_context_attention[True-False-True-1-16-16-32-7] _____________
2025-04-11T03:52:12.5685294Z 
2025-04-11T03:52:12.5685452Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5685599Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5685693Z use_new_kcache_layout = True
2025-04-11T03:52:12.5685697Z 
2025-04-11T03:52:12.5685896Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5686010Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5686136Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5686279Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5686399Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5686512Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5686652Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5686789Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5686944Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5687039Z     def test_context_attention(
2025-04-11T03:52:12.5687119Z         bsz: int,
2025-04-11T03:52:12.5687209Z         block_size: int,
2025-04-11T03:52:12.5687300Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5687496Z         num_attn_heads: int,
2025-04-11T03:52:12.5687589Z         kv_group_num: int,
2025-04-11T03:52:12.5687678Z         same_context_len: bool,
2025-04-11T03:52:12.5687769Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5687859Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5687939Z     ):
2025-04-11T03:52:12.5688052Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5688242Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5688429Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5688601Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5688905Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5688982Z             return
2025-04-11T03:52:12.5689058Z     
2025-04-11T03:52:12.5689147Z         torch.manual_seed(123)
2025-04-11T03:52:12.5689249Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5689345Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5689439Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5689444Z 
2025-04-11T03:52:12.5689616Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5689730Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5689734Z 
2025-04-11T03:52:12.5689815Z device = None
2025-04-11T03:52:12.5689820Z 
2025-04-11T03:52:12.5689939Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5690090Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5690167Z     
2025-04-11T03:52:12.5690242Z         Args:
2025-04-11T03:52:12.5690417Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5690581Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5690699Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5690779Z         """
2025-04-11T03:52:12.5690858Z         _lazy_init()
2025-04-11T03:52:12.5690965Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5691068Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5691178Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5691458Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5691593Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5691760Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5691765Z 
2025-04-11T03:52:12.5692004Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5692181Z ____________ test_context_attention[True-False-True-1-16-16-32-32] _____________
2025-04-11T03:52:12.5692188Z 
2025-04-11T03:52:12.5692343Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5692495Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5692583Z use_new_kcache_layout = True
2025-04-11T03:52:12.5692588Z 
2025-04-11T03:52:12.5692792Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5692896Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5693015Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5693157Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5693277Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5693394Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5693531Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5693674Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5693938Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5694027Z     def test_context_attention(
2025-04-11T03:52:12.5694110Z         bsz: int,
2025-04-11T03:52:12.5694196Z         block_size: int,
2025-04-11T03:52:12.5694292Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5694376Z         num_attn_heads: int,
2025-04-11T03:52:12.5694468Z         kv_group_num: int,
2025-04-11T03:52:12.5694556Z         same_context_len: bool,
2025-04-11T03:52:12.5694643Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5694737Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5694904Z     ):
2025-04-11T03:52:12.5695021Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5695217Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5695402Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5695583Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5695750Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5695831Z             return
2025-04-11T03:52:12.5695903Z     
2025-04-11T03:52:12.5695997Z         torch.manual_seed(123)
2025-04-11T03:52:12.5696098Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5696189Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5696294Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5696298Z 
2025-04-11T03:52:12.5696467Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5696600Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5696605Z 
2025-04-11T03:52:12.5696683Z device = None
2025-04-11T03:52:12.5696687Z 
2025-04-11T03:52:12.5696807Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5696963Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5697041Z     
2025-04-11T03:52:12.5697121Z         Args:
2025-04-11T03:52:12.5697291Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5697464Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5697571Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5697652Z         """
2025-04-11T03:52:12.5697732Z         _lazy_init()
2025-04-11T03:52:12.5697830Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5697945Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5698051Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5698347Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5698488Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5698658Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5698663Z 
2025-04-11T03:52:12.5698903Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5699072Z _____________ test_context_attention[True-False-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.5699083Z 
2025-04-11T03:52:12.5699231Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5699380Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5699476Z use_new_kcache_layout = True
2025-04-11T03:52:12.5699483Z 
2025-04-11T03:52:12.5699687Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5699798Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5699919Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5700172Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5700295Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5700411Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5700554Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5700689Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5700844Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5700934Z     def test_context_attention(
2025-04-11T03:52:12.5701009Z         bsz: int,
2025-04-11T03:52:12.5701099Z         block_size: int,
2025-04-11T03:52:12.5701296Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5701386Z         num_attn_heads: int,
2025-04-11T03:52:12.5701470Z         kv_group_num: int,
2025-04-11T03:52:12.5701563Z         same_context_len: bool,
2025-04-11T03:52:12.5701650Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5701742Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5701825Z     ):
2025-04-11T03:52:12.5701937Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5702133Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5702315Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5702485Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5702648Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5702723Z             return
2025-04-11T03:52:12.5702802Z     
2025-04-11T03:52:12.5702886Z         torch.manual_seed(123)
2025-04-11T03:52:12.5702990Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5703081Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5703174Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5703178Z 
2025-04-11T03:52:12.5703353Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5703467Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5703471Z 
2025-04-11T03:52:12.5703555Z device = None
2025-04-11T03:52:12.5703559Z 
2025-04-11T03:52:12.5703675Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5703830Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5703903Z     
2025-04-11T03:52:12.5703978Z         Args:
2025-04-11T03:52:12.5704151Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5704321Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5704435Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5704512Z         """
2025-04-11T03:52:12.5704596Z         _lazy_init()
2025-04-11T03:52:12.5704693Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5704801Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5704914Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5705196Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5705338Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5705497Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5705501Z 
2025-04-11T03:52:12.5705743Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5705913Z _____________ test_context_attention[True-False-True-4-16-8-16-32] _____________
2025-04-11T03:52:12.5705917Z 
2025-04-11T03:52:12.5706086Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5706239Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5706456Z use_new_kcache_layout = True
2025-04-11T03:52:12.5706461Z 
2025-04-11T03:52:12.5706671Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5706779Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5706906Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5707048Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5707171Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5707288Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5707423Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5707657Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5707814Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5707915Z     def test_context_attention(
2025-04-11T03:52:12.5707998Z         bsz: int,
2025-04-11T03:52:12.5708091Z         block_size: int,
2025-04-11T03:52:12.5708194Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5708283Z         num_attn_heads: int,
2025-04-11T03:52:12.5708377Z         kv_group_num: int,
2025-04-11T03:52:12.5708505Z         same_context_len: bool,
2025-04-11T03:52:12.5708600Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5708692Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5708767Z     ):
2025-04-11T03:52:12.5708886Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5709078Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5709270Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5709449Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5709620Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5709699Z             return
2025-04-11T03:52:12.5709772Z     
2025-04-11T03:52:12.5709871Z         torch.manual_seed(123)
2025-04-11T03:52:12.5709972Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5710069Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5710164Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5710169Z 
2025-04-11T03:52:12.5710338Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5710458Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5710463Z 
2025-04-11T03:52:12.5710542Z device = None
2025-04-11T03:52:12.5710552Z 
2025-04-11T03:52:12.5710673Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5710823Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5710902Z     
2025-04-11T03:52:12.5710977Z         Args:
2025-04-11T03:52:12.5711155Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5711337Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5711447Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5711529Z         """
2025-04-11T03:52:12.5711611Z         _lazy_init()
2025-04-11T03:52:12.5711715Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5711818Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5711925Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5712219Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5712367Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5712529Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5712533Z 
2025-04-11T03:52:12.5712772Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5713070Z _____________ test_context_attention[True-False-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.5713074Z 
2025-04-11T03:52:12.5713223Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5713374Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5713486Z use_new_kcache_layout = True
2025-04-11T03:52:12.5713492Z 
2025-04-11T03:52:12.5713717Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5713831Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5714064Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5714208Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5714327Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5714451Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5714596Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5714729Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5714885Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5714976Z     def test_context_attention(
2025-04-11T03:52:12.5715059Z         bsz: int,
2025-04-11T03:52:12.5715141Z         block_size: int,
2025-04-11T03:52:12.5715238Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5715321Z         num_attn_heads: int,
2025-04-11T03:52:12.5715404Z         kv_group_num: int,
2025-04-11T03:52:12.5715496Z         same_context_len: bool,
2025-04-11T03:52:12.5715584Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5715676Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5715749Z     ):
2025-04-11T03:52:12.5715870Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5716078Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5716271Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5716451Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5716613Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5716693Z             return
2025-04-11T03:52:12.5716766Z     
2025-04-11T03:52:12.5716852Z         torch.manual_seed(123)
2025-04-11T03:52:12.5716955Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5717045Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5717144Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5717148Z 
2025-04-11T03:52:12.5717315Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5717430Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5717434Z 
2025-04-11T03:52:12.5717515Z device = None
2025-04-11T03:52:12.5717519Z 
2025-04-11T03:52:12.5717635Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5717791Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5717863Z     
2025-04-11T03:52:12.5717941Z         Args:
2025-04-11T03:52:12.5718107Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5718276Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5718381Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5718458Z         """
2025-04-11T03:52:12.5718544Z         _lazy_init()
2025-04-11T03:52:12.5718641Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5718746Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5718853Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5719131Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5719383Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5719543Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5719547Z 
2025-04-11T03:52:12.5719793Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5719957Z _____________ test_context_attention[True-False-True-4-16-8-32-32] _____________
2025-04-11T03:52:12.5719962Z 
2025-04-11T03:52:12.5720119Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5720356Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5720455Z use_new_kcache_layout = True
2025-04-11T03:52:12.5720460Z 
2025-04-11T03:52:12.5720670Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5720796Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5720914Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5721067Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5721190Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5721312Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5721452Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5721589Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5721742Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5721840Z     def test_context_attention(
2025-04-11T03:52:12.5721916Z         bsz: int,
2025-04-11T03:52:12.5722003Z         block_size: int,
2025-04-11T03:52:12.5722092Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5722179Z         num_attn_heads: int,
2025-04-11T03:52:12.5722262Z         kv_group_num: int,
2025-04-11T03:52:12.5722351Z         same_context_len: bool,
2025-04-11T03:52:12.5722442Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5722530Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5722605Z     ):
2025-04-11T03:52:12.5722716Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5722909Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5723095Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5723264Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5723435Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5723510Z             return
2025-04-11T03:52:12.5723586Z     
2025-04-11T03:52:12.5723672Z         torch.manual_seed(123)
2025-04-11T03:52:12.5723767Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5723866Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5723957Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5723961Z 
2025-04-11T03:52:12.5724136Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5724249Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5724253Z 
2025-04-11T03:52:12.5724332Z device = None
2025-04-11T03:52:12.5724337Z 
2025-04-11T03:52:12.5724454Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5724603Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5724685Z     
2025-04-11T03:52:12.5724758Z         Args:
2025-04-11T03:52:12.5724932Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5725098Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5725206Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5725405Z         """
2025-04-11T03:52:12.5725495Z         _lazy_init()
2025-04-11T03:52:12.5725600Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5725708Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5725820Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5726113Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5726257Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5726417Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5726539Z 
2025-04-11T03:52:12.5726775Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5726946Z _____________ test_context_attention[True-False-True-4-16-16-16-7] _____________
2025-04-11T03:52:12.5726950Z 
2025-04-11T03:52:12.5727104Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5727253Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5727341Z use_new_kcache_layout = True
2025-04-11T03:52:12.5727346Z 
2025-04-11T03:52:12.5727549Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5727657Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5727778Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5727917Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5728033Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5728155Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5728290Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5728431Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5728581Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5728678Z     def test_context_attention(
2025-04-11T03:52:12.5728757Z         bsz: int,
2025-04-11T03:52:12.5728838Z         block_size: int,
2025-04-11T03:52:12.5728935Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5729021Z         num_attn_heads: int,
2025-04-11T03:52:12.5729109Z         kv_group_num: int,
2025-04-11T03:52:12.5729196Z         same_context_len: bool,
2025-04-11T03:52:12.5729285Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5729379Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5729451Z     ):
2025-04-11T03:52:12.5729565Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5729761Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5729941Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5730119Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5730285Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5730374Z             return
2025-04-11T03:52:12.5730446Z     
2025-04-11T03:52:12.5730536Z         torch.manual_seed(123)
2025-04-11T03:52:12.5730637Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5730725Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5730829Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5730833Z 
2025-04-11T03:52:12.5730997Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5731110Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5731117Z 
2025-04-11T03:52:12.5731194Z device = None
2025-04-11T03:52:12.5731198Z 
2025-04-11T03:52:12.5731318Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5731468Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5731659Z     
2025-04-11T03:52:12.5731734Z         Args:
2025-04-11T03:52:12.5731903Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5732074Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5732181Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5732259Z         """
2025-04-11T03:52:12.5732337Z         _lazy_init()
2025-04-11T03:52:12.5732432Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5732538Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5732832Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5733120Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5733258Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5733427Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5733431Z 
2025-04-11T03:52:12.5733670Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5733845Z ____________ test_context_attention[True-False-True-4-16-16-16-32] _____________
2025-04-11T03:52:12.5733849Z 
2025-04-11T03:52:12.5734000Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5734144Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5734237Z use_new_kcache_layout = True
2025-04-11T03:52:12.5734242Z 
2025-04-11T03:52:12.5734444Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5734557Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5734675Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5734827Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5734956Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5735076Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5735220Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5735365Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5735522Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5735614Z     def test_context_attention(
2025-04-11T03:52:12.5735695Z         bsz: int,
2025-04-11T03:52:12.5735775Z         block_size: int,
2025-04-11T03:52:12.5735865Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5735958Z         num_attn_heads: int,
2025-04-11T03:52:12.5736041Z         kv_group_num: int,
2025-04-11T03:52:12.5736130Z         same_context_len: bool,
2025-04-11T03:52:12.5736213Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5736304Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5736384Z     ):
2025-04-11T03:52:12.5736498Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5736693Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5736876Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5737055Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5737218Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5737294Z             return
2025-04-11T03:52:12.5737371Z     
2025-04-11T03:52:12.5737462Z         torch.manual_seed(123)
2025-04-11T03:52:12.5737568Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5737658Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5737747Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5737755Z 
2025-04-11T03:52:12.5737923Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5738149Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5738153Z 
2025-04-11T03:52:12.5738235Z device = None
2025-04-11T03:52:12.5738239Z 
2025-04-11T03:52:12.5738356Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5738508Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5738580Z     
2025-04-11T03:52:12.5738660Z         Args:
2025-04-11T03:52:12.5738825Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5738988Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5739195Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5739270Z         """
2025-04-11T03:52:12.5739353Z         _lazy_init()
2025-04-11T03:52:12.5739449Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5739550Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5739665Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5739949Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5740094Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5740257Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5740261Z 
2025-04-11T03:52:12.5740502Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5740669Z _____________ test_context_attention[True-False-True-4-16-16-32-7] _____________
2025-04-11T03:52:12.5740676Z 
2025-04-11T03:52:12.5740832Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5740977Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5741065Z use_new_kcache_layout = True
2025-04-11T03:52:12.5741077Z 
2025-04-11T03:52:12.5741286Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5741400Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5741538Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5741680Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5741802Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5741915Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5742050Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5742196Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5742348Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5742445Z     def test_context_attention(
2025-04-11T03:52:12.5742522Z         bsz: int,
2025-04-11T03:52:12.5742606Z         block_size: int,
2025-04-11T03:52:12.5742698Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5742781Z         num_attn_heads: int,
2025-04-11T03:52:12.5742868Z         kv_group_num: int,
2025-04-11T03:52:12.5742954Z         same_context_len: bool,
2025-04-11T03:52:12.5743044Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5743130Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5743200Z     ):
2025-04-11T03:52:12.5743316Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5743513Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5743703Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5743876Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5744041Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5744118Z             return
2025-04-11T03:52:12.5744287Z     
2025-04-11T03:52:12.5744380Z         torch.manual_seed(123)
2025-04-11T03:52:12.5744482Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5744577Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5744669Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5744673Z 
2025-04-11T03:52:12.5744849Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5744963Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5744967Z 
2025-04-11T03:52:12.5745045Z device = None
2025-04-11T03:52:12.5745054Z 
2025-04-11T03:52:12.5745172Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5745418Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5745496Z     
2025-04-11T03:52:12.5745570Z         Args:
2025-04-11T03:52:12.5745746Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5745917Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5746022Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5746105Z         """
2025-04-11T03:52:12.5746186Z         _lazy_init()
2025-04-11T03:52:12.5746292Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5746396Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5746530Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5746826Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5746975Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5747141Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5747145Z 
2025-04-11T03:52:12.5747385Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5747559Z ____________ test_context_attention[True-False-True-4-16-16-32-32] _____________
2025-04-11T03:52:12.5747564Z 
2025-04-11T03:52:12.5747713Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5747862Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.5747950Z use_new_kcache_layout = True
2025-04-11T03:52:12.5747954Z 
2025-04-11T03:52:12.5748158Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5748267Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5748386Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5748572Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5748691Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5748806Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5748943Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5749086Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5749236Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5749325Z     def test_context_attention(
2025-04-11T03:52:12.5749406Z         bsz: int,
2025-04-11T03:52:12.5749491Z         block_size: int,
2025-04-11T03:52:12.5749585Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5749673Z         num_attn_heads: int,
2025-04-11T03:52:12.5749760Z         kv_group_num: int,
2025-04-11T03:52:12.5749850Z         same_context_len: bool,
2025-04-11T03:52:12.5749935Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5750034Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5750108Z     ):
2025-04-11T03:52:12.5750223Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5750420Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5750727Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5750904Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5751067Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5751146Z             return
2025-04-11T03:52:12.5751220Z     
2025-04-11T03:52:12.5751307Z         torch.manual_seed(123)
2025-04-11T03:52:12.5751412Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5751507Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5751603Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5751755Z 
2025-04-11T03:52:12.5751933Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5752049Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5752054Z 
2025-04-11T03:52:12.5752131Z device = None
2025-04-11T03:52:12.5752140Z 
2025-04-11T03:52:12.5752269Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5752422Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5752495Z     
2025-04-11T03:52:12.5752573Z         Args:
2025-04-11T03:52:12.5752742Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5752918Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5753031Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5753109Z         """
2025-04-11T03:52:12.5753203Z         _lazy_init()
2025-04-11T03:52:12.5753304Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5753411Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5753521Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5753812Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5753951Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5754111Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5754120Z 
2025-04-11T03:52:12.5754358Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5754523Z _____________ test_context_attention[True-False-False-1-16-8-16-7] _____________
2025-04-11T03:52:12.5754527Z 
2025-04-11T03:52:12.5754686Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5754832Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5754927Z use_new_kcache_layout = True
2025-04-11T03:52:12.5754931Z 
2025-04-11T03:52:12.5755129Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5755239Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5755358Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5755497Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5755623Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5755739Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5755880Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5756015Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5756170Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5756262Z     def test_context_attention(
2025-04-11T03:52:12.5756340Z         bsz: int,
2025-04-11T03:52:12.5756432Z         block_size: int,
2025-04-11T03:52:12.5756535Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5756623Z         num_attn_heads: int,
2025-04-11T03:52:12.5756714Z         kv_group_num: int,
2025-04-11T03:52:12.5756802Z         same_context_len: bool,
2025-04-11T03:52:12.5757020Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5757110Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5757187Z     ):
2025-04-11T03:52:12.5757300Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5757494Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5757676Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5757845Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5758015Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5758193Z             return
2025-04-11T03:52:12.5758271Z     
2025-04-11T03:52:12.5758359Z         torch.manual_seed(123)
2025-04-11T03:52:12.5758460Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5758556Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5758654Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5758657Z 
2025-04-11T03:52:12.5758829Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5758944Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5758948Z 
2025-04-11T03:52:12.5759033Z device = None
2025-04-11T03:52:12.5759037Z 
2025-04-11T03:52:12.5759154Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5759313Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5759385Z     
2025-04-11T03:52:12.5759459Z         Args:
2025-04-11T03:52:12.5759635Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5759803Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5759912Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5759986Z         """
2025-04-11T03:52:12.5760072Z         _lazy_init()
2025-04-11T03:52:12.5760174Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5760276Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5760392Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5760674Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5760815Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5760979Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5760983Z 
2025-04-11T03:52:12.5761232Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5761404Z ____________ test_context_attention[True-False-False-1-16-8-16-32] _____________
2025-04-11T03:52:12.5761408Z 
2025-04-11T03:52:12.5761560Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5761716Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5761807Z use_new_kcache_layout = True
2025-04-11T03:52:12.5761811Z 
2025-04-11T03:52:12.5762018Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5762121Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5762244Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5762384Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5762501Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5762621Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5762763Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5762907Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5763058Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5763274Z     def test_context_attention(
2025-04-11T03:52:12.5763352Z         bsz: int,
2025-04-11T03:52:12.5763434Z         block_size: int,
2025-04-11T03:52:12.5763533Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5763623Z         num_attn_heads: int,
2025-04-11T03:52:12.5763709Z         kv_group_num: int,
2025-04-11T03:52:12.5763794Z         same_context_len: bool,
2025-04-11T03:52:12.5763878Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5763973Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5764045Z     ):
2025-04-11T03:52:12.5764161Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5764353Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5764653Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5764825Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5764994Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5765077Z             return
2025-04-11T03:52:12.5765149Z     
2025-04-11T03:52:12.5765243Z         torch.manual_seed(123)
2025-04-11T03:52:12.5765343Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5765437Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5765530Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5765534Z 
2025-04-11T03:52:12.5765699Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5765817Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5765825Z 
2025-04-11T03:52:12.5765906Z device = None
2025-04-11T03:52:12.5765910Z 
2025-04-11T03:52:12.5766032Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5766184Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5766260Z     
2025-04-11T03:52:12.5766337Z         Args:
2025-04-11T03:52:12.5766505Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5766675Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5766780Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5766854Z         """
2025-04-11T03:52:12.5766933Z         _lazy_init()
2025-04-11T03:52:12.5767029Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5767137Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5767243Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5767536Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5767673Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5767836Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5767843Z 
2025-04-11T03:52:12.5768083Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5768261Z _____________ test_context_attention[True-False-False-1-16-8-32-7] _____________
2025-04-11T03:52:12.5768265Z 
2025-04-11T03:52:12.5768417Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5768562Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5768661Z use_new_kcache_layout = True
2025-04-11T03:52:12.5768665Z 
2025-04-11T03:52:12.5768866Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5768979Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5769097Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5769240Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5769360Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5769586Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5769748Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5769884Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5770044Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5770136Z     def test_context_attention(
2025-04-11T03:52:12.5770235Z         bsz: int,
2025-04-11T03:52:12.5770318Z         block_size: int,
2025-04-11T03:52:12.5770408Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5770505Z         num_attn_heads: int,
2025-04-11T03:52:12.5770679Z         kv_group_num: int,
2025-04-11T03:52:12.5770769Z         same_context_len: bool,
2025-04-11T03:52:12.5770856Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5770946Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5771025Z     ):
2025-04-11T03:52:12.5771137Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5771337Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5771521Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5771695Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5771858Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5771934Z             return
2025-04-11T03:52:12.5772013Z     
2025-04-11T03:52:12.5772099Z         torch.manual_seed(123)
2025-04-11T03:52:12.5772206Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5772300Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5772395Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5772399Z 
2025-04-11T03:52:12.5772572Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5772690Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5772702Z 
2025-04-11T03:52:12.5772781Z device = None
2025-04-11T03:52:12.5772786Z 
2025-04-11T03:52:12.5772903Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5773068Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5773149Z     
2025-04-11T03:52:12.5773229Z         Args:
2025-04-11T03:52:12.5773407Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5773581Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5773695Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5773767Z         """
2025-04-11T03:52:12.5773849Z         _lazy_init()
2025-04-11T03:52:12.5773943Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5774051Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5774157Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5774444Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5774588Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5774746Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5774750Z 
2025-04-11T03:52:12.5774995Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5775162Z ____________ test_context_attention[True-False-False-1-16-8-32-32] _____________
2025-04-11T03:52:12.5775169Z 
2025-04-11T03:52:12.5775324Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5775471Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5775565Z use_new_kcache_layout = True
2025-04-11T03:52:12.5775569Z 
2025-04-11T03:52:12.5775769Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5775988Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5776116Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5776259Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5776385Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5776503Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5776645Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5776781Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5777022Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5777117Z     def test_context_attention(
2025-04-11T03:52:12.5777193Z         bsz: int,
2025-04-11T03:52:12.5777290Z         block_size: int,
2025-04-11T03:52:12.5777378Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5777471Z         num_attn_heads: int,
2025-04-11T03:52:12.5777559Z         kv_group_num: int,
2025-04-11T03:52:12.5777643Z         same_context_len: bool,
2025-04-11T03:52:12.5777739Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5777827Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5777910Z     ):
2025-04-11T03:52:12.5778024Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5778221Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5778421Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5778601Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5778774Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5778851Z             return
2025-04-11T03:52:12.5778925Z     
2025-04-11T03:52:12.5779013Z         torch.manual_seed(123)
2025-04-11T03:52:12.5779115Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5779213Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5779304Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5779308Z 
2025-04-11T03:52:12.5779482Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5779596Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5779600Z 
2025-04-11T03:52:12.5779683Z device = None
2025-04-11T03:52:12.5779688Z 
2025-04-11T03:52:12.5779803Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5779954Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5780035Z     
2025-04-11T03:52:12.5780110Z         Args:
2025-04-11T03:52:12.5780280Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5780446Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5780560Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5780632Z         """
2025-04-11T03:52:12.5780711Z         _lazy_init()
2025-04-11T03:52:12.5780809Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5780913Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5781020Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5781301Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5781438Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5781607Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5781611Z 
2025-04-11T03:52:12.5781852Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5782027Z ____________ test_context_attention[True-False-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.5782140Z 
2025-04-11T03:52:12.5782294Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5782449Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5782537Z use_new_kcache_layout = True
2025-04-11T03:52:12.5782541Z 
2025-04-11T03:52:12.5782741Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5782845Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5782963Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5783109Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5783328Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5783453Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5783591Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5783736Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5783890Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5783978Z     def test_context_attention(
2025-04-11T03:52:12.5784065Z         bsz: int,
2025-04-11T03:52:12.5784145Z         block_size: int,
2025-04-11T03:52:12.5784238Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5784324Z         num_attn_heads: int,
2025-04-11T03:52:12.5784410Z         kv_group_num: int,
2025-04-11T03:52:12.5784500Z         same_context_len: bool,
2025-04-11T03:52:12.5784583Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5784677Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5784754Z     ):
2025-04-11T03:52:12.5784870Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5785060Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5785242Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5785417Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5785581Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5785666Z             return
2025-04-11T03:52:12.5785737Z     
2025-04-11T03:52:12.5785828Z         torch.manual_seed(123)
2025-04-11T03:52:12.5785928Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5786018Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5786113Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5786117Z 
2025-04-11T03:52:12.5786281Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5786399Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5786404Z 
2025-04-11T03:52:12.5786479Z device = None
2025-04-11T03:52:12.5786484Z 
2025-04-11T03:52:12.5786607Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5786756Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5786827Z     
2025-04-11T03:52:12.5786909Z         Args:
2025-04-11T03:52:12.5787076Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5787248Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5787354Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5787433Z         """
2025-04-11T03:52:12.5787512Z         _lazy_init()
2025-04-11T03:52:12.5787609Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5787721Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5787827Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5788119Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5788364Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5788577Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5788586Z 
2025-04-11T03:52:12.5788825Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5788992Z ____________ test_context_attention[True-False-False-1-16-16-16-32] ____________
2025-04-11T03:52:12.5788996Z 
2025-04-11T03:52:12.5789158Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5789306Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5789508Z use_new_kcache_layout = True
2025-04-11T03:52:12.5789513Z 
2025-04-11T03:52:12.5789715Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5789826Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5789947Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5790088Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5790213Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5790327Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5790468Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5790604Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5790768Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5790863Z     def test_context_attention(
2025-04-11T03:52:12.5790941Z         bsz: int,
2025-04-11T03:52:12.5791033Z         block_size: int,
2025-04-11T03:52:12.5791128Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5791222Z         num_attn_heads: int,
2025-04-11T03:52:12.5791309Z         kv_group_num: int,
2025-04-11T03:52:12.5791396Z         same_context_len: bool,
2025-04-11T03:52:12.5791494Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5791587Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5791667Z     ):
2025-04-11T03:52:12.5791780Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5791977Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5792162Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5792332Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5792500Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5792577Z             return
2025-04-11T03:52:12.5792653Z     
2025-04-11T03:52:12.5792740Z         torch.manual_seed(123)
2025-04-11T03:52:12.5792843Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5792933Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5793022Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5793032Z 
2025-04-11T03:52:12.5793201Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5793318Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5793323Z 
2025-04-11T03:52:12.5793411Z device = None
2025-04-11T03:52:12.5793415Z 
2025-04-11T03:52:12.5793537Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5793691Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5793765Z     
2025-04-11T03:52:12.5793838Z         Args:
2025-04-11T03:52:12.5794009Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5794177Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5794289Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5794361Z         """
2025-04-11T03:52:12.5794443Z         _lazy_init()
2025-04-11T03:52:12.5794652Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5794756Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5794869Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5795152Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5795297Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5795453Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5795458Z 
2025-04-11T03:52:12.5795702Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5795968Z ____________ test_context_attention[True-False-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.5795972Z 
2025-04-11T03:52:12.5796126Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5796272Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5796363Z use_new_kcache_layout = True
2025-04-11T03:52:12.5796368Z 
2025-04-11T03:52:12.5796575Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5796680Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5796809Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5796948Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5797070Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5797186Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5797324Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5797465Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5797618Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5797716Z     def test_context_attention(
2025-04-11T03:52:12.5797796Z         bsz: int,
2025-04-11T03:52:12.5797879Z         block_size: int,
2025-04-11T03:52:12.5797972Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5798057Z         num_attn_heads: int,
2025-04-11T03:52:12.5798145Z         kv_group_num: int,
2025-04-11T03:52:12.5798234Z         same_context_len: bool,
2025-04-11T03:52:12.5798325Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5798413Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5798485Z     ):
2025-04-11T03:52:12.5798605Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5798797Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5798991Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5799162Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5799335Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5799414Z             return
2025-04-11T03:52:12.5799486Z     
2025-04-11T03:52:12.5799580Z         torch.manual_seed(123)
2025-04-11T03:52:12.5799679Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5799773Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5799864Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5799868Z 
2025-04-11T03:52:12.5800038Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5800156Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5800160Z 
2025-04-11T03:52:12.5800236Z device = None
2025-04-11T03:52:12.5800244Z 
2025-04-11T03:52:12.5800365Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5800512Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5800589Z     
2025-04-11T03:52:12.5800664Z         Args:
2025-04-11T03:52:12.5800831Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5801135Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5801245Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5801328Z         """
2025-04-11T03:52:12.5801407Z         _lazy_init()
2025-04-11T03:52:12.5801510Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5801612Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5801719Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5802008Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5802239Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5802404Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5802409Z 
2025-04-11T03:52:12.5802652Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5802827Z ____________ test_context_attention[True-False-False-1-16-16-32-32] ____________
2025-04-11T03:52:12.5802831Z 
2025-04-11T03:52:12.5802984Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5803137Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5803227Z use_new_kcache_layout = True
2025-04-11T03:52:12.5803232Z 
2025-04-11T03:52:12.5803430Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5803539Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5803662Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5803837Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5803964Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5804094Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5804236Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5804372Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5804526Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5804616Z     def test_context_attention(
2025-04-11T03:52:12.5804697Z         bsz: int,
2025-04-11T03:52:12.5804780Z         block_size: int,
2025-04-11T03:52:12.5804868Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5804959Z         num_attn_heads: int,
2025-04-11T03:52:12.5805040Z         kv_group_num: int,
2025-04-11T03:52:12.5805137Z         same_context_len: bool,
2025-04-11T03:52:12.5805221Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5805314Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5805386Z     ):
2025-04-11T03:52:12.5805498Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5805697Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5805884Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5806060Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5806223Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5806301Z             return
2025-04-11T03:52:12.5806372Z     
2025-04-11T03:52:12.5806460Z         torch.manual_seed(123)
2025-04-11T03:52:12.5806566Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5806658Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5806756Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5806760Z 
2025-04-11T03:52:12.5806925Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5807039Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5807153Z 
2025-04-11T03:52:12.5807237Z device = None
2025-04-11T03:52:12.5807241Z 
2025-04-11T03:52:12.5807361Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5807519Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5807593Z     
2025-04-11T03:52:12.5807674Z         Args:
2025-04-11T03:52:12.5807841Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5808007Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5808119Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5808293Z         """
2025-04-11T03:52:12.5808377Z         _lazy_init()
2025-04-11T03:52:12.5808476Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5808586Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5808692Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5808978Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5809123Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5809283Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5809288Z 
2025-04-11T03:52:12.5809532Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5809699Z _____________ test_context_attention[True-False-False-4-16-8-16-7] _____________
2025-04-11T03:52:12.5809703Z 
2025-04-11T03:52:12.5809860Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5810009Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5810103Z use_new_kcache_layout = True
2025-04-11T03:52:12.5810107Z 
2025-04-11T03:52:12.5810307Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5810414Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5810537Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5810676Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5810801Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5810915Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5811053Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5811190Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5811343Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5811443Z     def test_context_attention(
2025-04-11T03:52:12.5811520Z         bsz: int,
2025-04-11T03:52:12.5811609Z         block_size: int,
2025-04-11T03:52:12.5811700Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5811787Z         num_attn_heads: int,
2025-04-11T03:52:12.5811884Z         kv_group_num: int,
2025-04-11T03:52:12.5811973Z         same_context_len: bool,
2025-04-11T03:52:12.5812064Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5812152Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5812231Z     ):
2025-04-11T03:52:12.5812341Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5812533Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5812720Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5812889Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5813063Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5813140Z             return
2025-04-11T03:52:12.5813217Z     
2025-04-11T03:52:12.5813302Z         torch.manual_seed(123)
2025-04-11T03:52:12.5813403Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5813653Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5813751Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5813755Z 
2025-04-11T03:52:12.5813923Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5814077Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5814084Z 
2025-04-11T03:52:12.5814193Z device = None
2025-04-11T03:52:12.5814199Z 
2025-04-11T03:52:12.5814325Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5814476Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5814661Z     
2025-04-11T03:52:12.5814737Z         Args:
2025-04-11T03:52:12.5814908Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5815072Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5815183Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5815261Z         """
2025-04-11T03:52:12.5815340Z         _lazy_init()
2025-04-11T03:52:12.5815446Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5815549Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5815666Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5815943Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5816077Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5816242Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5816250Z 
2025-04-11T03:52:12.5816488Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5816661Z ____________ test_context_attention[True-False-False-4-16-8-16-32] _____________
2025-04-11T03:52:12.5816669Z 
2025-04-11T03:52:12.5816819Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5816969Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5817059Z use_new_kcache_layout = True
2025-04-11T03:52:12.5817063Z 
2025-04-11T03:52:12.5817266Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5817369Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5817489Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5817634Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5817755Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5817877Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5818013Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5818158Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5818320Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5818414Z     def test_context_attention(
2025-04-11T03:52:12.5818503Z         bsz: int,
2025-04-11T03:52:12.5818584Z         block_size: int,
2025-04-11T03:52:12.5818677Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5818764Z         num_attn_heads: int,
2025-04-11T03:52:12.5818852Z         kv_group_num: int,
2025-04-11T03:52:12.5818938Z         same_context_len: bool,
2025-04-11T03:52:12.5819023Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5819118Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5819192Z     ):
2025-04-11T03:52:12.5819312Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5819500Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5819680Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5819856Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5820128Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5820214Z             return
2025-04-11T03:52:12.5820289Z     
2025-04-11T03:52:12.5820383Z         torch.manual_seed(123)
2025-04-11T03:52:12.5820483Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5820573Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5820672Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5820676Z 
2025-04-11T03:52:12.5820843Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5821073Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5821077Z 
2025-04-11T03:52:12.5821155Z device = None
2025-04-11T03:52:12.5821160Z 
2025-04-11T03:52:12.5821281Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5821434Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5821507Z     
2025-04-11T03:52:12.5821588Z         Args:
2025-04-11T03:52:12.5821755Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5821924Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5822030Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5822110Z         """
2025-04-11T03:52:12.5822188Z         _lazy_init()
2025-04-11T03:52:12.5822284Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5822391Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5822502Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5822792Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5822929Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5823103Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5823107Z 
2025-04-11T03:52:12.5823362Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5823542Z _____________ test_context_attention[True-False-False-4-16-8-32-7] _____________
2025-04-11T03:52:12.5823558Z 
2025-04-11T03:52:12.5823708Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5823853Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5823948Z use_new_kcache_layout = True
2025-04-11T03:52:12.5823954Z 
2025-04-11T03:52:12.5824150Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5824258Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5824377Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5824521Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5824640Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5824755Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5824895Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5825032Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5825187Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5825276Z     def test_context_attention(
2025-04-11T03:52:12.5825357Z         bsz: int,
2025-04-11T03:52:12.5825440Z         block_size: int,
2025-04-11T03:52:12.5825529Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5825620Z         num_attn_heads: int,
2025-04-11T03:52:12.5825703Z         kv_group_num: int,
2025-04-11T03:52:12.5825795Z         same_context_len: bool,
2025-04-11T03:52:12.5825878Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5825966Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5826163Z     ):
2025-04-11T03:52:12.5826276Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5826471Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5826654Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5826830Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5826993Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5827068Z             return
2025-04-11T03:52:12.5827146Z     
2025-04-11T03:52:12.5827332Z         torch.manual_seed(123)
2025-04-11T03:52:12.5827443Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5827535Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5827626Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5827636Z 
2025-04-11T03:52:12.5827806Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5827923Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5827927Z 
2025-04-11T03:52:12.5828011Z device = None
2025-04-11T03:52:12.5828016Z 
2025-04-11T03:52:12.5828133Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5828290Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5828365Z     
2025-04-11T03:52:12.5828476Z         Args:
2025-04-11T03:52:12.5828645Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5828811Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5828928Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5829002Z         """
2025-04-11T03:52:12.5829087Z         _lazy_init()
2025-04-11T03:52:12.5829186Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5829291Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5829400Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5829683Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5829836Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5830001Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5830006Z 
2025-04-11T03:52:12.5830251Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5830423Z ____________ test_context_attention[True-False-False-4-16-8-32-32] _____________
2025-04-11T03:52:12.5830427Z 
2025-04-11T03:52:12.5830581Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5830726Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5830819Z use_new_kcache_layout = True
2025-04-11T03:52:12.5830824Z 
2025-04-11T03:52:12.5831035Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5831141Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5831269Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5831410Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5831534Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5831649Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5831785Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5831927Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5832080Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5832181Z     def test_context_attention(
2025-04-11T03:52:12.5832258Z         bsz: int,
2025-04-11T03:52:12.5832343Z         block_size: int,
2025-04-11T03:52:12.5832557Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5832645Z         num_attn_heads: int,
2025-04-11T03:52:12.5832738Z         kv_group_num: int,
2025-04-11T03:52:12.5832823Z         same_context_len: bool,
2025-04-11T03:52:12.5832914Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5833006Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5833079Z     ):
2025-04-11T03:52:12.5833201Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5833398Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5833586Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5833990Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5834156Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5834237Z             return
2025-04-11T03:52:12.5834310Z     
2025-04-11T03:52:12.5834408Z         torch.manual_seed(123)
2025-04-11T03:52:12.5834513Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5834618Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5834725Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5834729Z 
2025-04-11T03:52:12.5834913Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5835025Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5835029Z 
2025-04-11T03:52:12.5835107Z device = None
2025-04-11T03:52:12.5835111Z 
2025-04-11T03:52:12.5835236Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5835388Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5835464Z     
2025-04-11T03:52:12.5835538Z         Args:
2025-04-11T03:52:12.5835713Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5835885Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5835992Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5836072Z         """
2025-04-11T03:52:12.5836150Z         _lazy_init()
2025-04-11T03:52:12.5836251Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5836354Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5836462Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5836754Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5836895Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5837058Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5837063Z 
2025-04-11T03:52:12.5837302Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5837479Z ____________ test_context_attention[True-False-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.5837483Z 
2025-04-11T03:52:12.5837633Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5837783Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5837872Z use_new_kcache_layout = True
2025-04-11T03:52:12.5837876Z 
2025-04-11T03:52:12.5838076Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5838183Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5838302Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5838450Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5838567Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5838688Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5838932Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5839073Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5839244Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5839348Z     def test_context_attention(
2025-04-11T03:52:12.5839440Z         bsz: int,
2025-04-11T03:52:12.5839525Z         block_size: int,
2025-04-11T03:52:12.5839619Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5839705Z         num_attn_heads: int,
2025-04-11T03:52:12.5839791Z         kv_group_num: int,
2025-04-11T03:52:12.5839886Z         same_context_len: bool,
2025-04-11T03:52:12.5840089Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5840184Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5840259Z     ):
2025-04-11T03:52:12.5840369Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5840568Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5840751Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5840929Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5841103Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5841184Z             return
2025-04-11T03:52:12.5841257Z     
2025-04-11T03:52:12.5841343Z         torch.manual_seed(123)
2025-04-11T03:52:12.5841451Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5841542Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5841643Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5841651Z 
2025-04-11T03:52:12.5841821Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5841942Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5841946Z 
2025-04-11T03:52:12.5842024Z device = None
2025-04-11T03:52:12.5842031Z 
2025-04-11T03:52:12.5842151Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5842307Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5842379Z     
2025-04-11T03:52:12.5842458Z         Args:
2025-04-11T03:52:12.5842627Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5842798Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5842908Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5842983Z         """
2025-04-11T03:52:12.5843067Z         _lazy_init()
2025-04-11T03:52:12.5843165Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5843277Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5843380Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5843677Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5843821Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5844012Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5844017Z 
2025-04-11T03:52:12.5844282Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5844454Z ____________ test_context_attention[True-False-False-4-16-16-16-32] ____________
2025-04-11T03:52:12.5844458Z 
2025-04-11T03:52:12.5844617Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5844766Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5844863Z use_new_kcache_layout = True
2025-04-11T03:52:12.5844867Z 
2025-04-11T03:52:12.5845069Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5845178Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5845406Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5845548Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5845673Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5845788Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5845937Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5846075Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5846232Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5846324Z     def test_context_attention(
2025-04-11T03:52:12.5846513Z         bsz: int,
2025-04-11T03:52:12.5846602Z         block_size: int,
2025-04-11T03:52:12.5846691Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5846783Z         num_attn_heads: int,
2025-04-11T03:52:12.5846866Z         kv_group_num: int,
2025-04-11T03:52:12.5846957Z         same_context_len: bool,
2025-04-11T03:52:12.5847057Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5847144Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5847225Z     ):
2025-04-11T03:52:12.5847336Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5847530Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5847715Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5847883Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5848052Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5848131Z             return
2025-04-11T03:52:12.5848210Z     
2025-04-11T03:52:12.5848298Z         torch.manual_seed(123)
2025-04-11T03:52:12.5848397Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5848497Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5848593Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5848597Z 
2025-04-11T03:52:12.5848768Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5848882Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5848886Z 
2025-04-11T03:52:12.5848969Z device = None
2025-04-11T03:52:12.5848973Z 
2025-04-11T03:52:12.5849089Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5849255Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5849334Z     
2025-04-11T03:52:12.5849409Z         Args:
2025-04-11T03:52:12.5849589Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5849758Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5849871Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5849948Z         """
2025-04-11T03:52:12.5850025Z         _lazy_init()
2025-04-11T03:52:12.5850126Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5850226Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5850339Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5850620Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5850760Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5850921Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5850928Z 
2025-04-11T03:52:12.5851166Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5851340Z ____________ test_context_attention[True-False-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.5851345Z 
2025-04-11T03:52:12.5851491Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5851750Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5851837Z use_new_kcache_layout = True
2025-04-11T03:52:12.5851842Z 
2025-04-11T03:52:12.5852042Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5852150Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5852271Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5852409Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5852525Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5852736Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5852876Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5853017Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5853166Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5853263Z     def test_context_attention(
2025-04-11T03:52:12.5853344Z         bsz: int,
2025-04-11T03:52:12.5853427Z         block_size: int,
2025-04-11T03:52:12.5853525Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5853607Z         num_attn_heads: int,
2025-04-11T03:52:12.5853698Z         kv_group_num: int,
2025-04-11T03:52:12.5853785Z         same_context_len: bool,
2025-04-11T03:52:12.5853872Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5853967Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5854048Z     ):
2025-04-11T03:52:12.5854169Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5854367Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5854561Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5854740Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5854907Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5854990Z             return
2025-04-11T03:52:12.5855062Z     
2025-04-11T03:52:12.5855155Z         torch.manual_seed(123)
2025-04-11T03:52:12.5855254Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5855347Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5855446Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5855450Z 
2025-04-11T03:52:12.5855617Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5855734Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5855741Z 
2025-04-11T03:52:12.5855820Z device = None
2025-04-11T03:52:12.5855824Z 
2025-04-11T03:52:12.5855944Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5856098Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5856179Z     
2025-04-11T03:52:12.5856254Z         Args:
2025-04-11T03:52:12.5856421Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5856592Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5856697Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5856777Z         """
2025-04-11T03:52:12.5856856Z         _lazy_init()
2025-04-11T03:52:12.5856953Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5857060Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5857167Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5857457Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5857595Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5857757Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5857870Z 
2025-04-11T03:52:12.5858110Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5858285Z ____________ test_context_attention[True-False-False-4-16-16-32-32] ____________
2025-04-11T03:52:12.5858290Z 
2025-04-11T03:52:12.5858442Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5858588Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.5858681Z use_new_kcache_layout = True
2025-04-11T03:52:12.5858685Z 
2025-04-11T03:52:12.5858886Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5859122Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5859243Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5859397Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5859522Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5859638Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5859782Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5859922Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5860077Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5860167Z     def test_context_attention(
2025-04-11T03:52:12.5860248Z         bsz: int,
2025-04-11T03:52:12.5860331Z         block_size: int,
2025-04-11T03:52:12.5860421Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5860509Z         num_attn_heads: int,
2025-04-11T03:52:12.5860596Z         kv_group_num: int,
2025-04-11T03:52:12.5860683Z         same_context_len: bool,
2025-04-11T03:52:12.5860769Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5860857Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5860935Z     ):
2025-04-11T03:52:12.5861045Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5861249Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5861433Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5861609Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5861773Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5861847Z             return
2025-04-11T03:52:12.5861925Z     
2025-04-11T03:52:12.5862012Z         torch.manual_seed(123)
2025-04-11T03:52:12.5862119Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5862212Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5862309Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5862313Z 
2025-04-11T03:52:12.5862478Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5862592Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5862597Z 
2025-04-11T03:52:12.5862680Z device = None
2025-04-11T03:52:12.5862684Z 
2025-04-11T03:52:12.5862804Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5862962Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5863034Z     
2025-04-11T03:52:12.5863117Z         Args:
2025-04-11T03:52:12.5863286Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5863452Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5863575Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5863661Z         """
2025-04-11T03:52:12.5863753Z         _lazy_init()
2025-04-11T03:52:12.5863857Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5863979Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5864201Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5864485Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5864629Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5864788Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5864792Z 
2025-04-11T03:52:12.5865041Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5865208Z _____________ test_context_attention[False-True-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.5865308Z 
2025-04-11T03:52:12.5865467Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5865615Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5865717Z use_new_kcache_layout = False
2025-04-11T03:52:12.5865722Z 
2025-04-11T03:52:12.5865928Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5866032Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5866157Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5866298Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5866423Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5866539Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5866681Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5866817Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5866973Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5867068Z     def test_context_attention(
2025-04-11T03:52:12.5867145Z         bsz: int,
2025-04-11T03:52:12.5867231Z         block_size: int,
2025-04-11T03:52:12.5867322Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5867411Z         num_attn_heads: int,
2025-04-11T03:52:12.5867513Z         kv_group_num: int,
2025-04-11T03:52:12.5867610Z         same_context_len: bool,
2025-04-11T03:52:12.5867709Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5867801Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5867888Z     ):
2025-04-11T03:52:12.5867998Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5868192Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5868379Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5868611Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5868779Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5868855Z             return
2025-04-11T03:52:12.5868937Z     
2025-04-11T03:52:12.5869033Z         torch.manual_seed(123)
2025-04-11T03:52:12.5869132Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5869227Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5869316Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5869321Z 
2025-04-11T03:52:12.5869492Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5869605Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5869610Z 
2025-04-11T03:52:12.5869708Z device = None
2025-04-11T03:52:12.5869712Z 
2025-04-11T03:52:12.5869842Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5869998Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5870078Z     
2025-04-11T03:52:12.5870153Z         Args:
2025-04-11T03:52:12.5870327Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5870491Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5870744Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5870821Z         """
2025-04-11T03:52:12.5870902Z         _lazy_init()
2025-04-11T03:52:12.5871007Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5871110Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5871221Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5871502Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5871638Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5871911Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5871915Z 
2025-04-11T03:52:12.5872158Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5872333Z _____________ test_context_attention[False-True-True-1-16-8-16-32] _____________
2025-04-11T03:52:12.5872341Z 
2025-04-11T03:52:12.5872492Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5872644Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5872735Z use_new_kcache_layout = False
2025-04-11T03:52:12.5872739Z 
2025-04-11T03:52:12.5872942Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5873046Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5873163Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5873310Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5873433Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5873566Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5873705Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5873847Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5873999Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5874094Z     def test_context_attention(
2025-04-11T03:52:12.5874175Z         bsz: int,
2025-04-11T03:52:12.5874258Z         block_size: int,
2025-04-11T03:52:12.5874353Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5874436Z         num_attn_heads: int,
2025-04-11T03:52:12.5874525Z         kv_group_num: int,
2025-04-11T03:52:12.5874612Z         same_context_len: bool,
2025-04-11T03:52:12.5874699Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5874795Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5874871Z     ):
2025-04-11T03:52:12.5874989Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5875178Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5875358Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5875537Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5875699Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5875781Z             return
2025-04-11T03:52:12.5875856Z     
2025-04-11T03:52:12.5875950Z         torch.manual_seed(123)
2025-04-11T03:52:12.5876048Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5876136Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5876230Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5876235Z 
2025-04-11T03:52:12.5876406Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5876524Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5876528Z 
2025-04-11T03:52:12.5876605Z device = None
2025-04-11T03:52:12.5876609Z 
2025-04-11T03:52:12.5876732Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5877002Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5877075Z     
2025-04-11T03:52:12.5877156Z         Args:
2025-04-11T03:52:12.5877321Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5877499Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5877606Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5877712Z         """
2025-04-11T03:52:12.5877791Z         _lazy_init()
2025-04-11T03:52:12.5877897Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5878108Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5878213Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5878500Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5878644Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5878807Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5878812Z 
2025-04-11T03:52:12.5879047Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5879214Z _____________ test_context_attention[False-True-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.5879222Z 
2025-04-11T03:52:12.5879371Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5879515Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5879646Z use_new_kcache_layout = False
2025-04-11T03:52:12.5879651Z 
2025-04-11T03:52:12.5879847Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5879958Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5880076Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5880232Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5880357Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5880478Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5880619Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5880755Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5880909Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5881001Z     def test_context_attention(
2025-04-11T03:52:12.5881078Z         bsz: int,
2025-04-11T03:52:12.5881168Z         block_size: int,
2025-04-11T03:52:12.5881256Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5881350Z         num_attn_heads: int,
2025-04-11T03:52:12.5881433Z         kv_group_num: int,
2025-04-11T03:52:12.5881529Z         same_context_len: bool,
2025-04-11T03:52:12.5881614Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5881706Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5881785Z     ):
2025-04-11T03:52:12.5881894Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5882088Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5882268Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5882436Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5882601Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5882678Z             return
2025-04-11T03:52:12.5882758Z     
2025-04-11T03:52:12.5882846Z         torch.manual_seed(123)
2025-04-11T03:52:12.5882948Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5883037Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5883127Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5883240Z 
2025-04-11T03:52:12.5883416Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5883533Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5883537Z 
2025-04-11T03:52:12.5883622Z device = None
2025-04-11T03:52:12.5883627Z 
2025-04-11T03:52:12.5883745Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5883902Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5883985Z     
2025-04-11T03:52:12.5884060Z         Args:
2025-04-11T03:52:12.5884234Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5884515Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5884637Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5884717Z         """
2025-04-11T03:52:12.5884811Z         _lazy_init()
2025-04-11T03:52:12.5884912Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5885020Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5885135Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5885418Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5885560Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5885717Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5885722Z 
2025-04-11T03:52:12.5885965Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5886136Z _____________ test_context_attention[False-True-True-1-16-8-32-32] _____________
2025-04-11T03:52:12.5886140Z 
2025-04-11T03:52:12.5886296Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5886443Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5886534Z use_new_kcache_layout = False
2025-04-11T03:52:12.5886539Z 
2025-04-11T03:52:12.5886744Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5886851Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5886974Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5887113Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5887239Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5887354Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5887493Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5887635Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5887785Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5887883Z     def test_context_attention(
2025-04-11T03:52:12.5887964Z         bsz: int,
2025-04-11T03:52:12.5888048Z         block_size: int,
2025-04-11T03:52:12.5888138Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5888226Z         num_attn_heads: int,
2025-04-11T03:52:12.5888316Z         kv_group_num: int,
2025-04-11T03:52:12.5888403Z         same_context_len: bool,
2025-04-11T03:52:12.5888497Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5888588Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5888662Z     ):
2025-04-11T03:52:12.5888777Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5888971Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5889157Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5889329Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5889609Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5889841Z             return
2025-04-11T03:52:12.5889946Z     
2025-04-11T03:52:12.5890096Z         torch.manual_seed(123)
2025-04-11T03:52:12.5890239Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5890441Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5890622Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5890628Z 
2025-04-11T03:52:12.5890830Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5891021Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5891026Z 
2025-04-11T03:52:12.5891376Z device = None
2025-04-11T03:52:12.5891381Z 
2025-04-11T03:52:12.5891617Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5891826Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5891967Z     
2025-04-11T03:52:12.5892061Z         Args:
2025-04-11T03:52:12.5892296Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5892553Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5892689Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5892829Z         """
2025-04-11T03:52:12.5892940Z         _lazy_init()
2025-04-11T03:52:12.5893096Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5893253Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5893386Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5893742Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5893914Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5894120Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5894126Z 
2025-04-11T03:52:12.5894403Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5894650Z _____________ test_context_attention[False-True-True-1-16-16-16-7] _____________
2025-04-11T03:52:12.5894655Z 
2025-04-11T03:52:12.5894832Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5895037Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5895158Z use_new_kcache_layout = False
2025-04-11T03:52:12.5895164Z 
2025-04-11T03:52:12.5895375Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5895676Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5895841Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5896095Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5896232Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5896459Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5896635Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5896843Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5897055Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5897197Z     def test_context_attention(
2025-04-11T03:52:12.5897353Z         bsz: int,
2025-04-11T03:52:12.5897469Z         block_size: int,
2025-04-11T03:52:12.5897629Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5897745Z         num_attn_heads: int,
2025-04-11T03:52:12.5897849Z         kv_group_num: int,
2025-04-11T03:52:12.5898017Z         same_context_len: bool,
2025-04-11T03:52:12.5898152Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5898348Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5898441Z     ):
2025-04-11T03:52:12.5898647Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5898989Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5899216Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5899587Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5899785Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5899930Z             return
2025-04-11T03:52:12.5900033Z     
2025-04-11T03:52:12.5900171Z         torch.manual_seed(123)
2025-04-11T03:52:12.5900319Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5900575Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5900735Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5900740Z 
2025-04-11T03:52:12.5900938Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5901123Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5901131Z 
2025-04-11T03:52:12.5901222Z device = None
2025-04-11T03:52:12.5901227Z 
2025-04-11T03:52:12.5901426Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5901604Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5901704Z     
2025-04-11T03:52:12.5901845Z         Args:
2025-04-11T03:52:12.5902042Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5902260Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5902407Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5902549Z         """
2025-04-11T03:52:12.5902658Z         _lazy_init()
2025-04-11T03:52:12.5902799Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5903026Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5903169Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5903530Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5903698Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5903933Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5903938Z 
2025-04-11T03:52:12.5904208Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5904436Z ____________ test_context_attention[False-True-True-1-16-16-16-32] _____________
2025-04-11T03:52:12.5904441Z 
2025-04-11T03:52:12.5904639Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5904815Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5904980Z use_new_kcache_layout = False
2025-04-11T03:52:12.5904985Z 
2025-04-11T03:52:12.5905239Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5905458Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5905620Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5905840Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5905991Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5906203Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5906374Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5906540Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5906760Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5906893Z     def test_context_attention(
2025-04-11T03:52:12.5907029Z         bsz: int,
2025-04-11T03:52:12.5907152Z         block_size: int,
2025-04-11T03:52:12.5907359Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5907505Z         num_attn_heads: int,
2025-04-11T03:52:12.5907742Z         kv_group_num: int,
2025-04-11T03:52:12.5907880Z         same_context_len: bool,
2025-04-11T03:52:12.5908007Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5908169Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5908277Z     ):
2025-04-11T03:52:12.5908467Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5908722Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5908918Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5909172Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5909498Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5909650Z             return
2025-04-11T03:52:12.5909754Z     
2025-04-11T03:52:12.5909869Z         torch.manual_seed(123)
2025-04-11T03:52:12.5910065Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5910208Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5910415Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5910420Z 
2025-04-11T03:52:12.5910615Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5910828Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5910833Z 
2025-04-11T03:52:12.5910942Z device = None
2025-04-11T03:52:12.5910947Z 
2025-04-11T03:52:12.5911212Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5911390Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5911510Z     
2025-04-11T03:52:12.5911676Z         Args:
2025-04-11T03:52:12.5911875Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5912102Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5912245Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5912380Z         """
2025-04-11T03:52:12.5912498Z         _lazy_init()
2025-04-11T03:52:12.5912642Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5912805Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5912941Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5913297Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5913448Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5913684Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5913692Z 
2025-04-11T03:52:12.5913961Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5914159Z _____________ test_context_attention[False-True-True-1-16-16-32-7] _____________
2025-04-11T03:52:12.5914199Z 
2025-04-11T03:52:12.5914390Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5914560Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5914741Z use_new_kcache_layout = False
2025-04-11T03:52:12.5914751Z 
2025-04-11T03:52:12.5914996Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5915180Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5915331Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5915668Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5915834Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5915980Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5916186Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5916340Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5916714Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5916840Z     def test_context_attention(
2025-04-11T03:52:12.5916979Z         bsz: int,
2025-04-11T03:52:12.5917091Z         block_size: int,
2025-04-11T03:52:12.5917212Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5917359Z         num_attn_heads: int,
2025-04-11T03:52:12.5917482Z         kv_group_num: int,
2025-04-11T03:52:12.5917630Z         same_context_len: bool,
2025-04-11T03:52:12.5917747Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5917866Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5917998Z     ):
2025-04-11T03:52:12.5918257Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5918523Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5918735Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5918978Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5919254Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5919381Z             return
2025-04-11T03:52:12.5919523Z     
2025-04-11T03:52:12.5919658Z         torch.manual_seed(123)
2025-04-11T03:52:12.5919864Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5920035Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5920208Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5920214Z 
2025-04-11T03:52:12.5920414Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5920598Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5920603Z 
2025-04-11T03:52:12.5920700Z device = None
2025-04-11T03:52:12.5920705Z 
2025-04-11T03:52:12.5920870Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5921108Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5921217Z     
2025-04-11T03:52:12.5921385Z         Args:
2025-04-11T03:52:12.5921585Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5921818Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5921965Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5922067Z         """
2025-04-11T03:52:12.5922207Z         _lazy_init()
2025-04-11T03:52:12.5922336Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5922484Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5922650Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5923097Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5923261Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5923454Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5923489Z 
2025-04-11T03:52:12.5923753Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5923945Z ____________ test_context_attention[False-True-True-1-16-16-32-32] _____________
2025-04-11T03:52:12.5923950Z 
2025-04-11T03:52:12.5924178Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.5924354Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5924511Z use_new_kcache_layout = False
2025-04-11T03:52:12.5924519Z 
2025-04-11T03:52:12.5924749Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5924913Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5925069Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5925285Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5925533Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5925677Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5925884Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5926035Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5926265Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5926384Z     def test_context_attention(
2025-04-11T03:52:12.5926489Z         bsz: int,
2025-04-11T03:52:12.5926640Z         block_size: int,
2025-04-11T03:52:12.5926870Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5927080Z         num_attn_heads: int,
2025-04-11T03:52:12.5927202Z         kv_group_num: int,
2025-04-11T03:52:12.5927355Z         same_context_len: bool,
2025-04-11T03:52:12.5927473Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5927592Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5927722Z     ):
2025-04-11T03:52:12.5927871Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5928152Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5928363Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5928601Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5928794Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5928887Z             return
2025-04-11T03:52:12.5929048Z     
2025-04-11T03:52:12.5929174Z         torch.manual_seed(123)
2025-04-11T03:52:12.5929340Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5929467Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5929608Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5929612Z 
2025-04-11T03:52:12.5929817Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5929984Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5930019Z 
2025-04-11T03:52:12.5930127Z device = None
2025-04-11T03:52:12.5930132Z 
2025-04-11T03:52:12.5930279Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5930495Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5930700Z     
2025-04-11T03:52:12.5930870Z         Args:
2025-04-11T03:52:12.5931082Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5931280Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5931455Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5931574Z         """
2025-04-11T03:52:12.5931717Z         _lazy_init()
2025-04-11T03:52:12.5931857Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5932026Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5932158Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5932483Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5932692Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5932893Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5932898Z 
2025-04-11T03:52:12.5933196Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5933397Z _____________ test_context_attention[False-True-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.5933402Z 
2025-04-11T03:52:12.5933613Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5933798Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5934075Z use_new_kcache_layout = False
2025-04-11T03:52:12.5934080Z 
2025-04-11T03:52:12.5934310Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5934477Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5934613Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5934790Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5935060Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5935203Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5935401Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5935682Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5935902Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5936037Z     def test_context_attention(
2025-04-11T03:52:12.5936147Z         bsz: int,
2025-04-11T03:52:12.5936297Z         block_size: int,
2025-04-11T03:52:12.5936418Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5936552Z         num_attn_heads: int,
2025-04-11T03:52:12.5936680Z         kv_group_num: int,
2025-04-11T03:52:12.5936837Z         same_context_len: bool,
2025-04-11T03:52:12.5936950Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5937069Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5937201Z     ):
2025-04-11T03:52:12.5937334Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5937611Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5937832Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5938041Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5938275Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5938396Z             return
2025-04-11T03:52:12.5938604Z     
2025-04-11T03:52:12.5938733Z         torch.manual_seed(123)
2025-04-11T03:52:12.5938894Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5939015Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5939140Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5939145Z 
2025-04-11T03:52:12.5939369Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5939520Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5939525Z 
2025-04-11T03:52:12.5939677Z device = None
2025-04-11T03:52:12.5939682Z 
2025-04-11T03:52:12.5939833Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5940059Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5940165Z     
2025-04-11T03:52:12.5940295Z         Args:
2025-04-11T03:52:12.5940507Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5940700Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5940865Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5940976Z         """
2025-04-11T03:52:12.5941099Z         _lazy_init()
2025-04-11T03:52:12.5941250Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5941419Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5941556Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5941878Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5942169Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5942372Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5942377Z 
2025-04-11T03:52:12.5942706Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5943016Z _____________ test_context_attention[False-True-True-4-16-8-16-32] _____________
2025-04-11T03:52:12.5943022Z 
2025-04-11T03:52:12.5943244Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5943415Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5943552Z use_new_kcache_layout = False
2025-04-11T03:52:12.5943557Z 
2025-04-11T03:52:12.5943796Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5943941Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5944237Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5944404Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5944584Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5944714Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5944946Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5945110Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5945295Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5945451Z     def test_context_attention(
2025-04-11T03:52:12.5945560Z         bsz: int,
2025-04-11T03:52:12.5945696Z         block_size: int,
2025-04-11T03:52:12.5945837Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5945981Z         num_attn_heads: int,
2025-04-11T03:52:12.5946170Z         kv_group_num: int,
2025-04-11T03:52:12.5946286Z         same_context_len: bool,
2025-04-11T03:52:12.5946425Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5946564Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5946707Z     ):
2025-04-11T03:52:12.5946852Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5947071Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5947319Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5947505Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5947761Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5947868Z             return
2025-04-11T03:52:12.5948004Z     
2025-04-11T03:52:12.5948124Z         torch.manual_seed(123)
2025-04-11T03:52:12.5948282Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5948442Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5948580Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5948585Z 
2025-04-11T03:52:12.5948808Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5948948Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5948953Z 
2025-04-11T03:52:12.5949104Z device = None
2025-04-11T03:52:12.5949108Z 
2025-04-11T03:52:12.5949240Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5949480Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5949583Z     
2025-04-11T03:52:12.5949762Z         Args:
2025-04-11T03:52:12.5949992Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5950196Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5985758Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5985930Z         """
2025-04-11T03:52:12.5986039Z         _lazy_init()
2025-04-11T03:52:12.5986147Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5986269Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5986389Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5986717Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5987092Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5987272Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5987279Z 
2025-04-11T03:52:12.5987551Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5987738Z _____________ test_context_attention[False-True-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.5987748Z 
2025-04-11T03:52:12.5987914Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5988228Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5988331Z use_new_kcache_layout = False
2025-04-11T03:52:12.5988337Z 
2025-04-11T03:52:12.5988616Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5988736Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5988865Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5989023Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5989144Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5989261Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5989407Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5989543Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5989701Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5989798Z     def test_context_attention(
2025-04-11T03:52:12.5989878Z         bsz: int,
2025-04-11T03:52:12.5989973Z         block_size: int,
2025-04-11T03:52:12.5990070Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5990163Z         num_attn_heads: int,
2025-04-11T03:52:12.5990248Z         kv_group_num: int,
2025-04-11T03:52:12.5990345Z         same_context_len: bool,
2025-04-11T03:52:12.5990433Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5990525Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5990607Z     ):
2025-04-11T03:52:12.5990725Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5990930Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5991118Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5991293Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5991464Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5991541Z             return
2025-04-11T03:52:12.5991620Z     
2025-04-11T03:52:12.5991711Z         torch.manual_seed(123)
2025-04-11T03:52:12.5991816Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5991913Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5992007Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5992011Z 
2025-04-11T03:52:12.5992190Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5992309Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5992314Z 
2025-04-11T03:52:12.5992399Z device = None
2025-04-11T03:52:12.5992405Z 
2025-04-11T03:52:12.5992529Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5992690Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5992764Z     
2025-04-11T03:52:12.5992846Z         Args:
2025-04-11T03:52:12.5993020Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.5993187Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.5993301Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.5993494Z         """
2025-04-11T03:52:12.5993578Z         _lazy_init()
2025-04-11T03:52:12.5993675Z         with torch.cuda.device(device):
2025-04-11T03:52:12.5993780Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.5993893Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.5994182Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.5994328Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.5994488Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.5994589Z 
2025-04-11T03:52:12.5994836Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.5995005Z _____________ test_context_attention[False-True-True-4-16-8-32-32] _____________
2025-04-11T03:52:12.5995009Z 
2025-04-11T03:52:12.5995168Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.5995320Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.5995411Z use_new_kcache_layout = False
2025-04-11T03:52:12.5995415Z 
2025-04-11T03:52:12.5995620Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.5995728Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.5995852Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.5995990Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.5996111Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.5996229Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.5996365Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.5996504Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.5996656Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.5996753Z     def test_context_attention(
2025-04-11T03:52:12.5996829Z         bsz: int,
2025-04-11T03:52:12.5996915Z         block_size: int,
2025-04-11T03:52:12.5997011Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.5997096Z         num_attn_heads: int,
2025-04-11T03:52:12.5997184Z         kv_group_num: int,
2025-04-11T03:52:12.5997271Z         same_context_len: bool,
2025-04-11T03:52:12.5997361Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.5997449Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.5997524Z     ):
2025-04-11T03:52:12.5997643Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.5997840Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.5998029Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.5998200Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.5998368Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.5998447Z             return
2025-04-11T03:52:12.5998518Z     
2025-04-11T03:52:12.5998615Z         torch.manual_seed(123)
2025-04-11T03:52:12.5998715Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.5998815Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.5998909Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.5998914Z 
2025-04-11T03:52:12.5999082Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.5999202Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.5999209Z 
2025-04-11T03:52:12.5999286Z device = None
2025-04-11T03:52:12.5999291Z 
2025-04-11T03:52:12.5999417Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.5999567Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.5999756Z     
2025-04-11T03:52:12.5999834Z         Args:
2025-04-11T03:52:12.6000004Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6000173Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6000285Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6000363Z         """
2025-04-11T03:52:12.6000444Z         _lazy_init()
2025-04-11T03:52:12.6000543Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6000647Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6000753Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6001142Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6001280Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6001444Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6001452Z 
2025-04-11T03:52:12.6001693Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6001864Z _____________ test_context_attention[False-True-True-4-16-16-16-7] _____________
2025-04-11T03:52:12.6001868Z 
2025-04-11T03:52:12.6002018Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6002164Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.6002253Z use_new_kcache_layout = False
2025-04-11T03:52:12.6002258Z 
2025-04-11T03:52:12.6002456Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6002573Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6002692Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6002832Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6002952Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6003071Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6003206Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6003340Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6003496Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6003588Z     def test_context_attention(
2025-04-11T03:52:12.6003667Z         bsz: int,
2025-04-11T03:52:12.6003749Z         block_size: int,
2025-04-11T03:52:12.6003844Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6003932Z         num_attn_heads: int,
2025-04-11T03:52:12.6004017Z         kv_group_num: int,
2025-04-11T03:52:12.6004109Z         same_context_len: bool,
2025-04-11T03:52:12.6004193Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6004283Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6004357Z     ):
2025-04-11T03:52:12.6004474Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6004678Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6004860Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6005036Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6005198Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6005276Z             return
2025-04-11T03:52:12.6005348Z     
2025-04-11T03:52:12.6005435Z         torch.manual_seed(123)
2025-04-11T03:52:12.6005543Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6005634Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6005730Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6005734Z 
2025-04-11T03:52:12.6005905Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6006123Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6006131Z 
2025-04-11T03:52:12.6006209Z device = None
2025-04-11T03:52:12.6006214Z 
2025-04-11T03:52:12.6006333Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6006491Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6006561Z     
2025-04-11T03:52:12.6006638Z         Args:
2025-04-11T03:52:12.6006807Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6006978Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6007198Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6007270Z         """
2025-04-11T03:52:12.6007352Z         _lazy_init()
2025-04-11T03:52:12.6007448Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6007554Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6007665Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6007951Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6008091Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6008249Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6008254Z 
2025-04-11T03:52:12.6008493Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6008665Z ____________ test_context_attention[False-True-True-4-16-16-16-32] _____________
2025-04-11T03:52:12.6008673Z 
2025-04-11T03:52:12.6008830Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6008970Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.6009064Z use_new_kcache_layout = False
2025-04-11T03:52:12.6009071Z 
2025-04-11T03:52:12.6009269Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6009376Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6009498Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6009639Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6009761Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6009875Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6010018Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6010155Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6010314Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6010407Z     def test_context_attention(
2025-04-11T03:52:12.6010483Z         bsz: int,
2025-04-11T03:52:12.6010571Z         block_size: int,
2025-04-11T03:52:12.6010664Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6010755Z         num_attn_heads: int,
2025-04-11T03:52:12.6010852Z         kv_group_num: int,
2025-04-11T03:52:12.6010940Z         same_context_len: bool,
2025-04-11T03:52:12.6011034Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6011123Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6011204Z     ):
2025-04-11T03:52:12.6011321Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6011515Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6011705Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6011879Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6012048Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6012123Z             return
2025-04-11T03:52:12.6012194Z     
2025-04-11T03:52:12.6012405Z         torch.manual_seed(123)
2025-04-11T03:52:12.6012505Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6012605Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6012694Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6012698Z 
2025-04-11T03:52:12.6012875Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6012989Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6012993Z 
2025-04-11T03:52:12.6013072Z device = None
2025-04-11T03:52:12.6013082Z 
2025-04-11T03:52:12.6013201Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6013449Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6013531Z     
2025-04-11T03:52:12.6013605Z         Args:
2025-04-11T03:52:12.6013777Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6013942Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6014055Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6014137Z         """
2025-04-11T03:52:12.6014216Z         _lazy_init()
2025-04-11T03:52:12.6014319Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6014422Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6014535Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6014822Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6014956Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6015121Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6015126Z 
2025-04-11T03:52:12.6015441Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6015624Z _____________ test_context_attention[False-True-True-4-16-16-32-7] _____________
2025-04-11T03:52:12.6015628Z 
2025-04-11T03:52:12.6015780Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6015926Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.6016017Z use_new_kcache_layout = False
2025-04-11T03:52:12.6016022Z 
2025-04-11T03:52:12.6016225Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6016331Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6016449Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6016598Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6016717Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6016834Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6016975Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6017121Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6017273Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6017367Z     def test_context_attention(
2025-04-11T03:52:12.6017451Z         bsz: int,
2025-04-11T03:52:12.6017535Z         block_size: int,
2025-04-11T03:52:12.6017633Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6017721Z         num_attn_heads: int,
2025-04-11T03:52:12.6017808Z         kv_group_num: int,
2025-04-11T03:52:12.6017901Z         same_context_len: bool,
2025-04-11T03:52:12.6017989Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6018089Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6018163Z     ):
2025-04-11T03:52:12.6018284Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6018478Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6018661Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6018954Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6019118Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6019200Z             return
2025-04-11T03:52:12.6019273Z     
2025-04-11T03:52:12.6019361Z         torch.manual_seed(123)
2025-04-11T03:52:12.6019468Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6019561Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6019660Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6019664Z 
2025-04-11T03:52:12.6019930Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6020050Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6020055Z 
2025-04-11T03:52:12.6020134Z device = None
2025-04-11T03:52:12.6020139Z 
2025-04-11T03:52:12.6020268Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6020420Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6020494Z     
2025-04-11T03:52:12.6020577Z         Args:
2025-04-11T03:52:12.6020746Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6020914Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6021021Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6021104Z         """
2025-04-11T03:52:12.6021185Z         _lazy_init()
2025-04-11T03:52:12.6021284Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6021397Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6021505Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6021796Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6021939Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6022099Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6022110Z 
2025-04-11T03:52:12.6022349Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6022522Z ____________ test_context_attention[False-True-True-4-16-16-32-32] _____________
2025-04-11T03:52:12.6022526Z 
2025-04-11T03:52:12.6022686Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6022830Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
2025-04-11T03:52:12.6022931Z use_new_kcache_layout = False
2025-04-11T03:52:12.6022936Z 
2025-04-11T03:52:12.6023138Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6023249Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6023372Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6023513Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6023636Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6023750Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6023896Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6024034Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6024191Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6024284Z     def test_context_attention(
2025-04-11T03:52:12.6024368Z         bsz: int,
2025-04-11T03:52:12.6024463Z         block_size: int,
2025-04-11T03:52:12.6024554Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6024645Z         num_attn_heads: int,
2025-04-11T03:52:12.6024730Z         kv_group_num: int,
2025-04-11T03:52:12.6024816Z         same_context_len: bool,
2025-04-11T03:52:12.6025012Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6025102Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6025183Z     ):
2025-04-11T03:52:12.6025299Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6025498Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6025681Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6025855Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6026024Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6026207Z             return
2025-04-11T03:52:12.6026289Z     
2025-04-11T03:52:12.6026378Z         torch.manual_seed(123)
2025-04-11T03:52:12.6026484Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6026577Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6026668Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6026676Z 
2025-04-11T03:52:12.6026850Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6026962Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6026967Z 
2025-04-11T03:52:12.6027052Z device = None
2025-04-11T03:52:12.6027057Z 
2025-04-11T03:52:12.6027174Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6027332Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6027404Z     
2025-04-11T03:52:12.6027477Z         Args:
2025-04-11T03:52:12.6027649Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6027818Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6027929Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6028004Z         """
2025-04-11T03:52:12.6028094Z         _lazy_init()
2025-04-11T03:52:12.6028192Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6028293Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6028463Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6028753Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6028899Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6029058Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6029063Z 
2025-04-11T03:52:12.6029308Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6029483Z _____________ test_context_attention[False-True-False-1-16-8-16-7] _____________
2025-04-11T03:52:12.6029487Z 
2025-04-11T03:52:12.6029639Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6029797Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6029887Z use_new_kcache_layout = False
2025-04-11T03:52:12.6029892Z 
2025-04-11T03:52:12.6030098Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6030204Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6030329Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6030470Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6030588Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6030707Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6030848Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6030992Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6031145Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6031241Z     def test_context_attention(
2025-04-11T03:52:12.6031441Z         bsz: int,
2025-04-11T03:52:12.6031526Z         block_size: int,
2025-04-11T03:52:12.6031627Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6031714Z         num_attn_heads: int,
2025-04-11T03:52:12.6031806Z         kv_group_num: int,
2025-04-11T03:52:12.6031894Z         same_context_len: bool,
2025-04-11T03:52:12.6031979Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6032075Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6032149Z     ):
2025-04-11T03:52:12.6032273Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6032469Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6032776Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6032949Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6033116Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6033204Z             return
2025-04-11T03:52:12.6033279Z     
2025-04-11T03:52:12.6033374Z         torch.manual_seed(123)
2025-04-11T03:52:12.6033478Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6033578Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6033673Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6033677Z 
2025-04-11T03:52:12.6033847Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6033970Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6033978Z 
2025-04-11T03:52:12.6034055Z device = None
2025-04-11T03:52:12.6034059Z 
2025-04-11T03:52:12.6034184Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6034337Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6034416Z     
2025-04-11T03:52:12.6034492Z         Args:
2025-04-11T03:52:12.6034666Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6034840Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6034949Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6035035Z         """
2025-04-11T03:52:12.6035119Z         _lazy_init()
2025-04-11T03:52:12.6035228Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6035332Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6035441Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6035741Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6035881Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6036043Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6036051Z 
2025-04-11T03:52:12.6036290Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6036465Z ____________ test_context_attention[False-True-False-1-16-8-16-32] _____________
2025-04-11T03:52:12.6036469Z 
2025-04-11T03:52:12.6036622Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6036773Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6036873Z use_new_kcache_layout = False
2025-04-11T03:52:12.6036877Z 
2025-04-11T03:52:12.6037077Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6037193Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6037313Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6037459Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6037578Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6037804Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6037942Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6038080Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6038242Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6038332Z     def test_context_attention(
2025-04-11T03:52:12.6038418Z         bsz: int,
2025-04-11T03:52:12.6038501Z         block_size: int,
2025-04-11T03:52:12.6038592Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6038685Z         num_attn_heads: int,
2025-04-11T03:52:12.6038768Z         kv_group_num: int,
2025-04-11T03:52:12.6038951Z         same_context_len: bool,
2025-04-11T03:52:12.6039039Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6039130Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6039211Z     ):
2025-04-11T03:52:12.6039326Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6039527Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6039707Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6039886Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6040048Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6040125Z             return
2025-04-11T03:52:12.6040205Z     
2025-04-11T03:52:12.6040294Z         torch.manual_seed(123)
2025-04-11T03:52:12.6040400Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6040495Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6040592Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6040596Z 
2025-04-11T03:52:12.6040762Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6040875Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6040889Z 
2025-04-11T03:52:12.6040966Z device = None
2025-04-11T03:52:12.6040970Z 
2025-04-11T03:52:12.6041088Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6041245Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6041320Z     
2025-04-11T03:52:12.6041400Z         Args:
2025-04-11T03:52:12.6041567Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6041730Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6041842Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6041920Z         """
2025-04-11T03:52:12.6042007Z         _lazy_init()
2025-04-11T03:52:12.6042105Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6042213Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6042319Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6042606Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6042750Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6042907Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6042912Z 
2025-04-11T03:52:12.6043157Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6043327Z _____________ test_context_attention[False-True-False-1-16-8-32-7] _____________
2025-04-11T03:52:12.6043331Z 
2025-04-11T03:52:12.6043490Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6043640Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6043737Z use_new_kcache_layout = False
2025-04-11T03:52:12.6043741Z 
2025-04-11T03:52:12.6043939Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6044157Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6044288Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6044429Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6044555Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6044674Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6044820Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6044956Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6045206Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6045305Z     def test_context_attention(
2025-04-11T03:52:12.6045384Z         bsz: int,
2025-04-11T03:52:12.6045473Z         block_size: int,
2025-04-11T03:52:12.6045564Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6045651Z         num_attn_heads: int,
2025-04-11T03:52:12.6045743Z         kv_group_num: int,
2025-04-11T03:52:12.6045829Z         same_context_len: bool,
2025-04-11T03:52:12.6045920Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6046012Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6046094Z     ):
2025-04-11T03:52:12.6046208Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6046404Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6046591Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6046761Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6046932Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6047009Z             return
2025-04-11T03:52:12.6047088Z     
2025-04-11T03:52:12.6047175Z         torch.manual_seed(123)
2025-04-11T03:52:12.6047282Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6047380Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6047473Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6047477Z 
2025-04-11T03:52:12.6047649Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6047763Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6047768Z 
2025-04-11T03:52:12.6047852Z device = None
2025-04-11T03:52:12.6047857Z 
2025-04-11T03:52:12.6047972Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6048122Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6048206Z     
2025-04-11T03:52:12.6048282Z         Args:
2025-04-11T03:52:12.6048455Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6048618Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6048728Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6048811Z         """
2025-04-11T03:52:12.6048890Z         _lazy_init()
2025-04-11T03:52:12.6048999Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6049103Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6049214Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6049495Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6049630Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6049799Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6049804Z 
2025-04-11T03:52:12.6050039Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6050216Z ____________ test_context_attention[False-True-False-1-16-8-32-32] _____________
2025-04-11T03:52:12.6050324Z 
2025-04-11T03:52:12.6050477Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6050632Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6050724Z use_new_kcache_layout = False
2025-04-11T03:52:12.6050729Z 
2025-04-11T03:52:12.6050933Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6051036Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6051153Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6051296Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6051528Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6051651Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6051791Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6051932Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6052088Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6052179Z     def test_context_attention(
2025-04-11T03:52:12.6052264Z         bsz: int,
2025-04-11T03:52:12.6052348Z         block_size: int,
2025-04-11T03:52:12.6052445Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6052531Z         num_attn_heads: int,
2025-04-11T03:52:12.6052615Z         kv_group_num: int,
2025-04-11T03:52:12.6052709Z         same_context_len: bool,
2025-04-11T03:52:12.6052796Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6052891Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6052969Z     ):
2025-04-11T03:52:12.6053087Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6053281Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6053462Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6053643Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6053805Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6053891Z             return
2025-04-11T03:52:12.6053964Z     
2025-04-11T03:52:12.6054050Z         torch.manual_seed(123)
2025-04-11T03:52:12.6054155Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6054249Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6054348Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6054353Z 
2025-04-11T03:52:12.6054519Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6054643Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6054647Z 
2025-04-11T03:52:12.6054726Z device = None
2025-04-11T03:52:12.6054730Z 
2025-04-11T03:52:12.6054853Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6055006Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6055079Z     
2025-04-11T03:52:12.6055161Z         Args:
2025-04-11T03:52:12.6055326Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6055496Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6055603Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6055683Z         """
2025-04-11T03:52:12.6055765Z         _lazy_init()
2025-04-11T03:52:12.6055862Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6055968Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6056078Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6056365Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6056501Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6056777Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6056787Z 
2025-04-11T03:52:12.6057025Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6057194Z ____________ test_context_attention[False-True-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.6057199Z 
2025-04-11T03:52:12.6057354Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6057500Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6057598Z use_new_kcache_layout = False
2025-04-11T03:52:12.6057691Z 
2025-04-11T03:52:12.6057898Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6058010Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6058129Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6058272Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6058396Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6058511Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6058653Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6058788Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6058944Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6059034Z     def test_context_attention(
2025-04-11T03:52:12.6059113Z         bsz: int,
2025-04-11T03:52:12.6059201Z         block_size: int,
2025-04-11T03:52:12.6059296Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6059384Z         num_attn_heads: int,
2025-04-11T03:52:12.6059467Z         kv_group_num: int,
2025-04-11T03:52:12.6059554Z         same_context_len: bool,
2025-04-11T03:52:12.6059646Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6059736Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6059815Z     ):
2025-04-11T03:52:12.6059925Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6060123Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6060300Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6060471Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6060642Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6060722Z             return
2025-04-11T03:52:12.6060806Z     
2025-04-11T03:52:12.6060893Z         torch.manual_seed(123)
2025-04-11T03:52:12.6061001Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6061092Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6061183Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6061187Z 
2025-04-11T03:52:12.6061363Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6061474Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6061478Z 
2025-04-11T03:52:12.6061564Z device = None
2025-04-11T03:52:12.6061568Z 
2025-04-11T03:52:12.6061684Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6061837Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6061911Z     
2025-04-11T03:52:12.6061986Z         Args:
2025-04-11T03:52:12.6062157Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6062324Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6062437Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6062510Z         """
2025-04-11T03:52:12.6062592Z         _lazy_init()
2025-04-11T03:52:12.6062690Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6062903Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6063013Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6063297Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6063437Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6063596Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6063601Z 
2025-04-11T03:52:12.6063845Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6064106Z ____________ test_context_attention[False-True-False-1-16-16-16-32] ____________
2025-04-11T03:52:12.6064111Z 
2025-04-11T03:52:12.6064262Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6064414Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6064506Z use_new_kcache_layout = False
2025-04-11T03:52:12.6064510Z 
2025-04-11T03:52:12.6064716Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6064819Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6064938Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6065076Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6065193Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6065313Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6065450Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6065595Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6065745Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6065839Z     def test_context_attention(
2025-04-11T03:52:12.6065914Z         bsz: int,
2025-04-11T03:52:12.6066000Z         block_size: int,
2025-04-11T03:52:12.6066097Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6066182Z         num_attn_heads: int,
2025-04-11T03:52:12.6066268Z         kv_group_num: int,
2025-04-11T03:52:12.6066356Z         same_context_len: bool,
2025-04-11T03:52:12.6066439Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6066537Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6066609Z     ):
2025-04-11T03:52:12.6066725Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6066918Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6067108Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6067280Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6067442Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6067527Z             return
2025-04-11T03:52:12.6067598Z     
2025-04-11T03:52:12.6067690Z         torch.manual_seed(123)
2025-04-11T03:52:12.6067790Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6067885Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6067975Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6067979Z 
2025-04-11T03:52:12.6068146Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6068262Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6068266Z 
2025-04-11T03:52:12.6068343Z device = None
2025-04-11T03:52:12.6068350Z 
2025-04-11T03:52:12.6068514Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6068668Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6068743Z     
2025-04-11T03:52:12.6068819Z         Args:
2025-04-11T03:52:12.6068987Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6069284Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6069392Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6069471Z         """
2025-04-11T03:52:12.6069550Z         _lazy_init()
2025-04-11T03:52:12.6069653Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6069754Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6069860Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6070149Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6070483Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6070647Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6070651Z 
2025-04-11T03:52:12.6070893Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6071074Z ____________ test_context_attention[False-True-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.6071079Z 
2025-04-11T03:52:12.6071228Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6071376Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6071474Z use_new_kcache_layout = False
2025-04-11T03:52:12.6071478Z 
2025-04-11T03:52:12.6071681Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6071794Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6071913Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6072058Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6072177Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6072300Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6072444Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6072585Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6072744Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6072835Z     def test_context_attention(
2025-04-11T03:52:12.6072916Z         bsz: int,
2025-04-11T03:52:12.6072997Z         block_size: int,
2025-04-11T03:52:12.6073089Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6073181Z         num_attn_heads: int,
2025-04-11T03:52:12.6073263Z         kv_group_num: int,
2025-04-11T03:52:12.6073350Z         same_context_len: bool,
2025-04-11T03:52:12.6073440Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6073530Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6073605Z     ):
2025-04-11T03:52:12.6073718Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6073918Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6074102Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6074275Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6074437Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6074513Z             return
2025-04-11T03:52:12.6074590Z     
2025-04-11T03:52:12.6074676Z         torch.manual_seed(123)
2025-04-11T03:52:12.6074778Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6074870Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6074971Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6074975Z 
2025-04-11T03:52:12.6075145Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6075257Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6075265Z 
2025-04-11T03:52:12.6075449Z device = None
2025-04-11T03:52:12.6075453Z 
2025-04-11T03:52:12.6075572Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6075729Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6075801Z     
2025-04-11T03:52:12.6075878Z         Args:
2025-04-11T03:52:12.6076047Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6076215Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6076333Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6076407Z         """
2025-04-11T03:52:12.6076604Z         _lazy_init()
2025-04-11T03:52:12.6076700Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6076810Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6076916Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6077204Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6077348Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6077507Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6077511Z 
2025-04-11T03:52:12.6077752Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6077925Z ____________ test_context_attention[False-True-False-1-16-16-32-32] ____________
2025-04-11T03:52:12.6077930Z 
2025-04-11T03:52:12.6078086Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6078235Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6078327Z use_new_kcache_layout = False
2025-04-11T03:52:12.6078331Z 
2025-04-11T03:52:12.6078532Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6078639Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6078759Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6078898Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6079017Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6079129Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6079269Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6079403Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6079556Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6079655Z     def test_context_attention(
2025-04-11T03:52:12.6079732Z         bsz: int,
2025-04-11T03:52:12.6079818Z         block_size: int,
2025-04-11T03:52:12.6079909Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6079994Z         num_attn_heads: int,
2025-04-11T03:52:12.6080082Z         kv_group_num: int,
2025-04-11T03:52:12.6080172Z         same_context_len: bool,
2025-04-11T03:52:12.6080258Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6080347Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6080423Z     ):
2025-04-11T03:52:12.6080536Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6080729Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6080912Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6081081Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6081246Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6081321Z             return
2025-04-11T03:52:12.6081398Z     
2025-04-11T03:52:12.6081482Z         torch.manual_seed(123)
2025-04-11T03:52:12.6081582Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6081781Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6081874Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6081878Z 
2025-04-11T03:52:12.6082059Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6082173Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6082178Z 
2025-04-11T03:52:12.6082257Z device = None
2025-04-11T03:52:12.6082261Z 
2025-04-11T03:52:12.6082381Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6082531Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6082702Z     
2025-04-11T03:52:12.6082777Z         Args:
2025-04-11T03:52:12.6082946Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6083110Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6083218Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6083300Z         """
2025-04-11T03:52:12.6083377Z         _lazy_init()
2025-04-11T03:52:12.6083478Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6083580Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6083693Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6083979Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6084117Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6084282Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6084290Z 
2025-04-11T03:52:12.6084529Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6084704Z _____________ test_context_attention[False-True-False-4-16-8-16-7] _____________
2025-04-11T03:52:12.6084709Z 
2025-04-11T03:52:12.6084862Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6085011Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6085101Z use_new_kcache_layout = False
2025-04-11T03:52:12.6085105Z 
2025-04-11T03:52:12.6085307Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6085413Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6085532Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6085676Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6085792Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6085914Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6086048Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6086190Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6086340Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6086435Z     def test_context_attention(
2025-04-11T03:52:12.6086516Z         bsz: int,
2025-04-11T03:52:12.6086598Z         block_size: int,
2025-04-11T03:52:12.6086691Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6086774Z         num_attn_heads: int,
2025-04-11T03:52:12.6086856Z         kv_group_num: int,
2025-04-11T03:52:12.6086945Z         same_context_len: bool,
2025-04-11T03:52:12.6087027Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6087120Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6087191Z     ):
2025-04-11T03:52:12.6087309Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6087504Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6087684Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6087859Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6088132Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6088213Z             return
2025-04-11T03:52:12.6088287Z     
2025-04-11T03:52:12.6088379Z         torch.manual_seed(123)
2025-04-11T03:52:12.6088481Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6088572Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6088667Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6088672Z 
2025-04-11T03:52:12.6088837Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6088954Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6089048Z 
2025-04-11T03:52:12.6089127Z device = None
2025-04-11T03:52:12.6089132Z 
2025-04-11T03:52:12.6089251Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6089402Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6089477Z     
2025-04-11T03:52:12.6089555Z         Args:
2025-04-11T03:52:12.6089720Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6089888Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6089995Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6090071Z         """
2025-04-11T03:52:12.6090151Z         _lazy_init()
2025-04-11T03:52:12.6090247Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6090352Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6090462Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6090752Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6090888Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6091051Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6091059Z 
2025-04-11T03:52:12.6091300Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6091469Z ____________ test_context_attention[False-True-False-4-16-8-16-32] _____________
2025-04-11T03:52:12.6091473Z 
2025-04-11T03:52:12.6091627Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6091774Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6091869Z use_new_kcache_layout = False
2025-04-11T03:52:12.6091873Z 
2025-04-11T03:52:12.6092074Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6092185Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6092305Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6092445Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6092568Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6092684Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6092830Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6092968Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6093121Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6093213Z     def test_context_attention(
2025-04-11T03:52:12.6093286Z         bsz: int,
2025-04-11T03:52:12.6093375Z         block_size: int,
2025-04-11T03:52:12.6093464Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6093555Z         num_attn_heads: int,
2025-04-11T03:52:12.6093639Z         kv_group_num: int,
2025-04-11T03:52:12.6093725Z         same_context_len: bool,
2025-04-11T03:52:12.6093817Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6093904Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6093982Z     ):
2025-04-11T03:52:12.6094191Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6094389Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6094574Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6094744Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6094908Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6094983Z             return
2025-04-11T03:52:12.6095061Z     
2025-04-11T03:52:12.6095146Z         torch.manual_seed(123)
2025-04-11T03:52:12.6095348Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6095438Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6095527Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6095531Z 
2025-04-11T03:52:12.6095703Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6095817Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6095821Z 
2025-04-11T03:52:12.6095903Z device = None
2025-04-11T03:52:12.6095907Z 
2025-04-11T03:52:12.6096022Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6096180Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6096251Z     
2025-04-11T03:52:12.6096326Z         Args:
2025-04-11T03:52:12.6096502Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6096666Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6096781Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6096853Z         """
2025-04-11T03:52:12.6096938Z         _lazy_init()
2025-04-11T03:52:12.6097033Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6097138Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6097252Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6097536Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6097676Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6097835Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6097840Z 
2025-04-11T03:52:12.6098079Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6098247Z _____________ test_context_attention[False-True-False-4-16-8-32-7] _____________
2025-04-11T03:52:12.6098255Z 
2025-04-11T03:52:12.6098403Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6098555Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6098644Z use_new_kcache_layout = False
2025-04-11T03:52:12.6098651Z 
2025-04-11T03:52:12.6098858Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6098965Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6099086Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6099227Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6099343Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6099462Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6099598Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6099745Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6099896Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6099994Z     def test_context_attention(
2025-04-11T03:52:12.6100069Z         bsz: int,
2025-04-11T03:52:12.6100150Z         block_size: int,
2025-04-11T03:52:12.6100358Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6100442Z         num_attn_heads: int,
2025-04-11T03:52:12.6100531Z         kv_group_num: int,
2025-04-11T03:52:12.6100617Z         same_context_len: bool,
2025-04-11T03:52:12.6100702Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6100793Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6100864Z     ):
2025-04-11T03:52:12.6100979Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6101168Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6101360Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6101643Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6101806Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6101887Z             return
2025-04-11T03:52:12.6101963Z     
2025-04-11T03:52:12.6102053Z         torch.manual_seed(123)
2025-04-11T03:52:12.6102151Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6102245Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6102335Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6102340Z 
2025-04-11T03:52:12.6102507Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6102626Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6102630Z 
2025-04-11T03:52:12.6102707Z device = None
2025-04-11T03:52:12.6102711Z 
2025-04-11T03:52:12.6102833Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6102987Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6103068Z     
2025-04-11T03:52:12.6103141Z         Args:
2025-04-11T03:52:12.6103308Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6103478Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6103586Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6103669Z         """
2025-04-11T03:52:12.6103748Z         _lazy_init()
2025-04-11T03:52:12.6103849Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6103950Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6104057Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6104345Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6104483Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6104649Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6104653Z 
2025-04-11T03:52:12.6104890Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6105067Z ____________ test_context_attention[False-True-False-4-16-8-32-32] _____________
2025-04-11T03:52:12.6105071Z 
2025-04-11T03:52:12.6105222Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6105372Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6105461Z use_new_kcache_layout = False
2025-04-11T03:52:12.6105466Z 
2025-04-11T03:52:12.6105664Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6105773Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6105890Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6106033Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6106147Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6106266Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6106399Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6106634Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6106790Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6106880Z     def test_context_attention(
2025-04-11T03:52:12.6106961Z         bsz: int,
2025-04-11T03:52:12.6107043Z         block_size: int,
2025-04-11T03:52:12.6107132Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6107220Z         num_attn_heads: int,
2025-04-11T03:52:12.6107304Z         kv_group_num: int,
2025-04-11T03:52:12.6107393Z         same_context_len: bool,
2025-04-11T03:52:12.6107477Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6107659Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6107734Z     ):
2025-04-11T03:52:12.6107845Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6108046Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6108232Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6108405Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6108606Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6108687Z             return
2025-04-11T03:52:12.6108757Z     
2025-04-11T03:52:12.6108841Z         torch.manual_seed(123)
2025-04-11T03:52:12.6108947Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6109037Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6109130Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6109137Z 
2025-04-11T03:52:12.6109301Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6109415Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6109422Z 
2025-04-11T03:52:12.6109499Z device = None
2025-04-11T03:52:12.6109504Z 
2025-04-11T03:52:12.6109622Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6109776Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6109851Z     
2025-04-11T03:52:12.6109929Z         Args:
2025-04-11T03:52:12.6110095Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6110262Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6110374Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6110447Z         """
2025-04-11T03:52:12.6110534Z         _lazy_init()
2025-04-11T03:52:12.6110635Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6110740Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6110848Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6111131Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6111274Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6111432Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6111436Z 
2025-04-11T03:52:12.6111675Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6111844Z ____________ test_context_attention[False-True-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.6111849Z 
2025-04-11T03:52:12.6112002Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6112149Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6112246Z use_new_kcache_layout = False
2025-04-11T03:52:12.6112250Z 
2025-04-11T03:52:12.6112448Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6112550Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6112834Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6112970Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6113097Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6113211Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6113354Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6113489Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6113641Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6113735Z     def test_context_attention(
2025-04-11T03:52:12.6113914Z         bsz: int,
2025-04-11T03:52:12.6114001Z         block_size: int,
2025-04-11T03:52:12.6114092Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6114176Z         num_attn_heads: int,
2025-04-11T03:52:12.6114262Z         kv_group_num: int,
2025-04-11T03:52:12.6114348Z         same_context_len: bool,
2025-04-11T03:52:12.6114444Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6114531Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6114607Z     ):
2025-04-11T03:52:12.6114719Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6114910Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6115094Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6115260Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6115429Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6115507Z             return
2025-04-11T03:52:12.6115581Z     
2025-04-11T03:52:12.6115667Z         torch.manual_seed(123)
2025-04-11T03:52:12.6115768Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6115872Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6116002Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6116008Z 
2025-04-11T03:52:12.6116183Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6116293Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6116298Z 
2025-04-11T03:52:12.6116376Z device = None
2025-04-11T03:52:12.6116381Z 
2025-04-11T03:52:12.6116501Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6116650Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6116728Z     
2025-04-11T03:52:12.6116802Z         Args:
2025-04-11T03:52:12.6116980Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6117146Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6117256Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6117329Z         """
2025-04-11T03:52:12.6117413Z         _lazy_init()
2025-04-11T03:52:12.6117516Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6117619Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6117729Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6118011Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6118147Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6118311Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6118315Z 
2025-04-11T03:52:12.6118556Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6118731Z ____________ test_context_attention[False-True-False-4-16-16-16-32] ____________
2025-04-11T03:52:12.6118735Z 
2025-04-11T03:52:12.6118885Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6119157Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6119247Z use_new_kcache_layout = False
2025-04-11T03:52:12.6119252Z 
2025-04-11T03:52:12.6119450Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6119555Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6119676Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6119820Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6119937Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6120057Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6120290Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6120429Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6120580Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6120672Z     def test_context_attention(
2025-04-11T03:52:12.6120754Z         bsz: int,
2025-04-11T03:52:12.6120838Z         block_size: int,
2025-04-11T03:52:12.6120934Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6121021Z         num_attn_heads: int,
2025-04-11T03:52:12.6121103Z         kv_group_num: int,
2025-04-11T03:52:12.6121196Z         same_context_len: bool,
2025-04-11T03:52:12.6121281Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6121374Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6121447Z     ):
2025-04-11T03:52:12.6121567Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6121759Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6121942Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6122118Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6122283Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6122364Z             return
2025-04-11T03:52:12.6122438Z     
2025-04-11T03:52:12.6122528Z         torch.manual_seed(123)
2025-04-11T03:52:12.6122627Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6122716Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6122811Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6122815Z 
2025-04-11T03:52:12.6122976Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6123092Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6123099Z 
2025-04-11T03:52:12.6123176Z device = None
2025-04-11T03:52:12.6123180Z 
2025-04-11T03:52:12.6123301Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6123452Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6123523Z     
2025-04-11T03:52:12.6123604Z         Args:
2025-04-11T03:52:12.6123770Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6123936Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6124042Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6124119Z         """
2025-04-11T03:52:12.6124198Z         _lazy_init()
2025-04-11T03:52:12.6124293Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6124399Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6124503Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6124794Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6124932Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6125089Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6125224Z 
2025-04-11T03:52:12.6125466Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6125633Z ____________ test_context_attention[False-True-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.6125638Z 
2025-04-11T03:52:12.6125795Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6125938Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6126037Z use_new_kcache_layout = False
2025-04-11T03:52:12.6126042Z 
2025-04-11T03:52:12.6126241Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6126445Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6126567Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6126705Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6126826Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6126944Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6127085Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6127222Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6127375Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6127463Z     def test_context_attention(
2025-04-11T03:52:12.6127538Z         bsz: int,
2025-04-11T03:52:12.6127625Z         block_size: int,
2025-04-11T03:52:12.6127714Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6127801Z         num_attn_heads: int,
2025-04-11T03:52:12.6127886Z         kv_group_num: int,
2025-04-11T03:52:12.6127971Z         same_context_len: bool,
2025-04-11T03:52:12.6128059Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6128148Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6128225Z     ):
2025-04-11T03:52:12.6128335Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6128531Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6128712Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6128882Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6129049Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6129124Z             return
2025-04-11T03:52:12.6129201Z     
2025-04-11T03:52:12.6129285Z         torch.manual_seed(123)
2025-04-11T03:52:12.6129387Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6129481Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6129572Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6129576Z 
2025-04-11T03:52:12.6129745Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6129858Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6129865Z 
2025-04-11T03:52:12.6129946Z device = None
2025-04-11T03:52:12.6129950Z 
2025-04-11T03:52:12.6130066Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6130216Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6130288Z     
2025-04-11T03:52:12.6130361Z         Args:
2025-04-11T03:52:12.6130532Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6130694Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6130807Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6130886Z         """
2025-04-11T03:52:12.6130970Z         _lazy_init()
2025-04-11T03:52:12.6131065Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6131173Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6131284Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6131679Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6131818Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6131975Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6131979Z 
2025-04-11T03:52:12.6132217Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6132383Z ____________ test_context_attention[False-True-False-4-16-16-32-32] ____________
2025-04-11T03:52:12.6132477Z 
2025-04-11T03:52:12.6132630Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6132778Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
2025-04-11T03:52:12.6132867Z use_new_kcache_layout = False
2025-04-11T03:52:12.6132871Z 
2025-04-11T03:52:12.6133078Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6133182Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6133301Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6133441Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6133561Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6133674Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6133812Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6133952Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6134105Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6134198Z     def test_context_attention(
2025-04-11T03:52:12.6134274Z         bsz: int,
2025-04-11T03:52:12.6134356Z         block_size: int,
2025-04-11T03:52:12.6134451Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6134539Z         num_attn_heads: int,
2025-04-11T03:52:12.6134625Z         kv_group_num: int,
2025-04-11T03:52:12.6134710Z         same_context_len: bool,
2025-04-11T03:52:12.6134798Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6134889Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6134963Z     ):
2025-04-11T03:52:12.6135079Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6135271Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6135454Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6135627Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6135790Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6135869Z             return
2025-04-11T03:52:12.6135944Z     
2025-04-11T03:52:12.6136034Z         torch.manual_seed(123)
2025-04-11T03:52:12.6136139Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6136232Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6136322Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6136326Z 
2025-04-11T03:52:12.6136492Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6136610Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6136614Z 
2025-04-11T03:52:12.6136690Z device = None
2025-04-11T03:52:12.6136694Z 
2025-04-11T03:52:12.6136817Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6136965Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6137041Z     
2025-04-11T03:52:12.6137113Z         Args:
2025-04-11T03:52:12.6137280Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6137448Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6137666Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6137743Z         """
2025-04-11T03:52:12.6137823Z         _lazy_init()
2025-04-11T03:52:12.6137929Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6138034Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6138143Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6138442Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6138577Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6138828Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6138833Z 
2025-04-11T03:52:12.6139071Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6139243Z _____________ test_context_attention[False-False-True-1-16-8-16-7] _____________
2025-04-11T03:52:12.6139250Z 
2025-04-11T03:52:12.6139400Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6139548Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6139636Z use_new_kcache_layout = False
2025-04-11T03:52:12.6139641Z 
2025-04-11T03:52:12.6139840Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6139955Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6140077Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6140221Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6140342Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6140460Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6140595Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6140732Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6140889Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6140979Z     def test_context_attention(
2025-04-11T03:52:12.6141057Z         bsz: int,
2025-04-11T03:52:12.6141140Z         block_size: int,
2025-04-11T03:52:12.6141232Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6141322Z         num_attn_heads: int,
2025-04-11T03:52:12.6141406Z         kv_group_num: int,
2025-04-11T03:52:12.6141497Z         same_context_len: bool,
2025-04-11T03:52:12.6141583Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6141677Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6141755Z     ):
2025-04-11T03:52:12.6141867Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6142061Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6142242Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6142416Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6142578Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6142661Z             return
2025-04-11T03:52:12.6142733Z     
2025-04-11T03:52:12.6142821Z         torch.manual_seed(123)
2025-04-11T03:52:12.6142933Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6143023Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6143123Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6143127Z 
2025-04-11T03:52:12.6143293Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6143408Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6143415Z 
2025-04-11T03:52:12.6143493Z device = None
2025-04-11T03:52:12.6143497Z 
2025-04-11T03:52:12.6143613Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6143873Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6143946Z     
2025-04-11T03:52:12.6144023Z         Args:
2025-04-11T03:52:12.6144191Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6144354Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6144466Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6144540Z         """
2025-04-11T03:52:12.6144624Z         _lazy_init()
2025-04-11T03:52:12.6144720Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6144922Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6145030Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6145315Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6145461Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6145621Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6145626Z 
2025-04-11T03:52:12.6145870Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6146040Z ____________ test_context_attention[False-False-True-1-16-8-16-32] _____________
2025-04-11T03:52:12.6146044Z 
2025-04-11T03:52:12.6146198Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6146342Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6146437Z use_new_kcache_layout = False
2025-04-11T03:52:12.6146441Z 
2025-04-11T03:52:12.6146637Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6146739Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6146861Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6147004Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6147124Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6147239Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6147381Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6147517Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6147667Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6147763Z     def test_context_attention(
2025-04-11T03:52:12.6147838Z         bsz: int,
2025-04-11T03:52:12.6147929Z         block_size: int,
2025-04-11T03:52:12.6148019Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6148105Z         num_attn_heads: int,
2025-04-11T03:52:12.6148194Z         kv_group_num: int,
2025-04-11T03:52:12.6148280Z         same_context_len: bool,
2025-04-11T03:52:12.6148372Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6148519Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6148603Z     ):
2025-04-11T03:52:12.6148717Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6148913Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6149101Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6149272Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6149439Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6149517Z             return
2025-04-11T03:52:12.6149594Z     
2025-04-11T03:52:12.6149684Z         torch.manual_seed(123)
2025-04-11T03:52:12.6149785Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6149881Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6149975Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6150109Z 
2025-04-11T03:52:12.6150281Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6150394Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6150398Z 
2025-04-11T03:52:12.6150480Z device = None
2025-04-11T03:52:12.6150485Z 
2025-04-11T03:52:12.6150603Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6150751Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6150828Z     
2025-04-11T03:52:12.6150903Z         Args:
2025-04-11T03:52:12.6151073Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6151344Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6151457Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6151530Z         """
2025-04-11T03:52:12.6151608Z         _lazy_init()
2025-04-11T03:52:12.6151713Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6151814Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6151923Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6152206Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6152341Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6152504Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6152509Z 
2025-04-11T03:52:12.6152746Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6152920Z _____________ test_context_attention[False-False-True-1-16-8-32-7] _____________
2025-04-11T03:52:12.6152924Z 
2025-04-11T03:52:12.6153071Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6153220Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6153315Z use_new_kcache_layout = False
2025-04-11T03:52:12.6153319Z 
2025-04-11T03:52:12.6153524Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6153632Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6153750Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6153892Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6154011Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6154133Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6154272Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6154411Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6154563Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6154655Z     def test_context_attention(
2025-04-11T03:52:12.6154739Z         bsz: int,
2025-04-11T03:52:12.6154822Z         block_size: int,
2025-04-11T03:52:12.6154913Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6154998Z         num_attn_heads: int,
2025-04-11T03:52:12.6155081Z         kv_group_num: int,
2025-04-11T03:52:12.6155169Z         same_context_len: bool,
2025-04-11T03:52:12.6155251Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6155344Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6155416Z     ):
2025-04-11T03:52:12.6155527Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6155716Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6155902Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6156080Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6156242Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6156432Z             return
2025-04-11T03:52:12.6156505Z     
2025-04-11T03:52:12.6156593Z         torch.manual_seed(123)
2025-04-11T03:52:12.6156691Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6156780Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6156876Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6156881Z 
2025-04-11T03:52:12.6157046Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6157160Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6157165Z 
2025-04-11T03:52:12.6157241Z device = None
2025-04-11T03:52:12.6157335Z 
2025-04-11T03:52:12.6157457Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6157609Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6157679Z     
2025-04-11T03:52:12.6157756Z         Args:
2025-04-11T03:52:12.6157925Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6158095Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6158200Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6158277Z         """
2025-04-11T03:52:12.6158356Z         _lazy_init()
2025-04-11T03:52:12.6158455Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6158560Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6158666Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6158954Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6159095Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6159254Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6159263Z 
2025-04-11T03:52:12.6159507Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6159677Z ____________ test_context_attention[False-False-True-1-16-8-32-32] _____________
2025-04-11T03:52:12.6159682Z 
2025-04-11T03:52:12.6159837Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6159982Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6160074Z use_new_kcache_layout = False
2025-04-11T03:52:12.6160079Z 
2025-04-11T03:52:12.6160278Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6160387Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6160509Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6160648Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6160770Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6160884Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6161029Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6161164Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6161321Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6161414Z     def test_context_attention(
2025-04-11T03:52:12.6161489Z         bsz: int,
2025-04-11T03:52:12.6161575Z         block_size: int,
2025-04-11T03:52:12.6161665Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6161752Z         num_attn_heads: int,
2025-04-11T03:52:12.6161836Z         kv_group_num: int,
2025-04-11T03:52:12.6161924Z         same_context_len: bool,
2025-04-11T03:52:12.6162014Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6162102Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6162179Z     ):
2025-04-11T03:52:12.6162290Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6162486Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6162778Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6162953Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6163125Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6163202Z             return
2025-04-11T03:52:12.6163278Z     
2025-04-11T03:52:12.6163367Z         torch.manual_seed(123)
2025-04-11T03:52:12.6163468Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6163560Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6163748Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6163753Z 
2025-04-11T03:52:12.6163921Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6164031Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6164039Z 
2025-04-11T03:52:12.6164119Z device = None
2025-04-11T03:52:12.6164123Z 
2025-04-11T03:52:12.6164240Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6164394Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6164465Z     
2025-04-11T03:52:12.6164538Z         Args:
2025-04-11T03:52:12.6164711Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6164873Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6164982Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6165057Z         """
2025-04-11T03:52:12.6165138Z         _lazy_init()
2025-04-11T03:52:12.6165233Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6165336Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6165444Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6165727Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6165864Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6166020Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6166025Z 
2025-04-11T03:52:12.6166262Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6166429Z ____________ test_context_attention[False-False-True-1-16-16-16-7] _____________
2025-04-11T03:52:12.6166433Z 
2025-04-11T03:52:12.6166579Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6166733Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6166822Z use_new_kcache_layout = False
2025-04-11T03:52:12.6166827Z 
2025-04-11T03:52:12.6167031Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6167138Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6167257Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6167396Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6167517Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6167629Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6167768Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6167909Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6168059Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6168153Z     def test_context_attention(
2025-04-11T03:52:12.6168228Z         bsz: int,
2025-04-11T03:52:12.6168310Z         block_size: int,
2025-04-11T03:52:12.6168404Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6168488Z         num_attn_heads: int,
2025-04-11T03:52:12.6168685Z         kv_group_num: int,
2025-04-11T03:52:12.6168773Z         same_context_len: bool,
2025-04-11T03:52:12.6168860Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6168949Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6169022Z     ):
2025-04-11T03:52:12.6169139Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6169332Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6169518Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6169688Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6170065Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6170149Z             return
2025-04-11T03:52:12.6170220Z     
2025-04-11T03:52:12.6170317Z         torch.manual_seed(123)
2025-04-11T03:52:12.6170416Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6170514Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6170611Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6170615Z 
2025-04-11T03:52:12.6170781Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6170906Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6170910Z 
2025-04-11T03:52:12.6170987Z device = None
2025-04-11T03:52:12.6170992Z 
2025-04-11T03:52:12.6171116Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6171270Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6171355Z     
2025-04-11T03:52:12.6171434Z         Args:
2025-04-11T03:52:12.6171600Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6171767Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6171874Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6171954Z         """
2025-04-11T03:52:12.6172031Z         _lazy_init()
2025-04-11T03:52:12.6172134Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6172236Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6172342Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6172629Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6172763Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6172924Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6172932Z 
2025-04-11T03:52:12.6173169Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6173342Z ____________ test_context_attention[False-False-True-1-16-16-16-32] ____________
2025-04-11T03:52:12.6173349Z 
2025-04-11T03:52:12.6173500Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6173646Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6173734Z use_new_kcache_layout = False
2025-04-11T03:52:12.6173738Z 
2025-04-11T03:52:12.6173935Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6174044Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6174160Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6174300Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6174418Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6174536Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6174670Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6174807Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6175093Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6175181Z     def test_context_attention(
2025-04-11T03:52:12.6175261Z         bsz: int,
2025-04-11T03:52:12.6175342Z         block_size: int,
2025-04-11T03:52:12.6175432Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6175522Z         num_attn_heads: int,
2025-04-11T03:52:12.6175604Z         kv_group_num: int,
2025-04-11T03:52:12.6175692Z         same_context_len: bool,
2025-04-11T03:52:12.6175778Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6175875Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6175947Z     ):
2025-04-11T03:52:12.6176152Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6176350Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6176534Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6176710Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6176879Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6176963Z             return
2025-04-11T03:52:12.6177034Z     
2025-04-11T03:52:12.6177124Z         torch.manual_seed(123)
2025-04-11T03:52:12.6177231Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6177322Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6177416Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6177421Z 
2025-04-11T03:52:12.6177585Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6177696Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6177706Z 
2025-04-11T03:52:12.6177784Z device = None
2025-04-11T03:52:12.6177788Z 
2025-04-11T03:52:12.6177906Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6178059Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6178134Z     
2025-04-11T03:52:12.6178212Z         Args:
2025-04-11T03:52:12.6178379Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6178544Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6178656Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6178729Z         """
2025-04-11T03:52:12.6178814Z         _lazy_init()
2025-04-11T03:52:12.6178910Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6179013Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6179123Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6179405Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6179543Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6179704Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6179708Z 
2025-04-11T03:52:12.6179951Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6180116Z ____________ test_context_attention[False-False-True-1-16-16-32-7] _____________
2025-04-11T03:52:12.6180120Z 
2025-04-11T03:52:12.6180274Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6180421Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6180515Z use_new_kcache_layout = False
2025-04-11T03:52:12.6180522Z 
2025-04-11T03:52:12.6180722Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6180829Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6180948Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6181087Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6181309Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6181421Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6181561Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6181695Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6181844Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6181940Z     def test_context_attention(
2025-04-11T03:52:12.6182015Z         bsz: int,
2025-04-11T03:52:12.6182098Z         block_size: int,
2025-04-11T03:52:12.6182189Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6182364Z         num_attn_heads: int,
2025-04-11T03:52:12.6182452Z         kv_group_num: int,
2025-04-11T03:52:12.6182540Z         same_context_len: bool,
2025-04-11T03:52:12.6182631Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6182719Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6182801Z     ):
2025-04-11T03:52:12.6182913Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6183102Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6183287Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6183455Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6183619Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6183696Z             return
2025-04-11T03:52:12.6183769Z     
2025-04-11T03:52:12.6183858Z         torch.manual_seed(123)
2025-04-11T03:52:12.6183955Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6184050Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6184138Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6184142Z 
2025-04-11T03:52:12.6184308Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6184424Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6184428Z 
2025-04-11T03:52:12.6184509Z device = None
2025-04-11T03:52:12.6184514Z 
2025-04-11T03:52:12.6184627Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6184775Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6184850Z     
2025-04-11T03:52:12.6184922Z         Args:
2025-04-11T03:52:12.6185089Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6185249Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6185360Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6185434Z         """
2025-04-11T03:52:12.6185513Z         _lazy_init()
2025-04-11T03:52:12.6185611Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6185715Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6185823Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6186101Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6186236Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6186396Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6186400Z 
2025-04-11T03:52:12.6186637Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6186816Z ____________ test_context_attention[False-False-True-1-16-16-32-32] ____________
2025-04-11T03:52:12.6186820Z 
2025-04-11T03:52:12.6186970Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6187121Z kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6187318Z use_new_kcache_layout = False
2025-04-11T03:52:12.6187322Z 
2025-04-11T03:52:12.6187521Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6187624Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6187740Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6187884Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6188000Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6188117Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6188253Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6188531Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6188685Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6188775Z     def test_context_attention(
2025-04-11T03:52:12.6188858Z         bsz: int,
2025-04-11T03:52:12.6188938Z         block_size: int,
2025-04-11T03:52:12.6189038Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6189123Z         num_attn_heads: int,
2025-04-11T03:52:12.6189208Z         kv_group_num: int,
2025-04-11T03:52:12.6189299Z         same_context_len: bool,
2025-04-11T03:52:12.6189383Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6189476Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6189550Z     ):
2025-04-11T03:52:12.6189664Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6189857Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6190040Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6190220Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6190383Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6190469Z             return
2025-04-11T03:52:12.6190540Z     
2025-04-11T03:52:12.6190629Z         torch.manual_seed(123)
2025-04-11T03:52:12.6190728Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6190819Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6190916Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6190920Z 
2025-04-11T03:52:12.6191083Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6191199Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6191203Z 
2025-04-11T03:52:12.6191279Z device = None
2025-04-11T03:52:12.6191283Z 
2025-04-11T03:52:12.6191406Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6191555Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6191625Z     
2025-04-11T03:52:12.6191704Z         Args:
2025-04-11T03:52:12.6191867Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6192035Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6192145Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6192224Z         """
2025-04-11T03:52:12.6192302Z         _lazy_init()
2025-04-11T03:52:12.6192397Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6192507Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6192611Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6192895Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6193034Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6193189Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6193196Z 
2025-04-11T03:52:12.6193431Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6193719Z _____________ test_context_attention[False-False-True-4-16-8-16-7] _____________
2025-04-11T03:52:12.6193723Z 
2025-04-11T03:52:12.6193876Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6194025Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6194125Z use_new_kcache_layout = False
2025-04-11T03:52:12.6194129Z 
2025-04-11T03:52:12.6194329Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6194440Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6194673Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6194810Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6194931Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6195047Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6195194Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6195329Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6195485Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6195575Z     def test_context_attention(
2025-04-11T03:52:12.6195651Z         bsz: int,
2025-04-11T03:52:12.6195734Z         block_size: int,
2025-04-11T03:52:12.6195824Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6195913Z         num_attn_heads: int,
2025-04-11T03:52:12.6195995Z         kv_group_num: int,
2025-04-11T03:52:12.6196080Z         same_context_len: bool,
2025-04-11T03:52:12.6196175Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6196263Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6196341Z     ):
2025-04-11T03:52:12.6196453Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6196649Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6196836Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6197008Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6197173Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6197249Z             return
2025-04-11T03:52:12.6197326Z     
2025-04-11T03:52:12.6197411Z         torch.manual_seed(123)
2025-04-11T03:52:12.6197514Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6197609Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6197700Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6197707Z 
2025-04-11T03:52:12.6197882Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6198001Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6198005Z 
2025-04-11T03:52:12.6198086Z device = None
2025-04-11T03:52:12.6198093Z 
2025-04-11T03:52:12.6198210Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6198362Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6198433Z     
2025-04-11T03:52:12.6198506Z         Args:
2025-04-11T03:52:12.6198675Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6198836Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6198947Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6199019Z         """
2025-04-11T03:52:12.6199098Z         _lazy_init()
2025-04-11T03:52:12.6199198Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6199300Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6199408Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6199691Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6199939Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6200101Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6200105Z 
2025-04-11T03:52:12.6200347Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6200516Z ____________ test_context_attention[False-False-True-4-16-8-16-32] _____________
2025-04-11T03:52:12.6200520Z 
2025-04-11T03:52:12.6200669Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6200918Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6201005Z use_new_kcache_layout = False
2025-04-11T03:52:12.6201010Z 
2025-04-11T03:52:12.6201209Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6201314Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6201436Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6201576Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6201696Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6201812Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6201949Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6202087Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6202238Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6202332Z     def test_context_attention(
2025-04-11T03:52:12.6202411Z         bsz: int,
2025-04-11T03:52:12.6202491Z         block_size: int,
2025-04-11T03:52:12.6202587Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6202670Z         num_attn_heads: int,
2025-04-11T03:52:12.6202758Z         kv_group_num: int,
2025-04-11T03:52:12.6202844Z         same_context_len: bool,
2025-04-11T03:52:12.6202938Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6203030Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6203106Z     ):
2025-04-11T03:52:12.6203220Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6203412Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6203597Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6203766Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6203926Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6204011Z             return
2025-04-11T03:52:12.6204083Z     
2025-04-11T03:52:12.6204171Z         torch.manual_seed(123)
2025-04-11T03:52:12.6204270Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6204363Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6204458Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6204462Z 
2025-04-11T03:52:12.6204629Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6204743Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6204747Z 
2025-04-11T03:52:12.6204824Z device = None
2025-04-11T03:52:12.6204827Z 
2025-04-11T03:52:12.6204946Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6205097Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6205172Z     
2025-04-11T03:52:12.6205245Z         Args:
2025-04-11T03:52:12.6205414Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6205579Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6205685Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6205871Z         """
2025-04-11T03:52:12.6205951Z         _lazy_init()
2025-04-11T03:52:12.6206051Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6206153Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6206259Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6206544Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6206679Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6206843Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6206941Z 
2025-04-11T03:52:12.6207188Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6207359Z _____________ test_context_attention[False-False-True-4-16-8-32-7] _____________
2025-04-11T03:52:12.6207363Z 
2025-04-11T03:52:12.6207511Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6207672Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6207763Z use_new_kcache_layout = False
2025-04-11T03:52:12.6207768Z 
2025-04-11T03:52:12.6207964Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6208083Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6208199Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6208344Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6208459Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6208578Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6208713Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6208847Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6209002Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6209094Z     def test_context_attention(
2025-04-11T03:52:12.6209173Z         bsz: int,
2025-04-11T03:52:12.6209253Z         block_size: int,
2025-04-11T03:52:12.6209341Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6209428Z         num_attn_heads: int,
2025-04-11T03:52:12.6209510Z         kv_group_num: int,
2025-04-11T03:52:12.6209600Z         same_context_len: bool,
2025-04-11T03:52:12.6209685Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6209775Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6209848Z     ):
2025-04-11T03:52:12.6209958Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6210157Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6210338Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6210514Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6210676Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6210754Z             return
2025-04-11T03:52:12.6210824Z     
2025-04-11T03:52:12.6210909Z         torch.manual_seed(123)
2025-04-11T03:52:12.6211010Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6211099Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6211190Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6211194Z 
2025-04-11T03:52:12.6211360Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6211472Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6211482Z 
2025-04-11T03:52:12.6211559Z device = None
2025-04-11T03:52:12.6211563Z 
2025-04-11T03:52:12.6211677Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6211833Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6212018Z     
2025-04-11T03:52:12.6212095Z         Args:
2025-04-11T03:52:12.6212263Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6212426Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6212536Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6212608Z         """
2025-04-11T03:52:12.6212693Z         _lazy_init()
2025-04-11T03:52:12.6212789Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6212896Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6213001Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6213378Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6213521Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6213679Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6213687Z 
2025-04-11T03:52:12.6213927Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6214096Z ____________ test_context_attention[False-False-True-4-16-8-32-32] _____________
2025-04-11T03:52:12.6214100Z 
2025-04-11T03:52:12.6214251Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6214395Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6214486Z use_new_kcache_layout = False
2025-04-11T03:52:12.6214490Z 
2025-04-11T03:52:12.6214689Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6214797Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6214918Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6215057Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6215181Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6215293Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6215432Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6215567Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6215718Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6215812Z     def test_context_attention(
2025-04-11T03:52:12.6215887Z         bsz: int,
2025-04-11T03:52:12.6215970Z         block_size: int,
2025-04-11T03:52:12.6216061Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6216144Z         num_attn_heads: int,
2025-04-11T03:52:12.6216235Z         kv_group_num: int,
2025-04-11T03:52:12.6216320Z         same_context_len: bool,
2025-04-11T03:52:12.6216409Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6216518Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6216618Z     ):
2025-04-11T03:52:12.6216737Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6216932Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6217118Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6217287Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6217459Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6217539Z             return
2025-04-11T03:52:12.6217617Z     
2025-04-11T03:52:12.6217706Z         torch.manual_seed(123)
2025-04-11T03:52:12.6217808Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6217905Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6217997Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6218001Z 
2025-04-11T03:52:12.6218174Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6218407Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6218411Z 
2025-04-11T03:52:12.6218499Z device = None
2025-04-11T03:52:12.6218504Z 
2025-04-11T03:52:12.6218623Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6218776Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6218851Z     
2025-04-11T03:52:12.6218926Z         Args:
2025-04-11T03:52:12.6219099Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6219264Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6219476Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6219550Z         """
2025-04-11T03:52:12.6219630Z         _lazy_init()
2025-04-11T03:52:12.6219732Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6219834Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6219948Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6220229Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6220364Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6220524Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6220529Z 
2025-04-11T03:52:12.6220766Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6220939Z ____________ test_context_attention[False-False-True-4-16-16-16-7] _____________
2025-04-11T03:52:12.6220946Z 
2025-04-11T03:52:12.6221096Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6221245Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6221334Z use_new_kcache_layout = False
2025-04-11T03:52:12.6221338Z 
2025-04-11T03:52:12.6221541Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6221646Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6221765Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6221905Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6222022Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6222140Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6222277Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6222416Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6222570Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6222660Z     def test_context_attention(
2025-04-11T03:52:12.6222744Z         bsz: int,
2025-04-11T03:52:12.6222824Z         block_size: int,
2025-04-11T03:52:12.6222917Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6223011Z         num_attn_heads: int,
2025-04-11T03:52:12.6223092Z         kv_group_num: int,
2025-04-11T03:52:12.6223182Z         same_context_len: bool,
2025-04-11T03:52:12.6223267Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6223359Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6223430Z     ):
2025-04-11T03:52:12.6223547Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6223737Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6223918Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6224096Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6224259Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6224339Z             return
2025-04-11T03:52:12.6224410Z     
2025-04-11T03:52:12.6224610Z         torch.manual_seed(123)
2025-04-11T03:52:12.6224709Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6224798Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6224893Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6224898Z 
2025-04-11T03:52:12.6225065Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6225182Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6225186Z 
2025-04-11T03:52:12.6225262Z device = None
2025-04-11T03:52:12.6225266Z 
2025-04-11T03:52:12.6225389Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6225630Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6225703Z     
2025-04-11T03:52:12.6225782Z         Args:
2025-04-11T03:52:12.6225945Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6226111Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6226222Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6226297Z         """
2025-04-11T03:52:12.6226377Z         _lazy_init()
2025-04-11T03:52:12.6226475Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6226584Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6226689Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6226972Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6227106Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6227266Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6227275Z 
2025-04-11T03:52:12.6227510Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6227678Z ____________ test_context_attention[False-False-True-4-16-16-16-32] ____________
2025-04-11T03:52:12.6227686Z 
2025-04-11T03:52:12.6227840Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6227985Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6228077Z use_new_kcache_layout = False
2025-04-11T03:52:12.6228081Z 
2025-04-11T03:52:12.6228278Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6228387Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6228542Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6228683Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6228803Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6228916Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6229057Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6229200Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6229356Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6229446Z     def test_context_attention(
2025-04-11T03:52:12.6229521Z         bsz: int,
2025-04-11T03:52:12.6229612Z         block_size: int,
2025-04-11T03:52:12.6229706Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6229792Z         num_attn_heads: int,
2025-04-11T03:52:12.6229875Z         kv_group_num: int,
2025-04-11T03:52:12.6229961Z         same_context_len: bool,
2025-04-11T03:52:12.6230048Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6230136Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6230220Z     ):
2025-04-11T03:52:12.6230332Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6230525Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6230707Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6230991Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6231160Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6231236Z             return
2025-04-11T03:52:12.6231314Z     
2025-04-11T03:52:12.6231399Z         torch.manual_seed(123)
2025-04-11T03:52:12.6231500Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6231592Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6231683Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6231687Z 
2025-04-11T03:52:12.6231978Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6232090Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6232095Z 
2025-04-11T03:52:12.6232175Z device = None
2025-04-11T03:52:12.6232179Z 
2025-04-11T03:52:12.6232295Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6232453Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6232523Z     
2025-04-11T03:52:12.6232597Z         Args:
2025-04-11T03:52:12.6232767Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6232930Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6233040Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6233114Z         """
2025-04-11T03:52:12.6233198Z         _lazy_init()
2025-04-11T03:52:12.6233295Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6233403Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6233512Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6233792Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6233933Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6234091Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6234095Z 
2025-04-11T03:52:12.6234334Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6234503Z ____________ test_context_attention[False-False-True-4-16-16-32-7] _____________
2025-04-11T03:52:12.6234507Z 
2025-04-11T03:52:12.6234656Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6234806Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6234896Z use_new_kcache_layout = False
2025-04-11T03:52:12.6234900Z 
2025-04-11T03:52:12.6235104Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6235208Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6235332Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6235474Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6235598Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6235711Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6235845Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6235983Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6236132Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6236225Z     def test_context_attention(
2025-04-11T03:52:12.6236299Z         bsz: int,
2025-04-11T03:52:12.6236384Z         block_size: int,
2025-04-11T03:52:12.6236478Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6236561Z         num_attn_heads: int,
2025-04-11T03:52:12.6236647Z         kv_group_num: int,
2025-04-11T03:52:12.6236733Z         same_context_len: bool,
2025-04-11T03:52:12.6236821Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6237015Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6237087Z     ):
2025-04-11T03:52:12.6237204Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6237397Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6237583Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6237753Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6237914Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6238086Z             return
2025-04-11T03:52:12.6238157Z     
2025-04-11T03:52:12.6238247Z         torch.manual_seed(123)
2025-04-11T03:52:12.6238347Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6238441Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6238533Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6238540Z 
2025-04-11T03:52:12.6238709Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6238826Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6238831Z 
2025-04-11T03:52:12.6238907Z device = None
2025-04-11T03:52:12.6238911Z 
2025-04-11T03:52:12.6239032Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6239182Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6239256Z     
2025-04-11T03:52:12.6239329Z         Args:
2025-04-11T03:52:12.6239497Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6239666Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6239772Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6239854Z         """
2025-04-11T03:52:12.6239931Z         _lazy_init()
2025-04-11T03:52:12.6240032Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6240135Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6240245Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6240535Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6240669Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6240835Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6240839Z 
2025-04-11T03:52:12.6241080Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6241253Z ____________ test_context_attention[False-False-True-4-16-16-32-32] ____________
2025-04-11T03:52:12.6241257Z 
2025-04-11T03:52:12.6241406Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6241556Z kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
2025-04-11T03:52:12.6241651Z use_new_kcache_layout = False
2025-04-11T03:52:12.6241655Z 
2025-04-11T03:52:12.6241853Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6241961Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6242079Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6242221Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6242335Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6242454Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6242594Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6242730Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6242889Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6242979Z     def test_context_attention(
2025-04-11T03:52:12.6243181Z         bsz: int,
2025-04-11T03:52:12.6243264Z         block_size: int,
2025-04-11T03:52:12.6243353Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6243439Z         num_attn_heads: int,
2025-04-11T03:52:12.6243521Z         kv_group_num: int,
2025-04-11T03:52:12.6243614Z         same_context_len: bool,
2025-04-11T03:52:12.6243702Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6243793Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6243864Z     ):
2025-04-11T03:52:12.6243973Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6244173Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6244463Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6244638Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6244801Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6244883Z             return
2025-04-11T03:52:12.6244954Z     
2025-04-11T03:52:12.6245038Z         torch.manual_seed(123)
2025-04-11T03:52:12.6245141Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6245230Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6245323Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6245327Z 
2025-04-11T03:52:12.6245492Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6245603Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6245611Z 
2025-04-11T03:52:12.6245692Z device = None
2025-04-11T03:52:12.6245697Z 
2025-04-11T03:52:12.6245811Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6245963Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6246035Z     
2025-04-11T03:52:12.6246112Z         Args:
2025-04-11T03:52:12.6246282Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6246446Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6246557Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6246629Z         """
2025-04-11T03:52:12.6246709Z         _lazy_init()
2025-04-11T03:52:12.6246807Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6246913Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6247017Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6247299Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6247442Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6247602Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6247610Z 
2025-04-11T03:52:12.6247851Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6248019Z ____________ test_context_attention[False-False-False-1-16-8-16-7] _____________
2025-04-11T03:52:12.6248024Z 
2025-04-11T03:52:12.6248179Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6248325Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6248419Z use_new_kcache_layout = False
2025-04-11T03:52:12.6248423Z 
2025-04-11T03:52:12.6248624Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6248731Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6248859Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6248996Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6249115Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6249339Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6249479Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6249611Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6249759Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6249853Z     def test_context_attention(
2025-04-11T03:52:12.6249928Z         bsz: int,
2025-04-11T03:52:12.6250011Z         block_size: int,
2025-04-11T03:52:12.6250099Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6250182Z         num_attn_heads: int,
2025-04-11T03:52:12.6250269Z         kv_group_num: int,
2025-04-11T03:52:12.6250447Z         same_context_len: bool,
2025-04-11T03:52:12.6250536Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6250624Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6250698Z     ):
2025-04-11T03:52:12.6250808Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6251003Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6251189Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6251360Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6251528Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6251603Z             return
2025-04-11T03:52:12.6251678Z     
2025-04-11T03:52:12.6251763Z         torch.manual_seed(123)
2025-04-11T03:52:12.6251863Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6251961Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6252050Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6252054Z 
2025-04-11T03:52:12.6252222Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6252336Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6252344Z 
2025-04-11T03:52:12.6252423Z device = None
2025-04-11T03:52:12.6252427Z 
2025-04-11T03:52:12.6252543Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6252691Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6252768Z     
2025-04-11T03:52:12.6252841Z         Args:
2025-04-11T03:52:12.6253009Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6253171Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6253282Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6253358Z         """
2025-04-11T03:52:12.6253440Z         _lazy_init()
2025-04-11T03:52:12.6253540Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6253645Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6253752Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6254032Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6254169Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6254328Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6254333Z 
2025-04-11T03:52:12.6254570Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6254745Z ____________ test_context_attention[False-False-False-1-16-8-16-32] ____________
2025-04-11T03:52:12.6254749Z 
2025-04-11T03:52:12.6254899Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6255047Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6255136Z use_new_kcache_layout = False
2025-04-11T03:52:12.6255140Z 
2025-04-11T03:52:12.6255339Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6255545Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6255663Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6255805Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6255922Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6256039Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6256173Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6256311Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6256462Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6256642Z     def test_context_attention(
2025-04-11T03:52:12.6256721Z         bsz: int,
2025-04-11T03:52:12.6256803Z         block_size: int,
2025-04-11T03:52:12.6256896Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6256980Z         num_attn_heads: int,
2025-04-11T03:52:12.6257067Z         kv_group_num: int,
2025-04-11T03:52:12.6257156Z         same_context_len: bool,
2025-04-11T03:52:12.6257242Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6257335Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6257408Z     ):
2025-04-11T03:52:12.6257522Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6257710Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6257888Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6258059Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6258221Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6258301Z             return
2025-04-11T03:52:12.6258372Z     
2025-04-11T03:52:12.6258467Z         torch.manual_seed(123)
2025-04-11T03:52:12.6258570Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6258665Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6258765Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6258770Z 
2025-04-11T03:52:12.6258935Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6259048Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6259052Z 
2025-04-11T03:52:12.6259127Z device = None
2025-04-11T03:52:12.6259131Z 
2025-04-11T03:52:12.6259250Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6259398Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6259472Z     
2025-04-11T03:52:12.6259548Z         Args:
2025-04-11T03:52:12.6259714Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6259878Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6259985Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6260061Z         """
2025-04-11T03:52:12.6260139Z         _lazy_init()
2025-04-11T03:52:12.6260235Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6260342Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6260447Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6260729Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6260864Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6261030Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6261034Z 
2025-04-11T03:52:12.6261269Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6261438Z ____________ test_context_attention[False-False-False-1-16-8-32-7] _____________
2025-04-11T03:52:12.6261553Z 
2025-04-11T03:52:12.6261704Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6261851Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6261946Z use_new_kcache_layout = False
2025-04-11T03:52:12.6261950Z 
2025-04-11T03:52:12.6262148Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6262256Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6262372Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6262511Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6262736Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6262849Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6262992Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6263129Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6263284Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6263375Z     def test_context_attention(
2025-04-11T03:52:12.6263453Z         bsz: int,
2025-04-11T03:52:12.6263545Z         block_size: int,
2025-04-11T03:52:12.6263636Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6263730Z         num_attn_heads: int,
2025-04-11T03:52:12.6263820Z         kv_group_num: int,
2025-04-11T03:52:12.6263910Z         same_context_len: bool,
2025-04-11T03:52:12.6263993Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6264082Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6264158Z     ):
2025-04-11T03:52:12.6264277Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6264469Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6264651Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6264821Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6264986Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6265062Z             return
2025-04-11T03:52:12.6265139Z     
2025-04-11T03:52:12.6265227Z         torch.manual_seed(123)
2025-04-11T03:52:12.6265329Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6265419Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6265509Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6265514Z 
2025-04-11T03:52:12.6265684Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6265798Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6265802Z 
2025-04-11T03:52:12.6265884Z device = None
2025-04-11T03:52:12.6265888Z 
2025-04-11T03:52:12.6266004Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6266162Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6266233Z     
2025-04-11T03:52:12.6266305Z         Args:
2025-04-11T03:52:12.6266475Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6266637Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6266745Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6266817Z         """
2025-04-11T03:52:12.6266898Z         _lazy_init()
2025-04-11T03:52:12.6266994Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6267093Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6267205Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6267488Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6267625Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6267973Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6267977Z 
2025-04-11T03:52:12.6268218Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6268388Z ____________ test_context_attention[False-False-False-1-16-8-32-32] ____________
2025-04-11T03:52:12.6268392Z 
2025-04-11T03:52:12.6268588Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6268735Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6268824Z use_new_kcache_layout = False
2025-04-11T03:52:12.6268939Z 
2025-04-11T03:52:12.6269144Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6269248Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6269370Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6269510Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6269630Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6269742Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6269877Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6270020Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6270169Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6270263Z     def test_context_attention(
2025-04-11T03:52:12.6270339Z         bsz: int,
2025-04-11T03:52:12.6270420Z         block_size: int,
2025-04-11T03:52:12.6270520Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6270603Z         num_attn_heads: int,
2025-04-11T03:52:12.6270690Z         kv_group_num: int,
2025-04-11T03:52:12.6270776Z         same_context_len: bool,
2025-04-11T03:52:12.6270863Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6270950Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6271025Z     ):
2025-04-11T03:52:12.6271143Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6271335Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6271520Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6271689Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6271849Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6271929Z             return
2025-04-11T03:52:12.6272004Z     
2025-04-11T03:52:12.6272093Z         torch.manual_seed(123)
2025-04-11T03:52:12.6272189Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6272282Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6272375Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6272379Z 
2025-04-11T03:52:12.6272548Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6272663Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6272667Z 
2025-04-11T03:52:12.6272744Z device = None
2025-04-11T03:52:12.6272748Z 
2025-04-11T03:52:12.6272869Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6273021Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6273096Z     
2025-04-11T03:52:12.6273170Z         Args:
2025-04-11T03:52:12.6273336Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6273506Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6273612Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6273692Z         """
2025-04-11T03:52:12.6273771Z         _lazy_init()
2025-04-11T03:52:12.6273879Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6274101Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6274210Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6274501Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6274640Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6274808Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6274813Z 
2025-04-11T03:52:12.6275058Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6275322Z ____________ test_context_attention[False-False-False-1-16-16-16-7] ____________
2025-04-11T03:52:12.6275326Z 
2025-04-11T03:52:12.6275479Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6275630Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6275723Z use_new_kcache_layout = False
2025-04-11T03:52:12.6275727Z 
2025-04-11T03:52:12.6275925Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6276035Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6276152Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6276296Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6276415Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6276533Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6276673Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6276815Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6276969Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6277059Z     def test_context_attention(
2025-04-11T03:52:12.6277139Z         bsz: int,
2025-04-11T03:52:12.6277221Z         block_size: int,
2025-04-11T03:52:12.6277310Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6277397Z         num_attn_heads: int,
2025-04-11T03:52:12.6277479Z         kv_group_num: int,
2025-04-11T03:52:12.6277566Z         same_context_len: bool,
2025-04-11T03:52:12.6277651Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6277741Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6277813Z     ):
2025-04-11T03:52:12.6277925Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6278119Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6278303Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6278474Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6278636Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6278718Z             return
2025-04-11T03:52:12.6278792Z     
2025-04-11T03:52:12.6278878Z         torch.manual_seed(123)
2025-04-11T03:52:12.6278979Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6279070Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6279166Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6279171Z 
2025-04-11T03:52:12.6279337Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6279448Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6279457Z 
2025-04-11T03:52:12.6279535Z device = None
2025-04-11T03:52:12.6279542Z 
2025-04-11T03:52:12.6279659Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6279817Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6279889Z     
2025-04-11T03:52:12.6279965Z         Args:
2025-04-11T03:52:12.6280128Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6280398Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6280506Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6280577Z         """
2025-04-11T03:52:12.6280661Z         _lazy_init()
2025-04-11T03:52:12.6280759Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6280867Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6280971Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6281256Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6281492Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6281649Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6281654Z 
2025-04-11T03:52:12.6281897Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6282075Z ___________ test_context_attention[False-False-False-1-16-16-16-32] ____________
2025-04-11T03:52:12.6282080Z 
2025-04-11T03:52:12.6282238Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6282385Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6282478Z use_new_kcache_layout = False
2025-04-11T03:52:12.6282482Z 
2025-04-11T03:52:12.6282681Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6282807Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6282930Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6283068Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6283188Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6283300Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6283442Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6283578Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6283727Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6283821Z     def test_context_attention(
2025-04-11T03:52:12.6283896Z         bsz: int,
2025-04-11T03:52:12.6283985Z         block_size: int,
2025-04-11T03:52:12.6284078Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6284162Z         num_attn_heads: int,
2025-04-11T03:52:12.6284253Z         kv_group_num: int,
2025-04-11T03:52:12.6284340Z         same_context_len: bool,
2025-04-11T03:52:12.6284433Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6284520Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6284596Z     ):
2025-04-11T03:52:12.6284706Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6284897Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6285088Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6285256Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6285423Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6285498Z             return
2025-04-11T03:52:12.6285573Z     
2025-04-11T03:52:12.6285657Z         torch.manual_seed(123)
2025-04-11T03:52:12.6285755Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6285851Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6285943Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6285947Z 
2025-04-11T03:52:12.6286116Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6286228Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6286233Z 
2025-04-11T03:52:12.6286429Z device = None
2025-04-11T03:52:12.6286433Z 
2025-04-11T03:52:12.6286552Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6286705Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6286781Z     
2025-04-11T03:52:12.6286854Z         Args:
2025-04-11T03:52:12.6287024Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6287187Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6287301Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6287374Z         """
2025-04-11T03:52:12.6287564Z         _lazy_init()
2025-04-11T03:52:12.6287667Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6287769Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6287882Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6288166Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6288305Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6288468Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6288473Z 
2025-04-11T03:52:12.6288709Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6288886Z ____________ test_context_attention[False-False-False-1-16-16-32-7] ____________
2025-04-11T03:52:12.6288890Z 
2025-04-11T03:52:12.6289039Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6289189Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6289278Z use_new_kcache_layout = False
2025-04-11T03:52:12.6289282Z 
2025-04-11T03:52:12.6289481Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6289589Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6289703Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6289847Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6289963Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6290082Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6290220Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6290358Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6290507Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6290601Z     def test_context_attention(
2025-04-11T03:52:12.6290684Z         bsz: int,
2025-04-11T03:52:12.6290764Z         block_size: int,
2025-04-11T03:52:12.6290856Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6290942Z         num_attn_heads: int,
2025-04-11T03:52:12.6291029Z         kv_group_num: int,
2025-04-11T03:52:12.6291120Z         same_context_len: bool,
2025-04-11T03:52:12.6291206Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6291299Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6291371Z     ):
2025-04-11T03:52:12.6291482Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6291675Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6291856Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6292028Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6292194Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6292275Z             return
2025-04-11T03:52:12.6292347Z     
2025-04-11T03:52:12.6292434Z         torch.manual_seed(123)
2025-04-11T03:52:12.6292530Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6292722Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6292818Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6292823Z 
2025-04-11T03:52:12.6292990Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6293110Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6293114Z 
2025-04-11T03:52:12.6293192Z device = None
2025-04-11T03:52:12.6293195Z 
2025-04-11T03:52:12.6293320Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6293470Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6293542Z     
2025-04-11T03:52:12.6293728Z         Args:
2025-04-11T03:52:12.6293897Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6294064Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6294171Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6294252Z         """
2025-04-11T03:52:12.6294331Z         _lazy_init()
2025-04-11T03:52:12.6294427Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6294535Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6294641Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6294928Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6295064Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6295230Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6295237Z 
2025-04-11T03:52:12.6295475Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6295651Z ___________ test_context_attention[False-False-False-1-16-16-32-32] ____________
2025-04-11T03:52:12.6295661Z 
2025-04-11T03:52:12.6295816Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6295963Z kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6296058Z use_new_kcache_layout = False
2025-04-11T03:52:12.6296062Z 
2025-04-11T03:52:12.6296258Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6296366Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6296484Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6296626Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6296741Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6296861Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6297005Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6297139Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6297296Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6297391Z     def test_context_attention(
2025-04-11T03:52:12.6297466Z         bsz: int,
2025-04-11T03:52:12.6297558Z         block_size: int,
2025-04-11T03:52:12.6297647Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6297740Z         num_attn_heads: int,
2025-04-11T03:52:12.6297821Z         kv_group_num: int,
2025-04-11T03:52:12.6297907Z         same_context_len: bool,
2025-04-11T03:52:12.6297994Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6298087Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6298162Z     ):
2025-04-11T03:52:12.6298278Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6298480Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6298665Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6298835Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6299115Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6299189Z             return
2025-04-11T03:52:12.6299269Z     
2025-04-11T03:52:12.6299357Z         torch.manual_seed(123)
2025-04-11T03:52:12.6299466Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6299560Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6299653Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6299658Z 
2025-04-11T03:52:12.6299832Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6299942Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6300044Z 
2025-04-11T03:52:12.6300131Z device = None
2025-04-11T03:52:12.6300136Z 
2025-04-11T03:52:12.6300254Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6300411Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6300487Z     
2025-04-11T03:52:12.6300559Z         Args:
2025-04-11T03:52:12.6300731Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6300897Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6301005Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6301077Z         """
2025-04-11T03:52:12.6301158Z         _lazy_init()
2025-04-11T03:52:12.6301254Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6301356Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6301468Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6301755Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6301892Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6302052Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6302056Z 
2025-04-11T03:52:12.6302301Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6302471Z ____________ test_context_attention[False-False-False-4-16-8-16-7] _____________
2025-04-11T03:52:12.6302475Z 
2025-04-11T03:52:12.6302630Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6302776Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6302868Z use_new_kcache_layout = False
2025-04-11T03:52:12.6302872Z 
2025-04-11T03:52:12.6303081Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6303183Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6303302Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6303444Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6303570Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6303683Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6303822Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6303959Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6304108Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6304203Z     def test_context_attention(
2025-04-11T03:52:12.6304277Z         bsz: int,
2025-04-11T03:52:12.6304357Z         block_size: int,
2025-04-11T03:52:12.6304449Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6304537Z         num_attn_heads: int,
2025-04-11T03:52:12.6304624Z         kv_group_num: int,
2025-04-11T03:52:12.6304708Z         same_context_len: bool,
2025-04-11T03:52:12.6304796Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6304884Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6304954Z     ):
2025-04-11T03:52:12.6305173Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6305364Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6305547Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6305715Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6305879Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6305955Z             return
2025-04-11T03:52:12.6306028Z     
2025-04-11T03:52:12.6306118Z         torch.manual_seed(123)
2025-04-11T03:52:12.6306318Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6306412Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6306505Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6306509Z 
2025-04-11T03:52:12.6306675Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6306793Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6306797Z 
2025-04-11T03:52:12.6306872Z device = None
2025-04-11T03:52:12.6306876Z 
2025-04-11T03:52:12.6306995Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6307142Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6307216Z     
2025-04-11T03:52:12.6307291Z         Args:
2025-04-11T03:52:12.6307456Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6307625Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6307734Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6307815Z         """
2025-04-11T03:52:12.6307893Z         _lazy_init()
2025-04-11T03:52:12.6307994Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6308098Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6308206Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6308538Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6308677Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6308842Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6308846Z 
2025-04-11T03:52:12.6309084Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6309257Z ____________ test_context_attention[False-False-False-4-16-8-16-32] ____________
2025-04-11T03:52:12.6309264Z 
2025-04-11T03:52:12.6309412Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6309559Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6309647Z use_new_kcache_layout = False
2025-04-11T03:52:12.6309654Z 
2025-04-11T03:52:12.6309852Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6309960Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6310082Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6310229Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6310344Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6310461Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6310600Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6310737Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6310891Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6310981Z     def test_context_attention(
2025-04-11T03:52:12.6311062Z         bsz: int,
2025-04-11T03:52:12.6311142Z         block_size: int,
2025-04-11T03:52:12.6311390Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6311479Z         num_attn_heads: int,
2025-04-11T03:52:12.6311563Z         kv_group_num: int,
2025-04-11T03:52:12.6311653Z         same_context_len: bool,
2025-04-11T03:52:12.6311738Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6311832Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6311903Z     ):
2025-04-11T03:52:12.6312016Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6312214Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6312393Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6312680Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6312841Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6312920Z             return
2025-04-11T03:52:12.6312995Z     
2025-04-11T03:52:12.6313081Z         torch.manual_seed(123)
2025-04-11T03:52:12.6313187Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6313276Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6313368Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6313372Z 
2025-04-11T03:52:12.6313541Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6313651Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6313659Z 
2025-04-11T03:52:12.6313735Z device = None
2025-04-11T03:52:12.6313739Z 
2025-04-11T03:52:12.6313858Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6314015Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6314087Z     
2025-04-11T03:52:12.6314166Z         Args:
2025-04-11T03:52:12.6314332Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6314501Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6314609Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6314681Z         """
2025-04-11T03:52:12.6314768Z         _lazy_init()
2025-04-11T03:52:12.6314866Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6314977Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6315086Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6315373Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6315518Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6315676Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6315680Z 
2025-04-11T03:52:12.6315922Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6316094Z ____________ test_context_attention[False-False-False-4-16-8-32-7] _____________
2025-04-11T03:52:12.6316098Z 
2025-04-11T03:52:12.6316252Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6316401Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6316494Z use_new_kcache_layout = False
2025-04-11T03:52:12.6316498Z 
2025-04-11T03:52:12.6316698Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6316803Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6316925Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6317074Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6317238Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6317355Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6317494Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6317741Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6317890Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6317984Z     def test_context_attention(
2025-04-11T03:52:12.6318059Z         bsz: int,
2025-04-11T03:52:12.6318143Z         block_size: int,
2025-04-11T03:52:12.6318233Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6318318Z         num_attn_heads: int,
2025-04-11T03:52:12.6318406Z         kv_group_num: int,
2025-04-11T03:52:12.6318490Z         same_context_len: bool,
2025-04-11T03:52:12.6318578Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6318764Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6318842Z     ):
2025-04-11T03:52:12.6318951Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6319144Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6319336Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6319506Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6319673Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6319752Z             return
2025-04-11T03:52:12.6319830Z     
2025-04-11T03:52:12.6319920Z         torch.manual_seed(123)
2025-04-11T03:52:12.6320020Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6320115Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6320204Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6320211Z 
2025-04-11T03:52:12.6320386Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6320497Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6320501Z 
2025-04-11T03:52:12.6320581Z device = None
2025-04-11T03:52:12.6320585Z 
2025-04-11T03:52:12.6320703Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6320851Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6320927Z     
2025-04-11T03:52:12.6320998Z         Args:
2025-04-11T03:52:12.6321166Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6321329Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6321437Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6321510Z         """
2025-04-11T03:52:12.6321587Z         _lazy_init()
2025-04-11T03:52:12.6321692Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6321793Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6321901Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6322183Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6322322Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6322484Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6322488Z 
2025-04-11T03:52:12.6322727Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6322903Z ____________ test_context_attention[False-False-False-4-16-8-32-32] ____________
2025-04-11T03:52:12.6322907Z 
2025-04-11T03:52:12.6323056Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6323208Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6323300Z use_new_kcache_layout = False
2025-04-11T03:52:12.6323304Z 
2025-04-11T03:52:12.6323507Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6323611Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6323837Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6323984Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6324099Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6324216Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6324355Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6324494Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6324644Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6324733Z     def test_context_attention(
2025-04-11T03:52:12.6324906Z         bsz: int,
2025-04-11T03:52:12.6324989Z         block_size: int,
2025-04-11T03:52:12.6325081Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6325166Z         num_attn_heads: int,
2025-04-11T03:52:12.6325259Z         kv_group_num: int,
2025-04-11T03:52:12.6325349Z         same_context_len: bool,
2025-04-11T03:52:12.6325439Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6325537Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6325609Z     ):
2025-04-11T03:52:12.6325722Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6325913Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6326093Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6326264Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6326426Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6326510Z             return
2025-04-11T03:52:12.6326583Z     
2025-04-11T03:52:12.6326671Z         torch.manual_seed(123)
2025-04-11T03:52:12.6326772Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6326862Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6326959Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6326964Z 
2025-04-11T03:52:12.6327129Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6327244Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6327248Z 
2025-04-11T03:52:12.6327324Z device = None
2025-04-11T03:52:12.6327327Z 
2025-04-11T03:52:12.6327447Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6327595Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6327667Z     
2025-04-11T03:52:12.6327747Z         Args:
2025-04-11T03:52:12.6327916Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6328083Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6328190Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6328266Z         """
2025-04-11T03:52:12.6328347Z         _lazy_init()
2025-04-11T03:52:12.6328444Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6328550Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6328652Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6328935Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6329072Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6329232Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6329237Z 
2025-04-11T03:52:12.6329477Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6329646Z ____________ test_context_attention[False-False-False-4-16-16-16-7] ____________
2025-04-11T03:52:12.6329654Z 
2025-04-11T03:52:12.6329803Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6330054Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6330149Z use_new_kcache_layout = False
2025-04-11T03:52:12.6330154Z 
2025-04-11T03:52:12.6330351Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6330463Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6330580Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6330723Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6330840Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6330957Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6331202Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6331339Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6331496Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6331590Z     def test_context_attention(
2025-04-11T03:52:12.6331664Z         bsz: int,
2025-04-11T03:52:12.6331752Z         block_size: int,
2025-04-11T03:52:12.6331844Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6331930Z         num_attn_heads: int,
2025-04-11T03:52:12.6332014Z         kv_group_num: int,
2025-04-11T03:52:12.6332104Z         same_context_len: bool,
2025-04-11T03:52:12.6332188Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6332276Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6332355Z     ):
2025-04-11T03:52:12.6332467Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6332663Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6332847Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6333018Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6333186Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6333262Z             return
2025-04-11T03:52:12.6333348Z     
2025-04-11T03:52:12.6333433Z         torch.manual_seed(123)
2025-04-11T03:52:12.6333534Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6333627Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6333716Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6333720Z 
2025-04-11T03:52:12.6333889Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6334003Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6334010Z 
2025-04-11T03:52:12.6334090Z device = None
2025-04-11T03:52:12.6334094Z 
2025-04-11T03:52:12.6334209Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6334362Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6334434Z     
2025-04-11T03:52:12.6334510Z         Args:
2025-04-11T03:52:12.6334679Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6334842Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6334955Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6335028Z         """
2025-04-11T03:52:12.6335108Z         _lazy_init()
2025-04-11T03:52:12.6335204Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6335304Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6335417Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6335700Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6335837Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6335992Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6336116Z 
2025-04-11T03:52:12.6336361Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6336534Z ___________ test_context_attention[False-False-False-4-16-16-16-32] ____________
2025-04-11T03:52:12.6336539Z 
2025-04-11T03:52:12.6336694Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6336838Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6336926Z use_new_kcache_layout = False
2025-04-11T03:52:12.6336931Z 
2025-04-11T03:52:12.6337133Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6337332Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6337455Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6337592Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6337712Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6337828Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6337966Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6338104Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6338251Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6338347Z     def test_context_attention(
2025-04-11T03:52:12.6338424Z         bsz: int,
2025-04-11T03:52:12.6338503Z         block_size: int,
2025-04-11T03:52:12.6338601Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6338686Z         num_attn_heads: int,
2025-04-11T03:52:12.6338777Z         kv_group_num: int,
2025-04-11T03:52:12.6338864Z         same_context_len: bool,
2025-04-11T03:52:12.6338956Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6339042Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6339114Z     ):
2025-04-11T03:52:12.6339230Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6339426Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6339610Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6339778Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6339946Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6340021Z             return
2025-04-11T03:52:12.6340093Z     
2025-04-11T03:52:12.6340186Z         torch.manual_seed(123)
2025-04-11T03:52:12.6340285Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6340382Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6340470Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6340474Z 
2025-04-11T03:52:12.6340641Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6340754Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6340762Z 
2025-04-11T03:52:12.6340839Z device = None
2025-04-11T03:52:12.6340843Z 
2025-04-11T03:52:12.6340963Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6341114Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6341190Z     
2025-04-11T03:52:12.6341264Z         Args:
2025-04-11T03:52:12.6341432Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6341599Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6341708Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6341785Z         """
2025-04-11T03:52:12.6341865Z         _lazy_init()
2025-04-11T03:52:12.6341967Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6342069Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6342173Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6342571Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6342709Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6342875Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6342879Z 
2025-04-11T03:52:12.6343118Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6343294Z ____________ test_context_attention[False-False-False-4-16-16-32-7] ____________
2025-04-11T03:52:12.6343391Z 
2025-04-11T03:52:12.6343543Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6343693Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6343782Z use_new_kcache_layout = False
2025-04-11T03:52:12.6343787Z 
2025-04-11T03:52:12.6343989Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6344100Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6344217Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6344360Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6344477Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6344596Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6344732Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6344866Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6345024Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6345113Z     def test_context_attention(
2025-04-11T03:52:12.6345191Z         bsz: int,
2025-04-11T03:52:12.6345272Z         block_size: int,
2025-04-11T03:52:12.6345361Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6345450Z         num_attn_heads: int,
2025-04-11T03:52:12.6345532Z         kv_group_num: int,
2025-04-11T03:52:12.6345621Z         same_context_len: bool,
2025-04-11T03:52:12.6345704Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6345795Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6345866Z     ):
2025-04-11T03:52:12.6345977Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6346171Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6346350Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6346531Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6346693Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6346770Z             return
2025-04-11T03:52:12.6346841Z     
2025-04-11T03:52:12.6346926Z         torch.manual_seed(123)
2025-04-11T03:52:12.6347034Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6347123Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6347215Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6347220Z 
2025-04-11T03:52:12.6347386Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6347497Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6347504Z 
2025-04-11T03:52:12.6347583Z device = None
2025-04-11T03:52:12.6347588Z 
2025-04-11T03:52:12.6347709Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6347865Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6347941Z     
2025-04-11T03:52:12.6348018Z         Args:
2025-04-11T03:52:12.6348185Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6348350Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6348635Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6348709Z         """
2025-04-11T03:52:12.6348797Z         _lazy_init()
2025-04-11T03:52:12.6348894Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6349001Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6349107Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6349392Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6349530Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6349800Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6349805Z 
2025-04-11T03:52:12.6350049Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6350223Z ___________ test_context_attention[False-False-False-4-16-16-32-32] ____________
2025-04-11T03:52:12.6350231Z 
2025-04-11T03:52:12.6350384Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6350530Z kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
2025-04-11T03:52:12.6350622Z use_new_kcache_layout = False
2025-04-11T03:52:12.6350626Z 
2025-04-11T03:52:12.6350825Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6350927Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.6351047Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6351185Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6351310Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6351423Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6351561Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6351698Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6351847Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6351943Z     def test_context_attention(
2025-04-11T03:52:12.6352018Z         bsz: int,
2025-04-11T03:52:12.6352101Z         block_size: int,
2025-04-11T03:52:12.6352189Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6352278Z         num_attn_heads: int,
2025-04-11T03:52:12.6352359Z         kv_group_num: int,
2025-04-11T03:52:12.6352444Z         same_context_len: bool,
2025-04-11T03:52:12.6352531Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6352617Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6352693Z     ):
2025-04-11T03:52:12.6352803Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6352991Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6353177Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6353348Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6353511Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6353586Z             return
2025-04-11T03:52:12.6353660Z     
2025-04-11T03:52:12.6353746Z         torch.manual_seed(123)
2025-04-11T03:52:12.6353845Z         # It's necessary to clear cache here.
2025-04-11T03:52:12.6353945Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6354035Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6354039Z 
2025-04-11T03:52:12.6354207Z tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117: 
2025-04-11T03:52:12.6354321Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6354325Z 
2025-04-11T03:52:12.6354404Z device = None
2025-04-11T03:52:12.6354409Z 
2025-04-11T03:52:12.6354522Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6354801Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6354877Z     
2025-04-11T03:52:12.6354951Z         Args:
2025-04-11T03:52:12.6355118Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6355281Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6355391Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6355466Z         """
2025-04-11T03:52:12.6355548Z         _lazy_init()
2025-04-11T03:52:12.6355656Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6355874Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6355988Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6356274Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6356413Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6356578Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6356582Z 
2025-04-11T03:52:12.6356824Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6356995Z ______________ test_flash_decoding[True-False-1-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.6357000Z 
2025-04-11T03:52:12.6357149Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6357320Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6357414Z use_new_kcache_layout = True
2025-04-11T03:52:12.6357418Z 
2025-04-11T03:52:12.6357618Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6357721Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6357837Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6357980Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6358094Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6358215Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6358351Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6358457Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6358591Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6358741Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6358832Z     def test_flash_decoding(
2025-04-11T03:52:12.6358910Z         bsz: int,
2025-04-11T03:52:12.6358994Z         block_size: int,
2025-04-11T03:52:12.6359084Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6359170Z         num_attn_heads: int,
2025-04-11T03:52:12.6359252Z         kv_group_num: int,
2025-04-11T03:52:12.6359335Z         same_context_len: bool,
2025-04-11T03:52:12.6359419Z         q_len: int,
2025-04-11T03:52:12.6359504Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6359596Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6359668Z     ):
2025-04-11T03:52:12.6359776Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6359973Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6360158Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6360333Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6360497Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6360660Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6360733Z     
2025-04-11T03:52:12.6360817Z         torch.manual_seed(123)
2025-04-11T03:52:12.6361018Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6361110Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6361114Z 
2025-04-11T03:52:12.6361274Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6361384Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6361388Z 
2025-04-11T03:52:12.6361469Z device = None
2025-04-11T03:52:12.6361473Z 
2025-04-11T03:52:12.6361588Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6361738Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6361813Z     
2025-04-11T03:52:12.6361978Z         Args:
2025-04-11T03:52:12.6362149Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6362313Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6362423Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6362499Z         """
2025-04-11T03:52:12.6362577Z         _lazy_init()
2025-04-11T03:52:12.6362678Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6362783Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6362892Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6363173Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6363313Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6363472Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6363479Z 
2025-04-11T03:52:12.6363718Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6363888Z _____________ test_flash_decoding[True-False-1-True-1-16-8-16-16] ______________
2025-04-11T03:52:12.6363892Z 
2025-04-11T03:52:12.6364041Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6364212Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6364298Z use_new_kcache_layout = True
2025-04-11T03:52:12.6364302Z 
2025-04-11T03:52:12.6364510Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6364614Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6364734Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6364870Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6364986Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6365111Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6365253Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6365362Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6365501Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6365658Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6365753Z     def test_flash_decoding(
2025-04-11T03:52:12.6365827Z         bsz: int,
2025-04-11T03:52:12.6365912Z         block_size: int,
2025-04-11T03:52:12.6366000Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6366085Z         num_attn_heads: int,
2025-04-11T03:52:12.6366168Z         kv_group_num: int,
2025-04-11T03:52:12.6366253Z         same_context_len: bool,
2025-04-11T03:52:12.6366335Z         q_len: int,
2025-04-11T03:52:12.6366422Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6366513Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6366588Z     ):
2025-04-11T03:52:12.6366698Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6366899Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6367079Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6367445Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6367612Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6367773Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6367847Z     
2025-04-11T03:52:12.6367933Z         torch.manual_seed(123)
2025-04-11T03:52:12.6368028Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6368119Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6368124Z 
2025-04-11T03:52:12.6368284Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6368495Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6368499Z 
2025-04-11T03:52:12.6368586Z device = None
2025-04-11T03:52:12.6368590Z 
2025-04-11T03:52:12.6368705Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6368866Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6368938Z     
2025-04-11T03:52:12.6369013Z         Args:
2025-04-11T03:52:12.6369184Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6369347Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6369456Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6369529Z         """
2025-04-11T03:52:12.6369609Z         _lazy_init()
2025-04-11T03:52:12.6369711Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6369816Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6369923Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6370203Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6370344Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6370503Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6370507Z 
2025-04-11T03:52:12.6370744Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6370912Z ______________ test_flash_decoding[True-False-1-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.6370916Z 
2025-04-11T03:52:12.6371065Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6371234Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6371328Z use_new_kcache_layout = True
2025-04-11T03:52:12.6371332Z 
2025-04-11T03:52:12.6371531Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6371636Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6371759Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6371901Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6372018Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6372137Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6372271Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6372380Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6372514Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6372670Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6372756Z     def test_flash_decoding(
2025-04-11T03:52:12.6372834Z         bsz: int,
2025-04-11T03:52:12.6372924Z         block_size: int,
2025-04-11T03:52:12.6373010Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6373096Z         num_attn_heads: int,
2025-04-11T03:52:12.6373179Z         kv_group_num: int,
2025-04-11T03:52:12.6373263Z         same_context_len: bool,
2025-04-11T03:52:12.6373453Z         q_len: int,
2025-04-11T03:52:12.6373539Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6373630Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6373701Z     ):
2025-04-11T03:52:12.6373812Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6374009Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6374191Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6374365Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6374625Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6374792Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6374864Z     
2025-04-11T03:52:12.6374952Z         torch.manual_seed(123)
2025-04-11T03:52:12.6375053Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6375148Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6375153Z 
2025-04-11T03:52:12.6375313Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6375426Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6375430Z 
2025-04-11T03:52:12.6375512Z device = None
2025-04-11T03:52:12.6375516Z 
2025-04-11T03:52:12.6375633Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6375790Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6375864Z     
2025-04-11T03:52:12.6375943Z         Args:
2025-04-11T03:52:12.6376115Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6376280Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6376392Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6376472Z         """
2025-04-11T03:52:12.6376550Z         _lazy_init()
2025-04-11T03:52:12.6376650Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6376751Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6376865Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6377157Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6377297Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6377455Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6377462Z 
2025-04-11T03:52:12.6377709Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6377880Z _____________ test_flash_decoding[True-False-1-True-1-16-8-32-16] ______________
2025-04-11T03:52:12.6377884Z 
2025-04-11T03:52:12.6378034Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6378206Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6378297Z use_new_kcache_layout = True
2025-04-11T03:52:12.6378301Z 
2025-04-11T03:52:12.6378505Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6378612Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6378735Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6378876Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6378995Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6379113Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6379255Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6379363Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6379500Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6379774Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6379864Z     def test_flash_decoding(
2025-04-11T03:52:12.6379941Z         bsz: int,
2025-04-11T03:52:12.6380031Z         block_size: int,
2025-04-11T03:52:12.6380123Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6380210Z         num_attn_heads: int,
2025-04-11T03:52:12.6380296Z         kv_group_num: int,
2025-04-11T03:52:12.6380387Z         same_context_len: bool,
2025-04-11T03:52:12.6380470Z         q_len: int,
2025-04-11T03:52:12.6380557Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6380652Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6380854Z     ):
2025-04-11T03:52:12.6380967Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6381166Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6381349Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6381532Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6381695Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6381857Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6381929Z     
2025-04-11T03:52:12.6382020Z         torch.manual_seed(123)
2025-04-11T03:52:12.6382109Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6382201Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6382205Z 
2025-04-11T03:52:12.6382365Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6382480Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6382484Z 
2025-04-11T03:52:12.6382567Z device = None
2025-04-11T03:52:12.6382571Z 
2025-04-11T03:52:12.6382689Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6382842Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6382915Z     
2025-04-11T03:52:12.6382990Z         Args:
2025-04-11T03:52:12.6383160Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6383322Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6383432Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6383506Z         """
2025-04-11T03:52:12.6383587Z         _lazy_init()
2025-04-11T03:52:12.6383682Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6383787Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6383896Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6384180Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6384319Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6384479Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6384484Z 
2025-04-11T03:52:12.6384729Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6384895Z _____________ test_flash_decoding[True-False-1-True-1-16-16-16-7] ______________
2025-04-11T03:52:12.6384900Z 
2025-04-11T03:52:12.6385052Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6385217Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6385313Z use_new_kcache_layout = True
2025-04-11T03:52:12.6385317Z 
2025-04-11T03:52:12.6385519Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6385624Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6385748Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6385991Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6386106Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6386226Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6386361Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6386469Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6386607Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6386759Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6386846Z     def test_flash_decoding(
2025-04-11T03:52:12.6387024Z         bsz: int,
2025-04-11T03:52:12.6387113Z         block_size: int,
2025-04-11T03:52:12.6387204Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6387292Z         num_attn_heads: int,
2025-04-11T03:52:12.6387375Z         kv_group_num: int,
2025-04-11T03:52:12.6387460Z         same_context_len: bool,
2025-04-11T03:52:12.6387549Z         q_len: int,
2025-04-11T03:52:12.6387634Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6387726Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6387798Z     ):
2025-04-11T03:52:12.6387913Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6388107Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6388290Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6388501Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6388669Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6388831Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6388905Z     
2025-04-11T03:52:12.6388997Z         torch.manual_seed(123)
2025-04-11T03:52:12.6389089Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6389184Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6389188Z 
2025-04-11T03:52:12.6389346Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6389456Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6389460Z 
2025-04-11T03:52:12.6389541Z device = None
2025-04-11T03:52:12.6389546Z 
2025-04-11T03:52:12.6389661Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6389812Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6389884Z     
2025-04-11T03:52:12.6389958Z         Args:
2025-04-11T03:52:12.6390134Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6390299Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6390414Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6390491Z         """
2025-04-11T03:52:12.6390572Z         _lazy_init()
2025-04-11T03:52:12.6390668Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6390773Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6390882Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6391163Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6391304Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6391459Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6391466Z 
2025-04-11T03:52:12.6391708Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6391876Z _____________ test_flash_decoding[True-False-1-True-1-16-16-16-16] _____________
2025-04-11T03:52:12.6391880Z 
2025-04-11T03:52:12.6392032Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6392320Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6392409Z use_new_kcache_layout = True
2025-04-11T03:52:12.6392414Z 
2025-04-11T03:52:12.6392614Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6392719Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6392841Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6392981Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6393099Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6393320Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6393457Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6393571Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6393710Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6393869Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6393958Z     def test_flash_decoding(
2025-04-11T03:52:12.6394034Z         bsz: int,
2025-04-11T03:52:12.6394121Z         block_size: int,
2025-04-11T03:52:12.6394215Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6394304Z         num_attn_heads: int,
2025-04-11T03:52:12.6394388Z         kv_group_num: int,
2025-04-11T03:52:12.6394478Z         same_context_len: bool,
2025-04-11T03:52:12.6394552Z         q_len: int,
2025-04-11T03:52:12.6394638Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6394731Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6394807Z     ):
2025-04-11T03:52:12.6394921Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6395116Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6395297Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6395476Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6395641Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6395802Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6395876Z     
2025-04-11T03:52:12.6395967Z         torch.manual_seed(123)
2025-04-11T03:52:12.6396057Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6396146Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6396150Z 
2025-04-11T03:52:12.6396305Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6396417Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6396421Z 
2025-04-11T03:52:12.6396504Z device = None
2025-04-11T03:52:12.6396508Z 
2025-04-11T03:52:12.6396624Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6396779Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6396851Z     
2025-04-11T03:52:12.6396924Z         Args:
2025-04-11T03:52:12.6397092Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6397253Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6397365Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6397439Z         """
2025-04-11T03:52:12.6397522Z         _lazy_init()
2025-04-11T03:52:12.6397620Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6397725Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6397836Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6398119Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6398262Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6398532Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6398537Z 
2025-04-11T03:52:12.6398780Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6398945Z _____________ test_flash_decoding[True-False-1-True-1-16-16-32-7] ______________
2025-04-11T03:52:12.6398949Z 
2025-04-11T03:52:12.6399102Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6399263Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6399444Z use_new_kcache_layout = True
2025-04-11T03:52:12.6399449Z 
2025-04-11T03:52:12.6399659Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6399762Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6399884Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6400028Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6400145Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6400260Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6400397Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6400505Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6400640Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6400792Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6400880Z     def test_flash_decoding(
2025-04-11T03:52:12.6400962Z         bsz: int,
2025-04-11T03:52:12.6401042Z         block_size: int,
2025-04-11T03:52:12.6401131Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6401219Z         num_attn_heads: int,
2025-04-11T03:52:12.6401301Z         kv_group_num: int,
2025-04-11T03:52:12.6401391Z         same_context_len: bool,
2025-04-11T03:52:12.6401470Z         q_len: int,
2025-04-11T03:52:12.6401558Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6401649Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6401724Z     ):
2025-04-11T03:52:12.6401839Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6402033Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6402216Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6402393Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6402559Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6402721Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6402793Z     
2025-04-11T03:52:12.6402881Z         torch.manual_seed(123)
2025-04-11T03:52:12.6402971Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6403063Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6403071Z 
2025-04-11T03:52:12.6403225Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6403337Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6403341Z 
2025-04-11T03:52:12.6403426Z device = None
2025-04-11T03:52:12.6403430Z 
2025-04-11T03:52:12.6403548Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6403700Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6403773Z     
2025-04-11T03:52:12.6403849Z         Args:
2025-04-11T03:52:12.6404019Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6404186Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6404297Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6404490Z         """
2025-04-11T03:52:12.6404578Z         _lazy_init()
2025-04-11T03:52:12.6404673Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6404775Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6404886Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6405166Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6405307Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6405464Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6405562Z 
2025-04-11T03:52:12.6405807Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6405975Z _____________ test_flash_decoding[True-False-1-True-1-16-16-32-16] _____________
2025-04-11T03:52:12.6405979Z 
2025-04-11T03:52:12.6406133Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6406300Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6406389Z use_new_kcache_layout = True
2025-04-11T03:52:12.6406394Z 
2025-04-11T03:52:12.6406596Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6406699Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6406818Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6406956Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6407078Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6407195Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6407331Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6407440Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6407574Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6407733Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6407822Z     def test_flash_decoding(
2025-04-11T03:52:12.6407900Z         bsz: int,
2025-04-11T03:52:12.6407983Z         block_size: int,
2025-04-11T03:52:12.6408074Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6408161Z         num_attn_heads: int,
2025-04-11T03:52:12.6408245Z         kv_group_num: int,
2025-04-11T03:52:12.6408333Z         same_context_len: bool,
2025-04-11T03:52:12.6408409Z         q_len: int,
2025-04-11T03:52:12.6408494Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6408586Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6408660Z     ):
2025-04-11T03:52:12.6408777Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6408968Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6409152Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6409324Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6409486Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6409648Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6409720Z     
2025-04-11T03:52:12.6409809Z         torch.manual_seed(123)
2025-04-11T03:52:12.6409898Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6409988Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6409995Z 
2025-04-11T03:52:12.6410149Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6410265Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6410270Z 
2025-04-11T03:52:12.6410353Z device = None
2025-04-11T03:52:12.6410356Z 
2025-04-11T03:52:12.6410476Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6410748Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6410822Z     
2025-04-11T03:52:12.6410899Z         Args:
2025-04-11T03:52:12.6411067Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6411232Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6411345Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6411421Z         """
2025-04-11T03:52:12.6411505Z         _lazy_init()
2025-04-11T03:52:12.6411601Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6411705Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6411921Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6412205Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6412351Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6412515Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6412520Z 
2025-04-11T03:52:12.6412767Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6412939Z ______________ test_flash_decoding[True-False-1-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.6412943Z 
2025-04-11T03:52:12.6413103Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6413269Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6413360Z use_new_kcache_layout = True
2025-04-11T03:52:12.6413371Z 
2025-04-11T03:52:12.6413570Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6413676Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6413800Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6413944Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6414066Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6414183Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6414322Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6414432Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6414571Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6414730Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6414820Z     def test_flash_decoding(
2025-04-11T03:52:12.6414905Z         bsz: int,
2025-04-11T03:52:12.6414991Z         block_size: int,
2025-04-11T03:52:12.6415084Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6415175Z         num_attn_heads: int,
2025-04-11T03:52:12.6415261Z         kv_group_num: int,
2025-04-11T03:52:12.6415355Z         same_context_len: bool,
2025-04-11T03:52:12.6415437Z         q_len: int,
2025-04-11T03:52:12.6415524Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6415620Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6415695Z     ):
2025-04-11T03:52:12.6415812Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6416006Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6416195Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6416371Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6416537Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6416707Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6416782Z     
2025-04-11T03:52:12.6416875Z         torch.manual_seed(123)
2025-04-11T03:52:12.6416968Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6417181Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6417186Z 
2025-04-11T03:52:12.6417340Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6417450Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6417455Z 
2025-04-11T03:52:12.6417536Z device = None
2025-04-11T03:52:12.6417540Z 
2025-04-11T03:52:12.6417665Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6417860Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6417934Z     
2025-04-11T03:52:12.6418011Z         Args:
2025-04-11T03:52:12.6418276Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6418440Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6418551Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6418628Z         """
2025-04-11T03:52:12.6418708Z         _lazy_init()
2025-04-11T03:52:12.6418805Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6418911Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6419017Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6419299Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6419439Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6419600Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6419608Z 
2025-04-11T03:52:12.6419851Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6420019Z _____________ test_flash_decoding[True-False-1-True-4-16-8-16-16] ______________
2025-04-11T03:52:12.6420023Z 
2025-04-11T03:52:12.6420175Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6420340Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6420430Z use_new_kcache_layout = True
2025-04-11T03:52:12.6420437Z 
2025-04-11T03:52:12.6420635Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6420737Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6420859Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6420999Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6421118Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6421233Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6421369Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6421476Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6421617Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6421776Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6421866Z     def test_flash_decoding(
2025-04-11T03:52:12.6421947Z         bsz: int,
2025-04-11T03:52:12.6422029Z         block_size: int,
2025-04-11T03:52:12.6422123Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6422211Z         num_attn_heads: int,
2025-04-11T03:52:12.6422295Z         kv_group_num: int,
2025-04-11T03:52:12.6422387Z         same_context_len: bool,
2025-04-11T03:52:12.6422461Z         q_len: int,
2025-04-11T03:52:12.6422548Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6422644Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6422721Z     ):
2025-04-11T03:52:12.6422842Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6423038Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6423222Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6423501Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6423666Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6423828Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6423900Z     
2025-04-11T03:52:12.6423993Z         torch.manual_seed(123)
2025-04-11T03:52:12.6424081Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6424176Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6424180Z 
2025-04-11T03:52:12.6424337Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6424547Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6424555Z 
2025-04-11T03:52:12.6424631Z device = None
2025-04-11T03:52:12.6424636Z 
2025-04-11T03:52:12.6424755Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6424911Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6424985Z     
2025-04-11T03:52:12.6425062Z         Args:
2025-04-11T03:52:12.6425228Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6425392Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6425503Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6425576Z         """
2025-04-11T03:52:12.6425666Z         _lazy_init()
2025-04-11T03:52:12.6425763Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6425870Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6425978Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6426264Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6426405Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6426563Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6426567Z 
2025-04-11T03:52:12.6426807Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6426970Z ______________ test_flash_decoding[True-False-1-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.6426974Z 
2025-04-11T03:52:12.6427126Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6427288Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6427381Z use_new_kcache_layout = True
2025-04-11T03:52:12.6427388Z 
2025-04-11T03:52:12.6427590Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6427694Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6427817Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6427957Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6428079Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6428191Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6428331Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6428464Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6428602Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6428757Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6428843Z     def test_flash_decoding(
2025-04-11T03:52:12.6428930Z         bsz: int,
2025-04-11T03:52:12.6429015Z         block_size: int,
2025-04-11T03:52:12.6429107Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6429197Z         num_attn_heads: int,
2025-04-11T03:52:12.6429280Z         kv_group_num: int,
2025-04-11T03:52:12.6429370Z         same_context_len: bool,
2025-04-11T03:52:12.6429581Z         q_len: int,
2025-04-11T03:52:12.6429672Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6429761Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6429834Z     ):
2025-04-11T03:52:12.6429948Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6430143Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6430336Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6430508Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6430672Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6430959Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6431031Z     
2025-04-11T03:52:12.6431124Z         torch.manual_seed(123)
2025-04-11T03:52:12.6431215Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6431313Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6431318Z 
2025-04-11T03:52:12.6431471Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6431583Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6431591Z 
2025-04-11T03:52:12.6431668Z device = None
2025-04-11T03:52:12.6431672Z 
2025-04-11T03:52:12.6431788Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6431943Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6432014Z     
2025-04-11T03:52:12.6432091Z         Args:
2025-04-11T03:52:12.6432266Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6432430Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6432548Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6432621Z         """
2025-04-11T03:52:12.6432707Z         _lazy_init()
2025-04-11T03:52:12.6432805Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6432913Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6433022Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6433306Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6433442Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6433599Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6433603Z 
2025-04-11T03:52:12.6433851Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6434016Z _____________ test_flash_decoding[True-False-1-True-4-16-8-32-16] ______________
2025-04-11T03:52:12.6434020Z 
2025-04-11T03:52:12.6434174Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6434338Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6434429Z use_new_kcache_layout = True
2025-04-11T03:52:12.6434433Z 
2025-04-11T03:52:12.6434630Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6434735Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6434855Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6434993Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6435115Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6435230Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6435370Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6435474Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6435608Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6435869Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6435956Z     def test_flash_decoding(
2025-04-11T03:52:12.6436036Z         bsz: int,
2025-04-11T03:52:12.6436117Z         block_size: int,
2025-04-11T03:52:12.6436209Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6436295Z         num_attn_heads: int,
2025-04-11T03:52:12.6436377Z         kv_group_num: int,
2025-04-11T03:52:12.6436468Z         same_context_len: bool,
2025-04-11T03:52:12.6436545Z         q_len: int,
2025-04-11T03:52:12.6436636Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6436726Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6436886Z     ):
2025-04-11T03:52:12.6437002Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6437195Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6437384Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6437560Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6437730Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6437887Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6437960Z     
2025-04-11T03:52:12.6438053Z         torch.manual_seed(123)
2025-04-11T03:52:12.6438144Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6438238Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6438243Z 
2025-04-11T03:52:12.6438398Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6438516Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6438520Z 
2025-04-11T03:52:12.6438597Z device = None
2025-04-11T03:52:12.6438601Z 
2025-04-11T03:52:12.6438717Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6438878Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6438951Z     
2025-04-11T03:52:12.6439030Z         Args:
2025-04-11T03:52:12.6439196Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6439367Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6439474Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6439547Z         """
2025-04-11T03:52:12.6439631Z         _lazy_init()
2025-04-11T03:52:12.6439729Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6439835Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6439947Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6440228Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6440370Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6440529Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6440533Z 
2025-04-11T03:52:12.6440777Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6440943Z _____________ test_flash_decoding[True-False-1-True-4-16-16-16-7] ______________
2025-04-11T03:52:12.6440948Z 
2025-04-11T03:52:12.6441104Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6441270Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6441361Z use_new_kcache_layout = True
2025-04-11T03:52:12.6441368Z 
2025-04-11T03:52:12.6441566Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6441671Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6441792Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6442042Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6442161Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6442275Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6442413Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6442519Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6442652Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6442808Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6442895Z     def test_flash_decoding(
2025-04-11T03:52:12.6442976Z         bsz: int,
2025-04-11T03:52:12.6443165Z         block_size: int,
2025-04-11T03:52:12.6443263Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6443347Z         num_attn_heads: int,
2025-04-11T03:52:12.6443433Z         kv_group_num: int,
2025-04-11T03:52:12.6443523Z         same_context_len: bool,
2025-04-11T03:52:12.6443605Z         q_len: int,
2025-04-11T03:52:12.6443696Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6443783Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6443855Z     ):
2025-04-11T03:52:12.6443973Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6444165Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6444353Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6444524Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6444694Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6444857Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6444930Z     
2025-04-11T03:52:12.6445023Z         torch.manual_seed(123)
2025-04-11T03:52:12.6445115Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6445215Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6445219Z 
2025-04-11T03:52:12.6445371Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6445487Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6445491Z 
2025-04-11T03:52:12.6445569Z device = None
2025-04-11T03:52:12.6445572Z 
2025-04-11T03:52:12.6445688Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6445841Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6445913Z     
2025-04-11T03:52:12.6445994Z         Args:
2025-04-11T03:52:12.6446166Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6446336Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6446442Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6446516Z         """
2025-04-11T03:52:12.6446605Z         _lazy_init()
2025-04-11T03:52:12.6446701Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6446807Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6446912Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6447191Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6447328Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6447484Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6447488Z 
2025-04-11T03:52:12.6447730Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6447894Z _____________ test_flash_decoding[True-False-1-True-4-16-16-16-16] _____________
2025-04-11T03:52:12.6447898Z 
2025-04-11T03:52:12.6448054Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6448327Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6448421Z use_new_kcache_layout = True
2025-04-11T03:52:12.6448426Z 
2025-04-11T03:52:12.6448623Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6448735Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6448851Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6448989Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6449110Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6449326Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6449472Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6449579Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6449718Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6449881Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6449971Z     def test_flash_decoding(
2025-04-11T03:52:12.6450060Z         bsz: int,
2025-04-11T03:52:12.6450146Z         block_size: int,
2025-04-11T03:52:12.6450248Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6450336Z         num_attn_heads: int,
2025-04-11T03:52:12.6450423Z         kv_group_num: int,
2025-04-11T03:52:12.6450519Z         same_context_len: bool,
2025-04-11T03:52:12.6450599Z         q_len: int,
2025-04-11T03:52:12.6450692Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6450783Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6450861Z     ):
2025-04-11T03:52:12.6450982Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6451176Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6451368Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6451546Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6451720Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6451881Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6451956Z     
2025-04-11T03:52:12.6452050Z         torch.manual_seed(123)
2025-04-11T03:52:12.6452143Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6452240Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6452245Z 
2025-04-11T03:52:12.6452401Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6452523Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6452528Z 
2025-04-11T03:52:12.6452608Z device = None
2025-04-11T03:52:12.6452612Z 
2025-04-11T03:52:12.6452730Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6452884Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6452961Z     
2025-04-11T03:52:12.6453043Z         Args:
2025-04-11T03:52:12.6453214Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6453386Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6453497Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6453574Z         """
2025-04-11T03:52:12.6453660Z         _lazy_init()
2025-04-11T03:52:12.6453759Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6453869Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6453981Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6454275Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6454413Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6454682Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6454687Z 
2025-04-11T03:52:12.6454932Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6455097Z _____________ test_flash_decoding[True-False-1-True-4-16-16-32-7] ______________
2025-04-11T03:52:12.6455102Z 
2025-04-11T03:52:12.6455252Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6455413Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6455506Z use_new_kcache_layout = True
2025-04-11T03:52:12.6455597Z 
2025-04-11T03:52:12.6455799Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6455906Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6456023Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6456165Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6456287Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6456400Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6456541Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6456647Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6456786Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6456937Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6457024Z     def test_flash_decoding(
2025-04-11T03:52:12.6457105Z         bsz: int,
2025-04-11T03:52:12.6457190Z         block_size: int,
2025-04-11T03:52:12.6457283Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6457365Z         num_attn_heads: int,
2025-04-11T03:52:12.6457447Z         kv_group_num: int,
2025-04-11T03:52:12.6457538Z         same_context_len: bool,
2025-04-11T03:52:12.6457615Z         q_len: int,
2025-04-11T03:52:12.6457710Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6457798Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6457870Z     ):
2025-04-11T03:52:12.6457983Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6458172Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6458363Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6458532Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6458697Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6458859Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6458934Z     
2025-04-11T03:52:12.6459018Z         torch.manual_seed(123)
2025-04-11T03:52:12.6459106Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6459205Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6459209Z 
2025-04-11T03:52:12.6459365Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6459485Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6459489Z 
2025-04-11T03:52:12.6459565Z device = None
2025-04-11T03:52:12.6459569Z 
2025-04-11T03:52:12.6459691Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6459840Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6459911Z     
2025-04-11T03:52:12.6459989Z         Args:
2025-04-11T03:52:12.6460159Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6460330Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6460434Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6460509Z         """
2025-04-11T03:52:12.6460702Z         _lazy_init()
2025-04-11T03:52:12.6460797Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6460905Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6461009Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6461296Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6461430Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6461589Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6461598Z 
2025-04-11T03:52:12.6462020Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6462189Z _____________ test_flash_decoding[True-False-1-True-4-16-16-32-16] _____________
2025-04-11T03:52:12.6462194Z 
2025-04-11T03:52:12.6462350Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6462513Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6462605Z use_new_kcache_layout = True
2025-04-11T03:52:12.6462609Z 
2025-04-11T03:52:12.6462807Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6462918Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6463036Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6463177Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6463297Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6463414Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6463555Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6463660Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6463800Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6463955Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6464042Z     def test_flash_decoding(
2025-04-11T03:52:12.6464121Z         bsz: int,
2025-04-11T03:52:12.6464203Z         block_size: int,
2025-04-11T03:52:12.6464298Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6464380Z         num_attn_heads: int,
2025-04-11T03:52:12.6464462Z         kv_group_num: int,
2025-04-11T03:52:12.6464551Z         same_context_len: bool,
2025-04-11T03:52:12.6464627Z         q_len: int,
2025-04-11T03:52:12.6464715Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6464802Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6464880Z     ):
2025-04-11T03:52:12.6464994Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6465186Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6465370Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6465547Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6465710Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6465870Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6465948Z     
2025-04-11T03:52:12.6466033Z         torch.manual_seed(123)
2025-04-11T03:52:12.6466126Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6466224Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6466228Z 
2025-04-11T03:52:12.6466380Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6466505Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6466509Z 
2025-04-11T03:52:12.6466586Z device = None
2025-04-11T03:52:12.6466590Z 
2025-04-11T03:52:12.6466716Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6466863Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6467044Z     
2025-04-11T03:52:12.6467125Z         Args:
2025-04-11T03:52:12.6467292Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6467463Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6467569Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6467645Z         """
2025-04-11T03:52:12.6467723Z         _lazy_init()
2025-04-11T03:52:12.6467820Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6467928Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6468130Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6468465Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6468601Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6468760Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6468770Z 
2025-04-11T03:52:12.6469006Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6469175Z _____________ test_flash_decoding[True-False-1-False-1-16-8-16-7] ______________
2025-04-11T03:52:12.6469180Z 
2025-04-11T03:52:12.6469337Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6469501Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6469594Z use_new_kcache_layout = True
2025-04-11T03:52:12.6469601Z 
2025-04-11T03:52:12.6469800Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6469908Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6470025Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6470165Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6470290Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6470405Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6470548Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6470657Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6470801Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6470955Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6471041Z     def test_flash_decoding(
2025-04-11T03:52:12.6471120Z         bsz: int,
2025-04-11T03:52:12.6471206Z         block_size: int,
2025-04-11T03:52:12.6471299Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6471381Z         num_attn_heads: int,
2025-04-11T03:52:12.6471462Z         kv_group_num: int,
2025-04-11T03:52:12.6471553Z         same_context_len: bool,
2025-04-11T03:52:12.6471631Z         q_len: int,
2025-04-11T03:52:12.6471727Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6471816Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6471890Z     ):
2025-04-11T03:52:12.6472004Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6472205Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6472398Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6472568Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6472737Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6472895Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6472971Z     
2025-04-11T03:52:12.6473058Z         torch.manual_seed(123)
2025-04-11T03:52:12.6473150Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6473373Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6473377Z 
2025-04-11T03:52:12.6473533Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6473651Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6473655Z 
2025-04-11T03:52:12.6473733Z device = None
2025-04-11T03:52:12.6473736Z 
2025-04-11T03:52:12.6473860Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6474011Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6474084Z     
2025-04-11T03:52:12.6474162Z         Args:
2025-04-11T03:52:12.6474330Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6474646Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6474753Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6474828Z         """
2025-04-11T03:52:12.6474909Z         _lazy_init()
2025-04-11T03:52:12.6475008Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6475115Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6475221Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6475506Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6475642Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6475803Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6475807Z 
2025-04-11T03:52:12.6476048Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6476218Z _____________ test_flash_decoding[True-False-1-False-1-16-8-16-16] _____________
2025-04-11T03:52:12.6476227Z 
2025-04-11T03:52:12.6476375Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6476539Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6476631Z use_new_kcache_layout = True
2025-04-11T03:52:12.6476635Z 
2025-04-11T03:52:12.6476832Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6476941Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6477057Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6477200Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6477316Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6477428Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6477576Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6477680Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6477818Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6477967Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6478058Z     def test_flash_decoding(
2025-04-11T03:52:12.6478140Z         bsz: int,
2025-04-11T03:52:12.6478220Z         block_size: int,
2025-04-11T03:52:12.6478316Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6478399Z         num_attn_heads: int,
2025-04-11T03:52:12.6478486Z         kv_group_num: int,
2025-04-11T03:52:12.6478570Z         same_context_len: bool,
2025-04-11T03:52:12.6478646Z         q_len: int,
2025-04-11T03:52:12.6478738Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6478827Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6478904Z     ):
2025-04-11T03:52:12.6479018Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6479207Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6479390Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6479670Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6479837Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6479994Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6480074Z     
2025-04-11T03:52:12.6480161Z         torch.manual_seed(123)
2025-04-11T03:52:12.6480253Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6480348Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6480352Z 
2025-04-11T03:52:12.6480505Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6480716Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6480721Z 
2025-04-11T03:52:12.6480796Z device = None
2025-04-11T03:52:12.6480801Z 
2025-04-11T03:52:12.6480920Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6481070Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6481149Z     
2025-04-11T03:52:12.6481227Z         Args:
2025-04-11T03:52:12.6481400Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6481571Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6481679Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6481761Z         """
2025-04-11T03:52:12.6481840Z         _lazy_init()
2025-04-11T03:52:12.6481940Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6482048Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6482159Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6482465Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6482600Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6482764Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6482769Z 
2025-04-11T03:52:12.6483008Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6483176Z _____________ test_flash_decoding[True-False-1-False-1-16-8-32-7] ______________
2025-04-11T03:52:12.6483185Z 
2025-04-11T03:52:12.6483333Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6483500Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6483592Z use_new_kcache_layout = True
2025-04-11T03:52:12.6483599Z 
2025-04-11T03:52:12.6483798Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6483907Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6484023Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6484170Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6484289Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6484401Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6484547Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6484649Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6484791Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6484940Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6485030Z     def test_flash_decoding(
2025-04-11T03:52:12.6485105Z         bsz: int,
2025-04-11T03:52:12.6485191Z         block_size: int,
2025-04-11T03:52:12.6485287Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6485372Z         num_attn_heads: int,
2025-04-11T03:52:12.6485463Z         kv_group_num: int,
2025-04-11T03:52:12.6485549Z         same_context_len: bool,
2025-04-11T03:52:12.6485624Z         q_len: int,
2025-04-11T03:52:12.6485829Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6485916Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6485992Z     ):
2025-04-11T03:52:12.6486107Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6486302Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6486492Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6486667Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6486835Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6487102Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6487178Z     
2025-04-11T03:52:12.6487266Z         torch.manual_seed(123)
2025-04-11T03:52:12.6487357Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6487462Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6487466Z 
2025-04-11T03:52:12.6487620Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6487734Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6487738Z 
2025-04-11T03:52:12.6487814Z device = None
2025-04-11T03:52:12.6487818Z 
2025-04-11T03:52:12.6487943Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6488091Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6488167Z     
2025-04-11T03:52:12.6488240Z         Args:
2025-04-11T03:52:12.6488406Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6488581Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6488687Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6488763Z         """
2025-04-11T03:52:12.6488847Z         _lazy_init()
2025-04-11T03:52:12.6488944Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6489052Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6489158Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6489458Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6489594Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6489754Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6489759Z 
2025-04-11T03:52:12.6489993Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6490168Z _____________ test_flash_decoding[True-False-1-False-1-16-8-32-16] _____________
2025-04-11T03:52:12.6490172Z 
2025-04-11T03:52:12.6490322Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6490491Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6490584Z use_new_kcache_layout = True
2025-04-11T03:52:12.6490588Z 
2025-04-11T03:52:12.6490786Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6490895Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6491011Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6491152Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6491268Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6491382Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6491531Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6491633Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6491775Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6491925Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6492127Z     def test_flash_decoding(
2025-04-11T03:52:12.6492203Z         bsz: int,
2025-04-11T03:52:12.6492284Z         block_size: int,
2025-04-11T03:52:12.6492385Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6492467Z         num_attn_heads: int,
2025-04-11T03:52:12.6492556Z         kv_group_num: int,
2025-04-11T03:52:12.6492642Z         same_context_len: bool,
2025-04-11T03:52:12.6492718Z         q_len: int,
2025-04-11T03:52:12.6492812Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6492902Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6492981Z     ):
2025-04-11T03:52:12.6493185Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6493377Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6493565Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6493739Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6493905Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6494066Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6494141Z     
2025-04-11T03:52:12.6494230Z         torch.manual_seed(123)
2025-04-11T03:52:12.6494320Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6494415Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6494419Z 
2025-04-11T03:52:12.6494574Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6494695Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6494699Z 
2025-04-11T03:52:12.6494776Z device = None
2025-04-11T03:52:12.6494781Z 
2025-04-11T03:52:12.6494901Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6495052Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6495130Z     
2025-04-11T03:52:12.6495203Z         Args:
2025-04-11T03:52:12.6495372Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6495540Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6495645Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6495723Z         """
2025-04-11T03:52:12.6495800Z         _lazy_init()
2025-04-11T03:52:12.6495896Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6496004Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6496113Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6496403Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6496536Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6496701Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6496705Z 
2025-04-11T03:52:12.6496944Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6497112Z _____________ test_flash_decoding[True-False-1-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.6497116Z 
2025-04-11T03:52:12.6497265Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6497428Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6497522Z use_new_kcache_layout = True
2025-04-11T03:52:12.6497530Z 
2025-04-11T03:52:12.6497726Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6497834Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6497951Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6498092Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6498339Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6498453Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6498594Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6498699Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6498842Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6498995Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6499085Z     def test_flash_decoding(
2025-04-11T03:52:12.6499164Z         bsz: int,
2025-04-11T03:52:12.6499340Z         block_size: int,
2025-04-11T03:52:12.6499434Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6499517Z         num_attn_heads: int,
2025-04-11T03:52:12.6499610Z         kv_group_num: int,
2025-04-11T03:52:12.6499695Z         same_context_len: bool,
2025-04-11T03:52:12.6499772Z         q_len: int,
2025-04-11T03:52:12.6499865Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6499953Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6500026Z     ):
2025-04-11T03:52:12.6500135Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6500328Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6500510Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6500681Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6500851Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6501009Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6501088Z     
2025-04-11T03:52:12.6501175Z         torch.manual_seed(123)
2025-04-11T03:52:12.6501271Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6501362Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6501369Z 
2025-04-11T03:52:12.6501525Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6501643Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6501647Z 
2025-04-11T03:52:12.6501724Z device = None
2025-04-11T03:52:12.6501728Z 
2025-04-11T03:52:12.6501852Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6502001Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6502077Z     
2025-04-11T03:52:12.6502149Z         Args:
2025-04-11T03:52:12.6502313Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6502482Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6502587Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6502664Z         """
2025-04-11T03:52:12.6502745Z         _lazy_init()
2025-04-11T03:52:12.6502845Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6502946Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6503050Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6503338Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6503471Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6503627Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6503631Z 
2025-04-11T03:52:12.6503865Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6504040Z ____________ test_flash_decoding[True-False-1-False-1-16-16-16-16] _____________
2025-04-11T03:52:12.6504045Z 
2025-04-11T03:52:12.6504194Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6504466Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6504561Z use_new_kcache_layout = True
2025-04-11T03:52:12.6504566Z 
2025-04-11T03:52:12.6504766Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6504878Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6504996Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6505139Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6505260Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6505373Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6505618Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6505723Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6505866Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6506017Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6506116Z     def test_flash_decoding(
2025-04-11T03:52:12.6506192Z         bsz: int,
2025-04-11T03:52:12.6506280Z         block_size: int,
2025-04-11T03:52:12.6506377Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6506461Z         num_attn_heads: int,
2025-04-11T03:52:12.6506552Z         kv_group_num: int,
2025-04-11T03:52:12.6506635Z         same_context_len: bool,
2025-04-11T03:52:12.6506712Z         q_len: int,
2025-04-11T03:52:12.6506802Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6506892Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6506970Z     ):
2025-04-11T03:52:12.6507082Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6507278Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6507460Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6507635Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6507805Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6507962Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6508041Z     
2025-04-11T03:52:12.6508127Z         torch.manual_seed(123)
2025-04-11T03:52:12.6508221Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6508310Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6508314Z 
2025-04-11T03:52:12.6508512Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6508636Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6508640Z 
2025-04-11T03:52:12.6508716Z device = None
2025-04-11T03:52:12.6508721Z 
2025-04-11T03:52:12.6508842Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6508990Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6509069Z     
2025-04-11T03:52:12.6509144Z         Args:
2025-04-11T03:52:12.6509312Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6509480Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6509586Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6509663Z         """
2025-04-11T03:52:12.6509741Z         _lazy_init()
2025-04-11T03:52:12.6509840Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6509945Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6510054Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6510343Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6510480Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6510765Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6510770Z 
2025-04-11T03:52:12.6511008Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6511178Z _____________ test_flash_decoding[True-False-1-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.6511183Z 
2025-04-11T03:52:12.6511333Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6511498Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6511586Z use_new_kcache_layout = True
2025-04-11T03:52:12.6511699Z 
2025-04-11T03:52:12.6511900Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6512014Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6512132Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6512275Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6512396Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6512512Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6512649Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6512752Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6512892Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6513040Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6513131Z     def test_flash_decoding(
2025-04-11T03:52:12.6513208Z         bsz: int,
2025-04-11T03:52:12.6513293Z         block_size: int,
2025-04-11T03:52:12.6513385Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6513469Z         num_attn_heads: int,
2025-04-11T03:52:12.6513559Z         kv_group_num: int,
2025-04-11T03:52:12.6513642Z         same_context_len: bool,
2025-04-11T03:52:12.6513721Z         q_len: int,
2025-04-11T03:52:12.6513812Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6513900Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6513979Z     ):
2025-04-11T03:52:12.6514091Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6514284Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6514466Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6514640Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6514805Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6514964Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6515043Z     
2025-04-11T03:52:12.6515129Z         torch.manual_seed(123)
2025-04-11T03:52:12.6515223Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6515312Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6515319Z 
2025-04-11T03:52:12.6515471Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6515587Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6515592Z 
2025-04-11T03:52:12.6515669Z device = None
2025-04-11T03:52:12.6515673Z 
2025-04-11T03:52:12.6515792Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6515940Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6516018Z     
2025-04-11T03:52:12.6516092Z         Args:
2025-04-11T03:52:12.6516257Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6516428Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6516533Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6516613Z         """
2025-04-11T03:52:12.6516693Z         _lazy_init()
2025-04-11T03:52:12.6516965Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6517068Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6517174Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6517459Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6517596Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6517757Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6517761Z 
2025-04-11T03:52:12.6518000Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6518297Z ____________ test_flash_decoding[True-False-1-False-1-16-16-32-16] _____________
2025-04-11T03:52:12.6518306Z 
2025-04-11T03:52:12.6518482Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6518656Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6518743Z use_new_kcache_layout = True
2025-04-11T03:52:12.6518747Z 
2025-04-11T03:52:12.6518943Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6519057Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6519174Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6519318Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6519434Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6519553Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6519692Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6519798Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6519939Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6520089Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6520184Z     def test_flash_decoding(
2025-04-11T03:52:12.6520259Z         bsz: int,
2025-04-11T03:52:12.6520346Z         block_size: int,
2025-04-11T03:52:12.6520436Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6520520Z         num_attn_heads: int,
2025-04-11T03:52:12.6520610Z         kv_group_num: int,
2025-04-11T03:52:12.6520694Z         same_context_len: bool,
2025-04-11T03:52:12.6520774Z         q_len: int,
2025-04-11T03:52:12.6520861Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6520950Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6521030Z     ):
2025-04-11T03:52:12.6521142Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6521339Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6521520Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6521693Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6521852Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6522007Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6522083Z     
2025-04-11T03:52:12.6522168Z         torch.manual_seed(123)
2025-04-11T03:52:12.6522259Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6522350Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6522354Z 
2025-04-11T03:52:12.6522507Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6522624Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6522628Z 
2025-04-11T03:52:12.6522703Z device = None
2025-04-11T03:52:12.6522708Z 
2025-04-11T03:52:12.6522831Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6522978Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6523188Z     
2025-04-11T03:52:12.6523265Z         Args:
2025-04-11T03:52:12.6523436Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6523597Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6523702Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6523780Z         """
2025-04-11T03:52:12.6523861Z         _lazy_init()
2025-04-11T03:52:12.6523962Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6524065Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6524270Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6524563Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6524699Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6524867Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6524871Z 
2025-04-11T03:52:12.6525108Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6525281Z _____________ test_flash_decoding[True-False-1-False-4-16-8-16-7] ______________
2025-04-11T03:52:12.6525285Z 
2025-04-11T03:52:12.6525435Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6525599Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6525687Z use_new_kcache_layout = True
2025-04-11T03:52:12.6525694Z 
2025-04-11T03:52:12.6525895Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6526002Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6526121Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6526265Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6526384Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6526503Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6526641Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6526746Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6526887Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6527038Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6527130Z     def test_flash_decoding(
2025-04-11T03:52:12.6527205Z         bsz: int,
2025-04-11T03:52:12.6527299Z         block_size: int,
2025-04-11T03:52:12.6527391Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6527474Z         num_attn_heads: int,
2025-04-11T03:52:12.6527564Z         kv_group_num: int,
2025-04-11T03:52:12.6527651Z         same_context_len: bool,
2025-04-11T03:52:12.6527734Z         q_len: int,
2025-04-11T03:52:12.6527830Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6527918Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6527998Z     ):
2025-04-11T03:52:12.6528110Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6528313Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6528496Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6528674Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6528836Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6528995Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6529072Z     
2025-04-11T03:52:12.6529162Z         torch.manual_seed(123)
2025-04-11T03:52:12.6529261Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6529355Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6529467Z 
2025-04-11T03:52:12.6529627Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6529745Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6529750Z 
2025-04-11T03:52:12.6529828Z device = None
2025-04-11T03:52:12.6529833Z 
2025-04-11T03:52:12.6529954Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6530105Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6530181Z     
2025-04-11T03:52:12.6530255Z         Args:
2025-04-11T03:52:12.6530426Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6530691Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6530799Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6530882Z         """
2025-04-11T03:52:12.6530960Z         _lazy_init()
2025-04-11T03:52:12.6531067Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6531172Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6531278Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6531569Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6531706Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6531867Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6531872Z 
2025-04-11T03:52:12.6532109Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6532282Z _____________ test_flash_decoding[True-False-1-False-4-16-8-16-16] _____________
2025-04-11T03:52:12.6532287Z 
2025-04-11T03:52:12.6532438Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6532608Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6532696Z use_new_kcache_layout = True
2025-04-11T03:52:12.6532700Z 
2025-04-11T03:52:12.6532901Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6533003Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6533121Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6533267Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6533383Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6533502Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6533641Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6533744Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6533886Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6534035Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6534132Z     def test_flash_decoding(
2025-04-11T03:52:12.6534212Z         bsz: int,
2025-04-11T03:52:12.6534299Z         block_size: int,
2025-04-11T03:52:12.6534389Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6534472Z         num_attn_heads: int,
2025-04-11T03:52:12.6534561Z         kv_group_num: int,
2025-04-11T03:52:12.6534646Z         same_context_len: bool,
2025-04-11T03:52:12.6534728Z         q_len: int,
2025-04-11T03:52:12.6534814Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6534904Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6534982Z     ):
2025-04-11T03:52:12.6535093Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6535292Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6535473Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6535648Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6535921Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6536078Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6536157Z     
2025-04-11T03:52:12.6536243Z         torch.manual_seed(123)
2025-04-11T03:52:12.6536339Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6536429Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6536432Z 
2025-04-11T03:52:12.6536594Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6536707Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6536823Z 
2025-04-11T03:52:12.6536903Z device = None
2025-04-11T03:52:12.6536914Z 
2025-04-11T03:52:12.6537031Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6537178Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6537259Z     
2025-04-11T03:52:12.6537331Z         Args:
2025-04-11T03:52:12.6537501Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6537666Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6537774Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6537852Z         """
2025-04-11T03:52:12.6537931Z         _lazy_init()
2025-04-11T03:52:12.6538034Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6538137Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6538251Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6538533Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6538668Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6538834Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6538838Z 
2025-04-11T03:52:12.6539075Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6539248Z _____________ test_flash_decoding[True-False-1-False-4-16-8-32-7] ______________
2025-04-11T03:52:12.6539253Z 
2025-04-11T03:52:12.6539401Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6539568Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6539654Z use_new_kcache_layout = True
2025-04-11T03:52:12.6539657Z 
2025-04-11T03:52:12.6539862Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6539968Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6540086Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6540227Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6540345Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6540464Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6540598Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6540707Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6540843Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6540992Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6541085Z     def test_flash_decoding(
2025-04-11T03:52:12.6541162Z         bsz: int,
2025-04-11T03:52:12.6541252Z         block_size: int,
2025-04-11T03:52:12.6541349Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6541435Z         num_attn_heads: int,
2025-04-11T03:52:12.6541525Z         kv_group_num: int,
2025-04-11T03:52:12.6541610Z         same_context_len: bool,
2025-04-11T03:52:12.6541687Z         q_len: int,
2025-04-11T03:52:12.6541885Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6541979Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6542058Z     ):
2025-04-11T03:52:12.6542169Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6542369Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6542555Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6542733Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6542896Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6543168Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6543245Z     
2025-04-11T03:52:12.6543332Z         torch.manual_seed(123)
2025-04-11T03:52:12.6543425Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6543516Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6543523Z 
2025-04-11T03:52:12.6543679Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6543794Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6543798Z 
2025-04-11T03:52:12.6543878Z device = None
2025-04-11T03:52:12.6543882Z 
2025-04-11T03:52:12.6544000Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6544146Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6544224Z     
2025-04-11T03:52:12.6544298Z         Args:
2025-04-11T03:52:12.6544473Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6544639Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6544745Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6544823Z         """
2025-04-11T03:52:12.6544903Z         _lazy_init()
2025-04-11T03:52:12.6545011Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6545116Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6545229Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6545509Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6545646Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6545807Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6545811Z 
2025-04-11T03:52:12.6546048Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6546223Z _____________ test_flash_decoding[True-False-1-False-4-16-8-32-16] _____________
2025-04-11T03:52:12.6546228Z 
2025-04-11T03:52:12.6546378Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6546544Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6546636Z use_new_kcache_layout = True
2025-04-11T03:52:12.6546640Z 
2025-04-11T03:52:12.6546840Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6546944Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6547061Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6547205Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6547321Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6547440Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6547579Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6547689Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6547828Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6547979Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6548182Z     def test_flash_decoding(
2025-04-11T03:52:12.6548259Z         bsz: int,
2025-04-11T03:52:12.6548344Z         block_size: int,
2025-04-11T03:52:12.6548478Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6548566Z         num_attn_heads: int,
2025-04-11T03:52:12.6548653Z         kv_group_num: int,
2025-04-11T03:52:12.6548738Z         same_context_len: bool,
2025-04-11T03:52:12.6548820Z         q_len: int,
2025-04-11T03:52:12.6548907Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6549002Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6549075Z     ):
2025-04-11T03:52:12.6549187Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6549487Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6549674Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6549849Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6550017Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6550178Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6550252Z     
2025-04-11T03:52:12.6550338Z         torch.manual_seed(123)
2025-04-11T03:52:12.6550430Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6550520Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6550524Z 
2025-04-11T03:52:12.6550681Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6550797Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6550804Z 
2025-04-11T03:52:12.6550889Z device = None
2025-04-11T03:52:12.6550893Z 
2025-04-11T03:52:12.6551009Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6551162Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6551245Z     
2025-04-11T03:52:12.6551320Z         Args:
2025-04-11T03:52:12.6551492Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6551657Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6551771Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6551845Z         """
2025-04-11T03:52:12.6551924Z         _lazy_init()
2025-04-11T03:52:12.6552029Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6552132Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6552246Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6552533Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6552672Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6552833Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6552837Z 
2025-04-11T03:52:12.6553073Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6553249Z _____________ test_flash_decoding[True-False-1-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.6553253Z 
2025-04-11T03:52:12.6553404Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6553573Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6553659Z use_new_kcache_layout = True
2025-04-11T03:52:12.6553663Z 
2025-04-11T03:52:12.6553868Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6553971Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6554087Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6554229Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6554469Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6554588Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6554727Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6554836Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6554973Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6555125Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6555219Z     def test_flash_decoding(
2025-04-11T03:52:12.6555294Z         bsz: int,
2025-04-11T03:52:12.6555379Z         block_size: int,
2025-04-11T03:52:12.6555570Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6555660Z         num_attn_heads: int,
2025-04-11T03:52:12.6555745Z         kv_group_num: int,
2025-04-11T03:52:12.6555832Z         same_context_len: bool,
2025-04-11T03:52:12.6555917Z         q_len: int,
2025-04-11T03:52:12.6556007Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6556103Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6556176Z     ):
2025-04-11T03:52:12.6556290Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6556489Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6556671Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6556847Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6557011Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6557174Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6557247Z     
2025-04-11T03:52:12.6557334Z         torch.manual_seed(123)
2025-04-11T03:52:12.6557430Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6557522Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6557528Z 
2025-04-11T03:52:12.6557688Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6557799Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6557803Z 
2025-04-11T03:52:12.6557884Z device = None
2025-04-11T03:52:12.6557888Z 
2025-04-11T03:52:12.6558004Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6558154Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6558228Z     
2025-04-11T03:52:12.6558302Z         Args:
2025-04-11T03:52:12.6558473Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6558638Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6558748Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6558821Z         """
2025-04-11T03:52:12.6558898Z         _lazy_init()
2025-04-11T03:52:12.6559001Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6559106Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6559216Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6559500Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6559636Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6559795Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6559799Z 
2025-04-11T03:52:12.6560038Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6560215Z ____________ test_flash_decoding[True-False-1-False-4-16-16-16-16] _____________
2025-04-11T03:52:12.6560219Z 
2025-04-11T03:52:12.6560371Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6560542Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6560731Z use_new_kcache_layout = True
2025-04-11T03:52:12.6560736Z 
2025-04-11T03:52:12.6560941Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6561047Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6561169Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6561310Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6561427Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6561545Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6561892Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6562003Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6562138Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6562291Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6562388Z     def test_flash_decoding(
2025-04-11T03:52:12.6562465Z         bsz: int,
2025-04-11T03:52:12.6562552Z         block_size: int,
2025-04-11T03:52:12.6562645Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6562734Z         num_attn_heads: int,
2025-04-11T03:52:12.6562817Z         kv_group_num: int,
2025-04-11T03:52:12.6562901Z         same_context_len: bool,
2025-04-11T03:52:12.6562985Z         q_len: int,
2025-04-11T03:52:12.6563084Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6563179Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6563253Z     ):
2025-04-11T03:52:12.6563367Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6563571Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6563752Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6563936Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6564105Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6564263Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6564335Z     
2025-04-11T03:52:12.6564421Z         torch.manual_seed(123)
2025-04-11T03:52:12.6564516Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6564608Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6564612Z 
2025-04-11T03:52:12.6564772Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6564887Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6564894Z 
2025-04-11T03:52:12.6564976Z device = None
2025-04-11T03:52:12.6564981Z 
2025-04-11T03:52:12.6565096Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6565257Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6565338Z     
2025-04-11T03:52:12.6565412Z         Args:
2025-04-11T03:52:12.6565586Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6565753Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6565867Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6565942Z         """
2025-04-11T03:52:12.6566020Z         _lazy_init()
2025-04-11T03:52:12.6566122Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6566224Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6566337Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6566627Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6566768Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6566928Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6567041Z 
2025-04-11T03:52:12.6567283Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6567460Z _____________ test_flash_decoding[True-False-1-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.6567464Z 
2025-04-11T03:52:12.6567617Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6567789Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6567882Z use_new_kcache_layout = True
2025-04-11T03:52:12.6567886Z 
2025-04-11T03:52:12.6568196Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6568302Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6568427Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6568566Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6568689Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6568812Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6568948Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6569060Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6569200Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6569357Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6569446Z     def test_flash_decoding(
2025-04-11T03:52:12.6569525Z         bsz: int,
2025-04-11T03:52:12.6569615Z         block_size: int,
2025-04-11T03:52:12.6569713Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6569802Z         num_attn_heads: int,
2025-04-11T03:52:12.6569888Z         kv_group_num: int,
2025-04-11T03:52:12.6569976Z         same_context_len: bool,
2025-04-11T03:52:12.6570062Z         q_len: int,
2025-04-11T03:52:12.6570149Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6570250Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6570324Z     ):
2025-04-11T03:52:12.6570438Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6570637Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6570819Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6570996Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6571158Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6571423Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6571580Z     
2025-04-11T03:52:12.6571699Z         torch.manual_seed(123)
2025-04-11T03:52:12.6571862Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6571998Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6572009Z 
2025-04-11T03:52:12.6572226Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6572383Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6572388Z 
2025-04-11T03:52:12.6572495Z device = None
2025-04-11T03:52:12.6572501Z 
2025-04-11T03:52:12.6572682Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6572870Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6572988Z     
2025-04-11T03:52:12.6573100Z         Args:
2025-04-11T03:52:12.6573344Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6573540Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6573673Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6573818Z         """
2025-04-11T03:52:12.6573911Z         _lazy_init()
2025-04-11T03:52:12.6574195Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6574329Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6574603Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6574925Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6575093Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6575312Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6575317Z 
2025-04-11T03:52:12.6575595Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6575950Z ____________ test_flash_decoding[True-False-1-False-4-16-16-32-16] _____________
2025-04-11T03:52:12.6575955Z 
2025-04-11T03:52:12.6576135Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6576359Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.6576467Z use_new_kcache_layout = True
2025-04-11T03:52:12.6576472Z 
2025-04-11T03:52:12.6576761Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6576894Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6577041Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6577242Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6577388Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6577560Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6577749Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6577912Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6578076Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6578255Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6578391Z     def test_flash_decoding(
2025-04-11T03:52:12.6578503Z         bsz: int,
2025-04-11T03:52:12.6578740Z         block_size: int,
2025-04-11T03:52:12.6578856Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6579003Z         num_attn_heads: int,
2025-04-11T03:52:12.6579118Z         kv_group_num: int,
2025-04-11T03:52:12.6579223Z         same_context_len: bool,
2025-04-11T03:52:12.6579392Z         q_len: int,
2025-04-11T03:52:12.6579511Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6579662Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6579767Z     ):
2025-04-11T03:52:12.6579911Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6580173Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6580402Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6580633Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6580831Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6581055Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6581154Z     
2025-04-11T03:52:12.6581316Z         torch.manual_seed(123)
2025-04-11T03:52:12.6581433Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6581551Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6581556Z 
2025-04-11T03:52:12.6581767Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6581920Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6581928Z 
2025-04-11T03:52:12.6582058Z device = None
2025-04-11T03:52:12.6582063Z 
2025-04-11T03:52:12.6582297Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6582512Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6582730Z     
2025-04-11T03:52:12.6582846Z         Args:
2025-04-11T03:52:12.6583064Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6583270Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6583456Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6583559Z         """
2025-04-11T03:52:12.6583709Z         _lazy_init()
2025-04-11T03:52:12.6583839Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6583957Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6584143Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6584570Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6584775Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6584960Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6584967Z 
2025-04-11T03:52:12.6585252Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6585455Z ______________ test_flash_decoding[True-False-5-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.6585460Z 
2025-04-11T03:52:12.6585693Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6585885Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6586001Z use_new_kcache_layout = True
2025-04-11T03:52:12.6586038Z 
2025-04-11T03:52:12.6586347Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6586467Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6586664Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6586840Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6587024Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6587172Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6587357Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6587503Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6587693Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6587907Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6588027Z     def test_flash_decoding(
2025-04-11T03:52:12.6588173Z         bsz: int,
2025-04-11T03:52:12.6588271Z         block_size: int,
2025-04-11T03:52:12.6588494Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6588608Z         num_attn_heads: int,
2025-04-11T03:52:12.6588718Z         kv_group_num: int,
2025-04-11T03:52:12.6588864Z         same_context_len: bool,
2025-04-11T03:52:12.6588969Z         q_len: int,
2025-04-11T03:52:12.6589111Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6589253Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6589353Z     ):
2025-04-11T03:52:12.6589533Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6589760Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6590067Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6590286Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6590516Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6590711Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6590814Z     
2025-04-11T03:52:12.6590970Z         torch.manual_seed(123)
2025-04-11T03:52:12.6591075Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6591261Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6591387Z 
2025-04-11T03:52:12.6591571Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6591746Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6591751Z 
2025-04-11T03:52:12.6591859Z device = None
2025-04-11T03:52:12.6591863Z 
2025-04-11T03:52:12.6592037Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6592223Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6592337Z     
2025-04-11T03:52:12.6592468Z         Args:
2025-04-11T03:52:12.6592666Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6593028Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6593151Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6593302Z         """
2025-04-11T03:52:12.6593414Z         _lazy_init()
2025-04-11T03:52:12.6593622Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6593795Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6593929Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6594274Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6594453Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6594682Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6594687Z 
2025-04-11T03:52:12.6594958Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6595161Z _____________ test_flash_decoding[True-False-5-True-1-16-8-16-16] ______________
2025-04-11T03:52:12.6595199Z 
2025-04-11T03:52:12.6595377Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6595582Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6595756Z use_new_kcache_layout = True
2025-04-11T03:52:12.6595760Z 
2025-04-11T03:52:12.6595999Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6596162Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6596306Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6596503Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6596658Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6596809Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6597011Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6597143Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6597325Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6597513Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6597682Z     def test_flash_decoding(
2025-04-11T03:52:12.6597873Z         bsz: int,
2025-04-11T03:52:12.6597984Z         block_size: int,
2025-04-11T03:52:12.6598136Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6598239Z         num_attn_heads: int,
2025-04-11T03:52:12.6598421Z         kv_group_num: int,
2025-04-11T03:52:12.6598538Z         same_context_len: bool,
2025-04-11T03:52:12.6598677Z         q_len: int,
2025-04-11T03:52:12.6598799Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6598922Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6599053Z     ):
2025-04-11T03:52:12.6599217Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6599477Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6599696Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6599899Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6600217Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6600422Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6600565Z     
2025-04-11T03:52:12.6600683Z         torch.manual_seed(123)
2025-04-11T03:52:12.6600846Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6600966Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6600970Z 
2025-04-11T03:52:12.6601185Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6601445Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6601562Z 
2025-04-11T03:52:12.6601673Z device = None
2025-04-11T03:52:12.6601679Z 
2025-04-11T03:52:12.6601830Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6621668Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6621811Z     
2025-04-11T03:52:12.6621910Z         Args:
2025-04-11T03:52:12.6622110Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6622342Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6622504Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6622586Z         """
2025-04-11T03:52:12.6622678Z         _lazy_init()
2025-04-11T03:52:12.6622786Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6622902Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6623015Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6623342Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6623497Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6623666Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6623678Z 
2025-04-11T03:52:12.6623943Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6624120Z ______________ test_flash_decoding[True-False-5-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.6624126Z 
2025-04-11T03:52:12.6624294Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6624467Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6624568Z use_new_kcache_layout = True
2025-04-11T03:52:12.6624574Z 
2025-04-11T03:52:12.6624785Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6624902Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6625030Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6625177Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6625312Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6625432Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6625577Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6625688Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6625832Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6625995Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6626094Z     def test_flash_decoding(
2025-04-11T03:52:12.6626184Z         bsz: int,
2025-04-11T03:52:12.6626275Z         block_size: int,
2025-04-11T03:52:12.6626386Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6626476Z         num_attn_heads: int,
2025-04-11T03:52:12.6626564Z         kv_group_num: int,
2025-04-11T03:52:12.6626662Z         same_context_len: bool,
2025-04-11T03:52:12.6626753Z         q_len: int,
2025-04-11T03:52:12.6626848Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6627160Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6627240Z     ):
2025-04-11T03:52:12.6627367Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6627572Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6627771Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6627954Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6628125Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6628486Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6628564Z     
2025-04-11T03:52:12.6628671Z         torch.manual_seed(123)
2025-04-11T03:52:12.6628769Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6628871Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6628880Z 
2025-04-11T03:52:12.6629040Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6629164Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6629169Z 
2025-04-11T03:52:12.6629250Z device = None
2025-04-11T03:52:12.6629255Z 
2025-04-11T03:52:12.6629380Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6629541Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6629617Z     
2025-04-11T03:52:12.6629701Z         Args:
2025-04-11T03:52:12.6629877Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6630058Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6630171Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6630248Z         """
2025-04-11T03:52:12.6630345Z         _lazy_init()
2025-04-11T03:52:12.6630448Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6630558Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6630669Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6630962Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6631109Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6631274Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6631279Z 
2025-04-11T03:52:12.6631527Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6631708Z _____________ test_flash_decoding[True-False-5-True-1-16-8-32-16] ______________
2025-04-11T03:52:12.6631713Z 
2025-04-11T03:52:12.6631871Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6632038Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6632137Z use_new_kcache_layout = True
2025-04-11T03:52:12.6632141Z 
2025-04-11T03:52:12.6632346Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6632457Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6632576Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6632719Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6632845Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6632962Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6633108Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6633215Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6633355Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6633516Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6633732Z     def test_flash_decoding(
2025-04-11T03:52:12.6633815Z         bsz: int,
2025-04-11T03:52:12.6633901Z         block_size: int,
2025-04-11T03:52:12.6634000Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6634087Z         num_attn_heads: int,
2025-04-11T03:52:12.6634174Z         kv_group_num: int,
2025-04-11T03:52:12.6634268Z         same_context_len: bool,
2025-04-11T03:52:12.6634348Z         q_len: int,
2025-04-11T03:52:12.6634443Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6634533Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6634609Z     ):
2025-04-11T03:52:12.6634729Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6635045Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6635238Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6655989Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6656208Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6656376Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6656450Z     
2025-04-11T03:52:12.6656549Z         torch.manual_seed(123)
2025-04-11T03:52:12.6656646Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6656742Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6656746Z 
2025-04-11T03:52:12.6656908Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6657031Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6657039Z 
2025-04-11T03:52:12.6657118Z device = None
2025-04-11T03:52:12.6657124Z 
2025-04-11T03:52:12.6657254Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6657414Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6657494Z     
2025-04-11T03:52:12.6657572Z         Args:
2025-04-11T03:52:12.6657742Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6657924Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6658039Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6658111Z         """
2025-04-11T03:52:12.6658195Z         _lazy_init()
2025-04-11T03:52:12.6658292Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6658400Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6658506Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6658807Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6658952Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6659110Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6659119Z 
2025-04-11T03:52:12.6659364Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6659536Z _____________ test_flash_decoding[True-False-5-True-1-16-16-16-7] ______________
2025-04-11T03:52:12.6659540Z 
2025-04-11T03:52:12.6659697Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6659858Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6659953Z use_new_kcache_layout = True
2025-04-11T03:52:12.6659958Z 
2025-04-11T03:52:12.6660160Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6660273Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6660394Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6660535Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6660782Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6660897Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6661041Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6661147Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6661287Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6661439Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6661530Z     def test_flash_decoding(
2025-04-11T03:52:12.6661610Z         bsz: int,
2025-04-11T03:52:12.6661694Z         block_size: int,
2025-04-11T03:52:12.6661880Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6661964Z         num_attn_heads: int,
2025-04-11T03:52:12.6662046Z         kv_group_num: int,
2025-04-11T03:52:12.6662137Z         same_context_len: bool,
2025-04-11T03:52:12.6662213Z         q_len: int,
2025-04-11T03:52:12.6662303Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6662395Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6662468Z     ):
2025-04-11T03:52:12.6662585Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6662780Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6662980Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6663152Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6663320Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6663485Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6663556Z     
2025-04-11T03:52:12.6663649Z         torch.manual_seed(123)
2025-04-11T03:52:12.6663740Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6663836Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6663844Z 
2025-04-11T03:52:12.6664003Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6664119Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6664124Z 
2025-04-11T03:52:12.6664200Z device = None
2025-04-11T03:52:12.6664204Z 
2025-04-11T03:52:12.6664327Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6664478Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6664549Z     
2025-04-11T03:52:12.6664636Z         Args:
2025-04-11T03:52:12.6664804Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6664986Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6665100Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6665177Z         """
2025-04-11T03:52:12.6665264Z         _lazy_init()
2025-04-11T03:52:12.6665362Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6665478Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6665588Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6665883Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6666024Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6666182Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6666193Z 
2025-04-11T03:52:12.6666436Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6666607Z _____________ test_flash_decoding[True-False-5-True-1-16-16-16-16] _____________
2025-04-11T03:52:12.6666611Z 
2025-04-11T03:52:12.6666792Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6666968Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6667183Z use_new_kcache_layout = True
2025-04-11T03:52:12.6667188Z 
2025-04-11T03:52:12.6667390Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6667502Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6667625Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6667767Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6667890Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6668003Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6668263Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6668369Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6668552Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6668705Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6668798Z     def test_flash_decoding(
2025-04-11T03:52:12.6668879Z         bsz: int,
2025-04-11T03:52:12.6668963Z         block_size: int,
2025-04-11T03:52:12.6689606Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6689718Z         num_attn_heads: int,
2025-04-11T03:52:12.6689804Z         kv_group_num: int,
2025-04-11T03:52:12.6689895Z         same_context_len: bool,
2025-04-11T03:52:12.6689971Z         q_len: int,
2025-04-11T03:52:12.6690059Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6690148Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6690222Z     ):
2025-04-11T03:52:12.6690337Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6690536Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6690721Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6690891Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6691060Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6691217Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6691291Z     
2025-04-11T03:52:12.6691380Z         torch.manual_seed(123)
2025-04-11T03:52:12.6691469Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6691565Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6691569Z 
2025-04-11T03:52:12.6691727Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6691844Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6691851Z 
2025-04-11T03:52:12.6691928Z device = None
2025-04-11T03:52:12.6691932Z 
2025-04-11T03:52:12.6692056Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6692205Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6692277Z     
2025-04-11T03:52:12.6692354Z         Args:
2025-04-11T03:52:12.6692522Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6692689Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6692797Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6692873Z         """
2025-04-11T03:52:12.6692952Z         _lazy_init()
2025-04-11T03:52:12.6693051Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6693158Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6693266Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6693558Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6693697Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6693854Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6693982Z 
2025-04-11T03:52:12.6694225Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6694395Z _____________ test_flash_decoding[True-False-5-True-1-16-16-32-7] ______________
2025-04-11T03:52:12.6694399Z 
2025-04-11T03:52:12.6694555Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6694718Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6694811Z use_new_kcache_layout = True
2025-04-11T03:52:12.6694814Z 
2025-04-11T03:52:12.6695013Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6695226Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6695344Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6695483Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6695608Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6695725Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6695870Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6695980Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6696124Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6696278Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6696367Z     def test_flash_decoding(
2025-04-11T03:52:12.6696451Z         bsz: int,
2025-04-11T03:52:12.6696536Z         block_size: int,
2025-04-11T03:52:12.6696636Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6696722Z         num_attn_heads: int,
2025-04-11T03:52:12.6696807Z         kv_group_num: int,
2025-04-11T03:52:12.6696900Z         same_context_len: bool,
2025-04-11T03:52:12.6696979Z         q_len: int,
2025-04-11T03:52:12.6697073Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6697167Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6697246Z     ):
2025-04-11T03:52:12.6697361Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6697558Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6697751Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6697926Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6698095Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6698264Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6698341Z     
2025-04-11T03:52:12.6698438Z         torch.manual_seed(123)
2025-04-11T03:52:12.6698532Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6698626Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6698637Z 
2025-04-11T03:52:12.6698796Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6698913Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6698918Z 
2025-04-11T03:52:12.6699003Z device = None
2025-04-11T03:52:12.6699007Z 
2025-04-11T03:52:12.6699128Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6699284Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6699360Z     
2025-04-11T03:52:12.6699440Z         Args:
2025-04-11T03:52:12.6699610Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6699783Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6699899Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6699977Z         """
2025-04-11T03:52:12.6700060Z         _lazy_init()
2025-04-11T03:52:12.6700161Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6700367Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6700480Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6700770Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6700923Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6701082Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6701086Z 
2025-04-11T03:52:12.6701332Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6701593Z _____________ test_flash_decoding[True-False-5-True-1-16-16-32-16] _____________
2025-04-11T03:52:12.6701597Z 
2025-04-11T03:52:12.6701755Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6701919Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6702015Z use_new_kcache_layout = True
2025-04-11T03:52:12.6702019Z 
2025-04-11T03:52:12.6702227Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6702331Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6702455Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6702592Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6702716Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6702833Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6702972Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6703084Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6703219Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6703375Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6703467Z     def test_flash_decoding(
2025-04-11T03:52:12.6703549Z         bsz: int,
2025-04-11T03:52:12.6703632Z         block_size: int,
2025-04-11T03:52:12.6703721Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6703810Z         num_attn_heads: int,
2025-04-11T03:52:12.6703891Z         kv_group_num: int,
2025-04-11T03:52:12.6703978Z         same_context_len: bool,
2025-04-11T03:52:12.6704054Z         q_len: int,
2025-04-11T03:52:12.6704140Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6704232Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6704304Z     ):
2025-04-11T03:52:12.6704419Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6704618Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6704802Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6704977Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6705141Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6705303Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6705374Z     
2025-04-11T03:52:12.6705468Z         torch.manual_seed(123)
2025-04-11T03:52:12.6705559Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6705649Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6705657Z 
2025-04-11T03:52:12.6705813Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6705924Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6705931Z 
2025-04-11T03:52:12.6706012Z device = None
2025-04-11T03:52:12.6706016Z 
2025-04-11T03:52:12.6706136Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6706292Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6706473Z     
2025-04-11T03:52:12.6706553Z         Args:
2025-04-11T03:52:12.6706723Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6706889Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6707001Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6707076Z         """
2025-04-11T03:52:12.6707159Z         _lazy_init()
2025-04-11T03:52:12.6707256Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6707357Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6707466Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6707848Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6707991Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6708152Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6708159Z 
2025-04-11T03:52:12.6708404Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6708622Z ______________ test_flash_decoding[True-False-5-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.6708626Z 
2025-04-11T03:52:12.6708783Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6708951Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6709041Z use_new_kcache_layout = True
2025-04-11T03:52:12.6709046Z 
2025-04-11T03:52:12.6709250Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6709358Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6709480Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6709621Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6709746Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6709860Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6709996Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6710104Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6710239Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6710396Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6710484Z     def test_flash_decoding(
2025-04-11T03:52:12.6710563Z         bsz: int,
2025-04-11T03:52:12.6710646Z         block_size: int,
2025-04-11T03:52:12.6710739Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6710829Z         num_attn_heads: int,
2025-04-11T03:52:12.6710912Z         kv_group_num: int,
2025-04-11T03:52:12.6711002Z         same_context_len: bool,
2025-04-11T03:52:12.6711076Z         q_len: int,
2025-04-11T03:52:12.6711161Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6711256Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6711329Z     ):
2025-04-11T03:52:12.6711443Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6711635Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6711820Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6711990Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6712151Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6712315Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6712387Z     
2025-04-11T03:52:12.6712477Z         torch.manual_seed(123)
2025-04-11T03:52:12.6712567Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6712660Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6712667Z 
2025-04-11T03:52:12.6712958Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6713073Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6713077Z 
2025-04-11T03:52:12.6713160Z device = None
2025-04-11T03:52:12.6713165Z 
2025-04-11T03:52:12.6713281Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6713441Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6713514Z     
2025-04-11T03:52:12.6713593Z         Args:
2025-04-11T03:52:12.6713761Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6714041Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6714153Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6714229Z         """
2025-04-11T03:52:12.6714314Z         _lazy_init()
2025-04-11T03:52:12.6714413Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6714522Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6714633Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6714917Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6715058Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6715216Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6715220Z 
2025-04-11T03:52:12.6715463Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6715631Z _____________ test_flash_decoding[True-False-5-True-4-16-8-16-16] ______________
2025-04-11T03:52:12.6715636Z 
2025-04-11T03:52:12.6715790Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6715953Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6716044Z use_new_kcache_layout = True
2025-04-11T03:52:12.6716053Z 
2025-04-11T03:52:12.6716255Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6716357Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6716482Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6716623Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6716744Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6716856Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6716990Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6717104Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6717240Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6717397Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6717489Z     def test_flash_decoding(
2025-04-11T03:52:12.6717569Z         bsz: int,
2025-04-11T03:52:12.6717651Z         block_size: int,
2025-04-11T03:52:12.6717741Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6717828Z         num_attn_heads: int,
2025-04-11T03:52:12.6717910Z         kv_group_num: int,
2025-04-11T03:52:12.6718002Z         same_context_len: bool,
2025-04-11T03:52:12.6718080Z         q_len: int,
2025-04-11T03:52:12.6718167Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6718263Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6718335Z     ):
2025-04-11T03:52:12.6718451Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6718646Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6718834Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6719007Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6719281Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6719446Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6719538Z     
2025-04-11T03:52:12.6719653Z         torch.manual_seed(123)
2025-04-11T03:52:12.6719748Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6719846Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6719850Z 
2025-04-11T03:52:12.6720008Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6720122Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6720221Z 
2025-04-11T03:52:12.6720308Z device = None
2025-04-11T03:52:12.6720313Z 
2025-04-11T03:52:12.6720431Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6720586Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6720660Z     
2025-04-11T03:52:12.6720737Z         Args:
2025-04-11T03:52:12.6720903Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6721070Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6721182Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6721255Z         """
2025-04-11T03:52:12.6721335Z         _lazy_init()
2025-04-11T03:52:12.6721432Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6721542Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6721649Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6721940Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6722081Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6722239Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6722247Z 
2025-04-11T03:52:12.6722491Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6722661Z ______________ test_flash_decoding[True-False-5-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.6722666Z 
2025-04-11T03:52:12.6722819Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6722981Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6723074Z use_new_kcache_layout = True
2025-04-11T03:52:12.6723078Z 
2025-04-11T03:52:12.6723276Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6723383Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6723504Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6723641Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6723764Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6723881Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6724025Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6724129Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6724262Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6724415Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6724503Z     def test_flash_decoding(
2025-04-11T03:52:12.6724583Z         bsz: int,
2025-04-11T03:52:12.6724666Z         block_size: int,
2025-04-11T03:52:12.6724757Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6724849Z         num_attn_heads: int,
2025-04-11T03:52:12.6724932Z         kv_group_num: int,
2025-04-11T03:52:12.6725024Z         same_context_len: bool,
2025-04-11T03:52:12.6725100Z         q_len: int,
2025-04-11T03:52:12.6725185Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6725386Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6725459Z     ):
2025-04-11T03:52:12.6725577Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6725772Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6725960Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6726132Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6726293Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6726457Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6726624Z     
2025-04-11T03:52:12.6726718Z         torch.manual_seed(123)
2025-04-11T03:52:12.6726809Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6726907Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6726913Z 
2025-04-11T03:52:12.6727069Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6727184Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6727193Z 
2025-04-11T03:52:12.6727272Z device = None
2025-04-11T03:52:12.6727277Z 
2025-04-11T03:52:12.6727396Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6727550Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6727623Z     
2025-04-11T03:52:12.6727702Z         Args:
2025-04-11T03:52:12.6727868Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6728035Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6728149Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6728221Z         """
2025-04-11T03:52:12.6728306Z         _lazy_init()
2025-04-11T03:52:12.6728405Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6728513Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6728620Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6728905Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6729048Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6729207Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6729211Z 
2025-04-11T03:52:12.6729451Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6729622Z _____________ test_flash_decoding[True-False-5-True-4-16-8-32-16] ______________
2025-04-11T03:52:12.6729626Z 
2025-04-11T03:52:12.6729780Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6729941Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6730037Z use_new_kcache_layout = True
2025-04-11T03:52:12.6730041Z 
2025-04-11T03:52:12.6730239Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6730343Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6730466Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6730605Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6730729Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6730843Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6730985Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6731093Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6731227Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6731383Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6731605Z     def test_flash_decoding(
2025-04-11T03:52:12.6731687Z         bsz: int,
2025-04-11T03:52:12.6731770Z         block_size: int,
2025-04-11T03:52:12.6731860Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6731948Z         num_attn_heads: int,
2025-04-11T03:52:12.6732034Z         kv_group_num: int,
2025-04-11T03:52:12.6732124Z         same_context_len: bool,
2025-04-11T03:52:12.6732201Z         q_len: int,
2025-04-11T03:52:12.6732289Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6732380Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6732453Z     ):
2025-04-11T03:52:12.6732568Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6732864Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6733052Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6733224Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6733388Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6733548Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6733619Z     
2025-04-11T03:52:12.6733712Z         torch.manual_seed(123)
2025-04-11T03:52:12.6733804Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6733901Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6733906Z 
2025-04-11T03:52:12.6734061Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6734173Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6734186Z 
2025-04-11T03:52:12.6734262Z device = None
2025-04-11T03:52:12.6734267Z 
2025-04-11T03:52:12.6734384Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6734538Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6734610Z     
2025-04-11T03:52:12.6734695Z         Args:
2025-04-11T03:52:12.6734863Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6735028Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6735139Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6735210Z         """
2025-04-11T03:52:12.6735296Z         _lazy_init()
2025-04-11T03:52:12.6735391Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6735499Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6735603Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6735889Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6736029Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6736186Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6736193Z 
2025-04-11T03:52:12.6736435Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6736601Z _____________ test_flash_decoding[True-False-5-True-4-16-16-16-7] ______________
2025-04-11T03:52:12.6736605Z 
2025-04-11T03:52:12.6736758Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6736921Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6737011Z use_new_kcache_layout = True
2025-04-11T03:52:12.6737015Z 
2025-04-11T03:52:12.6737217Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6737323Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6737448Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6737588Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6737709Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6737948Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6738086Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6738190Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6738325Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6738485Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6738575Z     def test_flash_decoding(
2025-04-11T03:52:12.6738653Z         bsz: int,
2025-04-11T03:52:12.6738736Z         block_size: int,
2025-04-11T03:52:12.6738830Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6739007Z         num_attn_heads: int,
2025-04-11T03:52:12.6739090Z         kv_group_num: int,
2025-04-11T03:52:12.6739184Z         same_context_len: bool,
2025-04-11T03:52:12.6739261Z         q_len: int,
2025-04-11T03:52:12.6739350Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6739439Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6739517Z     ):
2025-04-11T03:52:12.6739635Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6739829Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6740016Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6740186Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6740351Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6740507Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6740583Z     
2025-04-11T03:52:12.6740674Z         torch.manual_seed(123)
2025-04-11T03:52:12.6740763Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6740855Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6740859Z 
2025-04-11T03:52:12.6741014Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6741131Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6741135Z 
2025-04-11T03:52:12.6741211Z device = None
2025-04-11T03:52:12.6741215Z 
2025-04-11T03:52:12.6741332Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6741485Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6741557Z     
2025-04-11T03:52:12.6741636Z         Args:
2025-04-11T03:52:12.6741802Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6741976Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6742080Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6742153Z         """
2025-04-11T03:52:12.6742236Z         _lazy_init()
2025-04-11T03:52:12.6742332Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6742444Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6742550Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6742832Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6742974Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6743129Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6743133Z 
2025-04-11T03:52:12.6743373Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6743545Z _____________ test_flash_decoding[True-False-5-True-4-16-16-16-16] _____________
2025-04-11T03:52:12.6743549Z 
2025-04-11T03:52:12.6743705Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6743866Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6744069Z use_new_kcache_layout = True
2025-04-11T03:52:12.6744073Z 
2025-04-11T03:52:12.6744274Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6744382Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6744499Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6744636Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6744757Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6744871Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6745007Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6745205Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6745345Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6745505Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6745597Z     def test_flash_decoding(
2025-04-11T03:52:12.6745686Z         bsz: int,
2025-04-11T03:52:12.6745774Z         block_size: int,
2025-04-11T03:52:12.6745873Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6745960Z         num_attn_heads: int,
2025-04-11T03:52:12.6746046Z         kv_group_num: int,
2025-04-11T03:52:12.6746145Z         same_context_len: bool,
2025-04-11T03:52:12.6746225Z         q_len: int,
2025-04-11T03:52:12.6746317Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6746409Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6746486Z     ):
2025-04-11T03:52:12.6746605Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6746807Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6746996Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6747175Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6747351Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6747512Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6747592Z     
2025-04-11T03:52:12.6747687Z         torch.manual_seed(123)
2025-04-11T03:52:12.6747779Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6747880Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6747884Z 
2025-04-11T03:52:12.6748040Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6748157Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6748165Z 
2025-04-11T03:52:12.6748244Z device = None
2025-04-11T03:52:12.6748248Z 
2025-04-11T03:52:12.6748368Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6748563Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6748634Z     
2025-04-11T03:52:12.6748716Z         Args:
2025-04-11T03:52:12.6748881Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6749051Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6749156Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6749228Z         """
2025-04-11T03:52:12.6749311Z         _lazy_init()
2025-04-11T03:52:12.6749406Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6749513Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6749618Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6749902Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6750042Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6750200Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6750316Z 
2025-04-11T03:52:12.6750558Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6750724Z _____________ test_flash_decoding[True-False-5-True-4-16-16-32-7] ______________
2025-04-11T03:52:12.6750728Z 
2025-04-11T03:52:12.6750881Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6751043Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6751134Z use_new_kcache_layout = True
2025-04-11T03:52:12.6751138Z 
2025-04-11T03:52:12.6751336Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6751638Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6751756Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6751897Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6752019Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6752134Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6752276Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6752379Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6752514Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6752670Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6752759Z     def test_flash_decoding(
2025-04-11T03:52:12.6752839Z         bsz: int,
2025-04-11T03:52:12.6752921Z         block_size: int,
2025-04-11T03:52:12.6753013Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6753098Z         num_attn_heads: int,
2025-04-11T03:52:12.6753181Z         kv_group_num: int,
2025-04-11T03:52:12.6753274Z         same_context_len: bool,
2025-04-11T03:52:12.6753349Z         q_len: int,
2025-04-11T03:52:12.6753440Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6753526Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6753601Z     ):
2025-04-11T03:52:12.6753719Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6753910Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6754094Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6754261Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6754424Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6754582Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6754657Z     
2025-04-11T03:52:12.6754746Z         torch.manual_seed(123)
2025-04-11T03:52:12.6754835Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6754932Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6754936Z 
2025-04-11T03:52:12.6755090Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6755209Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6755214Z 
2025-04-11T03:52:12.6755291Z device = None
2025-04-11T03:52:12.6755295Z 
2025-04-11T03:52:12.6755411Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6755560Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6755630Z     
2025-04-11T03:52:12.6755708Z         Args:
2025-04-11T03:52:12.6755872Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6756041Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6756146Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6756219Z         """
2025-04-11T03:52:12.6756300Z         _lazy_init()
2025-04-11T03:52:12.6756396Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6756617Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6756724Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6757011Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6757145Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6757303Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6757307Z 
2025-04-11T03:52:12.6757548Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6757830Z _____________ test_flash_decoding[True-False-5-True-4-16-16-32-16] _____________
2025-04-11T03:52:12.6757834Z 
2025-04-11T03:52:12.6757993Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6758155Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6758248Z use_new_kcache_layout = True
2025-04-11T03:52:12.6758252Z 
2025-04-11T03:52:12.6758452Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6758559Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6758677Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6758813Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6758935Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6759048Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6759185Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6759294Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6759434Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6759584Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6759669Z     def test_flash_decoding(
2025-04-11T03:52:12.6759753Z         bsz: int,
2025-04-11T03:52:12.6759834Z         block_size: int,
2025-04-11T03:52:12.6759926Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6760007Z         num_attn_heads: int,
2025-04-11T03:52:12.6760089Z         kv_group_num: int,
2025-04-11T03:52:12.6760179Z         same_context_len: bool,
2025-04-11T03:52:12.6760254Z         q_len: int,
2025-04-11T03:52:12.6760341Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6760429Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6760503Z     ):
2025-04-11T03:52:12.6760616Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6760815Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6761004Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6761175Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6761346Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6761502Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6761577Z     
2025-04-11T03:52:12.6761663Z         torch.manual_seed(123)
2025-04-11T03:52:12.6761751Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6761848Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6761852Z 
2025-04-11T03:52:12.6762005Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6762119Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6762126Z 
2025-04-11T03:52:12.6762201Z device = None
2025-04-11T03:52:12.6762205Z 
2025-04-11T03:52:12.6762325Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6762475Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6762545Z     
2025-04-11T03:52:12.6762733Z         Args:
2025-04-11T03:52:12.6762901Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6763074Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6763180Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6763252Z         """
2025-04-11T03:52:12.6763334Z         _lazy_init()
2025-04-11T03:52:12.6763432Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6763541Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6763646Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6764031Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6764167Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6764322Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6764333Z 
2025-04-11T03:52:12.6764570Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6764736Z _____________ test_flash_decoding[True-False-5-False-1-16-8-16-7] ______________
2025-04-11T03:52:12.6764740Z 
2025-04-11T03:52:12.6764893Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6765058Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6765151Z use_new_kcache_layout = True
2025-04-11T03:52:12.6765155Z 
2025-04-11T03:52:12.6765352Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6765465Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6765582Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6765720Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6765841Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6765958Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6766095Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6766196Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6766336Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6766486Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6766575Z     def test_flash_decoding(
2025-04-11T03:52:12.6766656Z         bsz: int,
2025-04-11T03:52:12.6766736Z         block_size: int,
2025-04-11T03:52:12.6766830Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6766917Z         num_attn_heads: int,
2025-04-11T03:52:12.6766998Z         kv_group_num: int,
2025-04-11T03:52:12.6767086Z         same_context_len: bool,
2025-04-11T03:52:12.6767161Z         q_len: int,
2025-04-11T03:52:12.6767251Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6767338Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6767418Z     ):
2025-04-11T03:52:12.6767532Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6767724Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6767908Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6768078Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6768242Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6768397Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6768476Z     
2025-04-11T03:52:12.6768563Z         torch.manual_seed(123)
2025-04-11T03:52:12.6768653Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6768748Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6768753Z 
2025-04-11T03:52:12.6768907Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6769135Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6769140Z 
2025-04-11T03:52:12.6769217Z device = None
2025-04-11T03:52:12.6769221Z 
2025-04-11T03:52:12.6769342Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6769492Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6769564Z     
2025-04-11T03:52:12.6769643Z         Args:
2025-04-11T03:52:12.6769811Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6769981Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6770182Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6770260Z         """
2025-04-11T03:52:12.6770340Z         _lazy_init()
2025-04-11T03:52:12.6770443Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6770556Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6770666Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6770958Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6771097Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6771258Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6771269Z 
2025-04-11T03:52:12.6771510Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6771682Z _____________ test_flash_decoding[True-False-5-False-1-16-8-16-16] _____________
2025-04-11T03:52:12.6771686Z 
2025-04-11T03:52:12.6771846Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6772011Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6772110Z use_new_kcache_layout = True
2025-04-11T03:52:12.6772114Z 
2025-04-11T03:52:12.6772314Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6772424Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6772543Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6772686Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6772812Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6772928Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6773073Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6773184Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6773329Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6773483Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6773573Z     def test_flash_decoding(
2025-04-11T03:52:12.6773659Z         bsz: int,
2025-04-11T03:52:12.6773745Z         block_size: int,
2025-04-11T03:52:12.6773844Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6773931Z         num_attn_heads: int,
2025-04-11T03:52:12.6774022Z         kv_group_num: int,
2025-04-11T03:52:12.6774113Z         same_context_len: bool,
2025-04-11T03:52:12.6774194Z         q_len: int,
2025-04-11T03:52:12.6774285Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6774376Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6774458Z     ):
2025-04-11T03:52:12.6774573Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6774766Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6774956Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6775130Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6775400Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6775557Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6775632Z     
2025-04-11T03:52:12.6775718Z         torch.manual_seed(123)
2025-04-11T03:52:12.6775808Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6775905Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6775909Z 
2025-04-11T03:52:12.6776064Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6776181Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6776277Z 
2025-04-11T03:52:12.6776355Z device = None
2025-04-11T03:52:12.6776360Z 
2025-04-11T03:52:12.6776481Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6776631Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6776702Z     
2025-04-11T03:52:12.6776783Z         Args:
2025-04-11T03:52:12.6776945Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6777112Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6777217Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6777295Z         """
2025-04-11T03:52:12.6777374Z         _lazy_init()
2025-04-11T03:52:12.6777469Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6777577Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6777683Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6777968Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6778106Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6778270Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6778277Z 
2025-04-11T03:52:12.6778515Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6778682Z _____________ test_flash_decoding[True-False-5-False-1-16-8-32-7] ______________
2025-04-11T03:52:12.6778690Z 
2025-04-11T03:52:12.6778838Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6779000Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6779094Z use_new_kcache_layout = True
2025-04-11T03:52:12.6779098Z 
2025-04-11T03:52:12.6779293Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6779404Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6779522Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6779665Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6779780Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6779897Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6780037Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6780142Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6780281Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6780432Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6780521Z     def test_flash_decoding(
2025-04-11T03:52:12.6780602Z         bsz: int,
2025-04-11T03:52:12.6780683Z         block_size: int,
2025-04-11T03:52:12.6780777Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6780863Z         num_attn_heads: int,
2025-04-11T03:52:12.6780950Z         kv_group_num: int,
2025-04-11T03:52:12.6781037Z         same_context_len: bool,
2025-04-11T03:52:12.6781114Z         q_len: int,
2025-04-11T03:52:12.6781205Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6781292Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6781490Z     ):
2025-04-11T03:52:12.6781601Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6781793Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6781977Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6782148Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6782315Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6782470Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6782666Z     
2025-04-11T03:52:12.6782752Z         torch.manual_seed(123)
2025-04-11T03:52:12.6782841Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6782943Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6782947Z 
2025-04-11T03:52:12.6783099Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6783221Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6783226Z 
2025-04-11T03:52:12.6783303Z device = None
2025-04-11T03:52:12.6783308Z 
2025-04-11T03:52:12.6783425Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6783573Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6783644Z     
2025-04-11T03:52:12.6783723Z         Args:
2025-04-11T03:52:12.6783887Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6784056Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6784164Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6784240Z         """
2025-04-11T03:52:12.6784321Z         _lazy_init()
2025-04-11T03:52:12.6784419Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6784534Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6784642Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6784926Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6785061Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6785218Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6785222Z 
2025-04-11T03:52:12.6785461Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6785632Z _____________ test_flash_decoding[True-False-5-False-1-16-8-32-16] _____________
2025-04-11T03:52:12.6785641Z 
2025-04-11T03:52:12.6785792Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6785959Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6786059Z use_new_kcache_layout = True
2025-04-11T03:52:12.6786063Z 
2025-04-11T03:52:12.6786261Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6786372Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6786489Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6786631Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6786748Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6786862Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6787008Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6787116Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6787256Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6787405Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6787496Z     def test_flash_decoding(
2025-04-11T03:52:12.6787681Z         bsz: int,
2025-04-11T03:52:12.6787763Z         block_size: int,
2025-04-11T03:52:12.6787858Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6787939Z         num_attn_heads: int,
2025-04-11T03:52:12.6788023Z         kv_group_num: int,
2025-04-11T03:52:12.6788107Z         same_context_len: bool,
2025-04-11T03:52:12.6788181Z         q_len: int,
2025-04-11T03:52:12.6788270Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6788358Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6788476Z     ):
2025-04-11T03:52:12.6788589Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6788782Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6789099Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6789270Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6789439Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6789596Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6789671Z     
2025-04-11T03:52:12.6789758Z         torch.manual_seed(123)
2025-04-11T03:52:12.6789847Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6789944Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6789948Z 
2025-04-11T03:52:12.6790101Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6790217Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6790224Z 
2025-04-11T03:52:12.6790301Z device = None
2025-04-11T03:52:12.6790305Z 
2025-04-11T03:52:12.6790424Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6790572Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6790646Z     
2025-04-11T03:52:12.6790723Z         Args:
2025-04-11T03:52:12.6790889Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6791057Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6791163Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6791240Z         """
2025-04-11T03:52:12.6791318Z         _lazy_init()
2025-04-11T03:52:12.6791413Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6791521Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6791625Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6791912Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6792051Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6792212Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6792219Z 
2025-04-11T03:52:12.6792458Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6792628Z _____________ test_flash_decoding[True-False-5-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.6792633Z 
2025-04-11T03:52:12.6792779Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6792942Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6793033Z use_new_kcache_layout = True
2025-04-11T03:52:12.6793037Z 
2025-04-11T03:52:12.6793234Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6793345Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6793462Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6793604Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6793721Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6793947Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6794090Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6794196Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6794336Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6794484Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6794575Z     def test_flash_decoding(
2025-04-11T03:52:12.6794654Z         bsz: int,
2025-04-11T03:52:12.6794735Z         block_size: int,
2025-04-11T03:52:12.6794828Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6795008Z         num_attn_heads: int,
2025-04-11T03:52:12.6795094Z         kv_group_num: int,
2025-04-11T03:52:12.6795179Z         same_context_len: bool,
2025-04-11T03:52:12.6795256Z         q_len: int,
2025-04-11T03:52:12.6795349Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6795436Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6795516Z     ):
2025-04-11T03:52:12.6795627Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6795822Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6796007Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6796179Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6796348Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6796507Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6796584Z     
2025-04-11T03:52:12.6796670Z         torch.manual_seed(123)
2025-04-11T03:52:12.6796758Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6796855Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6796858Z 
2025-04-11T03:52:12.6797012Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6797133Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6797137Z 
2025-04-11T03:52:12.6797213Z device = None
2025-04-11T03:52:12.6797217Z 
2025-04-11T03:52:12.6797336Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6797485Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6797557Z     
2025-04-11T03:52:12.6797632Z         Args:
2025-04-11T03:52:12.6797799Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6797969Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6798076Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6798154Z         """
2025-04-11T03:52:12.6798231Z         _lazy_init()
2025-04-11T03:52:12.6798325Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6798434Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6798539Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6798827Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6798961Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6799121Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6799126Z 
2025-04-11T03:52:12.6799365Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6799539Z ____________ test_flash_decoding[True-False-5-False-1-16-16-16-16] _____________
2025-04-11T03:52:12.6799547Z 
2025-04-11T03:52:12.6799697Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6799860Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6800067Z use_new_kcache_layout = True
2025-04-11T03:52:12.6800071Z 
2025-04-11T03:52:12.6800269Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6800377Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6800494Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6800636Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6800752Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6800864Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6801003Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6801205Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6801343Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6801494Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6801585Z     def test_flash_decoding(
2025-04-11T03:52:12.6801663Z         bsz: int,
2025-04-11T03:52:12.6801744Z         block_size: int,
2025-04-11T03:52:12.6801838Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6801922Z         num_attn_heads: int,
2025-04-11T03:52:12.6802010Z         kv_group_num: int,
2025-04-11T03:52:12.6802099Z         same_context_len: bool,
2025-04-11T03:52:12.6802175Z         q_len: int,
2025-04-11T03:52:12.6802266Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6802354Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6802432Z     ):
2025-04-11T03:52:12.6802544Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6802736Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6802921Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6803092Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6803264Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6803419Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6803498Z     
2025-04-11T03:52:12.6803583Z         torch.manual_seed(123)
2025-04-11T03:52:12.6803675Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6803764Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6803769Z 
2025-04-11T03:52:12.6803921Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6804039Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6804046Z 
2025-04-11T03:52:12.6804124Z device = None
2025-04-11T03:52:12.6804128Z 
2025-04-11T03:52:12.6804250Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6804400Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6804475Z     
2025-04-11T03:52:12.6804548Z         Args:
2025-04-11T03:52:12.6804716Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6804885Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6804992Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6805070Z         """
2025-04-11T03:52:12.6805149Z         _lazy_init()
2025-04-11T03:52:12.6805252Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6805355Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6805460Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6805750Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6805889Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6806047Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6806161Z 
2025-04-11T03:52:12.6806399Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6806569Z _____________ test_flash_decoding[True-False-5-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.6806574Z 
2025-04-11T03:52:12.6806721Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6806883Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6806975Z use_new_kcache_layout = True
2025-04-11T03:52:12.6806979Z 
2025-04-11T03:52:12.6807177Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6807380Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6807497Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6807639Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6807758Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6807879Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6808015Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6808123Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6808266Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6808418Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6808509Z     def test_flash_decoding(
2025-04-11T03:52:12.6808586Z         bsz: int,
2025-04-11T03:52:12.6808664Z         block_size: int,
2025-04-11T03:52:12.6808757Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6808847Z         num_attn_heads: int,
2025-04-11T03:52:12.6808934Z         kv_group_num: int,
2025-04-11T03:52:12.6809018Z         same_context_len: bool,
2025-04-11T03:52:12.6809092Z         q_len: int,
2025-04-11T03:52:12.6809180Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6809269Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6809351Z     ):
2025-04-11T03:52:12.6809463Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6809657Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6809838Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6810010Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6810178Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6810334Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6810415Z     
2025-04-11T03:52:12.6810500Z         torch.manual_seed(123)
2025-04-11T03:52:12.6810592Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6810684Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6810688Z 
2025-04-11T03:52:12.6810845Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6810964Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6810968Z 
2025-04-11T03:52:12.6811044Z device = None
2025-04-11T03:52:12.6811049Z 
2025-04-11T03:52:12.6811172Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6811322Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6811396Z     
2025-04-11T03:52:12.6811470Z         Args:
2025-04-11T03:52:12.6811639Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6811810Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6811919Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6811996Z         """
2025-04-11T03:52:12.6812073Z         _lazy_init()
2025-04-11T03:52:12.6812171Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6812378Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6812484Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6812773Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6812907Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6813065Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6813070Z 
2025-04-11T03:52:12.6813308Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6813487Z ____________ test_flash_decoding[True-False-5-False-1-16-16-32-16] _____________
2025-04-11T03:52:12.6813599Z 
2025-04-11T03:52:12.6813755Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6813924Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6814016Z use_new_kcache_layout = True
2025-04-11T03:52:12.6814020Z 
2025-04-11T03:52:12.6814217Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6814327Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6814443Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6814587Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6814703Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6814821Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6814956Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6815064Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6815206Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6815355Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6815445Z     def test_flash_decoding(
2025-04-11T03:52:12.6815523Z         bsz: int,
2025-04-11T03:52:12.6815603Z         block_size: int,
2025-04-11T03:52:12.6815696Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6815780Z         num_attn_heads: int,
2025-04-11T03:52:12.6815869Z         kv_group_num: int,
2025-04-11T03:52:12.6815954Z         same_context_len: bool,
2025-04-11T03:52:12.6816033Z         q_len: int,
2025-04-11T03:52:12.6816118Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6816208Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6816289Z     ):
2025-04-11T03:52:12.6816399Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6816592Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6816774Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6816944Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6817113Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6817270Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6817347Z     
2025-04-11T03:52:12.6817434Z         torch.manual_seed(123)
2025-04-11T03:52:12.6817526Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6817616Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6817620Z 
2025-04-11T03:52:12.6817773Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6817892Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6817899Z 
2025-04-11T03:52:12.6817976Z device = None
2025-04-11T03:52:12.6817980Z 
2025-04-11T03:52:12.6818099Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6818251Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6818328Z     
2025-04-11T03:52:12.6818403Z         Args:
2025-04-11T03:52:12.6818682Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6818852Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6818959Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6819036Z         """
2025-04-11T03:52:12.6819114Z         _lazy_init()
2025-04-11T03:52:12.6819216Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6819318Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6819423Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6819712Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6819958Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6820154Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6820166Z 
2025-04-11T03:52:12.6820417Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6820591Z _____________ test_flash_decoding[True-False-5-False-4-16-8-16-7] ______________
2025-04-11T03:52:12.6820595Z 
2025-04-11T03:52:12.6820743Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6820911Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6820997Z use_new_kcache_layout = True
2025-04-11T03:52:12.6821001Z 
2025-04-11T03:52:12.6821198Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6821309Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6821426Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6821573Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6821690Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6821812Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6821949Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6822054Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6822192Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6822342Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6822434Z     def test_flash_decoding(
2025-04-11T03:52:12.6822508Z         bsz: int,
2025-04-11T03:52:12.6822592Z         block_size: int,
2025-04-11T03:52:12.6822680Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6822765Z         num_attn_heads: int,
2025-04-11T03:52:12.6822850Z         kv_group_num: int,
2025-04-11T03:52:12.6822936Z         same_context_len: bool,
2025-04-11T03:52:12.6823018Z         q_len: int,
2025-04-11T03:52:12.6823103Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6823193Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6823273Z     ):
2025-04-11T03:52:12.6823384Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6823577Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6823758Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6823932Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6824097Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6824255Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6824335Z     
2025-04-11T03:52:12.6824421Z         torch.manual_seed(123)
2025-04-11T03:52:12.6824516Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6824609Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6824613Z 
2025-04-11T03:52:12.6824766Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6824994Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6824998Z 
2025-04-11T03:52:12.6825076Z device = None
2025-04-11T03:52:12.6825080Z 
2025-04-11T03:52:12.6825202Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6825351Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6825430Z     
2025-04-11T03:52:12.6825503Z         Args:
2025-04-11T03:52:12.6825673Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6825835Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6826037Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6826116Z         """
2025-04-11T03:52:12.6826195Z         _lazy_init()
2025-04-11T03:52:12.6826296Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6826398Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6826507Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6826790Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6826926Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6827088Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6827093Z 
2025-04-11T03:52:12.6827333Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6827506Z _____________ test_flash_decoding[True-False-5-False-4-16-8-16-16] _____________
2025-04-11T03:52:12.6827514Z 
2025-04-11T03:52:12.6827662Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6827831Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6827922Z use_new_kcache_layout = True
2025-04-11T03:52:12.6827926Z 
2025-04-11T03:52:12.6828127Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6828233Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6828349Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6828547Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6828663Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6828784Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6828920Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6829029Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6829169Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6829321Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6829414Z     def test_flash_decoding(
2025-04-11T03:52:12.6829493Z         bsz: int,
2025-04-11T03:52:12.6829581Z         block_size: int,
2025-04-11T03:52:12.6829669Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6829751Z         num_attn_heads: int,
2025-04-11T03:52:12.6829839Z         kv_group_num: int,
2025-04-11T03:52:12.6829924Z         same_context_len: bool,
2025-04-11T03:52:12.6830002Z         q_len: int,
2025-04-11T03:52:12.6830088Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6830176Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6830253Z     ):
2025-04-11T03:52:12.6830363Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6830560Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6830746Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6830921Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6831216Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6831376Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6831453Z     
2025-04-11T03:52:12.6831543Z         torch.manual_seed(123)
2025-04-11T03:52:12.6831637Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6831730Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6831734Z 
2025-04-11T03:52:12.6831893Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6832004Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6832008Z 
2025-04-11T03:52:12.6832197Z device = None
2025-04-11T03:52:12.6832202Z 
2025-04-11T03:52:12.6832324Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6832478Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6832555Z     
2025-04-11T03:52:12.6832628Z         Args:
2025-04-11T03:52:12.6832805Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6832970Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6833079Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6833155Z         """
2025-04-11T03:52:12.6833234Z         _lazy_init()
2025-04-11T03:52:12.6833332Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6833435Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6833540Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6833828Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6833970Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6834132Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6834140Z 
2025-04-11T03:52:12.6834383Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6834553Z _____________ test_flash_decoding[True-False-5-False-4-16-8-32-7] ______________
2025-04-11T03:52:12.6834557Z 
2025-04-11T03:52:12.6834706Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6834874Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6834962Z use_new_kcache_layout = True
2025-04-11T03:52:12.6834966Z 
2025-04-11T03:52:12.6835172Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6835280Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6835399Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6835543Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6835662Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6835782Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6835919Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6836023Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6836161Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6836313Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6836404Z     def test_flash_decoding(
2025-04-11T03:52:12.6836477Z         bsz: int,
2025-04-11T03:52:12.6836561Z         block_size: int,
2025-04-11T03:52:12.6836650Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6836736Z         num_attn_heads: int,
2025-04-11T03:52:12.6836824Z         kv_group_num: int,
2025-04-11T03:52:12.6836909Z         same_context_len: bool,
2025-04-11T03:52:12.6836987Z         q_len: int,
2025-04-11T03:52:12.6837073Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6837161Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6837349Z     ):
2025-04-11T03:52:12.6837460Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6837656Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6837838Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6838015Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6838179Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6838334Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6838507Z     
2025-04-11T03:52:12.6838594Z         torch.manual_seed(123)
2025-04-11T03:52:12.6838688Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6838778Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6838783Z 
2025-04-11T03:52:12.6838939Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6839055Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6839060Z 
2025-04-11T03:52:12.6839136Z device = None
2025-04-11T03:52:12.6839145Z 
2025-04-11T03:52:12.6839261Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6839411Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6839488Z     
2025-04-11T03:52:12.6839560Z         Args:
2025-04-11T03:52:12.6839729Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6839895Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6840010Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6840088Z         """
2025-04-11T03:52:12.6840166Z         _lazy_init()
2025-04-11T03:52:12.6840265Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6840367Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6840476Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6840756Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6840893Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6841055Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6841059Z 
2025-04-11T03:52:12.6841294Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6841465Z _____________ test_flash_decoding[True-False-5-False-4-16-8-32-16] _____________
2025-04-11T03:52:12.6841472Z 
2025-04-11T03:52:12.6841621Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6841787Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6841873Z use_new_kcache_layout = True
2025-04-11T03:52:12.6841881Z 
2025-04-11T03:52:12.6842082Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6842185Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6842302Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6842445Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6842561Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6842677Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6842815Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6842923Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6843059Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6843210Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6843303Z     def test_flash_decoding(
2025-04-11T03:52:12.6843484Z         bsz: int,
2025-04-11T03:52:12.6843568Z         block_size: int,
2025-04-11T03:52:12.6843660Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6843743Z         num_attn_heads: int,
2025-04-11T03:52:12.6843831Z         kv_group_num: int,
2025-04-11T03:52:12.6843917Z         same_context_len: bool,
2025-04-11T03:52:12.6843996Z         q_len: int,
2025-04-11T03:52:12.6844083Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6844169Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6844245Z     ):
2025-04-11T03:52:12.6844356Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6844554Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6844839Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6845015Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6845179Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6845340Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6845415Z     
2025-04-11T03:52:12.6845501Z         torch.manual_seed(123)
2025-04-11T03:52:12.6845594Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6845684Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6845688Z 
2025-04-11T03:52:12.6845844Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6845957Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6845960Z 
2025-04-11T03:52:12.6846042Z device = None
2025-04-11T03:52:12.6846047Z 
2025-04-11T03:52:12.6846162Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6846312Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6846391Z     
2025-04-11T03:52:12.6846466Z         Args:
2025-04-11T03:52:12.6846640Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6846805Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6846912Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6846988Z         """
2025-04-11T03:52:12.6847064Z         _lazy_init()
2025-04-11T03:52:12.6847165Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6847267Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6847374Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6847661Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6847802Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6847962Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6847966Z 
2025-04-11T03:52:12.6848206Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6848377Z _____________ test_flash_decoding[True-False-5-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.6848382Z 
2025-04-11T03:52:12.6848531Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6848699Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6848786Z use_new_kcache_layout = True
2025-04-11T03:52:12.6848790Z 
2025-04-11T03:52:12.6848992Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6849101Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6849218Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6849360Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6849475Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6849792Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6849929Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6850036Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6850172Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6850322Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6850414Z     def test_flash_decoding(
2025-04-11T03:52:12.6850488Z         bsz: int,
2025-04-11T03:52:12.6850573Z         block_size: int,
2025-04-11T03:52:12.6850664Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6850881Z         num_attn_heads: int,
2025-04-11T03:52:12.6850969Z         kv_group_num: int,
2025-04-11T03:52:12.6851053Z         same_context_len: bool,
2025-04-11T03:52:12.6851135Z         q_len: int,
2025-04-11T03:52:12.6851221Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6851309Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6851387Z     ):
2025-04-11T03:52:12.6851498Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6851691Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6851871Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6852043Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6852204Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6852365Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6852441Z     
2025-04-11T03:52:12.6852528Z         torch.manual_seed(123)
2025-04-11T03:52:12.6852621Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6852712Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6852716Z 
2025-04-11T03:52:12.6852873Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6852989Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6852992Z 
2025-04-11T03:52:12.6853071Z device = None
2025-04-11T03:52:12.6853076Z 
2025-04-11T03:52:12.6853193Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6853343Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6853420Z     
2025-04-11T03:52:12.6853495Z         Args:
2025-04-11T03:52:12.6853666Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6853832Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6853944Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6854015Z         """
2025-04-11T03:52:12.6854094Z         _lazy_init()
2025-04-11T03:52:12.6854193Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6854296Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6854408Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6854688Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6854823Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6854981Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6854985Z 
2025-04-11T03:52:12.6855216Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6855390Z ____________ test_flash_decoding[True-False-5-False-4-16-16-16-16] _____________
2025-04-11T03:52:12.6855397Z 
2025-04-11T03:52:12.6855548Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6855713Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6855801Z use_new_kcache_layout = True
2025-04-11T03:52:12.6855915Z 
2025-04-11T03:52:12.6856123Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6856228Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6856348Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6856491Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6856610Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6856730Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6856869Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6857070Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6857208Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6857357Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6857450Z     def test_flash_decoding(
2025-04-11T03:52:12.6857530Z         bsz: int,
2025-04-11T03:52:12.6857615Z         block_size: int,
2025-04-11T03:52:12.6857706Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6857793Z         num_attn_heads: int,
2025-04-11T03:52:12.6857875Z         kv_group_num: int,
2025-04-11T03:52:12.6857960Z         same_context_len: bool,
2025-04-11T03:52:12.6858043Z         q_len: int,
2025-04-11T03:52:12.6858128Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6858219Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6858294Z     ):
2025-04-11T03:52:12.6858403Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6858600Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6858785Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6858961Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6859125Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6859289Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6859362Z     
2025-04-11T03:52:12.6859449Z         torch.manual_seed(123)
2025-04-11T03:52:12.6859545Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6859635Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6859640Z 
2025-04-11T03:52:12.6859799Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6859912Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6859916Z 
2025-04-11T03:52:12.6859998Z device = None
2025-04-11T03:52:12.6860002Z 
2025-04-11T03:52:12.6860119Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6860269Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6860343Z     
2025-04-11T03:52:12.6860415Z         Args:
2025-04-11T03:52:12.6860588Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6860752Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6860862Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6860936Z         """
2025-04-11T03:52:12.6861014Z         _lazy_init()
2025-04-11T03:52:12.6861113Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6861214Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6861324Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6861603Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6861747Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6861904Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6861908Z 
2025-04-11T03:52:12.6862250Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6862420Z _____________ test_flash_decoding[True-False-5-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.6862424Z 
2025-04-11T03:52:12.6862575Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6862743Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6862835Z use_new_kcache_layout = True
2025-04-11T03:52:12.6862839Z 
2025-04-11T03:52:12.6863039Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6863242Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6863366Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6863504Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6863620Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6863741Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6863880Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6863985Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6864120Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6864273Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6864364Z     def test_flash_decoding(
2025-04-11T03:52:12.6864441Z         bsz: int,
2025-04-11T03:52:12.6864527Z         block_size: int,
2025-04-11T03:52:12.6864617Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6864702Z         num_attn_heads: int,
2025-04-11T03:52:12.6864789Z         kv_group_num: int,
2025-04-11T03:52:12.6864875Z         same_context_len: bool,
2025-04-11T03:52:12.6864953Z         q_len: int,
2025-04-11T03:52:12.6865037Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6865129Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6865203Z     ):
2025-04-11T03:52:12.6865313Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6865510Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6865691Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6865865Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6866030Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6866191Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6866265Z     
2025-04-11T03:52:12.6866352Z         torch.manual_seed(123)
2025-04-11T03:52:12.6866447Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6866537Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6866541Z 
2025-04-11T03:52:12.6866700Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6866815Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6866819Z 
2025-04-11T03:52:12.6866898Z device = None
2025-04-11T03:52:12.6866902Z 
2025-04-11T03:52:12.6867016Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6867165Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6867240Z     
2025-04-11T03:52:12.6867314Z         Args:
2025-04-11T03:52:12.6867482Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6867646Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6867759Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6867832Z         """
2025-04-11T03:52:12.6867908Z         _lazy_init()
2025-04-11T03:52:12.6868008Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6868107Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6868316Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6868650Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6868788Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6868945Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6868949Z 
2025-04-11T03:52:12.6869183Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6869355Z ____________ test_flash_decoding[True-False-5-False-4-16-16-32-16] _____________
2025-04-11T03:52:12.6869465Z 
2025-04-11T03:52:12.6869618Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6869787Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.6869876Z use_new_kcache_layout = True
2025-04-11T03:52:12.6869883Z 
2025-04-11T03:52:12.6870084Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6870191Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6870313Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6870449Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6870568Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6870686Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6870821Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6870931Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6871067Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6871222Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6871309Z     def test_flash_decoding(
2025-04-11T03:52:12.6871390Z         bsz: int,
2025-04-11T03:52:12.6871474Z         block_size: int,
2025-04-11T03:52:12.6871563Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6871649Z         num_attn_heads: int,
2025-04-11T03:52:12.6871732Z         kv_group_num: int,
2025-04-11T03:52:12.6871819Z         same_context_len: bool,
2025-04-11T03:52:12.6871900Z         q_len: int,
2025-04-11T03:52:12.6871983Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6872076Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6872149Z     ):
2025-04-11T03:52:12.6872260Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6872454Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6872638Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6872811Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6872972Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6873134Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6873208Z     
2025-04-11T03:52:12.6873294Z         torch.manual_seed(123)
2025-04-11T03:52:12.6873386Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6873477Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6873481Z 
2025-04-11T03:52:12.6873637Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6873749Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6873753Z 
2025-04-11T03:52:12.6873834Z device = None
2025-04-11T03:52:12.6873838Z 
2025-04-11T03:52:12.6873953Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6874104Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6874177Z     
2025-04-11T03:52:12.6874250Z         Args:
2025-04-11T03:52:12.6874548Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6874713Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6874824Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6874897Z         """
2025-04-11T03:52:12.6874976Z         _lazy_init()
2025-04-11T03:52:12.6875074Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6875176Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6875286Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6875570Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6875859Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6876016Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6876022Z 
2025-04-11T03:52:12.6876262Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6876432Z ______________ test_flash_decoding[False-True-1-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.6876436Z 
2025-04-11T03:52:12.6876585Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6876749Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6876838Z use_new_kcache_layout = False
2025-04-11T03:52:12.6876842Z 
2025-04-11T03:52:12.6877041Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6877149Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6877269Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6877406Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6877523Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6877642Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6877779Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6877888Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6878021Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6878174Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6878260Z     def test_flash_decoding(
2025-04-11T03:52:12.6878335Z         bsz: int,
2025-04-11T03:52:12.6878422Z         block_size: int,
2025-04-11T03:52:12.6878513Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6878599Z         num_attn_heads: int,
2025-04-11T03:52:12.6878683Z         kv_group_num: int,
2025-04-11T03:52:12.6878769Z         same_context_len: bool,
2025-04-11T03:52:12.6878849Z         q_len: int,
2025-04-11T03:52:12.6878933Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6879023Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6879099Z     ):
2025-04-11T03:52:12.6879206Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6879399Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6879580Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6879756Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6879918Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6880077Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6880153Z     
2025-04-11T03:52:12.6880243Z         torch.manual_seed(123)
2025-04-11T03:52:12.6880332Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6880423Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6880427Z 
2025-04-11T03:52:12.6880583Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6880808Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6880812Z 
2025-04-11T03:52:12.6880892Z device = None
2025-04-11T03:52:12.6880896Z 
2025-04-11T03:52:12.6881012Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6881164Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6881236Z     
2025-04-11T03:52:12.6881309Z         Args:
2025-04-11T03:52:12.6881480Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6881642Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6881845Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6881919Z         """
2025-04-11T03:52:12.6881996Z         _lazy_init()
2025-04-11T03:52:12.6882095Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6882197Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6882310Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6882592Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6882732Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6882889Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6882894Z 
2025-04-11T03:52:12.6883134Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6883300Z _____________ test_flash_decoding[False-True-1-True-1-16-8-16-16] ______________
2025-04-11T03:52:12.6883307Z 
2025-04-11T03:52:12.6883456Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6883623Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6883711Z use_new_kcache_layout = False
2025-04-11T03:52:12.6883719Z 
2025-04-11T03:52:12.6883921Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6884031Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6884151Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6884293Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6884406Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6884523Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6884659Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6884768Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6884902Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6885056Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6885142Z     def test_flash_decoding(
2025-04-11T03:52:12.6885215Z         bsz: int,
2025-04-11T03:52:12.6885304Z         block_size: int,
2025-04-11T03:52:12.6885393Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6885478Z         num_attn_heads: int,
2025-04-11T03:52:12.6885561Z         kv_group_num: int,
2025-04-11T03:52:12.6885646Z         same_context_len: bool,
2025-04-11T03:52:12.6885725Z         q_len: int,
2025-04-11T03:52:12.6885810Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6885905Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6885976Z     ):
2025-04-11T03:52:12.6886089Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6886278Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6886461Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6886633Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6886793Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6887063Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6887136Z     
2025-04-11T03:52:12.6887223Z         torch.manual_seed(123)
2025-04-11T03:52:12.6887313Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6887404Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6887408Z 
2025-04-11T03:52:12.6887569Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6887685Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6887689Z 
2025-04-11T03:52:12.6887770Z device = None
2025-04-11T03:52:12.6887870Z 
2025-04-11T03:52:12.6887988Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6888140Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6888212Z     
2025-04-11T03:52:12.6888286Z         Args:
2025-04-11T03:52:12.6888462Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6888626Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6888734Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6888806Z         """
2025-04-11T03:52:12.6888885Z         _lazy_init()
2025-04-11T03:52:12.6888981Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6889085Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6889195Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6889477Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6889619Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6889775Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6889779Z 
2025-04-11T03:52:12.6890025Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6890191Z ______________ test_flash_decoding[False-True-1-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.6890195Z 
2025-04-11T03:52:12.6890343Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6890507Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6890598Z use_new_kcache_layout = False
2025-04-11T03:52:12.6890602Z 
2025-04-11T03:52:12.6890806Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6890914Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6891036Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6891172Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6891290Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6891407Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6891542Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6891648Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6891785Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6891938Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6892024Z     def test_flash_decoding(
2025-04-11T03:52:12.6892097Z         bsz: int,
2025-04-11T03:52:12.6892183Z         block_size: int,
2025-04-11T03:52:12.6892272Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6892356Z         num_attn_heads: int,
2025-04-11T03:52:12.6892444Z         kv_group_num: int,
2025-04-11T03:52:12.6892532Z         same_context_len: bool,
2025-04-11T03:52:12.6892610Z         q_len: int,
2025-04-11T03:52:12.6892694Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6892786Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6892858Z     ):
2025-04-11T03:52:12.6893082Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6893273Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6893452Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6893624Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6893787Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6893946Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6894110Z     
2025-04-11T03:52:12.6894203Z         torch.manual_seed(123)
2025-04-11T03:52:12.6894295Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6894386Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6894391Z 
2025-04-11T03:52:12.6894548Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6894664Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6894668Z 
2025-04-11T03:52:12.6894750Z device = None
2025-04-11T03:52:12.6894755Z 
2025-04-11T03:52:12.6894872Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6895027Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6895099Z     
2025-04-11T03:52:12.6895172Z         Args:
2025-04-11T03:52:12.6895343Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6895507Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6895619Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6895691Z         """
2025-04-11T03:52:12.6895774Z         _lazy_init()
2025-04-11T03:52:12.6895870Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6895970Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6896086Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6896366Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6896505Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6896661Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6896665Z 
2025-04-11T03:52:12.6896907Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6897071Z _____________ test_flash_decoding[False-True-1-True-1-16-8-32-16] ______________
2025-04-11T03:52:12.6897078Z 
2025-04-11T03:52:12.6897231Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6897389Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6897477Z use_new_kcache_layout = False
2025-04-11T03:52:12.6897484Z 
2025-04-11T03:52:12.6897688Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6897792Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6897912Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6898052Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6898171Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6898283Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6898420Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6898528Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6898668Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6898820Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6898905Z     def test_flash_decoding(
2025-04-11T03:52:12.6898979Z         bsz: int,
2025-04-11T03:52:12.6899199Z         block_size: int,
2025-04-11T03:52:12.6899291Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6899379Z         num_attn_heads: int,
2025-04-11T03:52:12.6899462Z         kv_group_num: int,
2025-04-11T03:52:12.6899549Z         same_context_len: bool,
2025-04-11T03:52:12.6899625Z         q_len: int,
2025-04-11T03:52:12.6899709Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6899802Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6899874Z     ):
2025-04-11T03:52:12.6899987Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6900176Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6900454Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6900626Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6900788Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6900954Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6901027Z     
2025-04-11T03:52:12.6901121Z         torch.manual_seed(123)
2025-04-11T03:52:12.6901211Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6901305Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6901309Z 
2025-04-11T03:52:12.6901464Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6901579Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6901584Z 
2025-04-11T03:52:12.6901667Z device = None
2025-04-11T03:52:12.6901674Z 
2025-04-11T03:52:12.6901791Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6901942Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6902013Z     
2025-04-11T03:52:12.6902086Z         Args:
2025-04-11T03:52:12.6902256Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6902422Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6902532Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6902606Z         """
2025-04-11T03:52:12.6902688Z         _lazy_init()
2025-04-11T03:52:12.6902786Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6902886Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6902996Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6903276Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6903420Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6903577Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6903581Z 
2025-04-11T03:52:12.6903823Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6903989Z _____________ test_flash_decoding[False-True-1-True-1-16-16-16-7] ______________
2025-04-11T03:52:12.6903993Z 
2025-04-11T03:52:12.6904143Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6904304Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6904393Z use_new_kcache_layout = False
2025-04-11T03:52:12.6904397Z 
2025-04-11T03:52:12.6904602Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6904706Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6904829Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6904967Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6905088Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6905200Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6905444Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6905550Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6905684Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6905838Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6905925Z     def test_flash_decoding(
2025-04-11T03:52:12.6906006Z         bsz: int,
2025-04-11T03:52:12.6906087Z         block_size: int,
2025-04-11T03:52:12.6906175Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6906263Z         num_attn_heads: int,
2025-04-11T03:52:12.6906437Z         kv_group_num: int,
2025-04-11T03:52:12.6906526Z         same_context_len: bool,
2025-04-11T03:52:12.6906602Z         q_len: int,
2025-04-11T03:52:12.6906687Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6906782Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6906854Z     ):
2025-04-11T03:52:12.6906970Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6907160Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6907342Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6907514Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6907678Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6907836Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6907910Z     
2025-04-11T03:52:12.6907999Z         torch.manual_seed(123)
2025-04-11T03:52:12.6908088Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6908179Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6908187Z 
2025-04-11T03:52:12.6908341Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6908500Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6908505Z 
2025-04-11T03:52:12.6908590Z device = None
2025-04-11T03:52:12.6908594Z 
2025-04-11T03:52:12.6908713Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6908865Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6908935Z     
2025-04-11T03:52:12.6909011Z         Args:
2025-04-11T03:52:12.6909177Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6909340Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6909455Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6909527Z         """
2025-04-11T03:52:12.6909610Z         _lazy_init()
2025-04-11T03:52:12.6909708Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6909810Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6909925Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6910210Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6910348Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6910505Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6910509Z 
2025-04-11T03:52:12.6910755Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6910921Z _____________ test_flash_decoding[False-True-1-True-1-16-16-16-16] _____________
2025-04-11T03:52:12.6910928Z 
2025-04-11T03:52:12.6911082Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6911240Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6911329Z use_new_kcache_layout = False
2025-04-11T03:52:12.6911474Z 
2025-04-11T03:52:12.6911670Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6911775Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6911896Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6912034Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6912154Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6912267Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6912401Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6912507Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6912749Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6912903Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6912991Z     def test_flash_decoding(
2025-04-11T03:52:12.6913070Z         bsz: int,
2025-04-11T03:52:12.6913154Z         block_size: int,
2025-04-11T03:52:12.6913243Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6913330Z         num_attn_heads: int,
2025-04-11T03:52:12.6913411Z         kv_group_num: int,
2025-04-11T03:52:12.6913501Z         same_context_len: bool,
2025-04-11T03:52:12.6913577Z         q_len: int,
2025-04-11T03:52:12.6913661Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6913751Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6913822Z     ):
2025-04-11T03:52:12.6913934Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6914123Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6914307Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6914474Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6914634Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6914800Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6914870Z     
2025-04-11T03:52:12.6914960Z         torch.manual_seed(123)
2025-04-11T03:52:12.6915053Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6915147Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6915151Z 
2025-04-11T03:52:12.6915304Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6915416Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6915420Z 
2025-04-11T03:52:12.6915501Z device = None
2025-04-11T03:52:12.6915508Z 
2025-04-11T03:52:12.6915625Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6915777Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6915848Z     
2025-04-11T03:52:12.6915926Z         Args:
2025-04-11T03:52:12.6916091Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6916259Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6916369Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6916440Z         """
2025-04-11T03:52:12.6916522Z         _lazy_init()
2025-04-11T03:52:12.6916619Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6916722Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6916829Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6917111Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6917252Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6917412Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6917416Z 
2025-04-11T03:52:12.6917655Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6917932Z _____________ test_flash_decoding[False-True-1-True-1-16-16-32-7] ______________
2025-04-11T03:52:12.6917937Z 
2025-04-11T03:52:12.6918091Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6918253Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6918344Z use_new_kcache_layout = False
2025-04-11T03:52:12.6918352Z 
2025-04-11T03:52:12.6918553Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6918657Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6918877Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6919017Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6919139Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6919252Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6919395Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6919504Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6919640Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6919796Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6919881Z     def test_flash_decoding(
2025-04-11T03:52:12.6919960Z         bsz: int,
2025-04-11T03:52:12.6920044Z         block_size: int,
2025-04-11T03:52:12.6920134Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6920226Z         num_attn_heads: int,
2025-04-11T03:52:12.6920314Z         kv_group_num: int,
2025-04-11T03:52:12.6920402Z         same_context_len: bool,
2025-04-11T03:52:12.6920480Z         q_len: int,
2025-04-11T03:52:12.6920565Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6920668Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6920764Z     ):
2025-04-11T03:52:12.6920889Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6921081Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6921269Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6921438Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6921602Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6921761Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6921837Z     
2025-04-11T03:52:12.6921929Z         torch.manual_seed(123)
2025-04-11T03:52:12.6922018Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6922112Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6922116Z 
2025-04-11T03:52:12.6922270Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6922384Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6922392Z 
2025-04-11T03:52:12.6922468Z device = None
2025-04-11T03:52:12.6922473Z 
2025-04-11T03:52:12.6922587Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6922739Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6922812Z     
2025-04-11T03:52:12.6922889Z         Args:
2025-04-11T03:52:12.6923054Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6923218Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6923331Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6923406Z         """
2025-04-11T03:52:12.6923485Z         _lazy_init()
2025-04-11T03:52:12.6923582Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6923685Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6923908Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6924189Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6924329Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6924487Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6924492Z 
2025-04-11T03:52:12.6924733Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6924900Z _____________ test_flash_decoding[False-True-1-True-1-16-16-32-16] _____________
2025-04-11T03:52:12.6925002Z 
2025-04-11T03:52:12.6925157Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6925316Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6925408Z use_new_kcache_layout = False
2025-04-11T03:52:12.6925416Z 
2025-04-11T03:52:12.6925614Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6925717Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6925842Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6925979Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6926099Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6926215Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6926354Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6926457Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6926593Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6926746Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6926833Z     def test_flash_decoding(
2025-04-11T03:52:12.6926911Z         bsz: int,
2025-04-11T03:52:12.6926995Z         block_size: int,
2025-04-11T03:52:12.6927085Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6927173Z         num_attn_heads: int,
2025-04-11T03:52:12.6927255Z         kv_group_num: int,
2025-04-11T03:52:12.6927342Z         same_context_len: bool,
2025-04-11T03:52:12.6927417Z         q_len: int,
2025-04-11T03:52:12.6927506Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6927594Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6927667Z     ):
2025-04-11T03:52:12.6927781Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6927971Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6928159Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6928326Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6928487Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6928654Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6928725Z     
2025-04-11T03:52:12.6928815Z         torch.manual_seed(123)
2025-04-11T03:52:12.6928905Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6929004Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6929008Z 
2025-04-11T03:52:12.6929160Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6929272Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6929279Z 
2025-04-11T03:52:12.6929356Z device = None
2025-04-11T03:52:12.6929364Z 
2025-04-11T03:52:12.6929481Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6929633Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6929703Z     
2025-04-11T03:52:12.6929779Z         Args:
2025-04-11T03:52:12.6929944Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6930217Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6930327Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6930399Z         """
2025-04-11T03:52:12.6930483Z         _lazy_init()
2025-04-11T03:52:12.6930580Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6930686Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6930792Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6931076Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6931313Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6931470Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6931474Z 
2025-04-11T03:52:12.6931717Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6931889Z ______________ test_flash_decoding[False-True-1-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.6931893Z 
2025-04-11T03:52:12.6932046Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6932207Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6932302Z use_new_kcache_layout = False
2025-04-11T03:52:12.6932306Z 
2025-04-11T03:52:12.6932505Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6932609Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6932733Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6932872Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6932995Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6933107Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6933248Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6933352Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6933487Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6933645Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6933731Z     def test_flash_decoding(
2025-04-11T03:52:12.6933808Z         bsz: int,
2025-04-11T03:52:12.6933890Z         block_size: int,
2025-04-11T03:52:12.6933978Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6934066Z         num_attn_heads: int,
2025-04-11T03:52:12.6934154Z         kv_group_num: int,
2025-04-11T03:52:12.6934243Z         same_context_len: bool,
2025-04-11T03:52:12.6934319Z         q_len: int,
2025-04-11T03:52:12.6934407Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6934495Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6934567Z     ):
2025-04-11T03:52:12.6934687Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6934881Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6935065Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6935234Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6935398Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6935555Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6935630Z     
2025-04-11T03:52:12.6935721Z         torch.manual_seed(123)
2025-04-11T03:52:12.6935811Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6935905Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6935910Z 
2025-04-11T03:52:12.6936063Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6936297Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6936302Z 
2025-04-11T03:52:12.6936380Z device = None
2025-04-11T03:52:12.6936385Z 
2025-04-11T03:52:12.6936501Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6936656Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6936729Z     
2025-04-11T03:52:12.6936805Z         Args:
2025-04-11T03:52:12.6936972Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6937139Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6937337Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6937409Z         """
2025-04-11T03:52:12.6937491Z         _lazy_init()
2025-04-11T03:52:12.6937588Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6937696Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6937805Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6938089Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6938233Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6938392Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6938397Z 
2025-04-11T03:52:12.6938640Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6938808Z _____________ test_flash_decoding[False-True-1-True-4-16-8-16-16] ______________
2025-04-11T03:52:12.6938815Z 
2025-04-11T03:52:12.6938971Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6939133Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6939225Z use_new_kcache_layout = False
2025-04-11T03:52:12.6939233Z 
2025-04-11T03:52:12.6939432Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6939536Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6939658Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6939797Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6939918Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6940029Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6940168Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6940271Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6940414Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6940571Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6940658Z     def test_flash_decoding(
2025-04-11T03:52:12.6940740Z         bsz: int,
2025-04-11T03:52:12.6940824Z         block_size: int,
2025-04-11T03:52:12.6940917Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6941001Z         num_attn_heads: int,
2025-04-11T03:52:12.6941085Z         kv_group_num: int,
2025-04-11T03:52:12.6941175Z         same_context_len: bool,
2025-04-11T03:52:12.6941251Z         q_len: int,
2025-04-11T03:52:12.6941337Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6941425Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6941496Z     ):
2025-04-11T03:52:12.6941612Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6941803Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6941990Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6942161Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6942325Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6942631Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6942704Z     
2025-04-11T03:52:12.6942795Z         torch.manual_seed(123)
2025-04-11T03:52:12.6942885Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6942981Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6942985Z 
2025-04-11T03:52:12.6943138Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6943254Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6943259Z 
2025-04-11T03:52:12.6943338Z device = None
2025-04-11T03:52:12.6943448Z 
2025-04-11T03:52:12.6943567Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6943720Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6943791Z     
2025-04-11T03:52:12.6943872Z         Args:
2025-04-11T03:52:12.6944040Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6944216Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6944322Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6944396Z         """
2025-04-11T03:52:12.6944477Z         _lazy_init()
2025-04-11T03:52:12.6944572Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6944677Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6944783Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6945067Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6945212Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6945369Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6945374Z 
2025-04-11T03:52:12.6945617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6945791Z ______________ test_flash_decoding[False-True-1-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.6945795Z 
2025-04-11T03:52:12.6945950Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6946113Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6946205Z use_new_kcache_layout = False
2025-04-11T03:52:12.6946210Z 
2025-04-11T03:52:12.6946408Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6946516Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6946635Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6946776Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6946897Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6947009Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6947152Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6947253Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6947386Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6947542Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6947629Z     def test_flash_decoding(
2025-04-11T03:52:12.6947710Z         bsz: int,
2025-04-11T03:52:12.6947790Z         block_size: int,
2025-04-11T03:52:12.6947879Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6947962Z         num_attn_heads: int,
2025-04-11T03:52:12.6948046Z         kv_group_num: int,
2025-04-11T03:52:12.6948135Z         same_context_len: bool,
2025-04-11T03:52:12.6948211Z         q_len: int,
2025-04-11T03:52:12.6948299Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6948385Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6948508Z     ):
2025-04-11T03:52:12.6948625Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6949027Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6949212Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6949381Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6949547Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6949704Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6949776Z     
2025-04-11T03:52:12.6949975Z         torch.manual_seed(123)
2025-04-11T03:52:12.6950064Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6950160Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6950164Z 
2025-04-11T03:52:12.6950319Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6950435Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6950443Z 
2025-04-11T03:52:12.6950519Z device = None
2025-04-11T03:52:12.6950524Z 
2025-04-11T03:52:12.6950640Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6950792Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6950865Z     
2025-04-11T03:52:12.6950939Z         Args:
2025-04-11T03:52:12.6951104Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6951270Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6951379Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6951453Z         """
2025-04-11T03:52:12.6951533Z         _lazy_init()
2025-04-11T03:52:12.6951629Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6951732Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6951841Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6952126Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6952263Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6952418Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6952423Z 
2025-04-11T03:52:12.6952664Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6952829Z _____________ test_flash_decoding[False-True-1-True-4-16-8-32-16] ______________
2025-04-11T03:52:12.6952836Z 
2025-04-11T03:52:12.6952990Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6953151Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6953243Z use_new_kcache_layout = False
2025-04-11T03:52:12.6953248Z 
2025-04-11T03:52:12.6953448Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6953554Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6953671Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6953808Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6953928Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6954040Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6954179Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6954282Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6954424Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6954572Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6954658Z     def test_flash_decoding(
2025-04-11T03:52:12.6954739Z         bsz: int,
2025-04-11T03:52:12.6954818Z         block_size: int,
2025-04-11T03:52:12.6955023Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6955106Z         num_attn_heads: int,
2025-04-11T03:52:12.6955189Z         kv_group_num: int,
2025-04-11T03:52:12.6955281Z         same_context_len: bool,
2025-04-11T03:52:12.6955356Z         q_len: int,
2025-04-11T03:52:12.6955445Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6955532Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6955603Z     ):
2025-04-11T03:52:12.6955718Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6955908Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6956191Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6956361Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6956525Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6956686Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6956762Z     
2025-04-11T03:52:12.6956847Z         torch.manual_seed(123)
2025-04-11T03:52:12.6956937Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6957032Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6957037Z 
2025-04-11T03:52:12.6957187Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6957303Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6957308Z 
2025-04-11T03:52:12.6957383Z device = None
2025-04-11T03:52:12.6957391Z 
2025-04-11T03:52:12.6957510Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6957659Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6957729Z     
2025-04-11T03:52:12.6957808Z         Args:
2025-04-11T03:52:12.6957974Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6958145Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6958251Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6958321Z         """
2025-04-11T03:52:12.6958401Z         _lazy_init()
2025-04-11T03:52:12.6958497Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6958604Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6958710Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6959001Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6959139Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6959298Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6959306Z 
2025-04-11T03:52:12.6959545Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6959715Z _____________ test_flash_decoding[False-True-1-True-4-16-16-16-7] ______________
2025-04-11T03:52:12.6959719Z 
2025-04-11T03:52:12.6959871Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6960031Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6960122Z use_new_kcache_layout = False
2025-04-11T03:52:12.6960127Z 
2025-04-11T03:52:12.6960325Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6960433Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6960554Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6960691Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6960810Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6960923Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6961167Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6961273Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6961415Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6961566Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6961656Z     def test_flash_decoding(
2025-04-11T03:52:12.6961736Z         bsz: int,
2025-04-11T03:52:12.6961816Z         block_size: int,
2025-04-11T03:52:12.6961907Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6961992Z         num_attn_heads: int,
2025-04-11T03:52:12.6962073Z         kv_group_num: int,
2025-04-11T03:52:12.6962260Z         same_context_len: bool,
2025-04-11T03:52:12.6962336Z         q_len: int,
2025-04-11T03:52:12.6962426Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6962517Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6962593Z     ):
2025-04-11T03:52:12.6962703Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6962899Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6963083Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6963255Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6963417Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6963573Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6963648Z     
2025-04-11T03:52:12.6963737Z         torch.manual_seed(123)
2025-04-11T03:52:12.6963824Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6963920Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6963923Z 
2025-04-11T03:52:12.6964078Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6964193Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6964200Z 
2025-04-11T03:52:12.6964278Z device = None
2025-04-11T03:52:12.6964282Z 
2025-04-11T03:52:12.6964402Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6964549Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6964620Z     
2025-04-11T03:52:12.6964697Z         Args:
2025-04-11T03:52:12.6964861Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6965028Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6965138Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6965212Z         """
2025-04-11T03:52:12.6965289Z         _lazy_init()
2025-04-11T03:52:12.6965384Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6965488Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6965597Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6965885Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6966019Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6966178Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6966187Z 
2025-04-11T03:52:12.6966424Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6966589Z _____________ test_flash_decoding[False-True-1-True-4-16-16-16-16] _____________
2025-04-11T03:52:12.6966595Z 
2025-04-11T03:52:12.6966754Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6966912Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6967004Z use_new_kcache_layout = False
2025-04-11T03:52:12.6967008Z 
2025-04-11T03:52:12.6967315Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6967421Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6967539Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6967677Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6967801Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6967914Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6968054Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6968156Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6968405Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6968557Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6968644Z     def test_flash_decoding(
2025-04-11T03:52:12.6968725Z         bsz: int,
2025-04-11T03:52:12.6968809Z         block_size: int,
2025-04-11T03:52:12.6968903Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6968987Z         num_attn_heads: int,
2025-04-11T03:52:12.6969073Z         kv_group_num: int,
2025-04-11T03:52:12.6969158Z         same_context_len: bool,
2025-04-11T03:52:12.6969233Z         q_len: int,
2025-04-11T03:52:12.6969320Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6969408Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6969483Z     ):
2025-04-11T03:52:12.6969592Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6969783Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6969975Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6970148Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6970313Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6970474Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6970548Z     
2025-04-11T03:52:12.6970636Z         torch.manual_seed(123)
2025-04-11T03:52:12.6970725Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6970821Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6970826Z 
2025-04-11T03:52:12.6970977Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6971093Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6971097Z 
2025-04-11T03:52:12.6971174Z device = None
2025-04-11T03:52:12.6971178Z 
2025-04-11T03:52:12.6971303Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6971451Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6971521Z     
2025-04-11T03:52:12.6971601Z         Args:
2025-04-11T03:52:12.6971768Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6971940Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6972045Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6972121Z         """
2025-04-11T03:52:12.6972198Z         _lazy_init()
2025-04-11T03:52:12.6972295Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6972401Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6972510Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6972799Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6972938Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6973099Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6973103Z 
2025-04-11T03:52:12.6973342Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6973633Z _____________ test_flash_decoding[False-True-1-True-4-16-16-32-7] ______________
2025-04-11T03:52:12.6973640Z 
2025-04-11T03:52:12.6973789Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6973949Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6974039Z use_new_kcache_layout = False
2025-04-11T03:52:12.6974044Z 
2025-04-11T03:52:12.6974242Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6974349Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6974555Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6974695Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6974811Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6974923Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6975066Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6975169Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6975307Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6975459Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6975546Z     def test_flash_decoding(
2025-04-11T03:52:12.6975628Z         bsz: int,
2025-04-11T03:52:12.6975709Z         block_size: int,
2025-04-11T03:52:12.6975806Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6975889Z         num_attn_heads: int,
2025-04-11T03:52:12.6975977Z         kv_group_num: int,
2025-04-11T03:52:12.6976069Z         same_context_len: bool,
2025-04-11T03:52:12.6976145Z         q_len: int,
2025-04-11T03:52:12.6976238Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6976324Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6976399Z     ):
2025-04-11T03:52:12.6976512Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6976708Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6976891Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6977060Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6977230Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6977388Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6977463Z     
2025-04-11T03:52:12.6977553Z         torch.manual_seed(123)
2025-04-11T03:52:12.6977643Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6977740Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6977744Z 
2025-04-11T03:52:12.6977898Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6978019Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6978026Z 
2025-04-11T03:52:12.6978102Z device = None
2025-04-11T03:52:12.6978106Z 
2025-04-11T03:52:12.6978224Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6978372Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6978447Z     
2025-04-11T03:52:12.6978519Z         Args:
2025-04-11T03:52:12.6978687Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6978854Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6978964Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6979039Z         """
2025-04-11T03:52:12.6979116Z         _lazy_init()
2025-04-11T03:52:12.6979210Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6979314Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6979415Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6979820Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6979959Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6980119Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6980124Z 
2025-04-11T03:52:12.6980363Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6980530Z _____________ test_flash_decoding[False-True-1-True-4-16-16-32-16] _____________
2025-04-11T03:52:12.6980629Z 
2025-04-11T03:52:12.6980787Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.6980950Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6981043Z use_new_kcache_layout = False
2025-04-11T03:52:12.6981047Z 
2025-04-11T03:52:12.6981248Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6981355Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6981473Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6981614Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6981729Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6981840Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6981981Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6982085Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6982228Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6982377Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6982466Z     def test_flash_decoding(
2025-04-11T03:52:12.6982542Z         bsz: int,
2025-04-11T03:52:12.6982622Z         block_size: int,
2025-04-11T03:52:12.6982719Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6982802Z         num_attn_heads: int,
2025-04-11T03:52:12.6982887Z         kv_group_num: int,
2025-04-11T03:52:12.6982971Z         same_context_len: bool,
2025-04-11T03:52:12.6983044Z         q_len: int,
2025-04-11T03:52:12.6983132Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6983219Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6983291Z     ):
2025-04-11T03:52:12.6983403Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6983595Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6983783Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6983953Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6984118Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6984276Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6984353Z     
2025-04-11T03:52:12.6984439Z         torch.manual_seed(123)
2025-04-11T03:52:12.6984528Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6984624Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6984627Z 
2025-04-11T03:52:12.6984778Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6984896Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6984900Z 
2025-04-11T03:52:12.6984975Z device = None
2025-04-11T03:52:12.6984979Z 
2025-04-11T03:52:12.6985103Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6985251Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6985326Z     
2025-04-11T03:52:12.6985401Z         Args:
2025-04-11T03:52:12.6985565Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6985845Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6985952Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6986029Z         """
2025-04-11T03:52:12.6986107Z         _lazy_init()
2025-04-11T03:52:12.6986202Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6986308Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6986415Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6986704Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6986942Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6987103Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6987109Z 
2025-04-11T03:52:12.6987346Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6987517Z _____________ test_flash_decoding[False-True-1-False-1-16-8-16-7] ______________
2025-04-11T03:52:12.6987522Z 
2025-04-11T03:52:12.6987670Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6987831Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6987924Z use_new_kcache_layout = False
2025-04-11T03:52:12.6987928Z 
2025-04-11T03:52:12.6988128Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6988238Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6988358Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6988543Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6988660Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6988773Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6988921Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6989024Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6989163Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6989315Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6989408Z     def test_flash_decoding(
2025-04-11T03:52:12.6989482Z         bsz: int,
2025-04-11T03:52:12.6989562Z         block_size: int,
2025-04-11T03:52:12.6989659Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6989740Z         num_attn_heads: int,
2025-04-11T03:52:12.6989829Z         kv_group_num: int,
2025-04-11T03:52:12.6989917Z         same_context_len: bool,
2025-04-11T03:52:12.6989992Z         q_len: int,
2025-04-11T03:52:12.6990083Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6990172Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6990249Z     ):
2025-04-11T03:52:12.6990359Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6990551Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6990735Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6990906Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6991071Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6991227Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6991303Z     
2025-04-11T03:52:12.6991392Z         torch.manual_seed(123)
2025-04-11T03:52:12.6991482Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6991572Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6991576Z 
2025-04-11T03:52:12.6991729Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6991844Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6991983Z 
2025-04-11T03:52:12.6992066Z device = None
2025-04-11T03:52:12.6992070Z 
2025-04-11T03:52:12.6992193Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6992343Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6992417Z     
2025-04-11T03:52:12.6992490Z         Args:
2025-04-11T03:52:12.6992657Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6992825Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6993039Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.6993117Z         """
2025-04-11T03:52:12.6993196Z         _lazy_init()
2025-04-11T03:52:12.6993295Z         with torch.cuda.device(device):
2025-04-11T03:52:12.6993405Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.6993512Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.6993803Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.6993940Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.6994099Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.6994104Z 
2025-04-11T03:52:12.6994345Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.6994512Z _____________ test_flash_decoding[False-True-1-False-1-16-8-16-16] _____________
2025-04-11T03:52:12.6994519Z 
2025-04-11T03:52:12.6994669Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.6994833Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.6994925Z use_new_kcache_layout = False
2025-04-11T03:52:12.6994930Z 
2025-04-11T03:52:12.6995133Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.6995241Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.6995357Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.6995501Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.6995617Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.6995732Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.6995871Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.6995974Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.6996117Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.6996266Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.6996355Z     def test_flash_decoding(
2025-04-11T03:52:12.6996432Z         bsz: int,
2025-04-11T03:52:12.6996513Z         block_size: int,
2025-04-11T03:52:12.6996609Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.6996690Z         num_attn_heads: int,
2025-04-11T03:52:12.6996775Z         kv_group_num: int,
2025-04-11T03:52:12.6996860Z         same_context_len: bool,
2025-04-11T03:52:12.6996935Z         q_len: int,
2025-04-11T03:52:12.6997025Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.6997112Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.6997188Z     ):
2025-04-11T03:52:12.6997299Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.6997493Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.6997677Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.6997846Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.6998013Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.6998267Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.6998345Z     
2025-04-11T03:52:12.6998432Z         torch.manual_seed(123)
2025-04-11T03:52:12.6998524Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.6998613Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.6998617Z 
2025-04-11T03:52:12.6998769Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.6998886Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.6998890Z 
2025-04-11T03:52:12.6998966Z device = None
2025-04-11T03:52:12.6998971Z 
2025-04-11T03:52:12.6999187Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.6999336Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.6999410Z     
2025-04-11T03:52:12.6999485Z         Args:
2025-04-11T03:52:12.6999649Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.6999822Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.6999929Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7000008Z         """
2025-04-11T03:52:12.7000087Z         _lazy_init()
2025-04-11T03:52:12.7000187Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7000289Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7000394Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7000679Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7000819Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7000976Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7000980Z 
2025-04-11T03:52:12.7001218Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7001391Z _____________ test_flash_decoding[False-True-1-False-1-16-8-32-7] ______________
2025-04-11T03:52:12.7001395Z 
2025-04-11T03:52:12.7001544Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7001715Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7001805Z use_new_kcache_layout = False
2025-04-11T03:52:12.7001809Z 
2025-04-11T03:52:12.7002005Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7002115Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7002237Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7002378Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7002496Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7002614Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7002752Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7002856Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7002996Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7003148Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7003238Z     def test_flash_decoding(
2025-04-11T03:52:12.7003314Z         bsz: int,
2025-04-11T03:52:12.7003394Z         block_size: int,
2025-04-11T03:52:12.7003487Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7003571Z         num_attn_heads: int,
2025-04-11T03:52:12.7003655Z         kv_group_num: int,
2025-04-11T03:52:12.7003743Z         same_context_len: bool,
2025-04-11T03:52:12.7003821Z         q_len: int,
2025-04-11T03:52:12.7003906Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7003993Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7004068Z     ):
2025-04-11T03:52:12.7004176Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7004477Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7004662Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7004836Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7005004Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7005164Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7005245Z     
2025-04-11T03:52:12.7005334Z         torch.manual_seed(123)
2025-04-11T03:52:12.7005522Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7005615Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7005619Z 
2025-04-11T03:52:12.7005773Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7005891Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7005898Z 
2025-04-11T03:52:12.7005974Z device = None
2025-04-11T03:52:12.7005978Z 
2025-04-11T03:52:12.7006098Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7006248Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7006323Z     
2025-04-11T03:52:12.7006396Z         Args:
2025-04-11T03:52:12.7006560Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7006730Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7006835Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7006918Z         """
2025-04-11T03:52:12.7006995Z         _lazy_init()
2025-04-11T03:52:12.7007094Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7007197Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7007301Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7007593Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7007729Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7007892Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7007896Z 
2025-04-11T03:52:12.7008137Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7008310Z _____________ test_flash_decoding[False-True-1-False-1-16-8-32-16] _____________
2025-04-11T03:52:12.7008316Z 
2025-04-11T03:52:12.7008468Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7008634Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7008721Z use_new_kcache_layout = False
2025-04-11T03:52:12.7008725Z 
2025-04-11T03:52:12.7008924Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7009035Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7009151Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7009293Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7009412Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7009531Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7009666Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7009769Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7009915Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7010068Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7010156Z     def test_flash_decoding(
2025-04-11T03:52:12.7010232Z         bsz: int,
2025-04-11T03:52:12.7010312Z         block_size: int,
2025-04-11T03:52:12.7010517Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7010601Z         num_attn_heads: int,
2025-04-11T03:52:12.7010691Z         kv_group_num: int,
2025-04-11T03:52:12.7010777Z         same_context_len: bool,
2025-04-11T03:52:12.7010855Z         q_len: int,
2025-04-11T03:52:12.7010942Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7011030Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7011107Z     ):
2025-04-11T03:52:12.7011218Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7011416Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7011690Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7011859Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7012024Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7012183Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7012259Z     
2025-04-11T03:52:12.7012347Z         torch.manual_seed(123)
2025-04-11T03:52:12.7012440Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7012531Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7012535Z 
2025-04-11T03:52:12.7012689Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7012805Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7012809Z 
2025-04-11T03:52:12.7012884Z device = None
2025-04-11T03:52:12.7012889Z 
2025-04-11T03:52:12.7013016Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7013163Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7013237Z     
2025-04-11T03:52:12.7013312Z         Args:
2025-04-11T03:52:12.7013479Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7013653Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7013760Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7013838Z         """
2025-04-11T03:52:12.7013916Z         _lazy_init()
2025-04-11T03:52:12.7014015Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7014116Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7014220Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7014505Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7014644Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7014806Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7014811Z 
2025-04-11T03:52:12.7015047Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7015220Z _____________ test_flash_decoding[False-True-1-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.7015224Z 
2025-04-11T03:52:12.7015371Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7015536Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7015625Z use_new_kcache_layout = False
2025-04-11T03:52:12.7015629Z 
2025-04-11T03:52:12.7015826Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7015933Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7016053Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7016195Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7016310Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7016427Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7016689Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7016794Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7016934Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7017087Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7017177Z     def test_flash_decoding(
2025-04-11T03:52:12.7017253Z         bsz: int,
2025-04-11T03:52:12.7017339Z         block_size: int,
2025-04-11T03:52:12.7017429Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7017512Z         num_attn_heads: int,
2025-04-11T03:52:12.7017601Z         kv_group_num: int,
2025-04-11T03:52:12.7017784Z         same_context_len: bool,
2025-04-11T03:52:12.7017862Z         q_len: int,
2025-04-11T03:52:12.7017950Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7018039Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7018115Z     ):
2025-04-11T03:52:12.7018226Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7018425Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7018607Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7018781Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7018945Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7019102Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7019178Z     
2025-04-11T03:52:12.7019265Z         torch.manual_seed(123)
2025-04-11T03:52:12.7019364Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7019454Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7019458Z 
2025-04-11T03:52:12.7019616Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7019727Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7019734Z 
2025-04-11T03:52:12.7019808Z device = None
2025-04-11T03:52:12.7019813Z 
2025-04-11T03:52:12.7019935Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7020085Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7020160Z     
2025-04-11T03:52:12.7020233Z         Args:
2025-04-11T03:52:12.7020405Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7020568Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7020673Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7020754Z         """
2025-04-11T03:52:12.7020830Z         _lazy_init()
2025-04-11T03:52:12.7020932Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7021034Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7021141Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7021491Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7021631Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7021793Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7021797Z 
2025-04-11T03:52:12.7022036Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7022209Z ____________ test_flash_decoding[False-True-1-False-1-16-16-16-16] _____________
2025-04-11T03:52:12.7022213Z 
2025-04-11T03:52:12.7022371Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7022536Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7022624Z use_new_kcache_layout = False
2025-04-11T03:52:12.7022628Z 
2025-04-11T03:52:12.7022827Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7023047Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7023168Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7023311Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7023428Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7023545Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7023680Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7023785Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7024022Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7024171Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7024263Z     def test_flash_decoding(
2025-04-11T03:52:12.7024339Z         bsz: int,
2025-04-11T03:52:12.7024431Z         block_size: int,
2025-04-11T03:52:12.7024523Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7024605Z         num_attn_heads: int,
2025-04-11T03:52:12.7024698Z         kv_group_num: int,
2025-04-11T03:52:12.7024784Z         same_context_len: bool,
2025-04-11T03:52:12.7024863Z         q_len: int,
2025-04-11T03:52:12.7024949Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7025038Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7025116Z     ):
2025-04-11T03:52:12.7025226Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7025421Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7025604Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7025783Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7025946Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7026106Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7026181Z     
2025-04-11T03:52:12.7026268Z         torch.manual_seed(123)
2025-04-11T03:52:12.7026362Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7026452Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7026456Z 
2025-04-11T03:52:12.7026610Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7026723Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7026727Z 
2025-04-11T03:52:12.7026803Z device = None
2025-04-11T03:52:12.7026810Z 
2025-04-11T03:52:12.7026928Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7027078Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7027155Z     
2025-04-11T03:52:12.7027230Z         Args:
2025-04-11T03:52:12.7027402Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7027569Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7027674Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7027751Z         """
2025-04-11T03:52:12.7027829Z         _lazy_init()
2025-04-11T03:52:12.7027929Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7028031Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7028136Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7028466Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7028608Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7028768Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7028772Z 
2025-04-11T03:52:12.7029013Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7029302Z _____________ test_flash_decoding[False-True-1-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.7029306Z 
2025-04-11T03:52:12.7029456Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7029621Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7029710Z use_new_kcache_layout = False
2025-04-11T03:52:12.7029714Z 
2025-04-11T03:52:12.7029916Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7030021Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7030260Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7030405Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7030522Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7030639Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7030779Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7030887Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7031022Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7031172Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7031264Z     def test_flash_decoding(
2025-04-11T03:52:12.7031341Z         bsz: int,
2025-04-11T03:52:12.7031427Z         block_size: int,
2025-04-11T03:52:12.7031518Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7031600Z         num_attn_heads: int,
2025-04-11T03:52:12.7031687Z         kv_group_num: int,
2025-04-11T03:52:12.7031775Z         same_context_len: bool,
2025-04-11T03:52:12.7031853Z         q_len: int,
2025-04-11T03:52:12.7031940Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7032027Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7032104Z     ):
2025-04-11T03:52:12.7032214Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7032413Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7032594Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7032769Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7032930Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7033085Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7033163Z     
2025-04-11T03:52:12.7033252Z         torch.manual_seed(123)
2025-04-11T03:52:12.7033349Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7033444Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7033448Z 
2025-04-11T03:52:12.7033608Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7033719Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7033726Z 
2025-04-11T03:52:12.7033804Z device = None
2025-04-11T03:52:12.7033811Z 
2025-04-11T03:52:12.7033929Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7034077Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7034153Z     
2025-04-11T03:52:12.7034226Z         Args:
2025-04-11T03:52:12.7034397Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7034562Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7034667Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7034746Z         """
2025-04-11T03:52:12.7034824Z         _lazy_init()
2025-04-11T03:52:12.7034925Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7035025Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7035132Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7035521Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7035657Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7035820Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7035825Z 
2025-04-11T03:52:12.7036063Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7036239Z ____________ test_flash_decoding[False-True-1-False-1-16-16-32-16] _____________
2025-04-11T03:52:12.7036243Z 
2025-04-11T03:52:12.7036497Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7036664Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7036752Z use_new_kcache_layout = False
2025-04-11T03:52:12.7036757Z 
2025-04-11T03:52:12.7036958Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7037065Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7037183Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7037329Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7037447Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7037566Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7037704Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7037810Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7037948Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7038098Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7038193Z     def test_flash_decoding(
2025-04-11T03:52:12.7038269Z         bsz: int,
2025-04-11T03:52:12.7038354Z         block_size: int,
2025-04-11T03:52:12.7038447Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7038528Z         num_attn_heads: int,
2025-04-11T03:52:12.7038615Z         kv_group_num: int,
2025-04-11T03:52:12.7038699Z         same_context_len: bool,
2025-04-11T03:52:12.7038779Z         q_len: int,
2025-04-11T03:52:12.7038864Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7038955Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7039029Z     ):
2025-04-11T03:52:12.7039139Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7039333Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7039511Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7039688Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7039847Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7040011Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7040081Z     
2025-04-11T03:52:12.7040166Z         torch.manual_seed(123)
2025-04-11T03:52:12.7040261Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7040351Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7040355Z 
2025-04-11T03:52:12.7040512Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7040622Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7040626Z 
2025-04-11T03:52:12.7040706Z device = None
2025-04-11T03:52:12.7040710Z 
2025-04-11T03:52:12.7040826Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7040978Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7041054Z     
2025-04-11T03:52:12.7041127Z         Args:
2025-04-11T03:52:12.7041295Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7041562Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7041671Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7041745Z         """
2025-04-11T03:52:12.7041824Z         _lazy_init()
2025-04-11T03:52:12.7041926Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7042027Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7042138Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7042419Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7042736Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7042896Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7042900Z 
2025-04-11T03:52:12.7043142Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7043322Z _____________ test_flash_decoding[False-True-1-False-4-16-8-16-7] ______________
2025-04-11T03:52:12.7043326Z 
2025-04-11T03:52:12.7043479Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7043651Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7043746Z use_new_kcache_layout = False
2025-04-11T03:52:12.7043750Z 
2025-04-11T03:52:12.7043957Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7044064Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7044188Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7044334Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7044456Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7044577Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7044721Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7044833Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7044972Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7045125Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7045220Z     def test_flash_decoding(
2025-04-11T03:52:12.7045301Z         bsz: int,
2025-04-11T03:52:12.7045389Z         block_size: int,
2025-04-11T03:52:12.7045483Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7045569Z         num_attn_heads: int,
2025-04-11T03:52:12.7045659Z         kv_group_num: int,
2025-04-11T03:52:12.7045750Z         same_context_len: bool,
2025-04-11T03:52:12.7045831Z         q_len: int,
2025-04-11T03:52:12.7045920Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7046012Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7046091Z     ):
2025-04-11T03:52:12.7046205Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7046410Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7046593Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7046769Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7046933Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7047097Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7047171Z     
2025-04-11T03:52:12.7047260Z         torch.manual_seed(123)
2025-04-11T03:52:12.7047366Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7047460Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7047464Z 
2025-04-11T03:52:12.7047626Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7047742Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7047849Z 
2025-04-11T03:52:12.7047932Z device = None
2025-04-11T03:52:12.7047936Z 
2025-04-11T03:52:12.7048055Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7048204Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7048278Z     
2025-04-11T03:52:12.7048350Z         Args:
2025-04-11T03:52:12.7048520Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7048683Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7048793Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7048957Z         """
2025-04-11T03:52:12.7049036Z         _lazy_init()
2025-04-11T03:52:12.7049137Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7049239Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7049346Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7049632Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7049768Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7049931Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7049935Z 
2025-04-11T03:52:12.7050170Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7050344Z _____________ test_flash_decoding[False-True-1-False-4-16-8-16-16] _____________
2025-04-11T03:52:12.7050348Z 
2025-04-11T03:52:12.7050499Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7050666Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7050755Z use_new_kcache_layout = False
2025-04-11T03:52:12.7050760Z 
2025-04-11T03:52:12.7050961Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7051070Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7051192Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7051330Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7051447Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7051567Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7051702Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7051810Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7051948Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7052103Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7052197Z     def test_flash_decoding(
2025-04-11T03:52:12.7052273Z         bsz: int,
2025-04-11T03:52:12.7052358Z         block_size: int,
2025-04-11T03:52:12.7052452Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7052538Z         num_attn_heads: int,
2025-04-11T03:52:12.7052621Z         kv_group_num: int,
2025-04-11T03:52:12.7052706Z         same_context_len: bool,
2025-04-11T03:52:12.7052786Z         q_len: int,
2025-04-11T03:52:12.7052871Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7052966Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7053039Z     ):
2025-04-11T03:52:12.7053150Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7053345Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7053525Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7053702Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7053863Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7054135Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7054205Z     
2025-04-11T03:52:12.7054296Z         torch.manual_seed(123)
2025-04-11T03:52:12.7054392Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7054483Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7054487Z 
2025-04-11T03:52:12.7054646Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7054760Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7054764Z 
2025-04-11T03:52:12.7054844Z device = None
2025-04-11T03:52:12.7054849Z 
2025-04-11T03:52:12.7054966Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7055210Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7055285Z     
2025-04-11T03:52:12.7055359Z         Args:
2025-04-11T03:52:12.7055531Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7055697Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7055806Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7055878Z         """
2025-04-11T03:52:12.7055955Z         _lazy_init()
2025-04-11T03:52:12.7056056Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7056159Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7056271Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7056557Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7056701Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7056858Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7056862Z 
2025-04-11T03:52:12.7057106Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7057281Z _____________ test_flash_decoding[False-True-1-False-4-16-8-32-7] ______________
2025-04-11T03:52:12.7057285Z 
2025-04-11T03:52:12.7057433Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7057601Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7057690Z use_new_kcache_layout = False
2025-04-11T03:52:12.7057694Z 
2025-04-11T03:52:12.7057898Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7058000Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7058121Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7058263Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7058378Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7058495Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7058630Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7058739Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7058875Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7059026Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7059114Z     def test_flash_decoding(
2025-04-11T03:52:12.7059189Z         bsz: int,
2025-04-11T03:52:12.7059275Z         block_size: int,
2025-04-11T03:52:12.7059364Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7059450Z         num_attn_heads: int,
2025-04-11T03:52:12.7059533Z         kv_group_num: int,
2025-04-11T03:52:12.7059623Z         same_context_len: bool,
2025-04-11T03:52:12.7059706Z         q_len: int,
2025-04-11T03:52:12.7059789Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7059884Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7059954Z     ):
2025-04-11T03:52:12.7060066Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7060364Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7060543Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7060717Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7060878Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7061036Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7061107Z     
2025-04-11T03:52:12.7061196Z         torch.manual_seed(123)
2025-04-11T03:52:12.7061399Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7061489Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7061494Z 
2025-04-11T03:52:12.7061655Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7061772Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7061779Z 
2025-04-11T03:52:12.7061860Z device = None
2025-04-11T03:52:12.7061864Z 
2025-04-11T03:52:12.7061979Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7062135Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7062206Z     
2025-04-11T03:52:12.7062280Z         Args:
2025-04-11T03:52:12.7062450Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7062615Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7062725Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7062802Z         """
2025-04-11T03:52:12.7062880Z         _lazy_init()
2025-04-11T03:52:12.7062981Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7063080Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7063189Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7063472Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7063612Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7063770Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7063775Z 
2025-04-11T03:52:12.7064013Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7064183Z _____________ test_flash_decoding[False-True-1-False-4-16-8-32-16] _____________
2025-04-11T03:52:12.7064186Z 
2025-04-11T03:52:12.7064340Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7064506Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7064595Z use_new_kcache_layout = False
2025-04-11T03:52:12.7064599Z 
2025-04-11T03:52:12.7064800Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7064908Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7065029Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7065165Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7065279Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7065395Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7065531Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7065639Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7065774Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7065930Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7066016Z     def test_flash_decoding(
2025-04-11T03:52:12.7066092Z         bsz: int,
2025-04-11T03:52:12.7066177Z         block_size: int,
2025-04-11T03:52:12.7066378Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7066466Z         num_attn_heads: int,
2025-04-11T03:52:12.7066549Z         kv_group_num: int,
2025-04-11T03:52:12.7066632Z         same_context_len: bool,
2025-04-11T03:52:12.7066713Z         q_len: int,
2025-04-11T03:52:12.7066797Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7066887Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7066959Z     ):
2025-04-11T03:52:12.7067068Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7067262Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7067444Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7067726Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7067887Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7068053Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7068125Z     
2025-04-11T03:52:12.7068211Z         torch.manual_seed(123)
2025-04-11T03:52:12.7068306Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7068396Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7068400Z 
2025-04-11T03:52:12.7068592Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7068706Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7068711Z 
2025-04-11T03:52:12.7068790Z device = None
2025-04-11T03:52:12.7068795Z 
2025-04-11T03:52:12.7068910Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7069066Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7069137Z     
2025-04-11T03:52:12.7069209Z         Args:
2025-04-11T03:52:12.7069381Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7069548Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7069659Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7069732Z         """
2025-04-11T03:52:12.7069809Z         _lazy_init()
2025-04-11T03:52:12.7069910Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7070010Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7070123Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7070407Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7070553Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7070709Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7070713Z 
2025-04-11T03:52:12.7070954Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7071121Z _____________ test_flash_decoding[False-True-1-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.7071125Z 
2025-04-11T03:52:12.7071275Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7071439Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7071528Z use_new_kcache_layout = False
2025-04-11T03:52:12.7071532Z 
2025-04-11T03:52:12.7071730Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7071833Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7071952Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7072094Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7072208Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7072325Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7072461Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7072682Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7072822Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7072981Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7073072Z     def test_flash_decoding(
2025-04-11T03:52:12.7073151Z         bsz: int,
2025-04-11T03:52:12.7073243Z         block_size: int,
2025-04-11T03:52:12.7073335Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7073425Z         num_attn_heads: int,
2025-04-11T03:52:12.7073511Z         kv_group_num: int,
2025-04-11T03:52:12.7073704Z         same_context_len: bool,
2025-04-11T03:52:12.7073784Z         q_len: int,
2025-04-11T03:52:12.7073871Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7073963Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7074036Z     ):
2025-04-11T03:52:12.7074150Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7074345Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7074529Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7074704Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7074868Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7075028Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7075099Z     
2025-04-11T03:52:12.7075189Z         torch.manual_seed(123)
2025-04-11T03:52:12.7075282Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7075371Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7075375Z 
2025-04-11T03:52:12.7075532Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7075645Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7075652Z 
2025-04-11T03:52:12.7075735Z device = None
2025-04-11T03:52:12.7075739Z 
2025-04-11T03:52:12.7075855Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7076012Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7076083Z     
2025-04-11T03:52:12.7076156Z         Args:
2025-04-11T03:52:12.7076327Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7076492Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7076603Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7076679Z         """
2025-04-11T03:52:12.7076759Z         _lazy_init()
2025-04-11T03:52:12.7076856Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7076961Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7077070Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7077355Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7077495Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7077652Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7077656Z 
2025-04-11T03:52:12.7077896Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7078066Z ____________ test_flash_decoding[False-True-1-False-4-16-16-16-16] _____________
2025-04-11T03:52:12.7078070Z 
2025-04-11T03:52:12.7078223Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7078388Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7078479Z use_new_kcache_layout = False
2025-04-11T03:52:12.7078483Z 
2025-04-11T03:52:12.7078686Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7078904Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7079025Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7079164Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7079282Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7079399Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7079535Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7079643Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7079778Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7080034Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7080124Z     def test_flash_decoding(
2025-04-11T03:52:12.7080201Z         bsz: int,
2025-04-11T03:52:12.7080287Z         block_size: int,
2025-04-11T03:52:12.7080377Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7080467Z         num_attn_heads: int,
2025-04-11T03:52:12.7080552Z         kv_group_num: int,
2025-04-11T03:52:12.7080642Z         same_context_len: bool,
2025-04-11T03:52:12.7080719Z         q_len: int,
2025-04-11T03:52:12.7080806Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7080898Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7080971Z     ):
2025-04-11T03:52:12.7081085Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7081277Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7081458Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7081634Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7081795Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7081957Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7082032Z     
2025-04-11T03:52:12.7082119Z         torch.manual_seed(123)
2025-04-11T03:52:12.7082209Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7082301Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7082305Z 
2025-04-11T03:52:12.7082463Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7082574Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7082578Z 
2025-04-11T03:52:12.7082660Z device = None
2025-04-11T03:52:12.7082665Z 
2025-04-11T03:52:12.7082781Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7082936Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7083007Z     
2025-04-11T03:52:12.7083080Z         Args:
2025-04-11T03:52:12.7083251Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7083417Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7083528Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7083602Z         """
2025-04-11T03:52:12.7083683Z         _lazy_init()
2025-04-11T03:52:12.7083779Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7083879Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7083987Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7084268Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7084411Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7084567Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7084572Z 
2025-04-11T03:52:12.7084810Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7085109Z _____________ test_flash_decoding[False-True-1-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.7085113Z 
2025-04-11T03:52:12.7085265Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7085427Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7085514Z use_new_kcache_layout = False
2025-04-11T03:52:12.7085518Z 
2025-04-11T03:52:12.7085721Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7085824Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7085944Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7086183Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7086303Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7086416Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7086555Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7086667Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7086801Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7086959Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7087047Z     def test_flash_decoding(
2025-04-11T03:52:12.7087121Z         bsz: int,
2025-04-11T03:52:12.7087207Z         block_size: int,
2025-04-11T03:52:12.7087297Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7087382Z         num_attn_heads: int,
2025-04-11T03:52:12.7087466Z         kv_group_num: int,
2025-04-11T03:52:12.7087555Z         same_context_len: bool,
2025-04-11T03:52:12.7087635Z         q_len: int,
2025-04-11T03:52:12.7087722Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7087813Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7087884Z     ):
2025-04-11T03:52:12.7088001Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7088194Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7088376Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7088551Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7088716Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7088877Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7088951Z     
2025-04-11T03:52:12.7089039Z         torch.manual_seed(123)
2025-04-11T03:52:12.7089131Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7089220Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7089224Z 
2025-04-11T03:52:12.7089387Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7089498Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7089505Z 
2025-04-11T03:52:12.7089587Z device = None
2025-04-11T03:52:12.7089591Z 
2025-04-11T03:52:12.7089708Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7089861Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7089932Z     
2025-04-11T03:52:12.7090005Z         Args:
2025-04-11T03:52:12.7090180Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7090346Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7090455Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7090531Z         """
2025-04-11T03:52:12.7090612Z         _lazy_init()
2025-04-11T03:52:12.7090711Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7090814Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7090925Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7091312Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7091455Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7091614Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7091619Z 
2025-04-11T03:52:12.7091862Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7092030Z ____________ test_flash_decoding[False-True-1-False-4-16-16-32-16] _____________
2025-04-11T03:52:12.7092034Z 
2025-04-11T03:52:12.7092186Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7092474Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
2025-04-11T03:52:12.7092564Z use_new_kcache_layout = False
2025-04-11T03:52:12.7092568Z 
2025-04-11T03:52:12.7092769Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7092877Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7092997Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7093134Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7093256Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7093368Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7093506Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7093615Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7093750Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7093907Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7093994Z     def test_flash_decoding(
2025-04-11T03:52:12.7094076Z         bsz: int,
2025-04-11T03:52:12.7094159Z         block_size: int,
2025-04-11T03:52:12.7094250Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7094347Z         num_attn_heads: int,
2025-04-11T03:52:12.7094430Z         kv_group_num: int,
2025-04-11T03:52:12.7094518Z         same_context_len: bool,
2025-04-11T03:52:12.7094594Z         q_len: int,
2025-04-11T03:52:12.7094679Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7094773Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7094846Z     ):
2025-04-11T03:52:12.7094961Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7095150Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7095333Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7095509Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7095668Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7095828Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7095902Z     
2025-04-11T03:52:12.7095991Z         torch.manual_seed(123)
2025-04-11T03:52:12.7096079Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7096169Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7096177Z 
2025-04-11T03:52:12.7096328Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7096440Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7096445Z 
2025-04-11T03:52:12.7096525Z device = None
2025-04-11T03:52:12.7096530Z 
2025-04-11T03:52:12.7096644Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7096799Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7096870Z     
2025-04-11T03:52:12.7096948Z         Args:
2025-04-11T03:52:12.7097115Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7097381Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7097492Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7097565Z         """
2025-04-11T03:52:12.7097647Z         _lazy_init()
2025-04-11T03:52:12.7097743Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7097846Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7097956Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7098238Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7098471Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7098632Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7098637Z 
2025-04-11T03:52:12.7098881Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7099053Z ______________ test_flash_decoding[False-True-5-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.7099057Z 
2025-04-11T03:52:12.7099214Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7099378Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7099468Z use_new_kcache_layout = False
2025-04-11T03:52:12.7099479Z 
2025-04-11T03:52:12.7099680Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7099784Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7099907Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7100048Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7100167Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7100281Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7100417Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7100528Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7100665Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7100820Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7100906Z     def test_flash_decoding(
2025-04-11T03:52:12.7100985Z         bsz: int,
2025-04-11T03:52:12.7101068Z         block_size: int,
2025-04-11T03:52:12.7101159Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7101248Z         num_attn_heads: int,
2025-04-11T03:52:12.7101332Z         kv_group_num: int,
2025-04-11T03:52:12.7101419Z         same_context_len: bool,
2025-04-11T03:52:12.7101499Z         q_len: int,
2025-04-11T03:52:12.7101584Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7101676Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7101747Z     ):
2025-04-11T03:52:12.7101866Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7102059Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7102244Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7102412Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7102572Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7102732Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7102805Z     
2025-04-11T03:52:12.7102896Z         torch.manual_seed(123)
2025-04-11T03:52:12.7102989Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7103079Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7103087Z 
2025-04-11T03:52:12.7103241Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7103354Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7103464Z 
2025-04-11T03:52:12.7103549Z device = None
2025-04-11T03:52:12.7103553Z 
2025-04-11T03:52:12.7103668Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7103821Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7103894Z     
2025-04-11T03:52:12.7103971Z         Args:
2025-04-11T03:52:12.7104137Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7104300Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7104408Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7104578Z         """
2025-04-11T03:52:12.7104660Z         _lazy_init()
2025-04-11T03:52:12.7104758Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7104862Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7104971Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7105259Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7105399Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7105557Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7105561Z 
2025-04-11T03:52:12.7105807Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7105976Z _____________ test_flash_decoding[False-True-5-True-1-16-8-16-16] ______________
2025-04-11T03:52:12.7105980Z 
2025-04-11T03:52:12.7106134Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7106298Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7106386Z use_new_kcache_layout = False
2025-04-11T03:52:12.7106393Z 
2025-04-11T03:52:12.7106589Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7106696Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7106821Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7106962Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7107082Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7107196Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7107333Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7107445Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7107581Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7107741Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7107829Z     def test_flash_decoding(
2025-04-11T03:52:12.7107908Z         bsz: int,
2025-04-11T03:52:12.7107988Z         block_size: int,
2025-04-11T03:52:12.7108077Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7108168Z         num_attn_heads: int,
2025-04-11T03:52:12.7108251Z         kv_group_num: int,
2025-04-11T03:52:12.7108342Z         same_context_len: bool,
2025-04-11T03:52:12.7108459Z         q_len: int,
2025-04-11T03:52:12.7108545Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7108639Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7108710Z     ):
2025-04-11T03:52:12.7108824Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7109016Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7109202Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7109378Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7109541Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7109703Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7109909Z     
2025-04-11T03:52:12.7110001Z         torch.manual_seed(123)
2025-04-11T03:52:12.7110090Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7110185Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7110188Z 
2025-04-11T03:52:12.7110344Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7110458Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7110462Z 
2025-04-11T03:52:12.7110542Z device = None
2025-04-11T03:52:12.7110546Z 
2025-04-11T03:52:12.7110662Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7110923Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7110993Z     
2025-04-11T03:52:12.7111071Z         Args:
2025-04-11T03:52:12.7111236Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7111401Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7111511Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7111585Z         """
2025-04-11T03:52:12.7111674Z         _lazy_init()
2025-04-11T03:52:12.7111770Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7111876Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7111979Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7112263Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7112411Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7112569Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7112574Z 
2025-04-11T03:52:12.7112817Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7112984Z ______________ test_flash_decoding[False-True-5-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.7112988Z 
2025-04-11T03:52:12.7113142Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7113302Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7113394Z use_new_kcache_layout = False
2025-04-11T03:52:12.7113398Z 
2025-04-11T03:52:12.7113598Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7113703Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7113826Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7113965Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7114087Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7114199Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7114339Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7114446Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7114582Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7114738Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7114824Z     def test_flash_decoding(
2025-04-11T03:52:12.7114904Z         bsz: int,
2025-04-11T03:52:12.7114987Z         block_size: int,
2025-04-11T03:52:12.7115075Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7115164Z         num_attn_heads: int,
2025-04-11T03:52:12.7115245Z         kv_group_num: int,
2025-04-11T03:52:12.7115332Z         same_context_len: bool,
2025-04-11T03:52:12.7115411Z         q_len: int,
2025-04-11T03:52:12.7115502Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7115589Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7115663Z     ):
2025-04-11T03:52:12.7115773Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7116076Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7116265Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7116435Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7116601Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7116764Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7116839Z     
2025-04-11T03:52:12.7116929Z         torch.manual_seed(123)
2025-04-11T03:52:12.7117114Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7117208Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7117212Z 
2025-04-11T03:52:12.7117364Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7117478Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7117489Z 
2025-04-11T03:52:12.7117566Z device = None
2025-04-11T03:52:12.7117570Z 
2025-04-11T03:52:12.7117685Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7117837Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7117908Z     
2025-04-11T03:52:12.7117983Z         Args:
2025-04-11T03:52:12.7118148Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7118310Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7118422Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7118499Z         """
2025-04-11T03:52:12.7118582Z         _lazy_init()
2025-04-11T03:52:12.7118678Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7118781Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7118886Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7119172Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7119310Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7119470Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7119474Z 
2025-04-11T03:52:12.7119716Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7119880Z _____________ test_flash_decoding[False-True-5-True-1-16-8-32-16] ______________
2025-04-11T03:52:12.7119884Z 
2025-04-11T03:52:12.7120033Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7120198Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7120292Z use_new_kcache_layout = False
2025-04-11T03:52:12.7120297Z 
2025-04-11T03:52:12.7120493Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7120599Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7120719Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7120857Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7120976Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7121087Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7121224Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7121326Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7121459Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7121615Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7121702Z     def test_flash_decoding(
2025-04-11T03:52:12.7121785Z         bsz: int,
2025-04-11T03:52:12.7121879Z         block_size: int,
2025-04-11T03:52:12.7121998Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7122194Z         num_attn_heads: int,
2025-04-11T03:52:12.7122280Z         kv_group_num: int,
2025-04-11T03:52:12.7122370Z         same_context_len: bool,
2025-04-11T03:52:12.7122446Z         q_len: int,
2025-04-11T03:52:12.7122532Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7122622Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7122696Z     ):
2025-04-11T03:52:12.7122809Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7123001Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7123185Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7123455Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7123619Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7123776Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7123850Z     
2025-04-11T03:52:12.7123942Z         torch.manual_seed(123)
2025-04-11T03:52:12.7124031Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7124124Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7124128Z 
2025-04-11T03:52:12.7124281Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7124394Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7124404Z 
2025-04-11T03:52:12.7124480Z device = None
2025-04-11T03:52:12.7124485Z 
2025-04-11T03:52:12.7124602Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7124759Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7124830Z     
2025-04-11T03:52:12.7124906Z         Args:
2025-04-11T03:52:12.7125073Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7125240Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7125348Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7125422Z         """
2025-04-11T03:52:12.7125506Z         _lazy_init()
2025-04-11T03:52:12.7125604Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7125708Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7125816Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7126101Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7126239Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7126400Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7126405Z 
2025-04-11T03:52:12.7126649Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7126819Z _____________ test_flash_decoding[False-True-5-True-1-16-16-16-7] ______________
2025-04-11T03:52:12.7126823Z 
2025-04-11T03:52:12.7126977Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7127136Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7127227Z use_new_kcache_layout = False
2025-04-11T03:52:12.7127231Z 
2025-04-11T03:52:12.7127431Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7127537Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7127659Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7127804Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7127927Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7128039Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7128184Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7128400Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7128537Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7128691Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7128778Z     def test_flash_decoding(
2025-04-11T03:52:12.7128860Z         bsz: int,
2025-04-11T03:52:12.7128941Z         block_size: int,
2025-04-11T03:52:12.7129034Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7129116Z         num_attn_heads: int,
2025-04-11T03:52:12.7129198Z         kv_group_num: int,
2025-04-11T03:52:12.7129288Z         same_context_len: bool,
2025-04-11T03:52:12.7129469Z         q_len: int,
2025-04-11T03:52:12.7129562Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7129650Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7129724Z     ):
2025-04-11T03:52:12.7129839Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7130033Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7130217Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7130389Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7130556Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7130712Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7130788Z     
2025-04-11T03:52:12.7130879Z         torch.manual_seed(123)
2025-04-11T03:52:12.7130967Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7131067Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7131071Z 
2025-04-11T03:52:12.7131222Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7131340Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7131347Z 
2025-04-11T03:52:12.7131424Z device = None
2025-04-11T03:52:12.7131428Z 
2025-04-11T03:52:12.7131545Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7131703Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7131775Z     
2025-04-11T03:52:12.7131852Z         Args:
2025-04-11T03:52:12.7132017Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7132187Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7132293Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7132369Z         """
2025-04-11T03:52:12.7132450Z         _lazy_init()
2025-04-11T03:52:12.7132545Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7132651Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7132754Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7133037Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7133178Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7133333Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7133338Z 
2025-04-11T03:52:12.7133581Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7133748Z _____________ test_flash_decoding[False-True-5-True-1-16-16-16-16] _____________
2025-04-11T03:52:12.7133753Z 
2025-04-11T03:52:12.7133905Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7134073Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7134163Z use_new_kcache_layout = False
2025-04-11T03:52:12.7134167Z 
2025-04-11T03:52:12.7134367Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7134587Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7134707Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7134844Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7134967Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7135082Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7135222Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7135331Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7135466Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7135721Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7135809Z     def test_flash_decoding(
2025-04-11T03:52:12.7135889Z         bsz: int,
2025-04-11T03:52:12.7135970Z         block_size: int,
2025-04-11T03:52:12.7136063Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7136152Z         num_attn_heads: int,
2025-04-11T03:52:12.7136235Z         kv_group_num: int,
2025-04-11T03:52:12.7136329Z         same_context_len: bool,
2025-04-11T03:52:12.7136405Z         q_len: int,
2025-04-11T03:52:12.7136494Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7136582Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7136653Z     ):
2025-04-11T03:52:12.7136770Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7136961Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7137148Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7137321Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7137487Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7137643Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7137717Z     
2025-04-11T03:52:12.7137811Z         torch.manual_seed(123)
2025-04-11T03:52:12.7137900Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7137995Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7137999Z 
2025-04-11T03:52:12.7138154Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7138269Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7138273Z 
2025-04-11T03:52:12.7138350Z device = None
2025-04-11T03:52:12.7138355Z 
2025-04-11T03:52:12.7138474Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7138630Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7138703Z     
2025-04-11T03:52:12.7138782Z         Args:
2025-04-11T03:52:12.7138952Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7139117Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7139226Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7139301Z         """
2025-04-11T03:52:12.7139384Z         _lazy_init()
2025-04-11T03:52:12.7139479Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7139586Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7139693Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7139979Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7140114Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7140273Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7140277Z 
2025-04-11T03:52:12.7140523Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7140806Z _____________ test_flash_decoding[False-True-5-True-1-16-16-32-7] ______________
2025-04-11T03:52:12.7140810Z 
2025-04-11T03:52:12.7140963Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7141124Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7141217Z use_new_kcache_layout = False
2025-04-11T03:52:12.7141222Z 
2025-04-11T03:52:12.7141419Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7141524Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7141642Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7141945Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7142070Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7142184Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7142324Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7142432Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7142573Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7142722Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7142810Z     def test_flash_decoding(
2025-04-11T03:52:12.7142892Z         bsz: int,
2025-04-11T03:52:12.7142972Z         block_size: int,
2025-04-11T03:52:12.7143069Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7143150Z         num_attn_heads: int,
2025-04-11T03:52:12.7143231Z         kv_group_num: int,
2025-04-11T03:52:12.7143322Z         same_context_len: bool,
2025-04-11T03:52:12.7143401Z         q_len: int,
2025-04-11T03:52:12.7143491Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7143579Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7143651Z     ):
2025-04-11T03:52:12.7143770Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7143960Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7144147Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7144313Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7144476Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7144634Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7144707Z     
2025-04-11T03:52:12.7144794Z         torch.manual_seed(123)
2025-04-11T03:52:12.7144883Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7144979Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7144983Z 
2025-04-11T03:52:12.7145135Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7145253Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7145260Z 
2025-04-11T03:52:12.7145337Z device = None
2025-04-11T03:52:12.7145341Z 
2025-04-11T03:52:12.7145463Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7145610Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7145681Z     
2025-04-11T03:52:12.7145757Z         Args:
2025-04-11T03:52:12.7145921Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7146087Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7146193Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7146268Z         """
2025-04-11T03:52:12.7146350Z         _lazy_init()
2025-04-11T03:52:12.7146445Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7146553Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7146660Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7146945Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7147188Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7147350Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7147359Z 
2025-04-11T03:52:12.7147597Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7147766Z _____________ test_flash_decoding[False-True-5-True-1-16-16-32-16] _____________
2025-04-11T03:52:12.7147771Z 
2025-04-11T03:52:12.7147927Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7148178Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7148270Z use_new_kcache_layout = False
2025-04-11T03:52:12.7148274Z 
2025-04-11T03:52:12.7148514Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7148630Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7148748Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7148886Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7149009Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7149126Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7149266Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7149369Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7149509Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7149665Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7149750Z     def test_flash_decoding(
2025-04-11T03:52:12.7149834Z         bsz: int,
2025-04-11T03:52:12.7149916Z         block_size: int,
2025-04-11T03:52:12.7150009Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7150097Z         num_attn_heads: int,
2025-04-11T03:52:12.7150179Z         kv_group_num: int,
2025-04-11T03:52:12.7150269Z         same_context_len: bool,
2025-04-11T03:52:12.7150345Z         q_len: int,
2025-04-11T03:52:12.7150433Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7150522Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7150596Z     ):
2025-04-11T03:52:12.7150704Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7150897Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7151083Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7151256Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7151422Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7151582Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7151661Z     
2025-04-11T03:52:12.7151749Z         torch.manual_seed(123)
2025-04-11T03:52:12.7151837Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7151930Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7151934Z 
2025-04-11T03:52:12.7152088Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7152202Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7152206Z 
2025-04-11T03:52:12.7152285Z device = None
2025-04-11T03:52:12.7152289Z 
2025-04-11T03:52:12.7152412Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7152566Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7152638Z     
2025-04-11T03:52:12.7152718Z         Args:
2025-04-11T03:52:12.7152882Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7153048Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7153297Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7153373Z         """
2025-04-11T03:52:12.7153452Z         _lazy_init()
2025-04-11T03:52:12.7153548Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7153656Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7153760Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7154051Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7154188Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7154468Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7154478Z 
2025-04-11T03:52:12.7154715Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7154885Z ______________ test_flash_decoding[False-True-5-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.7154892Z 
2025-04-11T03:52:12.7155046Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7155207Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7155299Z use_new_kcache_layout = False
2025-04-11T03:52:12.7155303Z 
2025-04-11T03:52:12.7155500Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7155606Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7155723Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7155864Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7155984Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7156096Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7156238Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7156345Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7156482Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7156634Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7156721Z     def test_flash_decoding(
2025-04-11T03:52:12.7156801Z         bsz: int,
2025-04-11T03:52:12.7156882Z         block_size: int,
2025-04-11T03:52:12.7156974Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7157057Z         num_attn_heads: int,
2025-04-11T03:52:12.7157139Z         kv_group_num: int,
2025-04-11T03:52:12.7157227Z         same_context_len: bool,
2025-04-11T03:52:12.7157304Z         q_len: int,
2025-04-11T03:52:12.7157396Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7157484Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7157560Z     ):
2025-04-11T03:52:12.7157668Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7157861Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7158045Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7158213Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7158381Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7158536Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7158612Z     
2025-04-11T03:52:12.7158697Z         torch.manual_seed(123)
2025-04-11T03:52:12.7158786Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7158885Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7158889Z 
2025-04-11T03:52:12.7159042Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7159156Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7159269Z 
2025-04-11T03:52:12.7159352Z device = None
2025-04-11T03:52:12.7159357Z 
2025-04-11T03:52:12.7159479Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7159628Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7159699Z     
2025-04-11T03:52:12.7159775Z         Args:
2025-04-11T03:52:12.7159943Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7160112Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7160218Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7160392Z         """
2025-04-11T03:52:12.7160471Z         _lazy_init()
2025-04-11T03:52:12.7160569Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7160675Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7160783Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7161074Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7161215Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7161380Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7161385Z 
2025-04-11T03:52:12.7161625Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7161796Z _____________ test_flash_decoding[False-True-5-True-4-16-8-16-16] ______________
2025-04-11T03:52:12.7161805Z 
2025-04-11T03:52:12.7161958Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7162127Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7162223Z use_new_kcache_layout = False
2025-04-11T03:52:12.7162227Z 
2025-04-11T03:52:12.7162426Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7162540Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7162657Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7162800Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7162924Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7163042Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7163185Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7163291Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7163435Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7163595Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7163685Z     def test_flash_decoding(
2025-04-11T03:52:12.7163764Z         bsz: int,
2025-04-11T03:52:12.7163846Z         block_size: int,
2025-04-11T03:52:12.7163940Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7164027Z         num_attn_heads: int,
2025-04-11T03:52:12.7164114Z         kv_group_num: int,
2025-04-11T03:52:12.7164200Z         same_context_len: bool,
2025-04-11T03:52:12.7164276Z         q_len: int,
2025-04-11T03:52:12.7164365Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7164453Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7164531Z     ):
2025-04-11T03:52:12.7164643Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7164837Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7165021Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7165200Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7165371Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7165534Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7165724Z     
2025-04-11T03:52:12.7165815Z         torch.manual_seed(123)
2025-04-11T03:52:12.7165905Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7166005Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7166009Z 
2025-04-11T03:52:12.7166164Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7166280Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7166285Z 
2025-04-11T03:52:12.7166361Z device = None
2025-04-11T03:52:12.7166365Z 
2025-04-11T03:52:12.7166487Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7166740Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7166814Z     
2025-04-11T03:52:12.7166896Z         Args:
2025-04-11T03:52:12.7167062Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7167235Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7167344Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7167423Z         """
2025-04-11T03:52:12.7167502Z         _lazy_init()
2025-04-11T03:52:12.7167599Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7167710Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7167813Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7168101Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7168236Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7168400Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7168404Z 
2025-04-11T03:52:12.7168642Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7168809Z ______________ test_flash_decoding[False-True-5-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.7168821Z 
2025-04-11T03:52:12.7168969Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7169129Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7169223Z use_new_kcache_layout = False
2025-04-11T03:52:12.7169227Z 
2025-04-11T03:52:12.7169424Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7169533Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7169650Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7169797Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7169915Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7170027Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7170167Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7170277Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7170418Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7170569Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7170662Z     def test_flash_decoding(
2025-04-11T03:52:12.7170739Z         bsz: int,
2025-04-11T03:52:12.7170820Z         block_size: int,
2025-04-11T03:52:12.7170914Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7170997Z         num_attn_heads: int,
2025-04-11T03:52:12.7171084Z         kv_group_num: int,
2025-04-11T03:52:12.7171171Z         same_context_len: bool,
2025-04-11T03:52:12.7171248Z         q_len: int,
2025-04-11T03:52:12.7171337Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7171425Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7171501Z     ):
2025-04-11T03:52:12.7171611Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7171801Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7172091Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7172266Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7172437Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7172596Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7172671Z     
2025-04-11T03:52:12.7172759Z         torch.manual_seed(123)
2025-04-11T03:52:12.7172847Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7173038Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7173043Z 
2025-04-11T03:52:12.7173199Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7173315Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7173319Z 
2025-04-11T03:52:12.7173400Z device = None
2025-04-11T03:52:12.7173404Z 
2025-04-11T03:52:12.7173524Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7173671Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7173748Z     
2025-04-11T03:52:12.7173822Z         Args:
2025-04-11T03:52:12.7173988Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7174157Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7174264Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7174344Z         """
2025-04-11T03:52:12.7174426Z         _lazy_init()
2025-04-11T03:52:12.7174521Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7174628Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7174732Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7175022Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7175160Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7175322Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7175327Z 
2025-04-11T03:52:12.7175565Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7175735Z _____________ test_flash_decoding[False-True-5-True-4-16-8-32-16] ______________
2025-04-11T03:52:12.7175740Z 
2025-04-11T03:52:12.7175888Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7176052Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7176145Z use_new_kcache_layout = False
2025-04-11T03:52:12.7176149Z 
2025-04-11T03:52:12.7176348Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7176459Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7176576Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7176722Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7176840Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7176954Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7177095Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7177198Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7177343Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7177497Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7177590Z     def test_flash_decoding(
2025-04-11T03:52:12.7177666Z         bsz: int,
2025-04-11T03:52:12.7177748Z         block_size: int,
2025-04-11T03:52:12.7177842Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7178056Z         num_attn_heads: int,
2025-04-11T03:52:12.7178142Z         kv_group_num: int,
2025-04-11T03:52:12.7178227Z         same_context_len: bool,
2025-04-11T03:52:12.7178304Z         q_len: int,
2025-04-11T03:52:12.7178394Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7178482Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7178557Z     ):
2025-04-11T03:52:12.7178669Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7178862Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7179050Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7179329Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7179498Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7179658Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7179738Z     
2025-04-11T03:52:12.7179824Z         torch.manual_seed(123)
2025-04-11T03:52:12.7179914Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7180012Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7180016Z 
2025-04-11T03:52:12.7180171Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7180293Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7180297Z 
2025-04-11T03:52:12.7180373Z device = None
2025-04-11T03:52:12.7180380Z 
2025-04-11T03:52:12.7180501Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7180656Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7180733Z     
2025-04-11T03:52:12.7180806Z         Args:
2025-04-11T03:52:12.7180972Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7181143Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7181253Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7181331Z         """
2025-04-11T03:52:12.7181410Z         _lazy_init()
2025-04-11T03:52:12.7181505Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7181613Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7181718Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7182007Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7182144Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7182309Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7182313Z 
2025-04-11T03:52:12.7182551Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7182723Z _____________ test_flash_decoding[False-True-5-True-4-16-16-16-7] ______________
2025-04-11T03:52:12.7182730Z 
2025-04-11T03:52:12.7182882Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7183046Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7183137Z use_new_kcache_layout = False
2025-04-11T03:52:12.7183140Z 
2025-04-11T03:52:12.7183340Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7183447Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7183561Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7183709Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7183825Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7183938Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7184079Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7184292Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7184432Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7184583Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7184673Z     def test_flash_decoding(
2025-04-11T03:52:12.7184750Z         bsz: int,
2025-04-11T03:52:12.7184833Z         block_size: int,
2025-04-11T03:52:12.7184929Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7185012Z         num_attn_heads: int,
2025-04-11T03:52:12.7185100Z         kv_group_num: int,
2025-04-11T03:52:12.7185184Z         same_context_len: bool,
2025-04-11T03:52:12.7185350Z         q_len: int,
2025-04-11T03:52:12.7185443Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7185532Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7185609Z     ):
2025-04-11T03:52:12.7185719Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7185912Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7186097Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7186265Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7186432Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7186589Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7186666Z     
2025-04-11T03:52:12.7186752Z         torch.manual_seed(123)
2025-04-11T03:52:12.7186846Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7186941Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7186945Z 
2025-04-11T03:52:12.7187101Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7187219Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7187223Z 
2025-04-11T03:52:12.7187304Z device = None
2025-04-11T03:52:12.7187308Z 
2025-04-11T03:52:12.7187428Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7187576Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7187649Z     
2025-04-11T03:52:12.7187721Z         Args:
2025-04-11T03:52:12.7187885Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7188052Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7188157Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7188234Z         """
2025-04-11T03:52:12.7188315Z         _lazy_init()
2025-04-11T03:52:12.7188452Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7188553Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7188660Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7188949Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7189087Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7189249Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7189254Z 
2025-04-11T03:52:12.7189492Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7189662Z _____________ test_flash_decoding[False-True-5-True-4-16-16-16-16] _____________
2025-04-11T03:52:12.7189666Z 
2025-04-11T03:52:12.7189818Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7189980Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7190071Z use_new_kcache_layout = False
2025-04-11T03:52:12.7190075Z 
2025-04-11T03:52:12.7190274Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7190512Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7190633Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7190776Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7190892Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7191012Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7191150Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7191257Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7191397Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7191658Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7191752Z     def test_flash_decoding(
2025-04-11T03:52:12.7191827Z         bsz: int,
2025-04-11T03:52:12.7191908Z         block_size: int,
2025-04-11T03:52:12.7192000Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7192082Z         num_attn_heads: int,
2025-04-11T03:52:12.7192173Z         kv_group_num: int,
2025-04-11T03:52:12.7192258Z         same_context_len: bool,
2025-04-11T03:52:12.7192335Z         q_len: int,
2025-04-11T03:52:12.7192420Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7192508Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7192587Z     ):
2025-04-11T03:52:12.7192695Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7192889Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7193067Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7193239Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7193404Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7193565Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7193645Z     
2025-04-11T03:52:12.7193731Z         torch.manual_seed(123)
2025-04-11T03:52:12.7193822Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7193911Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7193915Z 
2025-04-11T03:52:12.7194067Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7194181Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7194185Z 
2025-04-11T03:52:12.7194261Z device = None
2025-04-11T03:52:12.7194265Z 
2025-04-11T03:52:12.7194388Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7194537Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7194616Z     
2025-04-11T03:52:12.7194690Z         Args:
2025-04-11T03:52:12.7194856Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7195026Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7195133Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7195211Z         """
2025-04-11T03:52:12.7195289Z         _lazy_init()
2025-04-11T03:52:12.7195389Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7195490Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7195594Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7195877Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7196015Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7196181Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7196185Z 
2025-04-11T03:52:12.7196424Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7196588Z _____________ test_flash_decoding[False-True-5-True-4-16-16-32-7] ______________
2025-04-11T03:52:12.7196701Z 
2025-04-11T03:52:12.7196851Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7197015Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7197104Z use_new_kcache_layout = False
2025-04-11T03:52:12.7197109Z 
2025-04-11T03:52:12.7197305Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7197412Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7197529Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7197774Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7197893Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7198010Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7198145Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7198254Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7198396Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7198547Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7198642Z     def test_flash_decoding(
2025-04-11T03:52:12.7198718Z         bsz: int,
2025-04-11T03:52:12.7198801Z         block_size: int,
2025-04-11T03:52:12.7198894Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7198978Z         num_attn_heads: int,
2025-04-11T03:52:12.7199066Z         kv_group_num: int,
2025-04-11T03:52:12.7199152Z         same_context_len: bool,
2025-04-11T03:52:12.7199231Z         q_len: int,
2025-04-11T03:52:12.7199321Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7199408Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7199486Z     ):
2025-04-11T03:52:12.7199595Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7199787Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7199971Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7200142Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7200310Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7200466Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7200543Z     
2025-04-11T03:52:12.7200629Z         torch.manual_seed(123)
2025-04-11T03:52:12.7200721Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7200815Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7200819Z 
2025-04-11T03:52:12.7200972Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7201086Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7201090Z 
2025-04-11T03:52:12.7201169Z device = None
2025-04-11T03:52:12.7201173Z 
2025-04-11T03:52:12.7201295Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7201445Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7201519Z     
2025-04-11T03:52:12.7201591Z         Args:
2025-04-11T03:52:12.7201756Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7201924Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7202028Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7202107Z         """
2025-04-11T03:52:12.7202191Z         _lazy_init()
2025-04-11T03:52:12.7202299Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7202399Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7202503Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7202791Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7203051Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7203214Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7203219Z 
2025-04-11T03:52:12.7203461Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7203634Z _____________ test_flash_decoding[False-True-5-True-4-16-16-32-16] _____________
2025-04-11T03:52:12.7203638Z 
2025-04-11T03:52:12.7203792Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7204055Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7204145Z use_new_kcache_layout = False
2025-04-11T03:52:12.7204150Z 
2025-04-11T03:52:12.7204344Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7204456Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7204575Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7204719Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7204837Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7204957Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7205094Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7205198Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7205338Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7205491Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7205585Z     def test_flash_decoding(
2025-04-11T03:52:12.7205661Z         bsz: int,
2025-04-11T03:52:12.7205749Z         block_size: int,
2025-04-11T03:52:12.7205837Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7205922Z         num_attn_heads: int,
2025-04-11T03:52:12.7206012Z         kv_group_num: int,
2025-04-11T03:52:12.7206097Z         same_context_len: bool,
2025-04-11T03:52:12.7206175Z         q_len: int,
2025-04-11T03:52:12.7206260Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7206350Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7206428Z     ):
2025-04-11T03:52:12.7206541Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7206735Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7206919Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7207096Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7207258Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7207413Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7207492Z     
2025-04-11T03:52:12.7207578Z         torch.manual_seed(123)
2025-04-11T03:52:12.7207673Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7207763Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7207767Z 
2025-04-11T03:52:12.7207923Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7208037Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7208041Z 
2025-04-11T03:52:12.7208119Z device = None
2025-04-11T03:52:12.7208123Z 
2025-04-11T03:52:12.7208243Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7208391Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7208472Z     
2025-04-11T03:52:12.7208545Z         Args:
2025-04-11T03:52:12.7208720Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7208883Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7209097Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7209174Z         """
2025-04-11T03:52:12.7209252Z         _lazy_init()
2025-04-11T03:52:12.7209350Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7209452Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7209557Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7209841Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7209978Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7210237Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7210242Z 
2025-04-11T03:52:12.7210480Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7210653Z _____________ test_flash_decoding[False-True-5-False-1-16-8-16-7] ______________
2025-04-11T03:52:12.7210659Z 
2025-04-11T03:52:12.7210811Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7210976Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7211063Z use_new_kcache_layout = False
2025-04-11T03:52:12.7211068Z 
2025-04-11T03:52:12.7211264Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7211374Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7211494Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7211639Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7211756Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7211874Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7212010Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7212119Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7212260Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7212411Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7212504Z     def test_flash_decoding(
2025-04-11T03:52:12.7212581Z         bsz: int,
2025-04-11T03:52:12.7212664Z         block_size: int,
2025-04-11T03:52:12.7212753Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7212835Z         num_attn_heads: int,
2025-04-11T03:52:12.7212926Z         kv_group_num: int,
2025-04-11T03:52:12.7213011Z         same_context_len: bool,
2025-04-11T03:52:12.7213091Z         q_len: int,
2025-04-11T03:52:12.7213179Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7213266Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7213343Z     ):
2025-04-11T03:52:12.7213454Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7213648Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7213831Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7214005Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7214167Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7214325Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7214403Z     
2025-04-11T03:52:12.7214489Z         torch.manual_seed(123)
2025-04-11T03:52:12.7214583Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7214679Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7214683Z 
2025-04-11T03:52:12.7214841Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7214953Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7214957Z 
2025-04-11T03:52:12.7215158Z device = None
2025-04-11T03:52:12.7215166Z 
2025-04-11T03:52:12.7215283Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7215433Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7215509Z     
2025-04-11T03:52:12.7215582Z         Args:
2025-04-11T03:52:12.7215749Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7215915Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7216022Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7216099Z         """
2025-04-11T03:52:12.7216268Z         _lazy_init()
2025-04-11T03:52:12.7216371Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7216476Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7216584Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7216871Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7217011Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7217174Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7217178Z 
2025-04-11T03:52:12.7217417Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7217590Z _____________ test_flash_decoding[False-True-5-False-1-16-8-16-16] _____________
2025-04-11T03:52:12.7217594Z 
2025-04-11T03:52:12.7217745Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7217917Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7218006Z use_new_kcache_layout = False
2025-04-11T03:52:12.7218010Z 
2025-04-11T03:52:12.7218213Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7218322Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7218440Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7218586Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7218704Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7218821Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7218959Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7219067Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7219205Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7219363Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7219456Z     def test_flash_decoding(
2025-04-11T03:52:12.7219531Z         bsz: int,
2025-04-11T03:52:12.7219617Z         block_size: int,
2025-04-11T03:52:12.7219708Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7219791Z         num_attn_heads: int,
2025-04-11T03:52:12.7219885Z         kv_group_num: int,
2025-04-11T03:52:12.7219970Z         same_context_len: bool,
2025-04-11T03:52:12.7220047Z         q_len: int,
2025-04-11T03:52:12.7220132Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7220221Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7220298Z     ):
2025-04-11T03:52:12.7220410Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7220609Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7220791Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7220969Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7221134Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7221294Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7221474Z     
2025-04-11T03:52:12.7221562Z         torch.manual_seed(123)
2025-04-11T03:52:12.7221658Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7221749Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7221753Z 
2025-04-11T03:52:12.7221909Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7222021Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7222025Z 
2025-04-11T03:52:12.7222101Z device = None
2025-04-11T03:52:12.7222109Z 
2025-04-11T03:52:12.7222227Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7222374Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7222588Z     
2025-04-11T03:52:12.7222666Z         Args:
2025-04-11T03:52:12.7222839Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7223005Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7223116Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7223193Z         """
2025-04-11T03:52:12.7223273Z         _lazy_init()
2025-04-11T03:52:12.7223377Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7223477Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7223586Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7223873Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7224009Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7224177Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7224182Z 
2025-04-11T03:52:12.7224422Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7224596Z _____________ test_flash_decoding[False-True-5-False-1-16-8-32-7] ______________
2025-04-11T03:52:12.7224602Z 
2025-04-11T03:52:12.7224752Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7224919Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7225008Z use_new_kcache_layout = False
2025-04-11T03:52:12.7225012Z 
2025-04-11T03:52:12.7225213Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7225316Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7225433Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7225578Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7225697Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7225817Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7225955Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7226067Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7226206Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7229728Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7229822Z     def test_flash_decoding(
2025-04-11T03:52:12.7229900Z         bsz: int,
2025-04-11T03:52:12.7229987Z         block_size: int,
2025-04-11T03:52:12.7230077Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7230161Z         num_attn_heads: int,
2025-04-11T03:52:12.7230245Z         kv_group_num: int,
2025-04-11T03:52:12.7230330Z         same_context_len: bool,
2025-04-11T03:52:12.7230411Z         q_len: int,
2025-04-11T03:52:12.7230500Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7230592Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7230665Z     ):
2025-04-11T03:52:12.7230775Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7230970Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7231288Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7231463Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7231625Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7231785Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7231859Z     
2025-04-11T03:52:12.7231947Z         torch.manual_seed(123)
2025-04-11T03:52:12.7232045Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7232238Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7232243Z 
2025-04-11T03:52:12.7232406Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7232518Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7232522Z 
2025-04-11T03:52:12.7232606Z device = None
2025-04-11T03:52:12.7232614Z 
2025-04-11T03:52:12.7232731Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7232880Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7232954Z     
2025-04-11T03:52:12.7233029Z         Args:
2025-04-11T03:52:12.7233199Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7233365Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7233477Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7233549Z         """
2025-04-11T03:52:12.7233630Z         _lazy_init()
2025-04-11T03:52:12.7233732Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7233836Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7233945Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7234232Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7234381Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7234539Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7234543Z 
2025-04-11T03:52:12.7234788Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7234965Z _____________ test_flash_decoding[False-True-5-False-1-16-8-32-16] _____________
2025-04-11T03:52:12.7234969Z 
2025-04-11T03:52:12.7235118Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7235290Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7235379Z use_new_kcache_layout = False
2025-04-11T03:52:12.7235383Z 
2025-04-11T03:52:12.7235590Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7235701Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7235824Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7235967Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7236084Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7236205Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7236342Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7236453Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7236587Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7236739Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7236835Z     def test_flash_decoding(
2025-04-11T03:52:12.7236909Z         bsz: int,
2025-04-11T03:52:12.7236996Z         block_size: int,
2025-04-11T03:52:12.7237085Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7237174Z         num_attn_heads: int,
2025-04-11T03:52:12.7237362Z         kv_group_num: int,
2025-04-11T03:52:12.7237449Z         same_context_len: bool,
2025-04-11T03:52:12.7237531Z         q_len: int,
2025-04-11T03:52:12.7237618Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7237714Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7237787Z     ):
2025-04-11T03:52:12.7237898Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7238098Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7238279Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7238548Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7238709Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7238873Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7238950Z     
2025-04-11T03:52:12.7239039Z         torch.manual_seed(123)
2025-04-11T03:52:12.7239137Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7239230Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7239234Z 
2025-04-11T03:52:12.7239393Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7239505Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7239509Z 
2025-04-11T03:52:12.7239591Z device = None
2025-04-11T03:52:12.7239595Z 
2025-04-11T03:52:12.7239715Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7239867Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7239946Z     
2025-04-11T03:52:12.7240020Z         Args:
2025-04-11T03:52:12.7240192Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7240358Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7240471Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7240545Z         """
2025-04-11T03:52:12.7240622Z         _lazy_init()
2025-04-11T03:52:12.7240721Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7240823Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7240932Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7241216Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7241357Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7241518Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7241523Z 
2025-04-11T03:52:12.7241759Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7241932Z _____________ test_flash_decoding[False-True-5-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.7241938Z 
2025-04-11T03:52:12.7242091Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7242260Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7242347Z use_new_kcache_layout = False
2025-04-11T03:52:12.7242351Z 
2025-04-11T03:52:12.7242552Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7242656Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7242777Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7242920Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7243038Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7243158Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7243294Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7243402Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7243714Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7243871Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7243958Z     def test_flash_decoding(
2025-04-11T03:52:12.7244036Z         bsz: int,
2025-04-11T03:52:12.7244121Z         block_size: int,
2025-04-11T03:52:12.7244211Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7244301Z         num_attn_heads: int,
2025-04-11T03:52:12.7244388Z         kv_group_num: int,
2025-04-11T03:52:12.7244476Z         same_context_len: bool,
2025-04-11T03:52:12.7244558Z         q_len: int,
2025-04-11T03:52:12.7244747Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7244841Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7244916Z     ):
2025-04-11T03:52:12.7245031Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7245230Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7245416Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7245593Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7245757Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7245919Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7245991Z     
2025-04-11T03:52:12.7246079Z         torch.manual_seed(123)
2025-04-11T03:52:12.7246174Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7246267Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7246271Z 
2025-04-11T03:52:12.7246429Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7246542Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7246546Z 
2025-04-11T03:52:12.7246628Z device = None
2025-04-11T03:52:12.7246635Z 
2025-04-11T03:52:12.7246753Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7246911Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7246983Z     
2025-04-11T03:52:12.7247059Z         Args:
2025-04-11T03:52:12.7247232Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7247398Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7247510Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7247584Z         """
2025-04-11T03:52:12.7247665Z         _lazy_init()
2025-04-11T03:52:12.7247765Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7247868Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7247977Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7248262Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7248406Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7248564Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7248568Z 
2025-04-11T03:52:12.7248810Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7248988Z ____________ test_flash_decoding[False-True-5-False-1-16-16-16-16] _____________
2025-04-11T03:52:12.7248992Z 
2025-04-11T03:52:12.7249144Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7249316Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7249406Z use_new_kcache_layout = False
2025-04-11T03:52:12.7249410Z 
2025-04-11T03:52:12.7249614Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7249824Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7249951Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7250093Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7250210Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7250330Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7250467Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7250577Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7250713Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7250872Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7251085Z     def test_flash_decoding(
2025-04-11T03:52:12.7251163Z         bsz: int,
2025-04-11T03:52:12.7251251Z         block_size: int,
2025-04-11T03:52:12.7251344Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7251437Z         num_attn_heads: int,
2025-04-11T03:52:12.7251524Z         kv_group_num: int,
2025-04-11T03:52:12.7251613Z         same_context_len: bool,
2025-04-11T03:52:12.7251696Z         q_len: int,
2025-04-11T03:52:12.7251781Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7251873Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7251946Z     ):
2025-04-11T03:52:12.7252057Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7252255Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7252436Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7252613Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7252774Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7252935Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7253011Z     
2025-04-11T03:52:12.7253100Z         torch.manual_seed(123)
2025-04-11T03:52:12.7253197Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7253289Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7253293Z 
2025-04-11T03:52:12.7253451Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7253563Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7253567Z 
2025-04-11T03:52:12.7253648Z device = None
2025-04-11T03:52:12.7253652Z 
2025-04-11T03:52:12.7253768Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7253926Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7254002Z     
2025-04-11T03:52:12.7254075Z         Args:
2025-04-11T03:52:12.7254246Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7254411Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7254523Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7254597Z         """
2025-04-11T03:52:12.7254676Z         _lazy_init()
2025-04-11T03:52:12.7254778Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7254879Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7254991Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7255271Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7255411Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7255571Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7255575Z 
2025-04-11T03:52:12.7255817Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7255983Z _____________ test_flash_decoding[False-True-5-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.7256097Z 
2025-04-11T03:52:12.7256251Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7256418Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7256507Z use_new_kcache_layout = False
2025-04-11T03:52:12.7256512Z 
2025-04-11T03:52:12.7256714Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7256819Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7256940Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7257080Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7257287Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7257406Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7257553Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7257665Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7257804Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7257962Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7258053Z     def test_flash_decoding(
2025-04-11T03:52:12.7258130Z         bsz: int,
2025-04-11T03:52:12.7258225Z         block_size: int,
2025-04-11T03:52:12.7258318Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7258408Z         num_attn_heads: int,
2025-04-11T03:52:12.7258491Z         kv_group_num: int,
2025-04-11T03:52:12.7258577Z         same_context_len: bool,
2025-04-11T03:52:12.7258661Z         q_len: int,
2025-04-11T03:52:12.7258754Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7258849Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7258922Z     ):
2025-04-11T03:52:12.7259033Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7259231Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7259416Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7259596Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7259758Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7259919Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7259993Z     
2025-04-11T03:52:12.7260085Z         torch.manual_seed(123)
2025-04-11T03:52:12.7260176Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7260268Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7260277Z 
2025-04-11T03:52:12.7260439Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7260553Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7260557Z 
2025-04-11T03:52:12.7260640Z device = None
2025-04-11T03:52:12.7260649Z 
2025-04-11T03:52:12.7260767Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7260920Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7260993Z     
2025-04-11T03:52:12.7261068Z         Args:
2025-04-11T03:52:12.7261243Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7261407Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7261517Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7261591Z         """
2025-04-11T03:52:12.7261671Z         _lazy_init()
2025-04-11T03:52:12.7261777Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7261880Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7261995Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7262277Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7262532Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7262691Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7262696Z 
2025-04-11T03:52:12.7262940Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7263109Z ____________ test_flash_decoding[False-True-5-False-1-16-16-32-16] _____________
2025-04-11T03:52:12.7263113Z 
2025-04-11T03:52:12.7263267Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7263535Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7263628Z use_new_kcache_layout = False
2025-04-11T03:52:12.7263632Z 
2025-04-11T03:52:12.7263841Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7263948Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7264074Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7264213Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7264332Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7264455Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7264594Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7264708Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7264847Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7265003Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7265097Z     def test_flash_decoding(
2025-04-11T03:52:12.7265176Z         bsz: int,
2025-04-11T03:52:12.7265266Z         block_size: int,
2025-04-11T03:52:12.7265356Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7265451Z         num_attn_heads: int,
2025-04-11T03:52:12.7265543Z         kv_group_num: int,
2025-04-11T03:52:12.7265631Z         same_context_len: bool,
2025-04-11T03:52:12.7265715Z         q_len: int,
2025-04-11T03:52:12.7265802Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7265900Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7265974Z     ):
2025-04-11T03:52:12.7266093Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7266286Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7266469Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7266652Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7266815Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7266980Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7267055Z     
2025-04-11T03:52:12.7267147Z         torch.manual_seed(123)
2025-04-11T03:52:12.7267237Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7267329Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7267333Z 
2025-04-11T03:52:12.7267495Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7267610Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7267614Z 
2025-04-11T03:52:12.7267699Z device = None
2025-04-11T03:52:12.7267703Z 
2025-04-11T03:52:12.7267820Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7267980Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7268056Z     
2025-04-11T03:52:12.7268131Z         Args:
2025-04-11T03:52:12.7268303Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7268502Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7268738Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7268815Z         """
2025-04-11T03:52:12.7268905Z         _lazy_init()
2025-04-11T03:52:12.7269004Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7269110Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7269230Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7269513Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7269662Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7269934Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7269938Z 
2025-04-11T03:52:12.7270186Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7270357Z _____________ test_flash_decoding[False-True-5-False-4-16-8-16-7] ______________
2025-04-11T03:52:12.7270364Z 
2025-04-11T03:52:12.7270518Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7270692Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7270783Z use_new_kcache_layout = False
2025-04-11T03:52:12.7270787Z 
2025-04-11T03:52:12.7270993Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7271099Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7271229Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7271368Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7271493Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7271608Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7271744Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7271856Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7271996Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7272152Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7272244Z     def test_flash_decoding(
2025-04-11T03:52:12.7272321Z         bsz: int,
2025-04-11T03:52:12.7272411Z         block_size: int,
2025-04-11T03:52:12.7272501Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7272592Z         num_attn_heads: int,
2025-04-11T03:52:12.7272676Z         kv_group_num: int,
2025-04-11T03:52:12.7272765Z         same_context_len: bool,
2025-04-11T03:52:12.7323196Z         q_len: int,
2025-04-11T03:52:12.7323397Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7323502Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7323597Z     ):
2025-04-11T03:52:12.7323732Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7323963Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7324195Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7324392Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7324585Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7324766Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7324856Z     
2025-04-11T03:52:12.7324959Z         torch.manual_seed(123)
2025-04-11T03:52:12.7325069Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7325173Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7325184Z 
2025-04-11T03:52:12.7325361Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7325498Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7325503Z 
2025-04-11T03:52:12.7325591Z device = None
2025-04-11T03:52:12.7325809Z 
2025-04-11T03:52:12.7325965Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7326146Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7326234Z     
2025-04-11T03:52:12.7326317Z         Args:
2025-04-11T03:52:12.7326514Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7326708Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7326825Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7326918Z         """
2025-04-11T03:52:12.7327008Z         _lazy_init()
2025-04-11T03:52:12.7327266Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7327392Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7327511Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7327827Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7327976Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7328154Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7328159Z 
2025-04-11T03:52:12.7328416Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7328596Z _____________ test_flash_decoding[False-True-5-False-4-16-8-16-16] _____________
2025-04-11T03:52:12.7328611Z 
2025-04-11T03:52:12.7328772Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7328946Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7329056Z use_new_kcache_layout = False
2025-04-11T03:52:12.7329061Z 
2025-04-11T03:52:12.7329268Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7329394Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7329526Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7329684Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7329809Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7329931Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7330083Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7330195Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7330345Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7330502Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7330611Z     def test_flash_decoding(
2025-04-11T03:52:12.7330693Z         bsz: int,
2025-04-11T03:52:12.7330783Z         block_size: int,
2025-04-11T03:52:12.7330893Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7330984Z         num_attn_heads: int,
2025-04-11T03:52:12.7331087Z         kv_group_num: int,
2025-04-11T03:52:12.7331181Z         same_context_len: bool,
2025-04-11T03:52:12.7331266Z         q_len: int,
2025-04-11T03:52:12.7331368Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7331464Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7331550Z     ):
2025-04-11T03:52:12.7331674Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7331882Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7332085Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7332266Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7332448Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7332614Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7332816Z     
2025-04-11T03:52:12.7332912Z         torch.manual_seed(123)
2025-04-11T03:52:12.7333012Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7333125Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7333130Z 
2025-04-11T03:52:12.7333294Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7333425Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7333430Z 
2025-04-11T03:52:12.7333515Z device = None
2025-04-11T03:52:12.7333521Z 
2025-04-11T03:52:12.7333658Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7333815Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7334007Z     
2025-04-11T03:52:12.7334090Z         Args:
2025-04-11T03:52:12.7334272Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7334461Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7334584Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7334677Z         """
2025-04-11T03:52:12.7334765Z         _lazy_init()
2025-04-11T03:52:12.7334870Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7334992Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7335109Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7335416Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7335562Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7335740Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7335744Z 
2025-04-11T03:52:12.7335991Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7336175Z _____________ test_flash_decoding[False-True-5-False-4-16-8-32-7] ______________
2025-04-11T03:52:12.7336182Z 
2025-04-11T03:52:12.7336339Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7336510Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7336615Z use_new_kcache_layout = False
2025-04-11T03:52:12.7336619Z 
2025-04-11T03:52:12.7336824Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7336942Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7337066Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7337221Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7337347Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7337467Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7337620Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7337732Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7337889Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7338046Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7338149Z     def test_flash_decoding(
2025-04-11T03:52:12.7338232Z         bsz: int,
2025-04-11T03:52:12.7338322Z         block_size: int,
2025-04-11T03:52:12.7338429Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7338518Z         num_attn_heads: int,
2025-04-11T03:52:12.7338616Z         kv_group_num: int,
2025-04-11T03:52:12.7338708Z         same_context_len: bool,
2025-04-11T03:52:12.7338790Z         q_len: int,
2025-04-11T03:52:12.7338893Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7338986Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7339068Z     ):
2025-04-11T03:52:12.7339185Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7339396Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7339691Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7339867Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7340044Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7340207Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7340292Z     
2025-04-11T03:52:12.7340384Z         torch.manual_seed(123)
2025-04-11T03:52:12.7340487Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7340583Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7340751Z 
2025-04-11T03:52:12.7340916Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7341043Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7341048Z 
2025-04-11T03:52:12.7341131Z device = None
2025-04-11T03:52:12.7341140Z 
2025-04-11T03:52:12.7341276Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7341433Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7341520Z     
2025-04-11T03:52:12.7341602Z         Args:
2025-04-11T03:52:12.7341775Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7341956Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7342070Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7342158Z         """
2025-04-11T03:52:12.7342241Z         _lazy_init()
2025-04-11T03:52:12.7342358Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7342466Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7342582Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7342885Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7343036Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7343207Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7343211Z 
2025-04-11T03:52:12.7343458Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7343645Z _____________ test_flash_decoding[False-True-5-False-4-16-8-32-16] _____________
2025-04-11T03:52:12.7343649Z 
2025-04-11T03:52:12.7343807Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7343981Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7344085Z use_new_kcache_layout = False
2025-04-11T03:52:12.7344089Z 
2025-04-11T03:52:12.7344294Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7344417Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7344548Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7344705Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7344828Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7344945Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7345096Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7345209Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7345361Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7345517Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7345623Z     def test_flash_decoding(
2025-04-11T03:52:12.7345702Z         bsz: int,
2025-04-11T03:52:12.7345788Z         block_size: int,
2025-04-11T03:52:12.7345894Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7345982Z         num_attn_heads: int,
2025-04-11T03:52:12.7346184Z         kv_group_num: int,
2025-04-11T03:52:12.7346274Z         same_context_len: bool,
2025-04-11T03:52:12.7346358Z         q_len: int,
2025-04-11T03:52:12.7346457Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7346552Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7346637Z     ):
2025-04-11T03:52:12.7346753Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7346966Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7347157Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7347335Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7347643Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7347804Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7347893Z     
2025-04-11T03:52:12.7347990Z         torch.manual_seed(123)
2025-04-11T03:52:12.7348093Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7348188Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7348192Z 
2025-04-11T03:52:12.7348353Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7348527Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7348532Z 
2025-04-11T03:52:12.7348616Z device = None
2025-04-11T03:52:12.7348621Z 
2025-04-11T03:52:12.7348753Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7348912Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7349001Z     
2025-04-11T03:52:12.7349081Z         Args:
2025-04-11T03:52:12.7349252Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7349430Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7349543Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7349630Z         """
2025-04-11T03:52:12.7349713Z         _lazy_init()
2025-04-11T03:52:12.7349822Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7349930Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7350040Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7350338Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7350480Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7350656Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7350660Z 
2025-04-11T03:52:12.7350905Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7351093Z _____________ test_flash_decoding[False-True-5-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.7351100Z 
2025-04-11T03:52:12.7351255Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7351430Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7351525Z use_new_kcache_layout = False
2025-04-11T03:52:12.7351529Z 
2025-04-11T03:52:12.7351737Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7351854Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7351977Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7352124Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7352247Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7352373Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7352512Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7352620Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7352896Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7353050Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7353151Z     def test_flash_decoding(
2025-04-11T03:52:12.7353232Z         bsz: int,
2025-04-11T03:52:12.7353320Z         block_size: int,
2025-04-11T03:52:12.7353424Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7353512Z         num_attn_heads: int,
2025-04-11T03:52:12.7353609Z         kv_group_num: int,
2025-04-11T03:52:12.7353700Z         same_context_len: bool,
2025-04-11T03:52:12.7353791Z         q_len: int,
2025-04-11T03:52:12.7353989Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7354087Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7354173Z     ):
2025-04-11T03:52:12.7354288Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7354493Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7354684Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7354858Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7355037Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7355202Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7355290Z     
2025-04-11T03:52:12.7355384Z         torch.manual_seed(123)
2025-04-11T03:52:12.7355491Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7355586Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7355594Z 
2025-04-11T03:52:12.7355754Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7355884Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7355889Z 
2025-04-11T03:52:12.7355971Z device = None
2025-04-11T03:52:12.7355979Z 
2025-04-11T03:52:12.7356111Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7356267Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7356354Z     
2025-04-11T03:52:12.7356436Z         Args:
2025-04-11T03:52:12.7356608Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7356788Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7356898Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7356990Z         """
2025-04-11T03:52:12.7357074Z         _lazy_init()
2025-04-11T03:52:12.7357192Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7357302Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7357413Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7357717Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7357861Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7358032Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7358036Z 
2025-04-11T03:52:12.7358276Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7358458Z ____________ test_flash_decoding[False-True-5-False-4-16-16-16-16] _____________
2025-04-11T03:52:12.7358463Z 
2025-04-11T03:52:12.7358620Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7358793Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7358893Z use_new_kcache_layout = False
2025-04-11T03:52:12.7358897Z 
2025-04-11T03:52:12.7359101Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7359220Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7359451Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7359604Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7359724Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7359850Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7359994Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7360104Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7360253Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7360407Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7360603Z     def test_flash_decoding(
2025-04-11T03:52:12.7360687Z         bsz: int,
2025-04-11T03:52:12.7360783Z         block_size: int,
2025-04-11T03:52:12.7360881Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7360971Z         num_attn_heads: int,
2025-04-11T03:52:12.7361075Z         kv_group_num: int,
2025-04-11T03:52:12.7361168Z         same_context_len: bool,
2025-04-11T03:52:12.7361258Z         q_len: int,
2025-04-11T03:52:12.7361348Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7361441Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7361529Z     ):
2025-04-11T03:52:12.7361644Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7361852Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7362037Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7362223Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7362394Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7362556Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7362640Z     
2025-04-11T03:52:12.7362736Z         torch.manual_seed(123)
2025-04-11T03:52:12.7362842Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7362939Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7362943Z 
2025-04-11T03:52:12.7363099Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7363225Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7363230Z 
2025-04-11T03:52:12.7363311Z device = None
2025-04-11T03:52:12.7363316Z 
2025-04-11T03:52:12.7363444Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7363597Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7363686Z     
2025-04-11T03:52:12.7363764Z         Args:
2025-04-11T03:52:12.7363940Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7364105Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7364216Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7364304Z         """
2025-04-11T03:52:12.7364389Z         _lazy_init()
2025-04-11T03:52:12.7364496Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7364604Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7364714Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7365004Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7365143Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7365312Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7365319Z 
2025-04-11T03:52:12.7365562Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7365738Z _____________ test_flash_decoding[False-True-5-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.7365844Z 
2025-04-11T03:52:12.7365999Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7366173Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7366272Z use_new_kcache_layout = False
2025-04-11T03:52:12.7366276Z 
2025-04-11T03:52:12.7366482Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7366605Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7366728Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7366883Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7367100Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7367227Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7367368Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7367480Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7367634Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7367791Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7367894Z     def test_flash_decoding(
2025-04-11T03:52:12.7367976Z         bsz: int,
2025-04-11T03:52:12.7368073Z         block_size: int,
2025-04-11T03:52:12.7368169Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7368255Z         num_attn_heads: int,
2025-04-11T03:52:12.7368351Z         kv_group_num: int,
2025-04-11T03:52:12.7368441Z         same_context_len: bool,
2025-04-11T03:52:12.7368531Z         q_len: int,
2025-04-11T03:52:12.7368622Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7368716Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7368803Z     ):
2025-04-11T03:52:12.7368918Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7369127Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7369320Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7369506Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7369675Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7369839Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7369926Z     
2025-04-11T03:52:12.7370018Z         torch.manual_seed(123)
2025-04-11T03:52:12.7370122Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7370218Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7370227Z 
2025-04-11T03:52:12.7370393Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7370510Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7370514Z 
2025-04-11T03:52:12.7370597Z device = None
2025-04-11T03:52:12.7370607Z 
2025-04-11T03:52:12.7370737Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7370892Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7370976Z     
2025-04-11T03:52:12.7371054Z         Args:
2025-04-11T03:52:12.7371236Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7371404Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7371515Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7371600Z         """
2025-04-11T03:52:12.7371684Z         _lazy_init()
2025-04-11T03:52:12.7371795Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7371901Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7372009Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7372304Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7372581Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7372751Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7372755Z 
2025-04-11T03:52:12.7373001Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7373177Z ____________ test_flash_decoding[False-True-5-False-4-16-16-32-16] _____________
2025-04-11T03:52:12.7373181Z 
2025-04-11T03:52:12.7373334Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7373501Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
2025-04-11T03:52:12.7373702Z use_new_kcache_layout = False
2025-04-11T03:52:12.7373707Z 
2025-04-11T03:52:12.7373910Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7374017Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7374141Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7374283Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7374399Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7374520Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7374655Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7374763Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7374899Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7375049Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7375143Z     def test_flash_decoding(
2025-04-11T03:52:12.7375220Z         bsz: int,
2025-04-11T03:52:12.7375310Z         block_size: int,
2025-04-11T03:52:12.7375401Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7375485Z         num_attn_heads: int,
2025-04-11T03:52:12.7375580Z         kv_group_num: int,
2025-04-11T03:52:12.7375664Z         same_context_len: bool,
2025-04-11T03:52:12.7375745Z         q_len: int,
2025-04-11T03:52:12.7375830Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7375919Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7375991Z     ):
2025-04-11T03:52:12.7376101Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7376298Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7376477Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7376648Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7376817Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7376978Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7377049Z     
2025-04-11T03:52:12.7377140Z         torch.manual_seed(123)
2025-04-11T03:52:12.7377234Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7377324Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7377328Z 
2025-04-11T03:52:12.7377486Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7377602Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7377606Z 
2025-04-11T03:52:12.7377687Z device = None
2025-04-11T03:52:12.7377692Z 
2025-04-11T03:52:12.7377807Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7377959Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7378037Z     
2025-04-11T03:52:12.7378113Z         Args:
2025-04-11T03:52:12.7378284Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7378452Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7378670Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7378744Z         """
2025-04-11T03:52:12.7378824Z         _lazy_init()
2025-04-11T03:52:12.7378927Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7379030Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7379140Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7379426Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7379562Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7379721Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7379835Z 
2025-04-11T03:52:12.7380073Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7380246Z _____________ test_flash_decoding[False-False-1-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.7380253Z 
2025-04-11T03:52:12.7380403Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7380571Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7380660Z use_new_kcache_layout = False
2025-04-11T03:52:12.7380664Z 
2025-04-11T03:52:12.7380864Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7380968Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7381085Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7381228Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7381347Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7381465Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7381600Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7381708Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7381847Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7381997Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7382091Z     def test_flash_decoding(
2025-04-11T03:52:12.7382166Z         bsz: int,
2025-04-11T03:52:12.7382255Z         block_size: int,
2025-04-11T03:52:12.7382345Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7382432Z         num_attn_heads: int,
2025-04-11T03:52:12.7382516Z         kv_group_num: int,
2025-04-11T03:52:12.7382600Z         same_context_len: bool,
2025-04-11T03:52:12.7382682Z         q_len: int,
2025-04-11T03:52:12.7382766Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7382862Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7382934Z     ):
2025-04-11T03:52:12.7383046Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7383240Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7383421Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7383594Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7383754Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7383913Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7383984Z     
2025-04-11T03:52:12.7384070Z         torch.manual_seed(123)
2025-04-11T03:52:12.7384165Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7384255Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7384262Z 
2025-04-11T03:52:12.7384420Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7384532Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7384536Z 
2025-04-11T03:52:12.7384617Z device = None
2025-04-11T03:52:12.7384622Z 
2025-04-11T03:52:12.7384854Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7385006Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7385080Z     
2025-04-11T03:52:12.7385156Z         Args:
2025-04-11T03:52:12.7385327Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7385492Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7385606Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7385679Z         """
2025-04-11T03:52:12.7385756Z         _lazy_init()
2025-04-11T03:52:12.7385954Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7386061Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7386171Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7386456Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7386601Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7386757Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7386761Z 
2025-04-11T03:52:12.7387000Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7387173Z _____________ test_flash_decoding[False-False-1-True-1-16-8-16-16] _____________
2025-04-11T03:52:12.7387177Z 
2025-04-11T03:52:12.7387329Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7387495Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7387588Z use_new_kcache_layout = False
2025-04-11T03:52:12.7387592Z 
2025-04-11T03:52:12.7387794Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7387898Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7388023Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7388160Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7388276Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7388394Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7388573Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7388686Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7388822Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7388971Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7389068Z     def test_flash_decoding(
2025-04-11T03:52:12.7389144Z         bsz: int,
2025-04-11T03:52:12.7389230Z         block_size: int,
2025-04-11T03:52:12.7389321Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7389407Z         num_attn_heads: int,
2025-04-11T03:52:12.7389490Z         kv_group_num: int,
2025-04-11T03:52:12.7389579Z         same_context_len: bool,
2025-04-11T03:52:12.7389660Z         q_len: int,
2025-04-11T03:52:12.7389747Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7389840Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7389911Z     ):
2025-04-11T03:52:12.7390023Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7390217Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7390399Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7390573Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7390739Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7390899Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7390970Z     
2025-04-11T03:52:12.7391181Z         torch.manual_seed(123)
2025-04-11T03:52:12.7391275Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7391368Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7391373Z 
2025-04-11T03:52:12.7391531Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7391645Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7391650Z 
2025-04-11T03:52:12.7391731Z device = None
2025-04-11T03:52:12.7391735Z 
2025-04-11T03:52:12.7391854Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7392006Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7392205Z     
2025-04-11T03:52:12.7392282Z         Args:
2025-04-11T03:52:12.7392455Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7392623Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7392739Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7392813Z         """
2025-04-11T03:52:12.7392892Z         _lazy_init()
2025-04-11T03:52:12.7392994Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7393097Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7393209Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7393494Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7393636Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7393795Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7393802Z 
2025-04-11T03:52:12.7394046Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7394217Z _____________ test_flash_decoding[False-False-1-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.7394224Z 
2025-04-11T03:52:12.7394374Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7394544Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7394633Z use_new_kcache_layout = False
2025-04-11T03:52:12.7394638Z 
2025-04-11T03:52:12.7394842Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7394946Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7395069Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7395208Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7395329Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7395448Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7395582Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7395692Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7395832Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7395987Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7396075Z     def test_flash_decoding(
2025-04-11T03:52:12.7396151Z         bsz: int,
2025-04-11T03:52:12.7396239Z         block_size: int,
2025-04-11T03:52:12.7396331Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7396417Z         num_attn_heads: int,
2025-04-11T03:52:12.7396501Z         kv_group_num: int,
2025-04-11T03:52:12.7396587Z         same_context_len: bool,
2025-04-11T03:52:12.7396668Z         q_len: int,
2025-04-11T03:52:12.7396753Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7396847Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7396917Z     ):
2025-04-11T03:52:12.7397030Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7397227Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7397537Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7397718Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7397880Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7398043Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7398117Z     
2025-04-11T03:52:12.7398204Z         torch.manual_seed(123)
2025-04-11T03:52:12.7398300Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7398393Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7398488Z 
2025-04-11T03:52:12.7398648Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7398760Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7398765Z 
2025-04-11T03:52:12.7398847Z device = None
2025-04-11T03:52:12.7398851Z 
2025-04-11T03:52:12.7398972Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7399125Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7399198Z     
2025-04-11T03:52:12.7399272Z         Args:
2025-04-11T03:52:12.7399443Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7399608Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7399718Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7399794Z         """
2025-04-11T03:52:12.7399873Z         _lazy_init()
2025-04-11T03:52:12.7399975Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7400076Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7400187Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7400471Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7400614Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7400770Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7400774Z 
2025-04-11T03:52:12.7401018Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7401187Z _____________ test_flash_decoding[False-False-1-True-1-16-8-32-16] _____________
2025-04-11T03:52:12.7401191Z 
2025-04-11T03:52:12.7401341Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7401506Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7401599Z use_new_kcache_layout = False
2025-04-11T03:52:12.7401603Z 
2025-04-11T03:52:12.7401807Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7401911Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7402040Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7402180Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7402297Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7402414Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7402552Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7402662Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7402799Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7402956Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7403049Z     def test_flash_decoding(
2025-04-11T03:52:12.7403125Z         bsz: int,
2025-04-11T03:52:12.7403212Z         block_size: int,
2025-04-11T03:52:12.7403302Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7403390Z         num_attn_heads: int,
2025-04-11T03:52:12.7403475Z         kv_group_num: int,
2025-04-11T03:52:12.7403675Z         same_context_len: bool,
2025-04-11T03:52:12.7403757Z         q_len: int,
2025-04-11T03:52:12.7403843Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7403940Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7404011Z     ):
2025-04-11T03:52:12.7404122Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7404319Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7404502Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7404678Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7404948Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7405111Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7405183Z     
2025-04-11T03:52:12.7405278Z         torch.manual_seed(123)
2025-04-11T03:52:12.7405369Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7405460Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7405463Z 
2025-04-11T03:52:12.7405624Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7405738Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7405742Z 
2025-04-11T03:52:12.7405824Z device = None
2025-04-11T03:52:12.7405828Z 
2025-04-11T03:52:12.7405942Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7406097Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7406170Z     
2025-04-11T03:52:12.7406245Z         Args:
2025-04-11T03:52:12.7406415Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7406581Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7406693Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7406767Z         """
2025-04-11T03:52:12.7406852Z         _lazy_init()
2025-04-11T03:52:12.7406948Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7407052Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7407162Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7407444Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7407584Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7407739Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7407746Z 
2025-04-11T03:52:12.7407988Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7408156Z _____________ test_flash_decoding[False-False-1-True-1-16-16-16-7] _____________
2025-04-11T03:52:12.7408163Z 
2025-04-11T03:52:12.7408312Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7408479Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7408567Z use_new_kcache_layout = False
2025-04-11T03:52:12.7408571Z 
2025-04-11T03:52:12.7408777Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7408880Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7409002Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7409141Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7409261Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7409380Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7409516Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7409626Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7409873Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7410030Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7410119Z     def test_flash_decoding(
2025-04-11T03:52:12.7410198Z         bsz: int,
2025-04-11T03:52:12.7410286Z         block_size: int,
2025-04-11T03:52:12.7410377Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7410462Z         num_attn_heads: int,
2025-04-11T03:52:12.7410546Z         kv_group_num: int,
2025-04-11T03:52:12.7410633Z         same_context_len: bool,
2025-04-11T03:52:12.7410714Z         q_len: int,
2025-04-11T03:52:12.7410802Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7410992Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7411064Z     ):
2025-04-11T03:52:12.7411181Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7411372Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7411558Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7411733Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7411899Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7412061Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7412133Z     
2025-04-11T03:52:12.7412226Z         torch.manual_seed(123)
2025-04-11T03:52:12.7412318Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7412410Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7412419Z 
2025-04-11T03:52:12.7412580Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7412693Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7412698Z 
2025-04-11T03:52:12.7412779Z device = None
2025-04-11T03:52:12.7412784Z 
2025-04-11T03:52:12.7412907Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7413060Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7413131Z     
2025-04-11T03:52:12.7413205Z         Args:
2025-04-11T03:52:12.7413380Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7413546Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7413657Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7413730Z         """
2025-04-11T03:52:12.7413813Z         _lazy_init()
2025-04-11T03:52:12.7413911Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7414016Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7414126Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7414410Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7414556Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7414710Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7414714Z 
2025-04-11T03:52:12.7414954Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7415126Z ____________ test_flash_decoding[False-False-1-True-1-16-16-16-16] _____________
2025-04-11T03:52:12.7415130Z 
2025-04-11T03:52:12.7415284Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7415452Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7415544Z use_new_kcache_layout = False
2025-04-11T03:52:12.7415549Z 
2025-04-11T03:52:12.7415754Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7415858Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7416083Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7416221Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7416345Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7416460Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7416596Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7416707Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7416842Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7416996Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7417190Z     def test_flash_decoding(
2025-04-11T03:52:12.7417266Z         bsz: int,
2025-04-11T03:52:12.7417353Z         block_size: int,
2025-04-11T03:52:12.7417444Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7417534Z         num_attn_heads: int,
2025-04-11T03:52:12.7417618Z         kv_group_num: int,
2025-04-11T03:52:12.7417714Z         same_context_len: bool,
2025-04-11T03:52:12.7417791Z         q_len: int,
2025-04-11T03:52:12.7417877Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7417970Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7418042Z     ):
2025-04-11T03:52:12.7418159Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7418351Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7418532Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7418707Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7418874Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7419037Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7419108Z     
2025-04-11T03:52:12.7419212Z         torch.manual_seed(123)
2025-04-11T03:52:12.7419303Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7419396Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7419401Z 
2025-04-11T03:52:12.7419564Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7419678Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7419682Z 
2025-04-11T03:52:12.7419767Z device = None
2025-04-11T03:52:12.7419772Z 
2025-04-11T03:52:12.7419889Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7420041Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7420116Z     
2025-04-11T03:52:12.7420189Z         Args:
2025-04-11T03:52:12.7420359Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7420523Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7420636Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7420709Z         """
2025-04-11T03:52:12.7420792Z         _lazy_init()
2025-04-11T03:52:12.7420887Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7420994Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7421104Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7421385Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7421525Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7421681Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7421688Z 
2025-04-11T03:52:12.7421929Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7422095Z _____________ test_flash_decoding[False-False-1-True-1-16-16-32-7] _____________
2025-04-11T03:52:12.7422207Z 
2025-04-11T03:52:12.7422364Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7422528Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7422620Z use_new_kcache_layout = False
2025-04-11T03:52:12.7422625Z 
2025-04-11T03:52:12.7422831Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7422937Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7423059Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7423200Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7423412Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7423530Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7423682Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7423832Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7423978Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7424135Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7424226Z     def test_flash_decoding(
2025-04-11T03:52:12.7424307Z         bsz: int,
2025-04-11T03:52:12.7424389Z         block_size: int,
2025-04-11T03:52:12.7424480Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7424570Z         num_attn_heads: int,
2025-04-11T03:52:12.7424652Z         kv_group_num: int,
2025-04-11T03:52:12.7424743Z         same_context_len: bool,
2025-04-11T03:52:12.7424819Z         q_len: int,
2025-04-11T03:52:12.7424904Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7425001Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7425072Z     ):
2025-04-11T03:52:12.7425189Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7425383Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7425568Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7425742Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7425906Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7426068Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7426142Z     
2025-04-11T03:52:12.7426234Z         torch.manual_seed(123)
2025-04-11T03:52:12.7426324Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7426415Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7426424Z 
2025-04-11T03:52:12.7426581Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7426696Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7426700Z 
2025-04-11T03:52:12.7426782Z device = None
2025-04-11T03:52:12.7426786Z 
2025-04-11T03:52:12.7426907Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7427063Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7427136Z     
2025-04-11T03:52:12.7427213Z         Args:
2025-04-11T03:52:12.7427382Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7427548Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7427661Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7427732Z         """
2025-04-11T03:52:12.7427813Z         _lazy_init()
2025-04-11T03:52:12.7427913Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7428017Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7428126Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7428446Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7428708Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7428869Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7428874Z 
2025-04-11T03:52:12.7429119Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7429287Z ____________ test_flash_decoding[False-False-1-True-1-16-16-32-16] _____________
2025-04-11T03:52:12.7429291Z 
2025-04-11T03:52:12.7429446Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7429608Z kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7429800Z use_new_kcache_layout = False
2025-04-11T03:52:12.7429804Z 
2025-04-11T03:52:12.7430009Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7430112Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7430239Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7430378Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7430498Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7430612Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7430749Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7430859Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7430993Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7431147Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7431240Z     def test_flash_decoding(
2025-04-11T03:52:12.7431320Z         bsz: int,
2025-04-11T03:52:12.7431403Z         block_size: int,
2025-04-11T03:52:12.7431492Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7431580Z         num_attn_heads: int,
2025-04-11T03:52:12.7431662Z         kv_group_num: int,
2025-04-11T03:52:12.7431755Z         same_context_len: bool,
2025-04-11T03:52:12.7431830Z         q_len: int,
2025-04-11T03:52:12.7431917Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7432009Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7432080Z     ):
2025-04-11T03:52:12.7432195Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7432387Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7432571Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7432741Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7432909Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7433077Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7433148Z     
2025-04-11T03:52:12.7433241Z         torch.manual_seed(123)
2025-04-11T03:52:12.7433335Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7433425Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7433434Z 
2025-04-11T03:52:12.7433588Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7433704Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7433708Z 
2025-04-11T03:52:12.7433791Z device = None
2025-04-11T03:52:12.7433795Z 
2025-04-11T03:52:12.7433911Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7434067Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7434142Z     
2025-04-11T03:52:12.7434220Z         Args:
2025-04-11T03:52:12.7434386Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7434550Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7434660Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7434857Z         """
2025-04-11T03:52:12.7434940Z         _lazy_init()
2025-04-11T03:52:12.7435036Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7435139Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7435246Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7435527Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7435668Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7435826Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7435929Z 
2025-04-11T03:52:12.7436174Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7436342Z _____________ test_flash_decoding[False-False-1-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.7436348Z 
2025-04-11T03:52:12.7436502Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7436667Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7436760Z use_new_kcache_layout = False
2025-04-11T03:52:12.7436768Z 
2025-04-11T03:52:12.7436971Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7437079Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7437207Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7437349Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7437478Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7437596Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7437736Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7437848Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7437988Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7438145Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7438236Z     def test_flash_decoding(
2025-04-11T03:52:12.7438322Z         bsz: int,
2025-04-11T03:52:12.7438409Z         block_size: int,
2025-04-11T03:52:12.7438503Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7438597Z         num_attn_heads: int,
2025-04-11T03:52:12.7438683Z         kv_group_num: int,
2025-04-11T03:52:12.7438774Z         same_context_len: bool,
2025-04-11T03:52:12.7438856Z         q_len: int,
2025-04-11T03:52:12.7438945Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7439045Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7439120Z     ):
2025-04-11T03:52:12.7439239Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7439432Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7439624Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7439795Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7439963Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7440126Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7440202Z     
2025-04-11T03:52:12.7440297Z         torch.manual_seed(123)
2025-04-11T03:52:12.7440393Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7440491Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7440498Z 
2025-04-11T03:52:12.7440656Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7440772Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7440776Z 
2025-04-11T03:52:12.7440861Z device = None
2025-04-11T03:52:12.7440866Z 
2025-04-11T03:52:12.7441093Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7441251Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7441324Z     
2025-04-11T03:52:12.7441404Z         Args:
2025-04-11T03:52:12.7441570Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7441739Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7441851Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7441925Z         """
2025-04-11T03:52:12.7442008Z         _lazy_init()
2025-04-11T03:52:12.7442103Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7442406Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7442515Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7442797Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7442944Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7443102Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7443106Z 
2025-04-11T03:52:12.7443355Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7443522Z _____________ test_flash_decoding[False-False-1-True-4-16-8-16-16] _____________
2025-04-11T03:52:12.7443526Z 
2025-04-11T03:52:12.7443679Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7443840Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7443938Z use_new_kcache_layout = False
2025-04-11T03:52:12.7443943Z 
2025-04-11T03:52:12.7444143Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7444249Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7444375Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7444516Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7444635Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7444749Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7444886Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7444991Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7445125Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7445278Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7445370Z     def test_flash_decoding(
2025-04-11T03:52:12.7445449Z         bsz: int,
2025-04-11T03:52:12.7445532Z         block_size: int,
2025-04-11T03:52:12.7445623Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7445711Z         num_attn_heads: int,
2025-04-11T03:52:12.7445794Z         kv_group_num: int,
2025-04-11T03:52:12.7445890Z         same_context_len: bool,
2025-04-11T03:52:12.7445966Z         q_len: int,
2025-04-11T03:52:12.7446051Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7446144Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7446213Z     ):
2025-04-11T03:52:12.7446326Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7446517Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7446702Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7446871Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7447035Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7447195Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7447266Z     
2025-04-11T03:52:12.7447356Z         torch.manual_seed(123)
2025-04-11T03:52:12.7447557Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7447652Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7447657Z 
2025-04-11T03:52:12.7447813Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7447930Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7447938Z 
2025-04-11T03:52:12.7448015Z device = None
2025-04-11T03:52:12.7448020Z 
2025-04-11T03:52:12.7448141Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7448301Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7448469Z     
2025-04-11T03:52:12.7448545Z         Args:
2025-04-11T03:52:12.7448713Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7448877Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7448989Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7449065Z         """
2025-04-11T03:52:12.7449146Z         _lazy_init()
2025-04-11T03:52:12.7449243Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7449349Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7449456Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7449738Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7449879Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7450034Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7450042Z 
2025-04-11T03:52:12.7450286Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7450457Z _____________ test_flash_decoding[False-False-1-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.7450461Z 
2025-04-11T03:52:12.7450617Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7450780Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7450874Z use_new_kcache_layout = False
2025-04-11T03:52:12.7450878Z 
2025-04-11T03:52:12.7451079Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7451182Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7451303Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7451443Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7451568Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7451683Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7451825Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7451930Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7452073Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7452230Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7452319Z     def test_flash_decoding(
2025-04-11T03:52:12.7452397Z         bsz: int,
2025-04-11T03:52:12.7452480Z         block_size: int,
2025-04-11T03:52:12.7452571Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7452658Z         num_attn_heads: int,
2025-04-11T03:52:12.7452742Z         kv_group_num: int,
2025-04-11T03:52:12.7452830Z         same_context_len: bool,
2025-04-11T03:52:12.7452907Z         q_len: int,
2025-04-11T03:52:12.7453001Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7453093Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7453164Z     ):
2025-04-11T03:52:12.7453281Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7453475Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7453660Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7453968Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7454135Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7454292Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7454366Z     
2025-04-11T03:52:12.7454461Z         torch.manual_seed(123)
2025-04-11T03:52:12.7454551Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7454645Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7454649Z 
2025-04-11T03:52:12.7454898Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7455011Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7455021Z 
2025-04-11T03:52:12.7455099Z device = None
2025-04-11T03:52:12.7455103Z 
2025-04-11T03:52:12.7455221Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7455381Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7455452Z     
2025-04-11T03:52:12.7455531Z         Args:
2025-04-11T03:52:12.7455703Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7455867Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7455979Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7456053Z         """
2025-04-11T03:52:12.7456137Z         _lazy_init()
2025-04-11T03:52:12.7456232Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7456342Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7456447Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7456726Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7456869Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7457026Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7457031Z 
2025-04-11T03:52:12.7457277Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7457442Z _____________ test_flash_decoding[False-False-1-True-4-16-8-32-16] _____________
2025-04-11T03:52:12.7457446Z 
2025-04-11T03:52:12.7457598Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7457759Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7457854Z use_new_kcache_layout = False
2025-04-11T03:52:12.7457858Z 
2025-04-11T03:52:12.7458057Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7458160Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7458283Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7458419Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7458541Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7458655Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7458794Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7458898Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7459033Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7459189Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7459279Z     def test_flash_decoding(
2025-04-11T03:52:12.7459360Z         bsz: int,
2025-04-11T03:52:12.7459443Z         block_size: int,
2025-04-11T03:52:12.7459540Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7459623Z         num_attn_heads: int,
2025-04-11T03:52:12.7459705Z         kv_group_num: int,
2025-04-11T03:52:12.7459909Z         same_context_len: bool,
2025-04-11T03:52:12.7459988Z         q_len: int,
2025-04-11T03:52:12.7460077Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7460165Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7460237Z     ):
2025-04-11T03:52:12.7460354Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7460545Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7460730Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7460901Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7461174Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7461331Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7461402Z     
2025-04-11T03:52:12.7461493Z         torch.manual_seed(123)
2025-04-11T03:52:12.7461587Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7461684Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7461689Z 
2025-04-11T03:52:12.7461845Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7461962Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7461966Z 
2025-04-11T03:52:12.7462043Z device = None
2025-04-11T03:52:12.7462047Z 
2025-04-11T03:52:12.7462162Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7462318Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7462393Z     
2025-04-11T03:52:12.7462470Z         Args:
2025-04-11T03:52:12.7462635Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7462803Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7462908Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7462984Z         """
2025-04-11T03:52:12.7463066Z         _lazy_init()
2025-04-11T03:52:12.7463161Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7463270Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7463377Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7463656Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7463795Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7463952Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7463959Z 
2025-04-11T03:52:12.7464203Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7464366Z _____________ test_flash_decoding[False-False-1-True-4-16-16-16-7] _____________
2025-04-11T03:52:12.7464370Z 
2025-04-11T03:52:12.7464529Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7464692Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7464787Z use_new_kcache_layout = False
2025-04-11T03:52:12.7464791Z 
2025-04-11T03:52:12.7464987Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7465094Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7465212Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7465351Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7465477Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7465590Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7465730Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7465834Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7465973Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7466265Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7466355Z     def test_flash_decoding(
2025-04-11T03:52:12.7466437Z         bsz: int,
2025-04-11T03:52:12.7466521Z         block_size: int,
2025-04-11T03:52:12.7466614Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7466696Z         num_attn_heads: int,
2025-04-11T03:52:12.7466780Z         kv_group_num: int,
2025-04-11T03:52:12.7466871Z         same_context_len: bool,
2025-04-11T03:52:12.7466947Z         q_len: int,
2025-04-11T03:52:12.7467037Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7467223Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7467296Z     ):
2025-04-11T03:52:12.7467410Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7467605Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7467794Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7467972Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7468141Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7468300Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7468372Z     
2025-04-11T03:52:12.7468513Z         torch.manual_seed(123)
2025-04-11T03:52:12.7468605Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7468698Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7468702Z 
2025-04-11T03:52:12.7468861Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7468979Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7468983Z 
2025-04-11T03:52:12.7469060Z device = None
2025-04-11T03:52:12.7469064Z 
2025-04-11T03:52:12.7469181Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7469337Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7469409Z     
2025-04-11T03:52:12.7469488Z         Args:
2025-04-11T03:52:12.7469652Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7469819Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7469925Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7469998Z         """
2025-04-11T03:52:12.7470080Z         _lazy_init()
2025-04-11T03:52:12.7470175Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7470286Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7470391Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7470673Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7470817Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7470975Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7470979Z 
2025-04-11T03:52:12.7471218Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7471386Z ____________ test_flash_decoding[False-False-1-True-4-16-16-16-16] _____________
2025-04-11T03:52:12.7471390Z 
2025-04-11T03:52:12.7471546Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7471711Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7471809Z use_new_kcache_layout = False
2025-04-11T03:52:12.7471813Z 
2025-04-11T03:52:12.7472013Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7472119Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7472353Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7472491Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7472614Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7472727Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7472865Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7472971Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7473110Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7473262Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7473457Z     def test_flash_decoding(
2025-04-11T03:52:12.7473539Z         bsz: int,
2025-04-11T03:52:12.7473622Z         block_size: int,
2025-04-11T03:52:12.7473715Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7473799Z         num_attn_heads: int,
2025-04-11T03:52:12.7473879Z         kv_group_num: int,
2025-04-11T03:52:12.7473975Z         same_context_len: bool,
2025-04-11T03:52:12.7474050Z         q_len: int,
2025-04-11T03:52:12.7474138Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7474229Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7474301Z     ):
2025-04-11T03:52:12.7474419Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7474609Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7474793Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7474962Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7475131Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7475286Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7475357Z     
2025-04-11T03:52:12.7475448Z         torch.manual_seed(123)
2025-04-11T03:52:12.7475543Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7475636Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7475640Z 
2025-04-11T03:52:12.7475791Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7475905Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7475909Z 
2025-04-11T03:52:12.7475985Z device = None
2025-04-11T03:52:12.7475989Z 
2025-04-11T03:52:12.7476108Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7476258Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7476331Z     
2025-04-11T03:52:12.7476410Z         Args:
2025-04-11T03:52:12.7476575Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7476740Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7476846Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7476923Z         """
2025-04-11T03:52:12.7477005Z         _lazy_init()
2025-04-11T03:52:12.7477103Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7477207Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7477312Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7477596Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7477734Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7477888Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7477899Z 
2025-04-11T03:52:12.7478136Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7478301Z _____________ test_flash_decoding[False-False-1-True-4-16-16-32-7] _____________
2025-04-11T03:52:12.7478305Z 
2025-04-11T03:52:12.7478575Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7478739Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7478830Z use_new_kcache_layout = False
2025-04-11T03:52:12.7478834Z 
2025-04-11T03:52:12.7479036Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7479144Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7479261Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7479402Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7479627Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7479744Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7479886Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7479993Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7480135Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7480289Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7480378Z     def test_flash_decoding(
2025-04-11T03:52:12.7480457Z         bsz: int,
2025-04-11T03:52:12.7480539Z         block_size: int,
2025-04-11T03:52:12.7480634Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7480716Z         num_attn_heads: int,
2025-04-11T03:52:12.7480796Z         kv_group_num: int,
2025-04-11T03:52:12.7480885Z         same_context_len: bool,
2025-04-11T03:52:12.7480960Z         q_len: int,
2025-04-11T03:52:12.7481052Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7481143Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7481214Z     ):
2025-04-11T03:52:12.7481329Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7481522Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7481710Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7481883Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7482047Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7482205Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7482279Z     
2025-04-11T03:52:12.7482365Z         torch.manual_seed(123)
2025-04-11T03:52:12.7482454Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7482551Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7482555Z 
2025-04-11T03:52:12.7482715Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7482832Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7482836Z 
2025-04-11T03:52:12.7482914Z device = None
2025-04-11T03:52:12.7482918Z 
2025-04-11T03:52:12.7483036Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7483192Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7483262Z     
2025-04-11T03:52:12.7483339Z         Args:
2025-04-11T03:52:12.7483506Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7483675Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7483779Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7483856Z         """
2025-04-11T03:52:12.7483934Z         _lazy_init()
2025-04-11T03:52:12.7484030Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7484144Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7484251Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7484537Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7484783Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7484941Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7484948Z 
2025-04-11T03:52:12.7485186Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7485356Z ____________ test_flash_decoding[False-False-1-True-4-16-16-32-16] _____________
2025-04-11T03:52:12.7485361Z 
2025-04-11T03:52:12.7485514Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7485676Z kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7485866Z use_new_kcache_layout = False
2025-04-11T03:52:12.7485871Z 
2025-04-11T03:52:12.7486071Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7486178Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7486297Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7486438Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7486559Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7486673Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7486813Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7486921Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7487060Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7487211Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7487303Z     def test_flash_decoding(
2025-04-11T03:52:12.7487385Z         bsz: int,
2025-04-11T03:52:12.7487466Z         block_size: int,
2025-04-11T03:52:12.7487559Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7487641Z         num_attn_heads: int,
2025-04-11T03:52:12.7487722Z         kv_group_num: int,
2025-04-11T03:52:12.7487813Z         same_context_len: bool,
2025-04-11T03:52:12.7487889Z         q_len: int,
2025-04-11T03:52:12.7487981Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7488070Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7488143Z     ):
2025-04-11T03:52:12.7488252Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7488444Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7488628Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7488796Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7488966Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7489121Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7489197Z     
2025-04-11T03:52:12.7489284Z         torch.manual_seed(123)
2025-04-11T03:52:12.7489378Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7489474Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7489478Z 
2025-04-11T03:52:12.7489632Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7489747Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7489751Z 
2025-04-11T03:52:12.7489829Z device = None
2025-04-11T03:52:12.7489833Z 
2025-04-11T03:52:12.7489953Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7490104Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7490178Z     
2025-04-11T03:52:12.7490255Z         Args:
2025-04-11T03:52:12.7490419Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7490583Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7490687Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7490884Z         """
2025-04-11T03:52:12.7490963Z         _lazy_init()
2025-04-11T03:52:12.7491059Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7491169Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7491275Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7491563Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7491701Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7491857Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7491959Z 
2025-04-11T03:52:12.7492200Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7492374Z _____________ test_flash_decoding[False-False-1-False-1-16-8-16-7] _____________
2025-04-11T03:52:12.7492378Z 
2025-04-11T03:52:12.7492538Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7492704Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7492798Z use_new_kcache_layout = False
2025-04-11T03:52:12.7492802Z 
2025-04-11T03:52:12.7493000Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7493108Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7493226Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7493363Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7493484Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7493601Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7493740Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7493842Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7493984Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7494136Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7494225Z     def test_flash_decoding(
2025-04-11T03:52:12.7494304Z         bsz: int,
2025-04-11T03:52:12.7494386Z         block_size: int,
2025-04-11T03:52:12.7494478Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7494561Z         num_attn_heads: int,
2025-04-11T03:52:12.7494648Z         kv_group_num: int,
2025-04-11T03:52:12.7494732Z         same_context_len: bool,
2025-04-11T03:52:12.7494808Z         q_len: int,
2025-04-11T03:52:12.7494899Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7494991Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7495064Z     ):
2025-04-11T03:52:12.7495176Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7495368Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7495552Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7495726Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7495892Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7496049Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7496126Z     
2025-04-11T03:52:12.7496212Z         torch.manual_seed(123)
2025-04-11T03:52:12.7496301Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7496396Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7496400Z 
2025-04-11T03:52:12.7496563Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7496681Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7496685Z 
2025-04-11T03:52:12.7496761Z device = None
2025-04-11T03:52:12.7496765Z 
2025-04-11T03:52:12.7496888Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7497152Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7497223Z     
2025-04-11T03:52:12.7497303Z         Args:
2025-04-11T03:52:12.7497474Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7497644Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7497748Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7497824Z         """
2025-04-11T03:52:12.7497903Z         _lazy_init()
2025-04-11T03:52:12.7498000Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7498201Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7498310Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7498597Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7498736Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7498898Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7498903Z 
2025-04-11T03:52:12.7499147Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7499318Z ____________ test_flash_decoding[False-False-1-False-1-16-8-16-16] _____________
2025-04-11T03:52:12.7499326Z 
2025-04-11T03:52:12.7499475Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7499638Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7499734Z use_new_kcache_layout = False
2025-04-11T03:52:12.7499738Z 
2025-04-11T03:52:12.7499939Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7500046Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7500165Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7500311Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7500427Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7500542Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7500684Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7500789Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7500931Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7501080Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7501175Z     def test_flash_decoding(
2025-04-11T03:52:12.7501253Z         bsz: int,
2025-04-11T03:52:12.7501334Z         block_size: int,
2025-04-11T03:52:12.7501429Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7501514Z         num_attn_heads: int,
2025-04-11T03:52:12.7501598Z         kv_group_num: int,
2025-04-11T03:52:12.7501691Z         same_context_len: bool,
2025-04-11T03:52:12.7501767Z         q_len: int,
2025-04-11T03:52:12.7501860Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7501949Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7502024Z     ):
2025-04-11T03:52:12.7502137Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7502329Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7502514Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7502685Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7502854Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7503011Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7503087Z     
2025-04-11T03:52:12.7503174Z         torch.manual_seed(123)
2025-04-11T03:52:12.7503375Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7503471Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7503475Z 
2025-04-11T03:52:12.7503632Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7503753Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7503757Z 
2025-04-11T03:52:12.7503836Z device = None
2025-04-11T03:52:12.7503840Z 
2025-04-11T03:52:12.7503963Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7504113Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7504189Z     
2025-04-11T03:52:12.7504362Z         Args:
2025-04-11T03:52:12.7504529Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7504701Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7504806Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7504887Z         """
2025-04-11T03:52:12.7504965Z         _lazy_init()
2025-04-11T03:52:12.7505063Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7505172Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7505280Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7505569Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7505706Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7505866Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7505873Z 
2025-04-11T03:52:12.7506115Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7506283Z _____________ test_flash_decoding[False-False-1-False-1-16-8-32-7] _____________
2025-04-11T03:52:12.7506291Z 
2025-04-11T03:52:12.7506445Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7506609Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7506702Z use_new_kcache_layout = False
2025-04-11T03:52:12.7506706Z 
2025-04-11T03:52:12.7506905Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7507016Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7507136Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7507280Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7507396Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7507516Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7507658Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7507762Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7507906Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7508059Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7508150Z     def test_flash_decoding(
2025-04-11T03:52:12.7508226Z         bsz: int,
2025-04-11T03:52:12.7508309Z         block_size: int,
2025-04-11T03:52:12.7508403Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7508528Z         num_attn_heads: int,
2025-04-11T03:52:12.7508615Z         kv_group_num: int,
2025-04-11T03:52:12.7508704Z         same_context_len: bool,
2025-04-11T03:52:12.7508782Z         q_len: int,
2025-04-11T03:52:12.7508874Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7508960Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7509040Z     ):
2025-04-11T03:52:12.7509151Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7509343Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7509528Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7509819Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7509987Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7510144Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7510220Z     
2025-04-11T03:52:12.7510310Z         torch.manual_seed(123)
2025-04-11T03:52:12.7510402Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7510499Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7510503Z 
2025-04-11T03:52:12.7510772Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7510892Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7510897Z 
2025-04-11T03:52:12.7510977Z device = None
2025-04-11T03:52:12.7510981Z 
2025-04-11T03:52:12.7511107Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7511262Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7511337Z     
2025-04-11T03:52:12.7511411Z         Args:
2025-04-11T03:52:12.7511578Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7511744Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7511849Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7511932Z         """
2025-04-11T03:52:12.7512009Z         _lazy_init()
2025-04-11T03:52:12.7512104Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7512214Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7512321Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7512610Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7512751Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7512912Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7512915Z 
2025-04-11T03:52:12.7513156Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7513327Z ____________ test_flash_decoding[False-False-1-False-1-16-8-32-16] _____________
2025-04-11T03:52:12.7513331Z 
2025-04-11T03:52:12.7513481Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7513645Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7513745Z use_new_kcache_layout = False
2025-04-11T03:52:12.7513749Z 
2025-04-11T03:52:12.7513947Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7514057Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7514173Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7514318Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7514432Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7514547Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7514688Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7514793Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7514934Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7515084Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7515176Z     def test_flash_decoding(
2025-04-11T03:52:12.7515255Z         bsz: int,
2025-04-11T03:52:12.7515338Z         block_size: int,
2025-04-11T03:52:12.7515433Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7515515Z         num_attn_heads: int,
2025-04-11T03:52:12.7515601Z         kv_group_num: int,
2025-04-11T03:52:12.7515811Z         same_context_len: bool,
2025-04-11T03:52:12.7515889Z         q_len: int,
2025-04-11T03:52:12.7515979Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7516070Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7516145Z     ):
2025-04-11T03:52:12.7516256Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7516450Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7516629Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7516800Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7517066Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7517222Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7517297Z     
2025-04-11T03:52:12.7517385Z         torch.manual_seed(123)
2025-04-11T03:52:12.7517484Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7517577Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7517582Z 
2025-04-11T03:52:12.7517737Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7517855Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7517859Z 
2025-04-11T03:52:12.7517934Z device = None
2025-04-11T03:52:12.7517938Z 
2025-04-11T03:52:12.7518059Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7518211Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7518285Z     
2025-04-11T03:52:12.7518364Z         Args:
2025-04-11T03:52:12.7518529Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7518698Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7518801Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7518879Z         """
2025-04-11T03:52:12.7518958Z         _lazy_init()
2025-04-11T03:52:12.7519053Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7519158Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7519264Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7519550Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7519685Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7519846Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7519853Z 
2025-04-11T03:52:12.7520091Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7520268Z ____________ test_flash_decoding[False-False-1-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.7520272Z 
2025-04-11T03:52:12.7520428Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7520590Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7520683Z use_new_kcache_layout = False
2025-04-11T03:52:12.7520687Z 
2025-04-11T03:52:12.7520887Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7520996Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7521114Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7521255Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7521373Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7521490Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7521628Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7521734Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7521876Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7522134Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7522227Z     def test_flash_decoding(
2025-04-11T03:52:12.7522303Z         bsz: int,
2025-04-11T03:52:12.7522388Z         block_size: int,
2025-04-11T03:52:12.7522483Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7522567Z         num_attn_heads: int,
2025-04-11T03:52:12.7522654Z         kv_group_num: int,
2025-04-11T03:52:12.7522740Z         same_context_len: bool,
2025-04-11T03:52:12.7522816Z         q_len: int,
2025-04-11T03:52:12.7522907Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7522996Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7523167Z     ):
2025-04-11T03:52:12.7523280Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7523482Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7523665Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7523842Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7524012Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7524171Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7524253Z     
2025-04-11T03:52:12.7524373Z         torch.manual_seed(123)
2025-04-11T03:52:12.7524474Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7524567Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7524571Z 
2025-04-11T03:52:12.7524732Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7524852Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7524857Z 
2025-04-11T03:52:12.7524933Z device = None
2025-04-11T03:52:12.7524937Z 
2025-04-11T03:52:12.7525061Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7525215Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7525289Z     
2025-04-11T03:52:12.7525363Z         Args:
2025-04-11T03:52:12.7525529Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7525696Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7525801Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7525877Z         """
2025-04-11T03:52:12.7525954Z         _lazy_init()
2025-04-11T03:52:12.7526053Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7526160Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7526264Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7526554Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7526691Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7526849Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7526853Z 
2025-04-11T03:52:12.7527092Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7527267Z ____________ test_flash_decoding[False-False-1-False-1-16-16-16-16] ____________
2025-04-11T03:52:12.7527271Z 
2025-04-11T03:52:12.7527422Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7527593Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7527686Z use_new_kcache_layout = False
2025-04-11T03:52:12.7527690Z 
2025-04-11T03:52:12.7527891Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7527998Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7528116Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7528369Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7528485Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7528605Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7528744Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7528849Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7528992Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7529141Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7529233Z     def test_flash_decoding(
2025-04-11T03:52:12.7529402Z         bsz: int,
2025-04-11T03:52:12.7529486Z         block_size: int,
2025-04-11T03:52:12.7529582Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7529668Z         num_attn_heads: int,
2025-04-11T03:52:12.7529754Z         kv_group_num: int,
2025-04-11T03:52:12.7529839Z         same_context_len: bool,
2025-04-11T03:52:12.7529924Z         q_len: int,
2025-04-11T03:52:12.7530009Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7530096Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7530174Z     ):
2025-04-11T03:52:12.7530283Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7530477Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7530663Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7530833Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7531002Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7531160Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7531237Z     
2025-04-11T03:52:12.7531323Z         torch.manual_seed(123)
2025-04-11T03:52:12.7531420Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7531513Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7531517Z 
2025-04-11T03:52:12.7531672Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7531789Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7531793Z 
2025-04-11T03:52:12.7531870Z device = None
2025-04-11T03:52:12.7531874Z 
2025-04-11T03:52:12.7531996Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7532145Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7532218Z     
2025-04-11T03:52:12.7532294Z         Args:
2025-04-11T03:52:12.7532458Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7532627Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7532731Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7532813Z         """
2025-04-11T03:52:12.7532891Z         _lazy_init()
2025-04-11T03:52:12.7532992Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7533097Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7533206Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7533492Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7533626Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7533789Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7533795Z 
2025-04-11T03:52:12.7534036Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7534211Z ____________ test_flash_decoding[False-False-1-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.7534215Z 
2025-04-11T03:52:12.7534365Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7534644Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7534735Z use_new_kcache_layout = False
2025-04-11T03:52:12.7534739Z 
2025-04-11T03:52:12.7534936Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7535045Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7535162Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7535302Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7535418Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7535654Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7535792Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7535900Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7536045Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7536196Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7536289Z     def test_flash_decoding(
2025-04-11T03:52:12.7536365Z         bsz: int,
2025-04-11T03:52:12.7536450Z         block_size: int,
2025-04-11T03:52:12.7536540Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7536622Z         num_attn_heads: int,
2025-04-11T03:52:12.7536710Z         kv_group_num: int,
2025-04-11T03:52:12.7536797Z         same_context_len: bool,
2025-04-11T03:52:12.7536876Z         q_len: int,
2025-04-11T03:52:12.7536963Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7537054Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7537133Z     ):
2025-04-11T03:52:12.7537242Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7537436Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7537615Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7537788Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7537954Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7538109Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7538183Z     
2025-04-11T03:52:12.7538271Z         torch.manual_seed(123)
2025-04-11T03:52:12.7538361Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7538452Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7538456Z 
2025-04-11T03:52:12.7538609Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7538728Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7538733Z 
2025-04-11T03:52:12.7538810Z device = None
2025-04-11T03:52:12.7538815Z 
2025-04-11T03:52:12.7538936Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7539088Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7539163Z     
2025-04-11T03:52:12.7539235Z         Args:
2025-04-11T03:52:12.7539403Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7539566Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7539671Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7539748Z         """
2025-04-11T03:52:12.7539826Z         _lazy_init()
2025-04-11T03:52:12.7539925Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7540032Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7540137Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7540420Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7540736Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7540898Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7540903Z 
2025-04-11T03:52:12.7541143Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7541320Z ____________ test_flash_decoding[False-False-1-False-1-16-16-32-16] ____________
2025-04-11T03:52:12.7541324Z 
2025-04-11T03:52:12.7541475Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7541643Z kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7541833Z use_new_kcache_layout = False
2025-04-11T03:52:12.7541838Z 
2025-04-11T03:52:12.7542037Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7542145Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7542264Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7542408Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7542524Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7542639Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7542773Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7542877Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7543015Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7543164Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7543257Z     def test_flash_decoding(
2025-04-11T03:52:12.7543337Z         bsz: int,
2025-04-11T03:52:12.7543421Z         block_size: int,
2025-04-11T03:52:12.7543512Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7543595Z         num_attn_heads: int,
2025-04-11T03:52:12.7543681Z         kv_group_num: int,
2025-04-11T03:52:12.7543767Z         same_context_len: bool,
2025-04-11T03:52:12.7543847Z         q_len: int,
2025-04-11T03:52:12.7543933Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7544019Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7544094Z     ):
2025-04-11T03:52:12.7544204Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7544403Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7544584Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7544759Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7544928Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7545084Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7545158Z     
2025-04-11T03:52:12.7545245Z         torch.manual_seed(123)
2025-04-11T03:52:12.7545342Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7545434Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7545439Z 
2025-04-11T03:52:12.7545595Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7545709Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7545713Z 
2025-04-11T03:52:12.7545789Z device = None
2025-04-11T03:52:12.7545794Z 
2025-04-11T03:52:12.7545916Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7546066Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7546139Z     
2025-04-11T03:52:12.7546216Z         Args:
2025-04-11T03:52:12.7546381Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7546546Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7546650Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7546839Z         """
2025-04-11T03:52:12.7546919Z         _lazy_init()
2025-04-11T03:52:12.7547016Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7547119Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7547225Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7547514Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7547651Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7547811Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7547907Z 
2025-04-11T03:52:12.7548152Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7548324Z _____________ test_flash_decoding[False-False-1-False-4-16-8-16-7] _____________
2025-04-11T03:52:12.7548328Z 
2025-04-11T03:52:12.7548515Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7548685Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7548778Z use_new_kcache_layout = False
2025-04-11T03:52:12.7548782Z 
2025-04-11T03:52:12.7548986Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7549090Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7549210Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7549355Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7549473Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7549597Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7549734Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7549841Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7549979Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7550131Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7550226Z     def test_flash_decoding(
2025-04-11T03:52:12.7550302Z         bsz: int,
2025-04-11T03:52:12.7550392Z         block_size: int,
2025-04-11T03:52:12.7550481Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7550563Z         num_attn_heads: int,
2025-04-11T03:52:12.7550650Z         kv_group_num: int,
2025-04-11T03:52:12.7550735Z         same_context_len: bool,
2025-04-11T03:52:12.7550813Z         q_len: int,
2025-04-11T03:52:12.7550898Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7550986Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7551063Z     ):
2025-04-11T03:52:12.7551175Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7551369Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7551547Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7551722Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7551884Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7552039Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7552115Z     
2025-04-11T03:52:12.7552201Z         torch.manual_seed(123)
2025-04-11T03:52:12.7552293Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7552383Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7552387Z 
2025-04-11T03:52:12.7552545Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7552660Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7552664Z 
2025-04-11T03:52:12.7552741Z device = None
2025-04-11T03:52:12.7552749Z 
2025-04-11T03:52:12.7552864Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7553136Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7553213Z     
2025-04-11T03:52:12.7553288Z         Args:
2025-04-11T03:52:12.7553462Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7553630Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7553735Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7553812Z         """
2025-04-11T03:52:12.7553890Z         _lazy_init()
2025-04-11T03:52:12.7553993Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7554202Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7554311Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7554595Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7554733Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7554894Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7554898Z 
2025-04-11T03:52:12.7555137Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7555312Z ____________ test_flash_decoding[False-False-1-False-4-16-8-16-16] _____________
2025-04-11T03:52:12.7555316Z 
2025-04-11T03:52:12.7555465Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7555631Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7555726Z use_new_kcache_layout = False
2025-04-11T03:52:12.7555731Z 
2025-04-11T03:52:12.7555933Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7556040Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7556158Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7556305Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7556425Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7556542Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7556679Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7556789Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7556927Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7557075Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7557171Z     def test_flash_decoding(
2025-04-11T03:52:12.7557250Z         bsz: int,
2025-04-11T03:52:12.7557335Z         block_size: int,
2025-04-11T03:52:12.7557426Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7557511Z         num_attn_heads: int,
2025-04-11T03:52:12.7557596Z         kv_group_num: int,
2025-04-11T03:52:12.7557682Z         same_context_len: bool,
2025-04-11T03:52:12.7557765Z         q_len: int,
2025-04-11T03:52:12.7557851Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7557940Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7558018Z     ):
2025-04-11T03:52:12.7558131Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7558328Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7558507Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7558683Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7558850Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7559008Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7559083Z     
2025-04-11T03:52:12.7559172Z         torch.manual_seed(123)
2025-04-11T03:52:12.7559408Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7559498Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7559502Z 
2025-04-11T03:52:12.7559661Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7559775Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7559778Z 
2025-04-11T03:52:12.7559863Z device = None
2025-04-11T03:52:12.7559867Z 
2025-04-11T03:52:12.7559987Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7560138Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7560214Z     
2025-04-11T03:52:12.7560387Z         Args:
2025-04-11T03:52:12.7560556Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7560721Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7560827Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7560908Z         """
2025-04-11T03:52:12.7560985Z         _lazy_init()
2025-04-11T03:52:12.7561086Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7561189Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7561299Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7561584Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7561720Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7561883Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7561891Z 
2025-04-11T03:52:12.7562134Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7562306Z _____________ test_flash_decoding[False-False-1-False-4-16-8-32-7] _____________
2025-04-11T03:52:12.7562310Z 
2025-04-11T03:52:12.7562461Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7562634Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7562724Z use_new_kcache_layout = False
2025-04-11T03:52:12.7562729Z 
2025-04-11T03:52:12.7562935Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7563041Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7563160Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7563303Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7563421Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7563543Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7563680Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7563788Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7563923Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7564077Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7564171Z     def test_flash_decoding(
2025-04-11T03:52:12.7564248Z         bsz: int,
2025-04-11T03:52:12.7564333Z         block_size: int,
2025-04-11T03:52:12.7564424Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7564507Z         num_attn_heads: int,
2025-04-11T03:52:12.7564593Z         kv_group_num: int,
2025-04-11T03:52:12.7564681Z         same_context_len: bool,
2025-04-11T03:52:12.7564761Z         q_len: int,
2025-04-11T03:52:12.7564846Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7564937Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7565012Z     ):
2025-04-11T03:52:12.7565123Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7565323Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7565507Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7565794Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7565955Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7566115Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7566190Z     
2025-04-11T03:52:12.7566276Z         torch.manual_seed(123)
2025-04-11T03:52:12.7566372Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7566464Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7566468Z 
2025-04-11T03:52:12.7566629Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7566841Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7566845Z 
2025-04-11T03:52:12.7566926Z device = None
2025-04-11T03:52:12.7566931Z 
2025-04-11T03:52:12.7567048Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7567202Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7567277Z     
2025-04-11T03:52:12.7567353Z         Args:
2025-04-11T03:52:12.7567522Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7567687Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7567796Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7567869Z         """
2025-04-11T03:52:12.7567946Z         _lazy_init()
2025-04-11T03:52:12.7568047Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7568155Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7568266Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7568550Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7568691Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7568849Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7568854Z 
2025-04-11T03:52:12.7569092Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7569266Z ____________ test_flash_decoding[False-False-1-False-4-16-8-32-16] _____________
2025-04-11T03:52:12.7569270Z 
2025-04-11T03:52:12.7569419Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7569587Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7569679Z use_new_kcache_layout = False
2025-04-11T03:52:12.7569683Z 
2025-04-11T03:52:12.7569886Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7569991Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7570108Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7570254Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7570370Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7570490Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7570625Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7570736Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7570871Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7571022Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7571113Z     def test_flash_decoding(
2025-04-11T03:52:12.7571193Z         bsz: int,
2025-04-11T03:52:12.7571280Z         block_size: int,
2025-04-11T03:52:12.7571369Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7571457Z         num_attn_heads: int,
2025-04-11T03:52:12.7571540Z         kv_group_num: int,
2025-04-11T03:52:12.7571627Z         same_context_len: bool,
2025-04-11T03:52:12.7571815Z         q_len: int,
2025-04-11T03:52:12.7571902Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7571993Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7572065Z     ):
2025-04-11T03:52:12.7572177Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7572379Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7572563Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7572739Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7573024Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7573186Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7573259Z     
2025-04-11T03:52:12.7573346Z         torch.manual_seed(123)
2025-04-11T03:52:12.7573444Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7573537Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7573541Z 
2025-04-11T03:52:12.7573698Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7573809Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7573813Z 
2025-04-11T03:52:12.7573894Z device = None
2025-04-11T03:52:12.7573900Z 
2025-04-11T03:52:12.7574017Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7574166Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7574241Z     
2025-04-11T03:52:12.7574319Z         Args:
2025-04-11T03:52:12.7574491Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7574656Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7574767Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7574844Z         """
2025-04-11T03:52:12.7574923Z         _lazy_init()
2025-04-11T03:52:12.7575024Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7575128Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7575239Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7575521Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7575661Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7575818Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7575825Z 
2025-04-11T03:52:12.7576067Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7576243Z ____________ test_flash_decoding[False-False-1-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.7576247Z 
2025-04-11T03:52:12.7576397Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7576570Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7576660Z use_new_kcache_layout = False
2025-04-11T03:52:12.7576665Z 
2025-04-11T03:52:12.7576868Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7576972Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7577092Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7577233Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7577350Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7577473Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7577611Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7577721Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7577859Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7578115Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7578207Z     def test_flash_decoding(
2025-04-11T03:52:12.7578282Z         bsz: int,
2025-04-11T03:52:12.7578368Z         block_size: int,
2025-04-11T03:52:12.7578461Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7578546Z         num_attn_heads: int,
2025-04-11T03:52:12.7578629Z         kv_group_num: int,
2025-04-11T03:52:12.7578715Z         same_context_len: bool,
2025-04-11T03:52:12.7578795Z         q_len: int,
2025-04-11T03:52:12.7578881Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7578974Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7579144Z     ):
2025-04-11T03:52:12.7579258Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7579458Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7579642Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7579826Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7579991Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7580150Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7580221Z     
2025-04-11T03:52:12.7580307Z         torch.manual_seed(123)
2025-04-11T03:52:12.7580404Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7580497Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7580502Z 
2025-04-11T03:52:12.7580663Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7580780Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7580783Z 
2025-04-11T03:52:12.7580862Z device = None
2025-04-11T03:52:12.7580867Z 
2025-04-11T03:52:12.7580986Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7581138Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7581214Z     
2025-04-11T03:52:12.7581289Z         Args:
2025-04-11T03:52:12.7581464Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7581631Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7581739Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7581812Z         """
2025-04-11T03:52:12.7581890Z         _lazy_init()
2025-04-11T03:52:12.7581990Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7582097Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7582204Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7582486Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7582627Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7582787Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7582791Z 
2025-04-11T03:52:12.7583029Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7583210Z ____________ test_flash_decoding[False-False-1-False-4-16-16-16-16] ____________
2025-04-11T03:52:12.7583214Z 
2025-04-11T03:52:12.7583365Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7583535Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7583630Z use_new_kcache_layout = False
2025-04-11T03:52:12.7583634Z 
2025-04-11T03:52:12.7583838Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7583943Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7584066Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7584333Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7584449Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7584569Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7584703Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7584812Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7584945Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7585101Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7585190Z     def test_flash_decoding(
2025-04-11T03:52:12.7585363Z         bsz: int,
2025-04-11T03:52:12.7585452Z         block_size: int,
2025-04-11T03:52:12.7585541Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7585630Z         num_attn_heads: int,
2025-04-11T03:52:12.7585714Z         kv_group_num: int,
2025-04-11T03:52:12.7585800Z         same_context_len: bool,
2025-04-11T03:52:12.7585888Z         q_len: int,
2025-04-11T03:52:12.7585974Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7586069Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7586141Z     ):
2025-04-11T03:52:12.7586249Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7586445Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7586628Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7586802Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7586970Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7587130Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7587201Z     
2025-04-11T03:52:12.7587291Z         torch.manual_seed(123)
2025-04-11T03:52:12.7587387Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7587480Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7587484Z 
2025-04-11T03:52:12.7587643Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7587756Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7587761Z 
2025-04-11T03:52:12.7587841Z device = None
2025-04-11T03:52:12.7587845Z 
2025-04-11T03:52:12.7587961Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7588115Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7588185Z     
2025-04-11T03:52:12.7588261Z         Args:
2025-04-11T03:52:12.7588472Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7588640Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7588750Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7588827Z         """
2025-04-11T03:52:12.7588905Z         _lazy_init()
2025-04-11T03:52:12.7589005Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7589110Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7589218Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7589503Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7589642Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7589797Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7589805Z 
2025-04-11T03:52:12.7590043Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7590216Z ____________ test_flash_decoding[False-False-1-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.7590220Z 
2025-04-11T03:52:12.7590370Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7590650Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7590739Z use_new_kcache_layout = False
2025-04-11T03:52:12.7590744Z 
2025-04-11T03:52:12.7590947Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7591053Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7591172Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7591309Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7591426Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7591649Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7591785Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7591894Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7592030Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7592188Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7592277Z     def test_flash_decoding(
2025-04-11T03:52:12.7592352Z         bsz: int,
2025-04-11T03:52:12.7592441Z         block_size: int,
2025-04-11T03:52:12.7592530Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7592616Z         num_attn_heads: int,
2025-04-11T03:52:12.7592697Z         kv_group_num: int,
2025-04-11T03:52:12.7592785Z         same_context_len: bool,
2025-04-11T03:52:12.7592865Z         q_len: int,
2025-04-11T03:52:12.7592950Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7593042Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7593116Z     ):
2025-04-11T03:52:12.7593225Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7593422Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7593601Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7593781Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7593944Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7594104Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7594174Z     
2025-04-11T03:52:12.7594265Z         torch.manual_seed(123)
2025-04-11T03:52:12.7594354Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7594444Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7594448Z 
2025-04-11T03:52:12.7594607Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7594722Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7594726Z 
2025-04-11T03:52:12.7594806Z device = None
2025-04-11T03:52:12.7594810Z 
2025-04-11T03:52:12.7594926Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7595081Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7595151Z     
2025-04-11T03:52:12.7595225Z         Args:
2025-04-11T03:52:12.7595395Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7595558Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7595667Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7595742Z         """
2025-04-11T03:52:12.7595819Z         _lazy_init()
2025-04-11T03:52:12.7595920Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7596026Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7596139Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7596422Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7596560Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7596831Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7596836Z 
2025-04-11T03:52:12.7597079Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7597252Z ____________ test_flash_decoding[False-False-1-False-4-16-16-32-16] ____________
2025-04-11T03:52:12.7597256Z 
2025-04-11T03:52:12.7597408Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7597577Z kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
2025-04-11T03:52:12.7597757Z use_new_kcache_layout = False
2025-04-11T03:52:12.7597761Z 
2025-04-11T03:52:12.7597970Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7598074Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7598196Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7598342Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7598459Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7598579Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7598718Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7598826Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7598963Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7599117Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7599207Z     def test_flash_decoding(
2025-04-11T03:52:12.7599285Z         bsz: int,
2025-04-11T03:52:12.7599371Z         block_size: int,
2025-04-11T03:52:12.7599459Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7599544Z         num_attn_heads: int,
2025-04-11T03:52:12.7599627Z         kv_group_num: int,
2025-04-11T03:52:12.7599714Z         same_context_len: bool,
2025-04-11T03:52:12.7599799Z         q_len: int,
2025-04-11T03:52:12.7599883Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7599974Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7600045Z     ):
2025-04-11T03:52:12.7600159Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7600353Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7600533Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7600710Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7600875Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7601036Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7601106Z     
2025-04-11T03:52:12.7601194Z         torch.manual_seed(123)
2025-04-11T03:52:12.7601287Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7601377Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7601380Z 
2025-04-11T03:52:12.7601539Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7601651Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7601656Z 
2025-04-11T03:52:12.7601736Z device = None
2025-04-11T03:52:12.7601740Z 
2025-04-11T03:52:12.7601854Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7602009Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7602080Z     
2025-04-11T03:52:12.7602158Z         Args:
2025-04-11T03:52:12.7602329Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7602491Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7602600Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7602771Z         """
2025-04-11T03:52:12.7602854Z         _lazy_init()
2025-04-11T03:52:12.7602949Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7603053Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7603161Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7603441Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7603579Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7603734Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7603849Z 
2025-04-11T03:52:12.7604096Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7604265Z _____________ test_flash_decoding[False-False-5-True-1-16-8-16-7] ______________
2025-04-11T03:52:12.7604270Z 
2025-04-11T03:52:12.7604421Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7604595Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7604683Z use_new_kcache_layout = False
2025-04-11T03:52:12.7604687Z 
2025-04-11T03:52:12.7604897Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7605001Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7605121Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7605263Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7605384Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7605501Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7605636Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7605745Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7605881Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7606038Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7606125Z     def test_flash_decoding(
2025-04-11T03:52:12.7606200Z         bsz: int,
2025-04-11T03:52:12.7606285Z         block_size: int,
2025-04-11T03:52:12.7606375Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7606462Z         num_attn_heads: int,
2025-04-11T03:52:12.7606546Z         kv_group_num: int,
2025-04-11T03:52:12.7606635Z         same_context_len: bool,
2025-04-11T03:52:12.7606712Z         q_len: int,
2025-04-11T03:52:12.7606797Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7606891Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7606965Z     ):
2025-04-11T03:52:12.7607079Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7607273Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7607452Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7607630Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7607793Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7607952Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7608022Z     
2025-04-11T03:52:12.7608112Z         torch.manual_seed(123)
2025-04-11T03:52:12.7608200Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7608293Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7608297Z 
2025-04-11T03:52:12.7608455Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7608569Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7608573Z 
2025-04-11T03:52:12.7608654Z device = None
2025-04-11T03:52:12.7608657Z 
2025-04-11T03:52:12.7608773Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7609041Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7609113Z     
2025-04-11T03:52:12.7609188Z         Args:
2025-04-11T03:52:12.7609357Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7609520Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7609629Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7609702Z         """
2025-04-11T03:52:12.7609783Z         _lazy_init()
2025-04-11T03:52:12.7609880Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7610075Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7610187Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7610471Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7610610Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7610769Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7610773Z 
2025-04-11T03:52:12.7611018Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7611186Z _____________ test_flash_decoding[False-False-5-True-1-16-8-16-16] _____________
2025-04-11T03:52:12.7611190Z 
2025-04-11T03:52:12.7611345Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7611508Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7611599Z use_new_kcache_layout = False
2025-04-11T03:52:12.7611604Z 
2025-04-11T03:52:12.7611805Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7611906Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7612027Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7612167Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7612287Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7612399Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7612534Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7612643Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7612777Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7612928Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7613017Z     def test_flash_decoding(
2025-04-11T03:52:12.7613098Z         bsz: int,
2025-04-11T03:52:12.7613181Z         block_size: int,
2025-04-11T03:52:12.7613269Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7613355Z         num_attn_heads: int,
2025-04-11T03:52:12.7613437Z         kv_group_num: int,
2025-04-11T03:52:12.7613526Z         same_context_len: bool,
2025-04-11T03:52:12.7613605Z         q_len: int,
2025-04-11T03:52:12.7613691Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7613783Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7613854Z     ):
2025-04-11T03:52:12.7613968Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7614161Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7614340Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7614517Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7614685Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7614846Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7614918Z     
2025-04-11T03:52:12.7615009Z         torch.manual_seed(123)
2025-04-11T03:52:12.7615098Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7615292Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7615297Z 
2025-04-11T03:52:12.7615456Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7615567Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7615571Z 
2025-04-11T03:52:12.7615653Z device = None
2025-04-11T03:52:12.7615658Z 
2025-04-11T03:52:12.7615775Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7615928Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7615998Z     
2025-04-11T03:52:12.7616076Z         Args:
2025-04-11T03:52:12.7616338Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7616502Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7616610Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7616689Z         """
2025-04-11T03:52:12.7616771Z         _lazy_init()
2025-04-11T03:52:12.7616867Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7616969Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7617079Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7617361Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7617501Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7617655Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7617663Z 
2025-04-11T03:52:12.7617906Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7618073Z _____________ test_flash_decoding[False-False-5-True-1-16-8-32-7] ______________
2025-04-11T03:52:12.7618077Z 
2025-04-11T03:52:12.7618230Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7618396Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7618487Z use_new_kcache_layout = False
2025-04-11T03:52:12.7618491Z 
2025-04-11T03:52:12.7618695Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7618802Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7618926Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7619066Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7619188Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7619305Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7619440Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7619546Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7619683Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7619838Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7619927Z     def test_flash_decoding(
2025-04-11T03:52:12.7620007Z         bsz: int,
2025-04-11T03:52:12.7620092Z         block_size: int,
2025-04-11T03:52:12.7620181Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7620270Z         num_attn_heads: int,
2025-04-11T03:52:12.7620353Z         kv_group_num: int,
2025-04-11T03:52:12.7620441Z         same_context_len: bool,
2025-04-11T03:52:12.7620518Z         q_len: int,
2025-04-11T03:52:12.7620607Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7620698Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7620775Z     ):
2025-04-11T03:52:12.7620892Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7621084Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7621261Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7621551Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7621715Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7621878Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7621951Z     
2025-04-11T03:52:12.7622044Z         torch.manual_seed(123)
2025-04-11T03:52:12.7622134Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7622224Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7622232Z 
2025-04-11T03:52:12.7622389Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7622603Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7622607Z 
2025-04-11T03:52:12.7622689Z device = None
2025-04-11T03:52:12.7622694Z 
2025-04-11T03:52:12.7622812Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7622970Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7623042Z     
2025-04-11T03:52:12.7623124Z         Args:
2025-04-11T03:52:12.7623296Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7623466Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7623578Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7623655Z         """
2025-04-11T03:52:12.7623739Z         _lazy_init()
2025-04-11T03:52:12.7623838Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7623948Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7624059Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7624345Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7624490Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7624651Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7624656Z 
2025-04-11T03:52:12.7624917Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7625112Z _____________ test_flash_decoding[False-False-5-True-1-16-8-32-16] _____________
2025-04-11T03:52:12.7625119Z 
2025-04-11T03:52:12.7625270Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7625433Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7625528Z use_new_kcache_layout = False
2025-04-11T03:52:12.7625536Z 
2025-04-11T03:52:12.7625735Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7625838Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7625961Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7626106Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7626227Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7626342Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7626477Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7626585Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7626720Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7626876Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7626966Z     def test_flash_decoding(
2025-04-11T03:52:12.7627047Z         bsz: int,
2025-04-11T03:52:12.7627130Z         block_size: int,
2025-04-11T03:52:12.7627219Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7627306Z         num_attn_heads: int,
2025-04-11T03:52:12.7627388Z         kv_group_num: int,
2025-04-11T03:52:12.7627479Z         same_context_len: bool,
2025-04-11T03:52:12.7627672Z         q_len: int,
2025-04-11T03:52:12.7627759Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7627855Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7627927Z     ):
2025-04-11T03:52:12.7628047Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7628239Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7628472Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7628645Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7628930Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7629092Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7629164Z     
2025-04-11T03:52:12.7629256Z         torch.manual_seed(123)
2025-04-11T03:52:12.7629346Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7629444Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7629449Z 
2025-04-11T03:52:12.7629604Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7629716Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7629720Z 
2025-04-11T03:52:12.7629800Z device = None
2025-04-11T03:52:12.7629805Z 
2025-04-11T03:52:12.7629922Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7630077Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7630149Z     
2025-04-11T03:52:12.7630223Z         Args:
2025-04-11T03:52:12.7630390Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7630553Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7630663Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7630740Z         """
2025-04-11T03:52:12.7630821Z         _lazy_init()
2025-04-11T03:52:12.7630914Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7631014Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7631123Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7631404Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7631541Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7631697Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7631704Z 
2025-04-11T03:52:12.7631944Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7632111Z _____________ test_flash_decoding[False-False-5-True-1-16-16-16-7] _____________
2025-04-11T03:52:12.7632115Z 
2025-04-11T03:52:12.7632266Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7632434Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7632526Z use_new_kcache_layout = False
2025-04-11T03:52:12.7632535Z 
2025-04-11T03:52:12.7632734Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7632838Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7632958Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7633095Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7633215Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7633333Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7633467Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7633577Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7633711Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7633999Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7634087Z     def test_flash_decoding(
2025-04-11T03:52:12.7634166Z         bsz: int,
2025-04-11T03:52:12.7634251Z         block_size: int,
2025-04-11T03:52:12.7634341Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7634427Z         num_attn_heads: int,
2025-04-11T03:52:12.7634510Z         kv_group_num: int,
2025-04-11T03:52:12.7634601Z         same_context_len: bool,
2025-04-11T03:52:12.7634678Z         q_len: int,
2025-04-11T03:52:12.7634763Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7634856Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7635037Z     ):
2025-04-11T03:52:12.7635152Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7635343Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7635528Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7635703Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7635867Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7636027Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7636096Z     
2025-04-11T03:52:12.7636189Z         torch.manual_seed(123)
2025-04-11T03:52:12.7636277Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7636374Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7636378Z 
2025-04-11T03:52:12.7636531Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7636647Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7636655Z 
2025-04-11T03:52:12.7636732Z device = None
2025-04-11T03:52:12.7636736Z 
2025-04-11T03:52:12.7636853Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7637013Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7637083Z     
2025-04-11T03:52:12.7637163Z         Args:
2025-04-11T03:52:12.7637329Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7637493Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7637600Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7637674Z         """
2025-04-11T03:52:12.7637756Z         _lazy_init()
2025-04-11T03:52:12.7637850Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7637958Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7638065Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7638346Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7638487Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7638648Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7638652Z 
2025-04-11T03:52:12.7638897Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7639068Z ____________ test_flash_decoding[False-False-5-True-1-16-16-16-16] _____________
2025-04-11T03:52:12.7639073Z 
2025-04-11T03:52:12.7639228Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7639389Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7639485Z use_new_kcache_layout = False
2025-04-11T03:52:12.7639490Z 
2025-04-11T03:52:12.7639685Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7639789Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7639912Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7640249Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7640371Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7640485Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7640630Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7640736Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7640872Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7641027Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7641114Z     def test_flash_decoding(
2025-04-11T03:52:12.7641294Z         bsz: int,
2025-04-11T03:52:12.7641379Z         block_size: int,
2025-04-11T03:52:12.7641469Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7641556Z         num_attn_heads: int,
2025-04-11T03:52:12.7641638Z         kv_group_num: int,
2025-04-11T03:52:12.7641729Z         same_context_len: bool,
2025-04-11T03:52:12.7641808Z         q_len: int,
2025-04-11T03:52:12.7641897Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7641987Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7642058Z     ):
2025-04-11T03:52:12.7642173Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7642367Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7642552Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7642726Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7642891Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7643052Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7643122Z     
2025-04-11T03:52:12.7643215Z         torch.manual_seed(123)
2025-04-11T03:52:12.7643303Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7643403Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7643407Z 
2025-04-11T03:52:12.7643564Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7643675Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7643684Z 
2025-04-11T03:52:12.7643760Z device = None
2025-04-11T03:52:12.7643764Z 
2025-04-11T03:52:12.7643878Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7644030Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7644100Z     
2025-04-11T03:52:12.7644177Z         Args:
2025-04-11T03:52:12.7644347Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7644511Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7644618Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7644694Z         """
2025-04-11T03:52:12.7644776Z         _lazy_init()
2025-04-11T03:52:12.7644870Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7644977Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7645083Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7645371Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7645511Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7645666Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7645673Z 
2025-04-11T03:52:12.7645915Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7646087Z _____________ test_flash_decoding[False-False-5-True-1-16-16-32-7] _____________
2025-04-11T03:52:12.7646091Z 
2025-04-11T03:52:12.7646248Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7646521Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7646616Z use_new_kcache_layout = False
2025-04-11T03:52:12.7646621Z 
2025-04-11T03:52:12.7646820Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7646925Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7647049Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7647188Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7647308Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7647523Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7647665Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7647770Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7647907Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7648065Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7648153Z     def test_flash_decoding(
2025-04-11T03:52:12.7648232Z         bsz: int,
2025-04-11T03:52:12.7648315Z         block_size: int,
2025-04-11T03:52:12.7648405Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7648494Z         num_attn_heads: int,
2025-04-11T03:52:12.7648576Z         kv_group_num: int,
2025-04-11T03:52:12.7648666Z         same_context_len: bool,
2025-04-11T03:52:12.7648741Z         q_len: int,
2025-04-11T03:52:12.7648829Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7648916Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7648991Z     ):
2025-04-11T03:52:12.7649104Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7649295Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7649479Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7649656Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7649824Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7649982Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7650053Z     
2025-04-11T03:52:12.7650144Z         torch.manual_seed(123)
2025-04-11T03:52:12.7650234Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7650329Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7650333Z 
2025-04-11T03:52:12.7650488Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7650608Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7650612Z 
2025-04-11T03:52:12.7650689Z device = None
2025-04-11T03:52:12.7650693Z 
2025-04-11T03:52:12.7650815Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7650972Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7651042Z     
2025-04-11T03:52:12.7651120Z         Args:
2025-04-11T03:52:12.7651283Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7651452Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7651556Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7651628Z         """
2025-04-11T03:52:12.7651711Z         _lazy_init()
2025-04-11T03:52:12.7651803Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7651910Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7652017Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7652297Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7652434Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7652716Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7652720Z 
2025-04-11T03:52:12.7652962Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7653130Z ____________ test_flash_decoding[False-False-5-True-1-16-16-32-16] _____________
2025-04-11T03:52:12.7653134Z 
2025-04-11T03:52:12.7653289Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7653450Z kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7653543Z use_new_kcache_layout = False
2025-04-11T03:52:12.7653636Z 
2025-04-11T03:52:12.7653837Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7653946Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7654063Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7654206Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7654330Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7654443Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7654582Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7654686Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7654824Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7654979Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7655069Z     def test_flash_decoding(
2025-04-11T03:52:12.7655152Z         bsz: int,
2025-04-11T03:52:12.7655232Z         block_size: int,
2025-04-11T03:52:12.7655326Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7655410Z         num_attn_heads: int,
2025-04-11T03:52:12.7655493Z         kv_group_num: int,
2025-04-11T03:52:12.7655584Z         same_context_len: bool,
2025-04-11T03:52:12.7655668Z         q_len: int,
2025-04-11T03:52:12.7655757Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7655846Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7655916Z     ):
2025-04-11T03:52:12.7656034Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7656231Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7656413Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7656586Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7656752Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7656914Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7656984Z     
2025-04-11T03:52:12.7657077Z         torch.manual_seed(123)
2025-04-11T03:52:12.7657165Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7657267Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7657271Z 
2025-04-11T03:52:12.7657426Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7657544Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7657549Z 
2025-04-11T03:52:12.7657625Z device = None
2025-04-11T03:52:12.7657630Z 
2025-04-11T03:52:12.7657747Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7657905Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7657974Z     
2025-04-11T03:52:12.7658054Z         Args:
2025-04-11T03:52:12.7658228Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7658398Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7658503Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7658683Z         """
2025-04-11T03:52:12.7658765Z         _lazy_init()
2025-04-11T03:52:12.7658862Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7658971Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7659076Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7659363Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7659503Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7659661Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7659665Z 
2025-04-11T03:52:12.7660010Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7660179Z _____________ test_flash_decoding[False-False-5-True-4-16-8-16-7] ______________
2025-04-11T03:52:12.7660183Z 
2025-04-11T03:52:12.7660339Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7660506Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7660599Z use_new_kcache_layout = False
2025-04-11T03:52:12.7660603Z 
2025-04-11T03:52:12.7660801Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7660910Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7661030Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7661169Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7661292Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7661409Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7661547Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7661653Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7661791Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7661947Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7662036Z     def test_flash_decoding(
2025-04-11T03:52:12.7662117Z         bsz: int,
2025-04-11T03:52:12.7662199Z         block_size: int,
2025-04-11T03:52:12.7662294Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7662378Z         num_attn_heads: int,
2025-04-11T03:52:12.7662463Z         kv_group_num: int,
2025-04-11T03:52:12.7662552Z         same_context_len: bool,
2025-04-11T03:52:12.7662627Z         q_len: int,
2025-04-11T03:52:12.7662714Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7662802Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7662875Z     ):
2025-04-11T03:52:12.7662989Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7663182Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7663367Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7663540Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7663709Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7663869Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7663939Z     
2025-04-11T03:52:12.7664031Z         torch.manual_seed(123)
2025-04-11T03:52:12.7664119Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7664217Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7664221Z 
2025-04-11T03:52:12.7664376Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7664495Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7664499Z 
2025-04-11T03:52:12.7664575Z device = None
2025-04-11T03:52:12.7664580Z 
2025-04-11T03:52:12.7664699Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7664948Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7665019Z     
2025-04-11T03:52:12.7665097Z         Args:
2025-04-11T03:52:12.7665265Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7665433Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7665539Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7665612Z         """
2025-04-11T03:52:12.7665694Z         _lazy_init()
2025-04-11T03:52:12.7665789Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7665896Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7666105Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7666393Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7666527Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7666688Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7668800Z 
2025-04-11T03:52:12.7669059Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7669228Z _____________ test_flash_decoding[False-False-5-True-4-16-8-16-16] _____________
2025-04-11T03:52:12.7669233Z 
2025-04-11T03:52:12.7669389Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7669553Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7669645Z use_new_kcache_layout = False
2025-04-11T03:52:12.7669652Z 
2025-04-11T03:52:12.7669852Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7669956Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7670080Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7670224Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7670343Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7670500Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7670640Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7670749Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7670886Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7671040Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7671128Z     def test_flash_decoding(
2025-04-11T03:52:12.7671207Z         bsz: int,
2025-04-11T03:52:12.7671291Z         block_size: int,
2025-04-11T03:52:12.7671380Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7671467Z         num_attn_heads: int,
2025-04-11T03:52:12.7671551Z         kv_group_num: int,
2025-04-11T03:52:12.7671640Z         same_context_len: bool,
2025-04-11T03:52:12.7671720Z         q_len: int,
2025-04-11T03:52:12.7671807Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7671898Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7671971Z     ):
2025-04-11T03:52:12.7672084Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7672278Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7672459Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7672634Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7672795Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7672957Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7673028Z     
2025-04-11T03:52:12.7673122Z         torch.manual_seed(123)
2025-04-11T03:52:12.7673211Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7673423Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7673432Z 
2025-04-11T03:52:12.7673588Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7673705Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7673709Z 
2025-04-11T03:52:12.7673792Z device = None
2025-04-11T03:52:12.7673797Z 
2025-04-11T03:52:12.7673914Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7674066Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7674138Z     
2025-04-11T03:52:12.7674217Z         Args:
2025-04-11T03:52:12.7674442Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7674609Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7674721Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7674795Z         """
2025-04-11T03:52:12.7674880Z         _lazy_init()
2025-04-11T03:52:12.7674973Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7675074Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7675311Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7675599Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7675741Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7675900Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7675905Z 
2025-04-11T03:52:12.7676156Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7676323Z _____________ test_flash_decoding[False-False-5-True-4-16-8-32-7] ______________
2025-04-11T03:52:12.7676327Z 
2025-04-11T03:52:12.7676482Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7676646Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7676738Z use_new_kcache_layout = False
2025-04-11T03:52:12.7676753Z 
2025-04-11T03:52:12.7676952Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7677060Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7677182Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7677322Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7677442Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7677558Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7677695Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7677805Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7677941Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7678099Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7678187Z     def test_flash_decoding(
2025-04-11T03:52:12.7678269Z         bsz: int,
2025-04-11T03:52:12.7678350Z         block_size: int,
2025-04-11T03:52:12.7678439Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7678529Z         num_attn_heads: int,
2025-04-11T03:52:12.7678613Z         kv_group_num: int,
2025-04-11T03:52:12.7678701Z         same_context_len: bool,
2025-04-11T03:52:12.7678777Z         q_len: int,
2025-04-11T03:52:12.7678863Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7678953Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7679025Z     ):
2025-04-11T03:52:12.7679136Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7679331Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7679518Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7679795Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7679959Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7680122Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7680193Z     
2025-04-11T03:52:12.7680283Z         torch.manual_seed(123)
2025-04-11T03:52:12.7680371Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7680465Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7680469Z 
2025-04-11T03:52:12.7680625Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7680792Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7680797Z 
2025-04-11T03:52:12.7680877Z device = None
2025-04-11T03:52:12.7680882Z 
2025-04-11T03:52:12.7681000Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7681157Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7681227Z     
2025-04-11T03:52:12.7681305Z         Args:
2025-04-11T03:52:12.7681524Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7681692Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7681802Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7681876Z         """
2025-04-11T03:52:12.7681958Z         _lazy_init()
2025-04-11T03:52:12.7682056Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7682157Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7682267Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7682549Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7682692Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7682850Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7682855Z 
2025-04-11T03:52:12.7683101Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7683266Z _____________ test_flash_decoding[False-False-5-True-4-16-8-32-16] _____________
2025-04-11T03:52:12.7683270Z 
2025-04-11T03:52:12.7683425Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7683588Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7683678Z use_new_kcache_layout = False
2025-04-11T03:52:12.7683687Z 
2025-04-11T03:52:12.7683885Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7683990Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7684114Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7684258Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7684378Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7684493Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7684630Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7684739Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7684873Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7685029Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7685116Z     def test_flash_decoding(
2025-04-11T03:52:12.7685194Z         bsz: int,
2025-04-11T03:52:12.7685280Z         block_size: int,
2025-04-11T03:52:12.7685369Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7685456Z         num_attn_heads: int,
2025-04-11T03:52:12.7685538Z         kv_group_num: int,
2025-04-11T03:52:12.7685625Z         same_context_len: bool,
2025-04-11T03:52:12.7685809Z         q_len: int,
2025-04-11T03:52:12.7685897Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7685991Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7686065Z     ):
2025-04-11T03:52:12.7686180Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7686372Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7686557Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7686727Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7686890Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7687112Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7687184Z     
2025-04-11T03:52:12.7687276Z         torch.manual_seed(123)
2025-04-11T03:52:12.7687365Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7687466Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7687470Z 
2025-04-11T03:52:12.7687624Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7687794Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7687799Z 
2025-04-11T03:52:12.7687881Z device = None
2025-04-11T03:52:12.7687885Z 
2025-04-11T03:52:12.7688001Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7688154Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7688223Z     
2025-04-11T03:52:12.7688300Z         Args:
2025-04-11T03:52:12.7688468Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7688633Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7688741Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7688813Z         """
2025-04-11T03:52:12.7688900Z         _lazy_init()
2025-04-11T03:52:12.7688996Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7689105Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7689211Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7689494Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7689635Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7689792Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7689796Z 
2025-04-11T03:52:12.7690040Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7690205Z _____________ test_flash_decoding[False-False-5-True-4-16-16-16-7] _____________
2025-04-11T03:52:12.7690210Z 
2025-04-11T03:52:12.7690362Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7690526Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7690620Z use_new_kcache_layout = False
2025-04-11T03:52:12.7690626Z 
2025-04-11T03:52:12.7690827Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7690930Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7691050Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7691186Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7691307Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7691424Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7691563Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7691669Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7691805Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7692069Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7692157Z     def test_flash_decoding(
2025-04-11T03:52:12.7692234Z         bsz: int,
2025-04-11T03:52:12.7692321Z         block_size: int,
2025-04-11T03:52:12.7692409Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7692495Z         num_attn_heads: int,
2025-04-11T03:52:12.7692578Z         kv_group_num: int,
2025-04-11T03:52:12.7692668Z         same_context_len: bool,
2025-04-11T03:52:12.7692743Z         q_len: int,
2025-04-11T03:52:12.7692831Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7692920Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7693048Z     ):
2025-04-11T03:52:12.7693162Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7693356Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7693543Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7693718Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7693883Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7694103Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7694174Z     
2025-04-11T03:52:12.7694263Z         torch.manual_seed(123)
2025-04-11T03:52:12.7694355Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7694452Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7694456Z 
2025-04-11T03:52:12.7694609Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7694721Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7694729Z 
2025-04-11T03:52:12.7694804Z device = None
2025-04-11T03:52:12.7694809Z 
2025-04-11T03:52:12.7694924Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7695080Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7695153Z     
2025-04-11T03:52:12.7695230Z         Args:
2025-04-11T03:52:12.7695396Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7695561Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7695668Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7695741Z         """
2025-04-11T03:52:12.7695825Z         _lazy_init()
2025-04-11T03:52:12.7695919Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7696028Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7696136Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7696415Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7696555Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7696717Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7696721Z 
2025-04-11T03:52:12.7696964Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7697132Z ____________ test_flash_decoding[False-False-5-True-4-16-16-16-16] _____________
2025-04-11T03:52:12.7697136Z 
2025-04-11T03:52:12.7697290Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7697453Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7697547Z use_new_kcache_layout = False
2025-04-11T03:52:12.7697553Z 
2025-04-11T03:52:12.7697751Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7697855Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7697975Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7698245Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7698367Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7698484Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7698621Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7698727Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7698866Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7699021Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7699111Z     def test_flash_decoding(
2025-04-11T03:52:12.7699195Z         bsz: int,
2025-04-11T03:52:12.7699347Z         block_size: int,
2025-04-11T03:52:12.7699437Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7699526Z         num_attn_heads: int,
2025-04-11T03:52:12.7699609Z         kv_group_num: int,
2025-04-11T03:52:12.7699700Z         same_context_len: bool,
2025-04-11T03:52:12.7699777Z         q_len: int,
2025-04-11T03:52:12.7699871Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7699959Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7700088Z     ):
2025-04-11T03:52:12.7700205Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7700402Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7700590Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7700765Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7700935Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7701098Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7701169Z     
2025-04-11T03:52:12.7701262Z         torch.manual_seed(123)
2025-04-11T03:52:12.7701351Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7701450Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7701454Z 
2025-04-11T03:52:12.7701611Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7701726Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7701733Z 
2025-04-11T03:52:12.7701810Z device = None
2025-04-11T03:52:12.7701814Z 
2025-04-11T03:52:12.7701933Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7702088Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7702161Z     
2025-04-11T03:52:12.7702240Z         Args:
2025-04-11T03:52:12.7702412Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7702582Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7702686Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7702757Z         """
2025-04-11T03:52:12.7702846Z         _lazy_init()
2025-04-11T03:52:12.7702940Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7703045Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7703154Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7703438Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7703576Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7703733Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7703737Z 
2025-04-11T03:52:12.7703987Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7704154Z _____________ test_flash_decoding[False-False-5-True-4-16-16-32-7] _____________
2025-04-11T03:52:12.7704159Z 
2025-04-11T03:52:12.7704310Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7704579Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7704671Z use_new_kcache_layout = False
2025-04-11T03:52:12.7704677Z 
2025-04-11T03:52:12.7704876Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7704981Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7705101Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7705239Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7705359Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7705520Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7705661Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7705764Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7705903Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7706062Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7706149Z     def test_flash_decoding(
2025-04-11T03:52:12.7706230Z         bsz: int,
2025-04-11T03:52:12.7706372Z         block_size: int,
2025-04-11T03:52:12.7706465Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7706548Z         num_attn_heads: int,
2025-04-11T03:52:12.7706630Z         kv_group_num: int,
2025-04-11T03:52:12.7706721Z         same_context_len: bool,
2025-04-11T03:52:12.7706798Z         q_len: int,
2025-04-11T03:52:12.7706888Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7706976Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7707048Z     ):
2025-04-11T03:52:12.7707164Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7707355Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7707539Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7707712Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7707877Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7708037Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7708108Z     
2025-04-11T03:52:12.7708201Z         torch.manual_seed(123)
2025-04-11T03:52:12.7708291Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7708388Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7708392Z 
2025-04-11T03:52:12.7708590Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7708711Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7708715Z 
2025-04-11T03:52:12.7708791Z device = None
2025-04-11T03:52:12.7708795Z 
2025-04-11T03:52:12.7708913Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7709068Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7709142Z     
2025-04-11T03:52:12.7709220Z         Args:
2025-04-11T03:52:12.7709387Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7709559Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7709663Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7709738Z         """
2025-04-11T03:52:12.7709823Z         _lazy_init()
2025-04-11T03:52:12.7709919Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7710027Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7710136Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7710416Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7710557Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7710836Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7710841Z 
2025-04-11T03:52:12.7711085Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7711256Z ____________ test_flash_decoding[False-False-5-True-4-16-16-32-16] _____________
2025-04-11T03:52:12.7711260Z 
2025-04-11T03:52:12.7711416Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7711579Z kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7711675Z use_new_kcache_layout = False
2025-04-11T03:52:12.7711736Z 
2025-04-11T03:52:12.7711935Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7712041Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7712158Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7712294Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7712418Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7712588Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7712730Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7712835Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7712971Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7713126Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7713215Z     def test_flash_decoding(
2025-04-11T03:52:12.7713294Z         bsz: int,
2025-04-11T03:52:12.7713377Z         block_size: int,
2025-04-11T03:52:12.7713467Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7713550Z         num_attn_heads: int,
2025-04-11T03:52:12.7713632Z         kv_group_num: int,
2025-04-11T03:52:12.7713723Z         same_context_len: bool,
2025-04-11T03:52:12.7713798Z         q_len: int,
2025-04-11T03:52:12.7713890Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7713977Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7714051Z     ):
2025-04-11T03:52:12.7714166Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7714357Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7714542Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7714713Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7714881Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7715040Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7715112Z     
2025-04-11T03:52:12.7715204Z         torch.manual_seed(123)
2025-04-11T03:52:12.7715292Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7715389Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7715393Z 
2025-04-11T03:52:12.7715548Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7715667Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7715672Z 
2025-04-11T03:52:12.7715753Z device = None
2025-04-11T03:52:12.7715757Z 
2025-04-11T03:52:12.7715873Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7716026Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7716096Z     
2025-04-11T03:52:12.7716175Z         Args:
2025-04-11T03:52:12.7716338Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7716508Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7716611Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7716684Z         """
2025-04-11T03:52:12.7716878Z         _lazy_init()
2025-04-11T03:52:12.7716972Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7717078Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7717185Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7717470Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7717607Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7717760Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7717764Z 
2025-04-11T03:52:12.7718053Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7718225Z _____________ test_flash_decoding[False-False-5-False-1-16-8-16-7] _____________
2025-04-11T03:52:12.7718230Z 
2025-04-11T03:52:12.7718384Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7718552Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7718645Z use_new_kcache_layout = False
2025-04-11T03:52:12.7718703Z 
2025-04-11T03:52:12.7718902Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7719010Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7719128Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7719270Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7719392Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7719505Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7719645Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7719748Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7719890Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7720043Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7720133Z     def test_flash_decoding(
2025-04-11T03:52:12.7720216Z         bsz: int,
2025-04-11T03:52:12.7720300Z         block_size: int,
2025-04-11T03:52:12.7720392Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7720476Z         num_attn_heads: int,
2025-04-11T03:52:12.7720558Z         kv_group_num: int,
2025-04-11T03:52:12.7720647Z         same_context_len: bool,
2025-04-11T03:52:12.7720723Z         q_len: int,
2025-04-11T03:52:12.7720812Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7720899Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7720970Z     ):
2025-04-11T03:52:12.7721086Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7721276Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7721459Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7721633Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7721799Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7721955Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7722030Z     
2025-04-11T03:52:12.7722117Z         torch.manual_seed(123)
2025-04-11T03:52:12.7722204Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7722298Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7722302Z 
2025-04-11T03:52:12.7722455Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7722569Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7722573Z 
2025-04-11T03:52:12.7722649Z device = None
2025-04-11T03:52:12.7722653Z 
2025-04-11T03:52:12.7722774Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7722924Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7723133Z     
2025-04-11T03:52:12.7723214Z         Args:
2025-04-11T03:52:12.7723381Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7723552Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7723656Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7723729Z         """
2025-04-11T03:52:12.7723814Z         _lazy_init()
2025-04-11T03:52:12.7723910Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7724016Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7724175Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7724462Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7724597Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7724757Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7724765Z 
2025-04-11T03:52:12.7725047Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7725217Z ____________ test_flash_decoding[False-False-5-False-1-16-8-16-16] _____________
2025-04-11T03:52:12.7725221Z 
2025-04-11T03:52:12.7725373Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7725591Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7725692Z use_new_kcache_layout = False
2025-04-11T03:52:12.7725700Z 
2025-04-11T03:52:12.7725898Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7726005Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7726122Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7726258Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7726380Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7726495Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7726635Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7726737Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7726875Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7727025Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7727113Z     def test_flash_decoding(
2025-04-11T03:52:12.7727191Z         bsz: int,
2025-04-11T03:52:12.7727274Z         block_size: int,
2025-04-11T03:52:12.7727367Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7727448Z         num_attn_heads: int,
2025-04-11T03:52:12.7727530Z         kv_group_num: int,
2025-04-11T03:52:12.7727620Z         same_context_len: bool,
2025-04-11T03:52:12.7727695Z         q_len: int,
2025-04-11T03:52:12.7727787Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7727874Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7727952Z     ):
2025-04-11T03:52:12.7728063Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7728255Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7728440Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7728609Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7728773Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7728932Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7729008Z     
2025-04-11T03:52:12.7729096Z         torch.manual_seed(123)
2025-04-11T03:52:12.7729187Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7729397Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7729402Z 
2025-04-11T03:52:12.7729557Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7729675Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7729679Z 
2025-04-11T03:52:12.7729757Z device = None
2025-04-11T03:52:12.7729762Z 
2025-04-11T03:52:12.7729885Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7730038Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7730109Z     
2025-04-11T03:52:12.7730188Z         Args:
2025-04-11T03:52:12.7730354Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7730573Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7730679Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7730756Z         """
2025-04-11T03:52:12.7730839Z         _lazy_init()
2025-04-11T03:52:12.7730936Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7731043Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7731201Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7731485Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7731623Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7731779Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7731787Z 
2025-04-11T03:52:12.7732027Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7732193Z _____________ test_flash_decoding[False-False-5-False-1-16-8-32-7] _____________
2025-04-11T03:52:12.7732198Z 
2025-04-11T03:52:12.7732348Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7732516Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7732609Z use_new_kcache_layout = False
2025-04-11T03:52:12.7732616Z 
2025-04-11T03:52:12.7732812Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7732919Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7733036Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7733173Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7733294Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7733406Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7733551Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7733656Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7733794Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7733947Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7734040Z     def test_flash_decoding(
2025-04-11T03:52:12.7734119Z         bsz: int,
2025-04-11T03:52:12.7734203Z         block_size: int,
2025-04-11T03:52:12.7734295Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7734381Z         num_attn_heads: int,
2025-04-11T03:52:12.7734466Z         kv_group_num: int,
2025-04-11T03:52:12.7734557Z         same_context_len: bool,
2025-04-11T03:52:12.7734634Z         q_len: int,
2025-04-11T03:52:12.7734723Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7734811Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7734886Z     ):
2025-04-11T03:52:12.7734996Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7735186Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7735372Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7735654Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7735820Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7735979Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7736052Z     
2025-04-11T03:52:12.7736141Z         torch.manual_seed(123)
2025-04-11T03:52:12.7736231Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7736328Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7736332Z 
2025-04-11T03:52:12.7736485Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7736726Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7736731Z 
2025-04-11T03:52:12.7736811Z device = None
2025-04-11T03:52:12.7736815Z 
2025-04-11T03:52:12.7736938Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7737087Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7737160Z     
2025-04-11T03:52:12.7737240Z         Args:
2025-04-11T03:52:12.7737406Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7737636Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7737742Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7737817Z         """
2025-04-11T03:52:12.7737896Z         _lazy_init()
2025-04-11T03:52:12.7737993Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7738100Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7738207Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7738494Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7738631Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7738796Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7738800Z 
2025-04-11T03:52:12.7739037Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7739204Z ____________ test_flash_decoding[False-False-5-False-1-16-8-32-16] _____________
2025-04-11T03:52:12.7739212Z 
2025-04-11T03:52:12.7739359Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7739522Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7739618Z use_new_kcache_layout = False
2025-04-11T03:52:12.7739624Z 
2025-04-11T03:52:12.7739821Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7739928Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7740047Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7740185Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7740303Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7740417Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7740561Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7740666Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7740806Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7740956Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7741044Z     def test_flash_decoding(
2025-04-11T03:52:12.7741123Z         bsz: int,
2025-04-11T03:52:12.7741212Z         block_size: int,
2025-04-11T03:52:12.7741306Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7741389Z         num_attn_heads: int,
2025-04-11T03:52:12.7741478Z         kv_group_num: int,
2025-04-11T03:52:12.7741564Z         same_context_len: bool,
2025-04-11T03:52:12.7741640Z         q_len: int,
2025-04-11T03:52:12.7741840Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7741927Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7742003Z     ):
2025-04-11T03:52:12.7742119Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7742314Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7742504Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7742678Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7742846Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7743054Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7743128Z     
2025-04-11T03:52:12.7743215Z         torch.manual_seed(123)
2025-04-11T03:52:12.7743303Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7743406Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7743411Z 
2025-04-11T03:52:12.7743568Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7743743Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7743747Z 
2025-04-11T03:52:12.7743827Z device = None
2025-04-11T03:52:12.7743832Z 
2025-04-11T03:52:12.7743954Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7744107Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7744180Z     
2025-04-11T03:52:12.7744261Z         Args:
2025-04-11T03:52:12.7744428Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7744598Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7744703Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7744778Z         """
2025-04-11T03:52:12.7744865Z         _lazy_init()
2025-04-11T03:52:12.7744961Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7745069Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7745181Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7745466Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7745600Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7745764Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7745769Z 
2025-04-11T03:52:12.7746005Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7746175Z ____________ test_flash_decoding[False-False-5-False-1-16-16-16-7] _____________
2025-04-11T03:52:12.7746184Z 
2025-04-11T03:52:12.7746333Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7746503Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7746598Z use_new_kcache_layout = False
2025-04-11T03:52:12.7746604Z 
2025-04-11T03:52:12.7746801Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7746909Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7747028Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7747172Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7747288Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7747403Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7747546Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7747652Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7747788Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7747936Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7748152Z     def test_flash_decoding(
2025-04-11T03:52:12.7748230Z         bsz: int,
2025-04-11T03:52:12.7748314Z         block_size: int,
2025-04-11T03:52:12.7748407Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7748534Z         num_attn_heads: int,
2025-04-11T03:52:12.7748622Z         kv_group_num: int,
2025-04-11T03:52:12.7748709Z         same_context_len: bool,
2025-04-11T03:52:12.7748785Z         q_len: int,
2025-04-11T03:52:12.7748875Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7748964Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7749039Z     ):
2025-04-11T03:52:12.7749208Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7749401Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7749585Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7749759Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7749925Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7750143Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7750219Z     
2025-04-11T03:52:12.7750305Z         torch.manual_seed(123)
2025-04-11T03:52:12.7750395Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7750493Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7750497Z 
2025-04-11T03:52:12.7750650Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7750768Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7750773Z 
2025-04-11T03:52:12.7750850Z device = None
2025-04-11T03:52:12.7750854Z 
2025-04-11T03:52:12.7750973Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7751121Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7751197Z     
2025-04-11T03:52:12.7751271Z         Args:
2025-04-11T03:52:12.7751436Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7751608Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7751712Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7751790Z         """
2025-04-11T03:52:12.7751869Z         _lazy_init()
2025-04-11T03:52:12.7751963Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7752070Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7752178Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7752464Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7752599Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7752765Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7752769Z 
2025-04-11T03:52:12.7753007Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7753185Z ____________ test_flash_decoding[False-False-5-False-1-16-16-16-16] ____________
2025-04-11T03:52:12.7753189Z 
2025-04-11T03:52:12.7753341Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7753506Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7753599Z use_new_kcache_layout = False
2025-04-11T03:52:12.7753605Z 
2025-04-11T03:52:12.7753802Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7753910Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7754027Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7754169Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7754403Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7754518Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7754660Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7754764Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7754906Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7755055Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7755146Z     def test_flash_decoding(
2025-04-11T03:52:12.7755223Z         bsz: int,
2025-04-11T03:52:12.7755358Z         block_size: int,
2025-04-11T03:52:12.7755453Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7755537Z         num_attn_heads: int,
2025-04-11T03:52:12.7755623Z         kv_group_num: int,
2025-04-11T03:52:12.7755710Z         same_context_len: bool,
2025-04-11T03:52:12.7755786Z         q_len: int,
2025-04-11T03:52:12.7755883Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7755970Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7756046Z     ):
2025-04-11T03:52:12.7756228Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7756425Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7756607Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7756777Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7756943Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7757101Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7757176Z     
2025-04-11T03:52:12.7757261Z         torch.manual_seed(123)
2025-04-11T03:52:12.7757351Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7757451Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7757455Z 
2025-04-11T03:52:12.7757612Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7757736Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7757740Z 
2025-04-11T03:52:12.7757819Z device = None
2025-04-11T03:52:12.7757823Z 
2025-04-11T03:52:12.7757942Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7758092Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7758166Z     
2025-04-11T03:52:12.7758239Z         Args:
2025-04-11T03:52:12.7758404Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7758574Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7758678Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7758755Z         """
2025-04-11T03:52:12.7758836Z         _lazy_init()
2025-04-11T03:52:12.7758932Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7759045Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7759152Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7759438Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7759575Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7759738Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7759742Z 
2025-04-11T03:52:12.7759980Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7760154Z ____________ test_flash_decoding[False-False-5-False-1-16-16-32-7] _____________
2025-04-11T03:52:12.7760158Z 
2025-04-11T03:52:12.7760307Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7760584Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7760677Z use_new_kcache_layout = False
2025-04-11T03:52:12.7760683Z 
2025-04-11T03:52:12.7760882Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7760991Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7761109Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7761249Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7761365Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7761478Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7761672Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7761778Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7761922Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7762078Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7762178Z     def test_flash_decoding(
2025-04-11T03:52:12.7762258Z         bsz: int,
2025-04-11T03:52:12.7762403Z         block_size: int,
2025-04-11T03:52:12.7762499Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7762582Z         num_attn_heads: int,
2025-04-11T03:52:12.7762670Z         kv_group_num: int,
2025-04-11T03:52:12.7762758Z         same_context_len: bool,
2025-04-11T03:52:12.7762836Z         q_len: int,
2025-04-11T03:52:12.7762927Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7763015Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7763091Z     ):
2025-04-11T03:52:12.7763201Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7763396Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7763575Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7763748Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7763917Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7764074Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7764150Z     
2025-04-11T03:52:12.7764236Z         torch.manual_seed(123)
2025-04-11T03:52:12.7764332Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7764422Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7764426Z 
2025-04-11T03:52:12.7764579Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7764698Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7764703Z 
2025-04-11T03:52:12.7764778Z device = None
2025-04-11T03:52:12.7764783Z 
2025-04-11T03:52:12.7764905Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7765052Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7765134Z     
2025-04-11T03:52:12.7765208Z         Args:
2025-04-11T03:52:12.7765372Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7765540Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7765646Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7765723Z         """
2025-04-11T03:52:12.7765802Z         _lazy_init()
2025-04-11T03:52:12.7765900Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7766004Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7766111Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7766395Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7766530Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7766802Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7766806Z 
2025-04-11T03:52:12.7767042Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7767216Z ____________ test_flash_decoding[False-False-5-False-1-16-16-32-16] ____________
2025-04-11T03:52:12.7767221Z 
2025-04-11T03:52:12.7767372Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7767538Z kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7767632Z use_new_kcache_layout = False
2025-04-11T03:52:12.7767687Z 
2025-04-11T03:52:12.7767889Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7768000Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7768118Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7768260Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7768381Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7768503Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7768706Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7768812Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7768953Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7769101Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7769191Z     def test_flash_decoding(
2025-04-11T03:52:12.7769267Z         bsz: int,
2025-04-11T03:52:12.7769352Z         block_size: int,
2025-04-11T03:52:12.7769444Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7769528Z         num_attn_heads: int,
2025-04-11T03:52:12.7769615Z         kv_group_num: int,
2025-04-11T03:52:12.7769702Z         same_context_len: bool,
2025-04-11T03:52:12.7769780Z         q_len: int,
2025-04-11T03:52:12.7769871Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7769958Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7770034Z     ):
2025-04-11T03:52:12.7770144Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7770338Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7770519Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7770691Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7770859Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7771018Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7771095Z     
2025-04-11T03:52:12.7771185Z         torch.manual_seed(123)
2025-04-11T03:52:12.7771277Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7771373Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7771377Z 
2025-04-11T03:52:12.7771531Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7771652Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7771657Z 
2025-04-11T03:52:12.7771734Z device = None
2025-04-11T03:52:12.7771738Z 
2025-04-11T03:52:12.7771859Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7772007Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7772085Z     
2025-04-11T03:52:12.7772159Z         Args:
2025-04-11T03:52:12.7772322Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7772489Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7772594Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7772671Z         """
2025-04-11T03:52:12.7772863Z         _lazy_init()
2025-04-11T03:52:12.7772962Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7773066Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7773174Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7773462Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7773596Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7773758Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7773762Z 
2025-04-11T03:52:12.7773999Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7774233Z _____________ test_flash_decoding[False-False-5-False-4-16-8-16-7] _____________
2025-04-11T03:52:12.7774237Z 
2025-04-11T03:52:12.7774386Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7774556Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7774648Z use_new_kcache_layout = False
2025-04-11T03:52:12.7774698Z 
2025-04-11T03:52:12.7774898Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7775004Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7775123Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7775270Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7775388Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7775507Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7775644Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7775748Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7775889Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7776039Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7776135Z     def test_flash_decoding(
2025-04-11T03:52:12.7776211Z         bsz: int,
2025-04-11T03:52:12.7776297Z         block_size: int,
2025-04-11T03:52:12.7776391Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7776477Z         num_attn_heads: int,
2025-04-11T03:52:12.7776566Z         kv_group_num: int,
2025-04-11T03:52:12.7776650Z         same_context_len: bool,
2025-04-11T03:52:12.7776730Z         q_len: int,
2025-04-11T03:52:12.7776818Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7776907Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7776982Z     ):
2025-04-11T03:52:12.7777096Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7777293Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7777478Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7777652Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7777820Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7777979Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7778055Z     
2025-04-11T03:52:12.7778142Z         torch.manual_seed(123)
2025-04-11T03:52:12.7778234Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7778325Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7778329Z 
2025-04-11T03:52:12.7778483Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7778601Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7778606Z 
2025-04-11T03:52:12.7778683Z device = None
2025-04-11T03:52:12.7778687Z 
2025-04-11T03:52:12.7778810Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7778960Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7779142Z     
2025-04-11T03:52:12.7779217Z         Args:
2025-04-11T03:52:12.7779385Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7779560Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7779665Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7779744Z         """
2025-04-11T03:52:12.7779823Z         _lazy_init()
2025-04-11T03:52:12.7779922Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7780026Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7780181Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7780473Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7780608Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7780771Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7780775Z 
2025-04-11T03:52:12.7781013Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7781242Z ____________ test_flash_decoding[False-False-5-False-4-16-8-16-16] _____________
2025-04-11T03:52:12.7781246Z 
2025-04-11T03:52:12.7781399Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7781572Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7781663Z use_new_kcache_layout = False
2025-04-11T03:52:12.7781669Z 
2025-04-11T03:52:12.7781871Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7781981Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7782102Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7782249Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7782374Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7782494Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7782637Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7782743Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7782889Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7783042Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7783139Z     def test_flash_decoding(
2025-04-11T03:52:12.7783219Z         bsz: int,
2025-04-11T03:52:12.7783310Z         block_size: int,
2025-04-11T03:52:12.7783403Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7783488Z         num_attn_heads: int,
2025-04-11T03:52:12.7783580Z         kv_group_num: int,
2025-04-11T03:52:12.7783671Z         same_context_len: bool,
2025-04-11T03:52:12.7783753Z         q_len: int,
2025-04-11T03:52:12.7783848Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7783939Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7784019Z     ):
2025-04-11T03:52:12.7784136Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7784336Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7784520Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7784697Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7784864Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7785027Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7785105Z     
2025-04-11T03:52:12.7785195Z         torch.manual_seed(123)
2025-04-11T03:52:12.7785290Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7785386Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7785503Z 
2025-04-11T03:52:12.7785664Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7785781Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7785786Z 
2025-04-11T03:52:12.7785865Z device = None
2025-04-11T03:52:12.7785870Z 
2025-04-11T03:52:12.7785990Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7786141Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7786214Z     
2025-04-11T03:52:12.7786291Z         Args:
2025-04-11T03:52:12.7786460Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7786675Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7786781Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7786860Z         """
2025-04-11T03:52:12.7786940Z         _lazy_init()
2025-04-11T03:52:12.7787043Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7787146Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7787300Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7787588Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7787722Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7787883Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7787888Z 
2025-04-11T03:52:12.7788124Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7788298Z _____________ test_flash_decoding[False-False-5-False-4-16-8-32-7] _____________
2025-04-11T03:52:12.7788302Z 
2025-04-11T03:52:12.7788489Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7788662Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7788750Z use_new_kcache_layout = False
2025-04-11T03:52:12.7788756Z 
2025-04-11T03:52:12.7788951Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7789064Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7789183Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7789327Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7789443Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7789557Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7789696Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7789804Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7789942Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7790090Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7790186Z     def test_flash_decoding(
2025-04-11T03:52:12.7790261Z         bsz: int,
2025-04-11T03:52:12.7790347Z         block_size: int,
2025-04-11T03:52:12.7790436Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7790520Z         num_attn_heads: int,
2025-04-11T03:52:12.7790609Z         kv_group_num: int,
2025-04-11T03:52:12.7790695Z         same_context_len: bool,
2025-04-11T03:52:12.7790774Z         q_len: int,
2025-04-11T03:52:12.7790862Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7790949Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7791024Z     ):
2025-04-11T03:52:12.7791135Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7791329Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7791507Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7791806Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7791969Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7792130Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7792207Z     
2025-04-11T03:52:12.7792295Z         torch.manual_seed(123)
2025-04-11T03:52:12.7792391Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7792484Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7792488Z 
2025-04-11T03:52:12.7792645Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7792830Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7792835Z 
2025-04-11T03:52:12.7792913Z device = None
2025-04-11T03:52:12.7792917Z 
2025-04-11T03:52:12.7793038Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7793193Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7793280Z     
2025-04-11T03:52:12.7793357Z         Args:
2025-04-11T03:52:12.7793531Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7793761Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7793869Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7793948Z         """
2025-04-11T03:52:12.7794027Z         _lazy_init()
2025-04-11T03:52:12.7794126Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7794228Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7794334Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7794620Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7794753Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7794921Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7794925Z 
2025-04-11T03:52:12.7795162Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7795338Z ____________ test_flash_decoding[False-False-5-False-4-16-8-32-16] _____________
2025-04-11T03:52:12.7795341Z 
2025-04-11T03:52:12.7795491Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
2025-04-11T03:52:12.7795658Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7795746Z use_new_kcache_layout = False
2025-04-11T03:52:12.7795752Z 
2025-04-11T03:52:12.7795952Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7796056Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7796173Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7796316Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7796436Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7796555Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7796693Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7796801Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7796936Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7797083Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7797177Z     def test_flash_decoding(
2025-04-11T03:52:12.7797251Z         bsz: int,
2025-04-11T03:52:12.7797338Z         block_size: int,
2025-04-11T03:52:12.7797428Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7797509Z         num_attn_heads: int,
2025-04-11T03:52:12.7797596Z         kv_group_num: int,
2025-04-11T03:52:12.7797684Z         same_context_len: bool,
2025-04-11T03:52:12.7797761Z         q_len: int,
2025-04-11T03:52:12.7797958Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7798047Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7798122Z     ):
2025-04-11T03:52:12.7798236Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7798434Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7798615Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7798788Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7798951Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7799155Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7799230Z     
2025-04-11T03:52:12.7799316Z         torch.manual_seed(123)
2025-04-11T03:52:12.7799410Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7799503Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7799510Z 
2025-04-11T03:52:12.7799669Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7799839Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7799844Z 
2025-04-11T03:52:12.7799923Z device = None
2025-04-11T03:52:12.7799932Z 
2025-04-11T03:52:12.7800049Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7800203Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7800279Z     
2025-04-11T03:52:12.7800354Z         Args:
2025-04-11T03:52:12.7800524Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7800693Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7800798Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7800877Z         """
2025-04-11T03:52:12.7800955Z         _lazy_init()
2025-04-11T03:52:12.7801059Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7801162Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7801272Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7801553Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7801688Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7801848Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7801852Z 
2025-04-11T03:52:12.7802088Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7802264Z ____________ test_flash_decoding[False-False-5-False-4-16-16-16-7] _____________
2025-04-11T03:52:12.7802268Z 
2025-04-11T03:52:12.7802417Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7802588Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7802677Z use_new_kcache_layout = False
2025-04-11T03:52:12.7802683Z 
2025-04-11T03:52:12.7802885Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7802988Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7803105Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7803253Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7803371Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7803487Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7803626Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7803738Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7803872Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7804022Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7804230Z     def test_flash_decoding(
2025-04-11T03:52:12.7804308Z         bsz: int,
2025-04-11T03:52:12.7804395Z         block_size: int,
2025-04-11T03:52:12.7804484Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7804567Z         num_attn_heads: int,
2025-04-11T03:52:12.7804654Z         kv_group_num: int,
2025-04-11T03:52:12.7804737Z         same_context_len: bool,
2025-04-11T03:52:12.7804815Z         q_len: int,
2025-04-11T03:52:12.7804901Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7804994Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7805066Z     ):
2025-04-11T03:52:12.7805229Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7805431Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7805613Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7805786Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7805954Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7806172Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7806246Z     
2025-04-11T03:52:12.7806332Z         torch.manual_seed(123)
2025-04-11T03:52:12.7806427Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7806517Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7806521Z 
2025-04-11T03:52:12.7806678Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7806792Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7806796Z 
2025-04-11T03:52:12.7806878Z device = None
2025-04-11T03:52:12.7806882Z 
2025-04-11T03:52:12.7806999Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7807151Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7807227Z     
2025-04-11T03:52:12.7807302Z         Args:
2025-04-11T03:52:12.7807471Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7807637Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7807746Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7807819Z         """
2025-04-11T03:52:12.7807898Z         _lazy_init()
2025-04-11T03:52:12.7807999Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7808102Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7808212Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7808494Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7808629Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7808793Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7808797Z 
2025-04-11T03:52:12.7809040Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7809214Z ____________ test_flash_decoding[False-False-5-False-4-16-16-16-16] ____________
2025-04-11T03:52:12.7809218Z 
2025-04-11T03:52:12.7809371Z bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7809540Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7809631Z use_new_kcache_layout = False
2025-04-11T03:52:12.7809637Z 
2025-04-11T03:52:12.7809838Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7809941Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7810059Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7810201Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7810432Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7810553Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7810692Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7810801Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7810939Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7811090Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7811181Z     def test_flash_decoding(
2025-04-11T03:52:12.7811256Z         bsz: int,
2025-04-11T03:52:12.7811394Z         block_size: int,
2025-04-11T03:52:12.7811483Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7811568Z         num_attn_heads: int,
2025-04-11T03:52:12.7811655Z         kv_group_num: int,
2025-04-11T03:52:12.7811741Z         same_context_len: bool,
2025-04-11T03:52:12.7811823Z         q_len: int,
2025-04-11T03:52:12.7811911Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7812003Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7812075Z     ):
2025-04-11T03:52:12.7812248Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7812449Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7812633Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7812808Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7812973Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7813136Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7813208Z     
2025-04-11T03:52:12.7813294Z         torch.manual_seed(123)
2025-04-11T03:52:12.7813388Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7813479Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7813486Z 
2025-04-11T03:52:12.7813645Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7813761Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7813765Z 
2025-04-11T03:52:12.7813846Z device = None
2025-04-11T03:52:12.7813850Z 
2025-04-11T03:52:12.7813967Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7814117Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7814195Z     
2025-04-11T03:52:12.7814270Z         Args:
2025-04-11T03:52:12.7814441Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7814607Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7814716Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7814789Z         """
2025-04-11T03:52:12.7814869Z         _lazy_init()
2025-04-11T03:52:12.7814972Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7815073Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7815184Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7815466Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7815602Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7815760Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7815765Z 
2025-04-11T03:52:12.7816004Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7816181Z ____________ test_flash_decoding[False-False-5-False-4-16-16-32-7] _____________
2025-04-11T03:52:12.7816185Z 
2025-04-11T03:52:12.7816336Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7816626Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7816717Z use_new_kcache_layout = False
2025-04-11T03:52:12.7816724Z 
2025-04-11T03:52:12.7816927Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7817032Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7817151Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7817294Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7817410Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7817527Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7817732Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7817842Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7817980Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7818129Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7818227Z     def test_flash_decoding(
2025-04-11T03:52:12.7818302Z         bsz: int,
2025-04-11T03:52:12.7818449Z         block_size: int,
2025-04-11T03:52:12.7818542Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7818631Z         num_attn_heads: int,
2025-04-11T03:52:12.7818714Z         kv_group_num: int,
2025-04-11T03:52:12.7818799Z         same_context_len: bool,
2025-04-11T03:52:12.7818878Z         q_len: int,
2025-04-11T03:52:12.7818965Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7819056Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7819127Z     ):
2025-04-11T03:52:12.7819239Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7819442Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7819624Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7819799Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7819962Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7820122Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7820193Z     
2025-04-11T03:52:12.7820279Z         torch.manual_seed(123)
2025-04-11T03:52:12.7820374Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7820468Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7820473Z 
2025-04-11T03:52:12.7820631Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7820740Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7820747Z 
2025-04-11T03:52:12.7820826Z device = None
2025-04-11T03:52:12.7820830Z 
2025-04-11T03:52:12.7820950Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7821100Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7821180Z     
2025-04-11T03:52:12.7821256Z         Args:
2025-04-11T03:52:12.7821427Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7821593Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7821704Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7821776Z         """
2025-04-11T03:52:12.7821856Z         _lazy_init()
2025-04-11T03:52:12.7821957Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7822060Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7822170Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7822449Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7822588Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7822861Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7822866Z 
2025-04-11T03:52:12.7823104Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7823279Z ____________ test_flash_decoding[False-False-5-False-4-16-16-32-16] ____________
2025-04-11T03:52:12.7823284Z 
2025-04-11T03:52:12.7823437Z bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
2025-04-11T03:52:12.7823605Z kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
2025-04-11T03:52:12.7823692Z use_new_kcache_layout = False
2025-04-11T03:52:12.7823697Z 
2025-04-11T03:52:12.7823962Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7824077Z     @pytest.mark.parametrize("bsz", [7, 16])
2025-04-11T03:52:12.7824206Z     @pytest.mark.parametrize("block_size", [16, 32])
2025-04-11T03:52:12.7824348Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
2025-04-11T03:52:12.7824471Z     @pytest.mark.parametrize("num_attn_heads", [16])
2025-04-11T03:52:12.7824597Z     @pytest.mark.parametrize("kv_group_num", [1, 4])
2025-04-11T03:52:12.7824786Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7824896Z     @pytest.mark.parametrize("q_len", [1, 5])
2025-04-11T03:52:12.7825034Z     @pytest.mark.parametrize("use_alibi_slopes", [True, False])
2025-04-11T03:52:12.7825185Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7825280Z     def test_flash_decoding(
2025-04-11T03:52:12.7825359Z         bsz: int,
2025-04-11T03:52:12.7825451Z         block_size: int,
2025-04-11T03:52:12.7825546Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7825636Z         num_attn_heads: int,
2025-04-11T03:52:12.7825720Z         kv_group_num: int,
2025-04-11T03:52:12.7825808Z         same_context_len: bool,
2025-04-11T03:52:12.7825891Z         q_len: int,
2025-04-11T03:52:12.7825983Z         use_alibi_slopes: bool,
2025-04-11T03:52:12.7826084Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7826179Z     ):
2025-04-11T03:52:12.7826303Z         if use_new_kcache_layout and use_alibi_slopes:
2025-04-11T03:52:12.7826505Z             # TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
2025-04-11T03:52:12.7826687Z             # the code (alibi kernel) will be refactored later to avoid code duplication, when
2025-04-11T03:52:12.7826868Z             # the whole triton flow with new k cache layout has been supported and tested.
2025-04-11T03:52:12.7827029Z             # And tests for the alibi kernel using new kcache layout will be added then.
2025-04-11T03:52:12.7827190Z             pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T03:52:12.7827261Z     
2025-04-11T03:52:12.7827351Z         torch.manual_seed(123)
2025-04-11T03:52:12.7827447Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7827541Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7827550Z 
2025-04-11T03:52:12.7827711Z tests/test_infer/test_kernels/triton/test_decoding_attn.py:100: 
2025-04-11T03:52:12.7827830Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7827834Z 
2025-04-11T03:52:12.7827913Z device = None
2025-04-11T03:52:12.7827918Z 
2025-04-11T03:52:12.7828038Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7828190Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7828264Z     
2025-04-11T03:52:12.7828339Z         Args:
2025-04-11T03:52:12.7828554Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7828723Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7828833Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7828907Z         """
2025-04-11T03:52:12.7828987Z         _lazy_init()
2025-04-11T03:52:12.7829222Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7829331Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7829444Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7829728Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7829867Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7830024Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7830029Z 
2025-04-11T03:52:12.7830267Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7830504Z ________________ test_copy_kv_to_caches[True-1-True-16-16-16-7] ________________
2025-04-11T03:52:12.7830508Z 
2025-04-11T03:52:12.7830656Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7830819Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7830823Z 
2025-04-11T03:52:12.7831025Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7831196Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7831323Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7831461Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7831576Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7831714Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7831827Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7831984Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7832077Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7832155Z         bsz: int,
2025-04-11T03:52:12.7832242Z         block_size: int,
2025-04-11T03:52:12.7832331Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7832415Z         num_kv_heads: int,
2025-04-11T03:52:12.7832505Z         same_context_len: bool,
2025-04-11T03:52:12.7832583Z         n_tokens: int,
2025-04-11T03:52:12.7832678Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7832752Z     ):
2025-04-11T03:52:12.7832838Z         torch.manual_seed(123)
2025-04-11T03:52:12.7832931Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7833022Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7833026Z 
2025-04-11T03:52:12.7833185Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7833301Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7833307Z 
2025-04-11T03:52:12.7833387Z device = None
2025-04-11T03:52:12.7833392Z 
2025-04-11T03:52:12.7833508Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7833661Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7833738Z     
2025-04-11T03:52:12.7833812Z         Args:
2025-04-11T03:52:12.7833979Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7834147Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7834256Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7834329Z         """
2025-04-11T03:52:12.7834408Z         _lazy_init()
2025-04-11T03:52:12.7834509Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7834613Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7834724Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7835011Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7835146Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7835306Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7835493Z 
2025-04-11T03:52:12.7835736Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7835898Z _______________ test_copy_kv_to_caches[True-1-True-16-16-16-32] ________________
2025-04-11T03:52:12.7835903Z 
2025-04-11T03:52:12.7836052Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7836209Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7836213Z 
2025-04-11T03:52:12.7836410Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7836577Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7836707Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7836851Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7836967Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7837106Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7837220Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7837429Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7837524Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7837600Z         bsz: int,
2025-04-11T03:52:12.7837683Z         block_size: int,
2025-04-11T03:52:12.7837780Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7837864Z         num_kv_heads: int,
2025-04-11T03:52:12.7837954Z         same_context_len: bool,
2025-04-11T03:52:12.7838031Z         n_tokens: int,
2025-04-11T03:52:12.7838119Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7838197Z     ):
2025-04-11T03:52:12.7838287Z         torch.manual_seed(123)
2025-04-11T03:52:12.7838385Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7838476Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7838480Z 
2025-04-11T03:52:12.7838634Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7838749Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7838755Z 
2025-04-11T03:52:12.7838830Z device = None
2025-04-11T03:52:12.7838838Z 
2025-04-11T03:52:12.7838958Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7839109Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7839186Z     
2025-04-11T03:52:12.7839260Z         Args:
2025-04-11T03:52:12.7839430Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7839595Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7839707Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7839785Z         """
2025-04-11T03:52:12.7839864Z         _lazy_init()
2025-04-11T03:52:12.7839964Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7840064Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7840178Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7840459Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7840594Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7840752Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7840756Z 
2025-04-11T03:52:12.7840994Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7841151Z ________________ test_copy_kv_to_caches[True-1-True-16-16-32-7] ________________
2025-04-11T03:52:12.7841157Z 
2025-04-11T03:52:12.7841305Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7841458Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7841462Z 
2025-04-11T03:52:12.7841789Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7841901Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7842028Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7842164Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7842282Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7842420Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7842532Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7842680Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7842828Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7842904Z         bsz: int,
2025-04-11T03:52:12.7842987Z         block_size: int,
2025-04-11T03:52:12.7843084Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7843167Z         num_kv_heads: int,
2025-04-11T03:52:12.7843260Z         same_context_len: bool,
2025-04-11T03:52:12.7843338Z         n_tokens: int,
2025-04-11T03:52:12.7843426Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7843560Z     ):
2025-04-11T03:52:12.7843649Z         torch.manual_seed(123)
2025-04-11T03:52:12.7843740Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7843830Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7843833Z 
2025-04-11T03:52:12.7843988Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7844102Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7844106Z 
2025-04-11T03:52:12.7844181Z device = None
2025-04-11T03:52:12.7844187Z 
2025-04-11T03:52:12.7844311Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7844460Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7844537Z     
2025-04-11T03:52:12.7844611Z         Args:
2025-04-11T03:52:12.7844781Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7844952Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7845061Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7845139Z         """
2025-04-11T03:52:12.7845216Z         _lazy_init()
2025-04-11T03:52:12.7845316Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7845418Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7845523Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7845810Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7845947Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7846105Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7846110Z 
2025-04-11T03:52:12.7846347Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7846507Z _______________ test_copy_kv_to_caches[True-1-True-16-16-32-32] ________________
2025-04-11T03:52:12.7846513Z 
2025-04-11T03:52:12.7846663Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7846819Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7846823Z 
2025-04-11T03:52:12.7847019Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7847126Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7847249Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7847384Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7847501Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7847639Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7847752Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7848023Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7848113Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7848196Z         bsz: int,
2025-04-11T03:52:12.7848276Z         block_size: int,
2025-04-11T03:52:12.7848374Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7848456Z         num_kv_heads: int,
2025-04-11T03:52:12.7848546Z         same_context_len: bool,
2025-04-11T03:52:12.7848626Z         n_tokens: int,
2025-04-11T03:52:12.7848715Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7848793Z     ):
2025-04-11T03:52:12.7848878Z         torch.manual_seed(123)
2025-04-11T03:52:12.7849023Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7849114Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7849118Z 
2025-04-11T03:52:12.7849268Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7849386Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7849393Z 
2025-04-11T03:52:12.7849470Z device = None
2025-04-11T03:52:12.7849474Z 
2025-04-11T03:52:12.7849595Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7849793Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7849869Z     
2025-04-11T03:52:12.7849944Z         Args:
2025-04-11T03:52:12.7850110Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7850280Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7850387Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7850467Z         """
2025-04-11T03:52:12.7850545Z         _lazy_init()
2025-04-11T03:52:12.7850644Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7850748Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7850854Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7851145Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7851283Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7851446Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7851450Z 
2025-04-11T03:52:12.7851685Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7851844Z ________________ test_copy_kv_to_caches[True-1-True-16-16-64-7] ________________
2025-04-11T03:52:12.7851848Z 
2025-04-11T03:52:12.7851997Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7852154Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7852158Z 
2025-04-11T03:52:12.7852355Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7852463Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7852590Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7852724Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7852844Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7852980Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7853092Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7853243Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7853333Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7853416Z         bsz: int,
2025-04-11T03:52:12.7853499Z         block_size: int,
2025-04-11T03:52:12.7853596Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7853679Z         num_kv_heads: int,
2025-04-11T03:52:12.7853763Z         same_context_len: bool,
2025-04-11T03:52:12.7853847Z         n_tokens: int,
2025-04-11T03:52:12.7854043Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7854123Z     ):
2025-04-11T03:52:12.7854208Z         torch.manual_seed(123)
2025-04-11T03:52:12.7854301Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7854395Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7854398Z 
2025-04-11T03:52:12.7854549Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7854666Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7854670Z 
2025-04-11T03:52:12.7854745Z device = None
2025-04-11T03:52:12.7854749Z 
2025-04-11T03:52:12.7854872Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7855074Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7855152Z     
2025-04-11T03:52:12.7855228Z         Args:
2025-04-11T03:52:12.7855399Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7855573Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7855687Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7855807Z         """
2025-04-11T03:52:12.7855888Z         _lazy_init()
2025-04-11T03:52:12.7855983Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7856091Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7856200Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7856490Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7856625Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7856789Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7856794Z 
2025-04-11T03:52:12.7857035Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7857197Z _______________ test_copy_kv_to_caches[True-1-True-16-16-64-32] ________________
2025-04-11T03:52:12.7857201Z 
2025-04-11T03:52:12.7857353Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7857506Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7857516Z 
2025-04-11T03:52:12.7857714Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7857817Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7857946Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7858080Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7858202Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7858341Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7858450Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7858606Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7858702Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7858785Z         bsz: int,
2025-04-11T03:52:12.7858870Z         block_size: int,
2025-04-11T03:52:12.7858964Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7859050Z         num_kv_heads: int,
2025-04-11T03:52:12.7859138Z         same_context_len: bool,
2025-04-11T03:52:12.7859223Z         n_tokens: int,
2025-04-11T03:52:12.7859315Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7859391Z     ):
2025-04-11T03:52:12.7859478Z         torch.manual_seed(123)
2025-04-11T03:52:12.7859568Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7859668Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7859673Z 
2025-04-11T03:52:12.7859824Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7859944Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7859948Z 
2025-04-11T03:52:12.7860026Z device = None
2025-04-11T03:52:12.7860133Z 
2025-04-11T03:52:12.7860259Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7860411Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7860487Z     
2025-04-11T03:52:12.7860561Z         Args:
2025-04-11T03:52:12.7860734Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7860905Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7861013Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7861091Z         """
2025-04-11T03:52:12.7861230Z         _lazy_init()
2025-04-11T03:52:12.7861323Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7861431Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7861535Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7861819Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7861956Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7862189Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7862193Z 
2025-04-11T03:52:12.7862430Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7862584Z _______________ test_copy_kv_to_caches[True-1-False-16-16-16-7] ________________
2025-04-11T03:52:12.7862594Z 
2025-04-11T03:52:12.7862742Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7862898Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7862902Z 
2025-04-11T03:52:12.7863106Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7863211Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7863343Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7863475Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7863593Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7863732Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7863840Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7863995Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7864085Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7864168Z         bsz: int,
2025-04-11T03:52:12.7864250Z         block_size: int,
2025-04-11T03:52:12.7864344Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7864434Z         num_kv_heads: int,
2025-04-11T03:52:12.7864519Z         same_context_len: bool,
2025-04-11T03:52:12.7864601Z         n_tokens: int,
2025-04-11T03:52:12.7864690Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7864768Z     ):
2025-04-11T03:52:12.7864860Z         torch.manual_seed(123)
2025-04-11T03:52:12.7864948Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7865043Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7865050Z 
2025-04-11T03:52:12.7865201Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7865317Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7865321Z 
2025-04-11T03:52:12.7865397Z device = None
2025-04-11T03:52:12.7865402Z 
2025-04-11T03:52:12.7865524Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7865675Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7865749Z     
2025-04-11T03:52:12.7865827Z         Args:
2025-04-11T03:52:12.7865994Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7866163Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7866268Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7866459Z         """
2025-04-11T03:52:12.7866537Z         _lazy_init()
2025-04-11T03:52:12.7866637Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7866744Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7866851Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7867135Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7867269Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7867429Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7867489Z 
2025-04-11T03:52:12.7867728Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7867881Z _______________ test_copy_kv_to_caches[True-1-False-16-16-16-32] _______________
2025-04-11T03:52:12.7867888Z 
2025-04-11T03:52:12.7868041Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7868196Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7868257Z 
2025-04-11T03:52:12.7868496Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7868601Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7868727Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7868860Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7868977Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7869116Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7869224Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7869379Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7869469Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7869555Z         bsz: int,
2025-04-11T03:52:12.7869637Z         block_size: int,
2025-04-11T03:52:12.7869727Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7869818Z         num_kv_heads: int,
2025-04-11T03:52:12.7869903Z         same_context_len: bool,
2025-04-11T03:52:12.7869987Z         n_tokens: int,
2025-04-11T03:52:12.7870077Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7870149Z     ):
2025-04-11T03:52:12.7870243Z         torch.manual_seed(123)
2025-04-11T03:52:12.7870331Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7870427Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7870431Z 
2025-04-11T03:52:12.7870579Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7870697Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7870702Z 
2025-04-11T03:52:12.7870777Z device = None
2025-04-11T03:52:12.7870781Z 
2025-04-11T03:52:12.7870903Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7871055Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7871126Z     
2025-04-11T03:52:12.7871206Z         Args:
2025-04-11T03:52:12.7871371Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7871539Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7871644Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7871719Z         """
2025-04-11T03:52:12.7871802Z         _lazy_init()
2025-04-11T03:52:12.7871897Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7872005Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7872109Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7872394Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7872526Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7872808Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7872819Z 
2025-04-11T03:52:12.7873059Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7873211Z _______________ test_copy_kv_to_caches[True-1-False-16-16-32-7] ________________
2025-04-11T03:52:12.7873215Z 
2025-04-11T03:52:12.7873366Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7873518Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7873576Z 
2025-04-11T03:52:12.7873778Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7873884Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7874012Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7874149Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7874267Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7874409Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7874580Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7874733Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7874826Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7874907Z         bsz: int,
2025-04-11T03:52:12.7874989Z         block_size: int,
2025-04-11T03:52:12.7875078Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7875168Z         num_kv_heads: int,
2025-04-11T03:52:12.7875253Z         same_context_len: bool,
2025-04-11T03:52:12.7875335Z         n_tokens: int,
2025-04-11T03:52:12.7875422Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7875492Z     ):
2025-04-11T03:52:12.7875586Z         torch.manual_seed(123)
2025-04-11T03:52:12.7875677Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7875776Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7875780Z 
2025-04-11T03:52:12.7875934Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7876051Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7876056Z 
2025-04-11T03:52:12.7876131Z device = None
2025-04-11T03:52:12.7876135Z 
2025-04-11T03:52:12.7876253Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7876408Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7876479Z     
2025-04-11T03:52:12.7876562Z         Args:
2025-04-11T03:52:12.7876730Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7876899Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7877005Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7877077Z         """
2025-04-11T03:52:12.7877165Z         _lazy_init()
2025-04-11T03:52:12.7877259Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7877365Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7877471Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7877752Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7877892Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7878052Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7878057Z 
2025-04-11T03:52:12.7878297Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7878453Z _______________ test_copy_kv_to_caches[True-1-False-16-16-32-32] _______________
2025-04-11T03:52:12.7878456Z 
2025-04-11T03:52:12.7878611Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7878894Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7878899Z 
2025-04-11T03:52:12.7879099Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7879206Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7879333Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7879465Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7879581Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7879725Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7879882Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7880035Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7880125Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7880200Z         bsz: int,
2025-04-11T03:52:12.7880289Z         block_size: int,
2025-04-11T03:52:12.7880383Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7880473Z         num_kv_heads: int,
2025-04-11T03:52:12.7880557Z         same_context_len: bool,
2025-04-11T03:52:12.7880697Z         n_tokens: int,
2025-04-11T03:52:12.7880786Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7880857Z     ):
2025-04-11T03:52:12.7880949Z         torch.manual_seed(123)
2025-04-11T03:52:12.7881037Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7881131Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7881135Z 
2025-04-11T03:52:12.7881286Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7881400Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7881409Z 
2025-04-11T03:52:12.7881484Z device = None
2025-04-11T03:52:12.7881488Z 
2025-04-11T03:52:12.7881607Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7881762Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7881836Z     
2025-04-11T03:52:12.7881913Z         Args:
2025-04-11T03:52:12.7882081Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7882252Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7882361Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7882434Z         """
2025-04-11T03:52:12.7882519Z         _lazy_init()
2025-04-11T03:52:12.7882615Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7882719Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7882824Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7883110Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7883249Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7883405Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7883412Z 
2025-04-11T03:52:12.7883657Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7883813Z _______________ test_copy_kv_to_caches[True-1-False-16-16-64-7] ________________
2025-04-11T03:52:12.7883818Z 
2025-04-11T03:52:12.7883968Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7884118Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7884123Z 
2025-04-11T03:52:12.7884320Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7884427Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7884550Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7884686Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7884801Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7885052Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7885159Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7885315Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7885406Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7885481Z         bsz: int,
2025-04-11T03:52:12.7885571Z         block_size: int,
2025-04-11T03:52:12.7885661Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7885749Z         num_kv_heads: int,
2025-04-11T03:52:12.7885837Z         same_context_len: bool,
2025-04-11T03:52:12.7885916Z         n_tokens: int,
2025-04-11T03:52:12.7886072Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7886144Z     ):
2025-04-11T03:52:12.7886235Z         torch.manual_seed(123)
2025-04-11T03:52:12.7886324Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7886415Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7886422Z 
2025-04-11T03:52:12.7886575Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7886686Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7886746Z 
2025-04-11T03:52:12.7886829Z device = None
2025-04-11T03:52:12.7886834Z 
2025-04-11T03:52:12.7886952Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7887105Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7887177Z     
2025-04-11T03:52:12.7887254Z         Args:
2025-04-11T03:52:12.7887421Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7887586Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7887696Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7887770Z         """
2025-04-11T03:52:12.7887855Z         _lazy_init()
2025-04-11T03:52:12.7887950Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7888056Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7888163Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7888447Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7888584Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7888740Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7888745Z 
2025-04-11T03:52:12.7888984Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7889140Z _______________ test_copy_kv_to_caches[True-1-False-16-16-64-32] _______________
2025-04-11T03:52:12.7889144Z 
2025-04-11T03:52:12.7889294Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7889448Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T03:52:12.7889455Z 
2025-04-11T03:52:12.7889653Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7889761Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7889884Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7890020Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7890136Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7890278Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7890385Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7890535Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7890631Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7890708Z         bsz: int,
2025-04-11T03:52:12.7890797Z         block_size: int,
2025-04-11T03:52:12.7890887Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7890974Z         num_kv_heads: int,
2025-04-11T03:52:12.7891167Z         same_context_len: bool,
2025-04-11T03:52:12.7891245Z         n_tokens: int,
2025-04-11T03:52:12.7891339Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7891415Z     ):
2025-04-11T03:52:12.7891504Z         torch.manual_seed(123)
2025-04-11T03:52:12.7891592Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7891680Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7891684Z 
2025-04-11T03:52:12.7891839Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7891952Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7891956Z 
2025-04-11T03:52:12.7892084Z device = None
2025-04-11T03:52:12.7892088Z 
2025-04-11T03:52:12.7892206Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7892360Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7892434Z     
2025-04-11T03:52:12.7892508Z         Args:
2025-04-11T03:52:12.7892683Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7892851Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7893019Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7893093Z         """
2025-04-11T03:52:12.7893176Z         _lazy_init()
2025-04-11T03:52:12.7893270Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7893371Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7893482Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7893763Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7893905Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7894064Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7894068Z 
2025-04-11T03:52:12.7894309Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7894462Z ________________ test_copy_kv_to_caches[True-5-True-16-16-16-7] ________________
2025-04-11T03:52:12.7894467Z 
2025-04-11T03:52:12.7894620Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7894772Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7894777Z 
2025-04-11T03:52:12.7894975Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7895083Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7895207Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7895346Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7895461Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7895607Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7895719Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7895867Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7895965Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7896040Z         bsz: int,
2025-04-11T03:52:12.7896127Z         block_size: int,
2025-04-11T03:52:12.7896214Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7896295Z         num_kv_heads: int,
2025-04-11T03:52:12.7896384Z         same_context_len: bool,
2025-04-11T03:52:12.7896463Z         n_tokens: int,
2025-04-11T03:52:12.7896557Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7896628Z     ):
2025-04-11T03:52:12.7896720Z         torch.manual_seed(123)
2025-04-11T03:52:12.7896807Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7896895Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7896899Z 
2025-04-11T03:52:12.7897056Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7897168Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7897275Z 
2025-04-11T03:52:12.7897359Z device = None
2025-04-11T03:52:12.7897365Z 
2025-04-11T03:52:12.7897486Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7897643Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7897714Z     
2025-04-11T03:52:12.7897789Z         Args:
2025-04-11T03:52:12.7897961Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7898129Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7898288Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7898365Z         """
2025-04-11T03:52:12.7898448Z         _lazy_init()
2025-04-11T03:52:12.7898543Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7898644Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7898758Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7899037Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7899234Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7899390Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7899394Z 
2025-04-11T03:52:12.7899635Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7899788Z _______________ test_copy_kv_to_caches[True-5-True-16-16-16-32] ________________
2025-04-11T03:52:12.7899794Z 
2025-04-11T03:52:12.7899945Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7900102Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7900107Z 
2025-04-11T03:52:12.7900303Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7900416Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7900542Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7900683Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7900800Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7900944Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7901056Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7901205Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7901302Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7901381Z         bsz: int,
2025-04-11T03:52:12.7901471Z         block_size: int,
2025-04-11T03:52:12.7901562Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7901645Z         num_kv_heads: int,
2025-04-11T03:52:12.7901736Z         same_context_len: bool,
2025-04-11T03:52:12.7901813Z         n_tokens: int,
2025-04-11T03:52:12.7901912Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7901985Z     ):
2025-04-11T03:52:12.7902071Z         torch.manual_seed(123)
2025-04-11T03:52:12.7902167Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7902256Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7902260Z 
2025-04-11T03:52:12.7902416Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7902529Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7902533Z 
2025-04-11T03:52:12.7902615Z device = None
2025-04-11T03:52:12.7902619Z 
2025-04-11T03:52:12.7902738Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7902895Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7902966Z     
2025-04-11T03:52:12.7903039Z         Args:
2025-04-11T03:52:12.7903214Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7903498Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7903609Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7903685Z         """
2025-04-11T03:52:12.7903765Z         _lazy_init()
2025-04-11T03:52:12.7903865Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7903968Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7904077Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7904357Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7904547Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7904704Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7904709Z 
2025-04-11T03:52:12.7904942Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7905102Z ________________ test_copy_kv_to_caches[True-5-True-16-16-32-7] ________________
2025-04-11T03:52:12.7905106Z 
2025-04-11T03:52:12.7905312Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7905467Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7905471Z 
2025-04-11T03:52:12.7905667Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7905778Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7905903Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7906047Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7906162Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7906302Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7906413Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7906564Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7906660Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7906737Z         bsz: int,
2025-04-11T03:52:12.7906827Z         block_size: int,
2025-04-11T03:52:12.7906918Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7907003Z         num_kv_heads: int,
2025-04-11T03:52:12.7907093Z         same_context_len: bool,
2025-04-11T03:52:12.7907172Z         n_tokens: int,
2025-04-11T03:52:12.7907263Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7907334Z     ):
2025-04-11T03:52:12.7907418Z         torch.manual_seed(123)
2025-04-11T03:52:12.7907511Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7907604Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7907608Z 
2025-04-11T03:52:12.7907763Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7907878Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7907882Z 
2025-04-11T03:52:12.7907965Z device = None
2025-04-11T03:52:12.7907968Z 
2025-04-11T03:52:12.7908088Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7908239Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7908315Z     
2025-04-11T03:52:12.7908389Z         Args:
2025-04-11T03:52:12.7908630Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7908795Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7908905Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7908980Z         """
2025-04-11T03:52:12.7909060Z         _lazy_init()
2025-04-11T03:52:12.7909158Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7909261Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7909371Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7909647Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7909919Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7910078Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7910083Z 
2025-04-11T03:52:12.7910317Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7910475Z _______________ test_copy_kv_to_caches[True-5-True-16-16-32-32] ________________
2025-04-11T03:52:12.7910479Z 
2025-04-11T03:52:12.7910627Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7910844Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7910848Z 
2025-04-11T03:52:12.7911046Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7911155Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7911284Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7911426Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7911592Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7911727Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7911839Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7911988Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7912083Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7912159Z         bsz: int,
2025-04-11T03:52:12.7912240Z         block_size: int,
2025-04-11T03:52:12.7912336Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7912421Z         num_kv_heads: int,
2025-04-11T03:52:12.7912512Z         same_context_len: bool,
2025-04-11T03:52:12.7912591Z         n_tokens: int,
2025-04-11T03:52:12.7912685Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7912759Z     ):
2025-04-11T03:52:12.7912845Z         torch.manual_seed(123)
2025-04-11T03:52:12.7912937Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7913029Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7913035Z 
2025-04-11T03:52:12.7913190Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7913301Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7913305Z 
2025-04-11T03:52:12.7913384Z device = None
2025-04-11T03:52:12.7913388Z 
2025-04-11T03:52:12.7913509Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7913659Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7913735Z     
2025-04-11T03:52:12.7913810Z         Args:
2025-04-11T03:52:12.7913979Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7914142Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7914248Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7914327Z         """
2025-04-11T03:52:12.7914405Z         _lazy_init()
2025-04-11T03:52:12.7914506Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7914607Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7914713Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7914992Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7915125Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7915288Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7915292Z 
2025-04-11T03:52:12.7915526Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7915681Z ________________ test_copy_kv_to_caches[True-5-True-16-16-64-7] ________________
2025-04-11T03:52:12.7915798Z 
2025-04-11T03:52:12.7915946Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7916105Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7916109Z 
2025-04-11T03:52:12.7916304Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7916414Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7916541Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7916676Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7916795Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7916993Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7917107Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7917257Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7917348Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7917430Z         bsz: int,
2025-04-11T03:52:12.7917511Z         block_size: int,
2025-04-11T03:52:12.7917606Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7917749Z         num_kv_heads: int,
2025-04-11T03:52:12.7917837Z         same_context_len: bool,
2025-04-11T03:52:12.7917917Z         n_tokens: int,
2025-04-11T03:52:12.7918006Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7918081Z     ):
2025-04-11T03:52:12.7918168Z         torch.manual_seed(123)
2025-04-11T03:52:12.7918262Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7918357Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7918361Z 
2025-04-11T03:52:12.7918519Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7918631Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7918636Z 
2025-04-11T03:52:12.7918713Z device = None
2025-04-11T03:52:12.7918717Z 
2025-04-11T03:52:12.7918839Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7918993Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7919069Z     
2025-04-11T03:52:12.7919143Z         Args:
2025-04-11T03:52:12.7919313Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7919479Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7919586Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7919667Z         """
2025-04-11T03:52:12.7919748Z         _lazy_init()
2025-04-11T03:52:12.7919847Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7919956Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7920063Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7920351Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7920490Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7920655Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7920661Z 
2025-04-11T03:52:12.7920898Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7921057Z _______________ test_copy_kv_to_caches[True-5-True-16-16-64-32] ________________
2025-04-11T03:52:12.7921062Z 
2025-04-11T03:52:12.7921210Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7921366Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7921372Z 
2025-04-11T03:52:12.7921572Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7921682Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7921804Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7922047Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7922171Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7922312Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7922426Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7922577Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7922667Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7922746Z         bsz: int,
2025-04-11T03:52:12.7922827Z         block_size: int,
2025-04-11T03:52:12.7922922Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7923003Z         num_kv_heads: int,
2025-04-11T03:52:12.7923144Z         same_context_len: bool,
2025-04-11T03:52:12.7923225Z         n_tokens: int,
2025-04-11T03:52:12.7923314Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7923394Z     ):
2025-04-11T03:52:12.7923482Z         torch.manual_seed(123)
2025-04-11T03:52:12.7923578Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7923672Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7923675Z 
2025-04-11T03:52:12.7923825Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7924001Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7924006Z 
2025-04-11T03:52:12.7924081Z device = None
2025-04-11T03:52:12.7924085Z 
2025-04-11T03:52:12.7924207Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7924357Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7924429Z     
2025-04-11T03:52:12.7924505Z         Args:
2025-04-11T03:52:12.7924673Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7924841Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7924947Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7925027Z         """
2025-04-11T03:52:12.7925105Z         _lazy_init()
2025-04-11T03:52:12.7925208Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7925312Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7925417Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7925702Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7925837Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7925997Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7926003Z 
2025-04-11T03:52:12.7926235Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7926388Z _______________ test_copy_kv_to_caches[True-5-False-16-16-16-7] ________________
2025-04-11T03:52:12.7926392Z 
2025-04-11T03:52:12.7926537Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7926710Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7926718Z 
2025-04-11T03:52:12.7926945Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7927054Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7927180Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7927313Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7927434Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7927573Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7927692Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7927841Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7927930Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7928011Z         bsz: int,
2025-04-11T03:52:12.7928093Z         block_size: int,
2025-04-11T03:52:12.7928304Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7928386Z         num_kv_heads: int,
2025-04-11T03:52:12.7928475Z         same_context_len: bool,
2025-04-11T03:52:12.7928560Z         n_tokens: int,
2025-04-11T03:52:12.7928649Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7928729Z     ):
2025-04-11T03:52:12.7928817Z         torch.manual_seed(123)
2025-04-11T03:52:12.7928917Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7929007Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7929011Z 
2025-04-11T03:52:12.7929159Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7929335Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7929340Z 
2025-04-11T03:52:12.7929417Z device = None
2025-04-11T03:52:12.7929421Z 
2025-04-11T03:52:12.7929542Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7929692Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7929771Z     
2025-04-11T03:52:12.7929843Z         Args:
2025-04-11T03:52:12.7930010Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7930241Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7930345Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7930423Z         """
2025-04-11T03:52:12.7930502Z         _lazy_init()
2025-04-11T03:52:12.7930600Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7930700Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7930808Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7931092Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7931227Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7931390Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7931394Z 
2025-04-11T03:52:12.7931626Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7931787Z _______________ test_copy_kv_to_caches[True-5-False-16-16-16-32] _______________
2025-04-11T03:52:12.7931791Z 
2025-04-11T03:52:12.7931937Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7932090Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7932098Z 
2025-04-11T03:52:12.7932292Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7932398Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7932525Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7932656Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7932778Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7932915Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7933028Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7933179Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7933268Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7933348Z         bsz: int,
2025-04-11T03:52:12.7933428Z         block_size: int,
2025-04-11T03:52:12.7933520Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7933601Z         num_kv_heads: int,
2025-04-11T03:52:12.7933686Z         same_context_len: bool,
2025-04-11T03:52:12.7933768Z         n_tokens: int,
2025-04-11T03:52:12.7933855Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7933928Z     ):
2025-04-11T03:52:12.7934014Z         torch.manual_seed(123)
2025-04-11T03:52:12.7934102Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7934198Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7934396Z 
2025-04-11T03:52:12.7934548Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7942678Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7942698Z 
2025-04-11T03:52:12.7942825Z device = None
2025-04-11T03:52:12.7942830Z 
2025-04-11T03:52:12.7942981Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7943147Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7943228Z     
2025-04-11T03:52:12.7943310Z         Args:
2025-04-11T03:52:12.7943488Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7943794Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7943907Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7943988Z         """
2025-04-11T03:52:12.7944072Z         _lazy_init()
2025-04-11T03:52:12.7944180Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7944304Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7944417Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7944789Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7944935Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7945104Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7945109Z 
2025-04-11T03:52:12.7945359Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7945531Z _______________ test_copy_kv_to_caches[True-5-False-16-16-32-7] ________________
2025-04-11T03:52:12.7945535Z 
2025-04-11T03:52:12.7945691Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7945851Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7945862Z 
2025-04-11T03:52:12.7946068Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7946186Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7946323Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7946461Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7946582Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7946725Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7946841Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7946998Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7947096Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7947175Z         bsz: int,
2025-04-11T03:52:12.7947263Z         block_size: int,
2025-04-11T03:52:12.7947362Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7947453Z         num_kv_heads: int,
2025-04-11T03:52:12.7947548Z         same_context_len: bool,
2025-04-11T03:52:12.7947627Z         n_tokens: int,
2025-04-11T03:52:12.7947725Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7947805Z     ):
2025-04-11T03:52:12.7947894Z         torch.manual_seed(123)
2025-04-11T03:52:12.7947992Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7948087Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7948092Z 
2025-04-11T03:52:12.7948249Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7948368Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7948374Z 
2025-04-11T03:52:12.7948500Z device = None
2025-04-11T03:52:12.7948510Z 
2025-04-11T03:52:12.7948636Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7948791Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7948866Z     
2025-04-11T03:52:12.7949090Z         Args:
2025-04-11T03:52:12.7949270Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7949440Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7949552Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7949634Z         """
2025-04-11T03:52:12.7949714Z         _lazy_init()
2025-04-11T03:52:12.7949821Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7949927Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7950038Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7950324Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7950566Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7950734Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7950742Z 
2025-04-11T03:52:12.7950981Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7951191Z _______________ test_copy_kv_to_caches[True-5-False-16-16-32-32] _______________
2025-04-11T03:52:12.7951197Z 
2025-04-11T03:52:12.7951349Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7951510Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7951514Z 
2025-04-11T03:52:12.7951716Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7951827Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7951954Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7952091Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7952213Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7952357Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7952478Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7952626Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7952727Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7952805Z         bsz: int,
2025-04-11T03:52:12.7952890Z         block_size: int,
2025-04-11T03:52:12.7952987Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7953074Z         num_kv_heads: int,
2025-04-11T03:52:12.7953163Z         same_context_len: bool,
2025-04-11T03:52:12.7953243Z         n_tokens: int,
2025-04-11T03:52:12.7953332Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7953414Z     ):
2025-04-11T03:52:12.7953503Z         torch.manual_seed(123)
2025-04-11T03:52:12.7953600Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7953697Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7953701Z 
2025-04-11T03:52:12.7953862Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7953980Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7953984Z 
2025-04-11T03:52:12.7954060Z device = None
2025-04-11T03:52:12.7954067Z 
2025-04-11T03:52:12.7954189Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7954340Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7954415Z     
2025-04-11T03:52:12.7954489Z         Args:
2025-04-11T03:52:12.7954662Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7954830Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7954940Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7955018Z         """
2025-04-11T03:52:12.7955100Z         _lazy_init()
2025-04-11T03:52:12.7955200Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7955308Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7955522Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7955813Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7955954Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7956118Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7956123Z 
2025-04-11T03:52:12.7956363Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7956525Z _______________ test_copy_kv_to_caches[True-5-False-16-16-64-7] ________________
2025-04-11T03:52:12.7956579Z 
2025-04-11T03:52:12.7956733Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7956893Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7956897Z 
2025-04-11T03:52:12.7957097Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7957214Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7957404Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7957539Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7957661Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7957804Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7957916Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7958067Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7958160Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7958243Z         bsz: int,
2025-04-11T03:52:12.7958328Z         block_size: int,
2025-04-11T03:52:12.7958424Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7958506Z         num_kv_heads: int,
2025-04-11T03:52:12.7958599Z         same_context_len: bool,
2025-04-11T03:52:12.7958682Z         n_tokens: int,
2025-04-11T03:52:12.7958772Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7958850Z     ):
2025-04-11T03:52:12.7958938Z         torch.manual_seed(123)
2025-04-11T03:52:12.7959034Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7959126Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7959130Z 
2025-04-11T03:52:12.7959282Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7959401Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7959406Z 
2025-04-11T03:52:12.7959484Z device = None
2025-04-11T03:52:12.7959488Z 
2025-04-11T03:52:12.7959612Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7959761Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7959836Z     
2025-04-11T03:52:12.7959911Z         Args:
2025-04-11T03:52:12.7960079Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7960256Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7960365Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7960445Z         """
2025-04-11T03:52:12.7960526Z         _lazy_init()
2025-04-11T03:52:12.7960629Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7960733Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7960841Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7961130Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7961269Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7961437Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7961441Z 
2025-04-11T03:52:12.7961677Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7961943Z _______________ test_copy_kv_to_caches[True-5-False-16-16-64-32] _______________
2025-04-11T03:52:12.7961950Z 
2025-04-11T03:52:12.7962101Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7962260Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T03:52:12.7962264Z 
2025-04-11T03:52:12.7962467Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7962574Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7962701Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7962886Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7963009Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7963148Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7963264Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7963418Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7963506Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7963650Z         bsz: int,
2025-04-11T03:52:12.7963734Z         block_size: int,
2025-04-11T03:52:12.7963830Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7963914Z         num_kv_heads: int,
2025-04-11T03:52:12.7964002Z         same_context_len: bool,
2025-04-11T03:52:12.7964087Z         n_tokens: int,
2025-04-11T03:52:12.7964176Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7964252Z     ):
2025-04-11T03:52:12.7964340Z         torch.manual_seed(123)
2025-04-11T03:52:12.7964438Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7964532Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7964536Z 
2025-04-11T03:52:12.7964686Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7964807Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7964815Z 
2025-04-11T03:52:12.7964892Z device = None
2025-04-11T03:52:12.7964896Z 
2025-04-11T03:52:12.7965019Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7965171Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7965248Z     
2025-04-11T03:52:12.7965324Z         Args:
2025-04-11T03:52:12.7965493Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7965667Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7965775Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7965855Z         """
2025-04-11T03:52:12.7965935Z         _lazy_init()
2025-04-11T03:52:12.7966032Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7966140Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7966246Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7966537Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7966673Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7966836Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7966841Z 
2025-04-11T03:52:12.7967075Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7967232Z _______________ test_copy_kv_to_caches[False-1-True-16-16-16-7] ________________
2025-04-11T03:52:12.7967236Z 
2025-04-11T03:52:12.7967383Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7967537Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7967546Z 
2025-04-11T03:52:12.7967741Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7967955Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7968082Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7968217Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7968339Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7968477Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7968589Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7968744Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7968833Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7968919Z         bsz: int,
2025-04-11T03:52:12.7969055Z         block_size: int,
2025-04-11T03:52:12.7969150Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7969234Z         num_kv_heads: int,
2025-04-11T03:52:12.7969319Z         same_context_len: bool,
2025-04-11T03:52:12.7969406Z         n_tokens: int,
2025-04-11T03:52:12.7969495Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7969575Z     ):
2025-04-11T03:52:12.7969661Z         torch.manual_seed(123)
2025-04-11T03:52:12.7969753Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7969909Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7969913Z 
2025-04-11T03:52:12.7970065Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7970184Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7970189Z 
2025-04-11T03:52:12.7970265Z device = None
2025-04-11T03:52:12.7970269Z 
2025-04-11T03:52:12.7970392Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7970544Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7970622Z     
2025-04-11T03:52:12.7970697Z         Args:
2025-04-11T03:52:12.7970868Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7971040Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7971150Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7971229Z         """
2025-04-11T03:52:12.7971311Z         _lazy_init()
2025-04-11T03:52:12.7971409Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7971522Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7971628Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7971918Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7972054Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7972222Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7972227Z 
2025-04-11T03:52:12.7972464Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7972618Z _______________ test_copy_kv_to_caches[False-1-True-16-16-16-32] _______________
2025-04-11T03:52:12.7972631Z 
2025-04-11T03:52:12.7972784Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7972943Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7972947Z 
2025-04-11T03:52:12.7973151Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7973258Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7973388Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7973525Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7973646Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7973788Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7973899Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7974056Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7974252Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7974336Z         bsz: int,
2025-04-11T03:52:12.7974420Z         block_size: int,
2025-04-11T03:52:12.7974516Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7974609Z         num_kv_heads: int,
2025-04-11T03:52:12.7974697Z         same_context_len: bool,
2025-04-11T03:52:12.7974784Z         n_tokens: int,
2025-04-11T03:52:12.7974873Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7974951Z     ):
2025-04-11T03:52:12.7975036Z         torch.manual_seed(123)
2025-04-11T03:52:12.7975126Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7975225Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7975278Z 
2025-04-11T03:52:12.7975435Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7975555Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7975560Z 
2025-04-11T03:52:12.7975638Z device = None
2025-04-11T03:52:12.7975647Z 
2025-04-11T03:52:12.7975774Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7975928Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7976059Z     
2025-04-11T03:52:12.7976140Z         Args:
2025-04-11T03:52:12.7976307Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7976480Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7976587Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7976666Z         """
2025-04-11T03:52:12.7976748Z         _lazy_init()
2025-04-11T03:52:12.7976847Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7976961Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7977069Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7977355Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7977494Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7977654Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7977666Z 
2025-04-11T03:52:12.7977904Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7978060Z _______________ test_copy_kv_to_caches[False-1-True-16-16-32-7] ________________
2025-04-11T03:52:12.7978065Z 
2025-04-11T03:52:12.7978215Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7978365Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7978371Z 
2025-04-11T03:52:12.7978574Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7978680Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7978812Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7978948Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7979065Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7979211Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7979321Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7979476Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7979567Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7979651Z         bsz: int,
2025-04-11T03:52:12.7979737Z         block_size: int,
2025-04-11T03:52:12.7979828Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7979921Z         num_kv_heads: int,
2025-04-11T03:52:12.7980008Z         same_context_len: bool,
2025-04-11T03:52:12.7980094Z         n_tokens: int,
2025-04-11T03:52:12.7980185Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7980258Z     ):
2025-04-11T03:52:12.7980349Z         torch.manual_seed(123)
2025-04-11T03:52:12.7980552Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7980652Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7980656Z 
2025-04-11T03:52:12.7980809Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7980933Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7980937Z 
2025-04-11T03:52:12.7981017Z device = None
2025-04-11T03:52:12.7981021Z 
2025-04-11T03:52:12.7981146Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7981297Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7981370Z     
2025-04-11T03:52:12.7981502Z         Args:
2025-04-11T03:52:12.7981669Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7981844Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7981953Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7982032Z         """
2025-04-11T03:52:12.7982119Z         _lazy_init()
2025-04-11T03:52:12.7982215Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7982379Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7982486Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7982777Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7982914Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7983072Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7983085Z 
2025-04-11T03:52:12.7983325Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7983479Z _______________ test_copy_kv_to_caches[False-1-True-16-16-32-32] _______________
2025-04-11T03:52:12.7983484Z 
2025-04-11T03:52:12.7983646Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7983797Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7983803Z 
2025-04-11T03:52:12.7984011Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7984117Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7984247Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7984381Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7984497Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7984650Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7984758Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7984916Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7985004Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7985090Z         bsz: int,
2025-04-11T03:52:12.7985180Z         block_size: int,
2025-04-11T03:52:12.7985271Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7985365Z         num_kv_heads: int,
2025-04-11T03:52:12.7985457Z         same_context_len: bool,
2025-04-11T03:52:12.7985543Z         n_tokens: int,
2025-04-11T03:52:12.7985636Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7985711Z     ):
2025-04-11T03:52:12.7985809Z         torch.manual_seed(123)
2025-04-11T03:52:12.7985898Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7985995Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7986000Z 
2025-04-11T03:52:12.7986151Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7986273Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7986277Z 
2025-04-11T03:52:12.7986354Z device = None
2025-04-11T03:52:12.7986359Z 
2025-04-11T03:52:12.7986478Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7986742Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7986815Z     
2025-04-11T03:52:12.7986897Z         Args:
2025-04-11T03:52:12.7987070Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7987240Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7987346Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7987421Z         """
2025-04-11T03:52:12.7987508Z         _lazy_init()
2025-04-11T03:52:12.7987605Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7987712Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7987885Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7988171Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7988310Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7988536Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7988606Z 
2025-04-11T03:52:12.7988857Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7989014Z _______________ test_copy_kv_to_caches[False-1-True-16-16-64-7] ________________
2025-04-11T03:52:12.7989018Z 
2025-04-11T03:52:12.7989171Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7989324Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7989329Z 
2025-04-11T03:52:12.7989532Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7989635Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7989761Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7989890Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7990007Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7990148Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7990257Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7990412Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7990500Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7990578Z         bsz: int,
2025-04-11T03:52:12.7990658Z         block_size: int,
2025-04-11T03:52:12.7990745Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7990831Z         num_kv_heads: int,
2025-04-11T03:52:12.7990916Z         same_context_len: bool,
2025-04-11T03:52:12.7990998Z         n_tokens: int,
2025-04-11T03:52:12.7991086Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7991158Z     ):
2025-04-11T03:52:12.7991248Z         torch.manual_seed(123)
2025-04-11T03:52:12.7991337Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7991429Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7991439Z 
2025-04-11T03:52:12.7991588Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7991706Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7991710Z 
2025-04-11T03:52:12.7991786Z device = None
2025-04-11T03:52:12.7991790Z 
2025-04-11T03:52:12.7991906Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7992058Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7992126Z     
2025-04-11T03:52:12.7992203Z         Args:
2025-04-11T03:52:12.7992368Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7992539Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7992644Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7992715Z         """
2025-04-11T03:52:12.7992796Z         _lazy_init()
2025-04-11T03:52:12.7993022Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7993126Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7993234Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7993515Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7993653Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7993810Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7993814Z 
2025-04-11T03:52:12.7994055Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7994263Z _______________ test_copy_kv_to_caches[False-1-True-16-16-64-32] _______________
2025-04-11T03:52:12.7994267Z 
2025-04-11T03:52:12.7994418Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7994571Z same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7994575Z 
2025-04-11T03:52:12.7994776Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.7994936Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.7995066Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.7995201Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.7995319Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.7995463Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.7995574Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.7995728Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.7995820Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.7995900Z         bsz: int,
2025-04-11T03:52:12.7995991Z         block_size: int,
2025-04-11T03:52:12.7996083Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.7996178Z         num_kv_heads: int,
2025-04-11T03:52:12.7996267Z         same_context_len: bool,
2025-04-11T03:52:12.7996353Z         n_tokens: int,
2025-04-11T03:52:12.7996444Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.7996519Z     ):
2025-04-11T03:52:12.7996611Z         torch.manual_seed(123)
2025-04-11T03:52:12.7996704Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.7996800Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.7996804Z 
2025-04-11T03:52:12.7996955Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.7997072Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.7997080Z 
2025-04-11T03:52:12.7997160Z device = None
2025-04-11T03:52:12.7997164Z 
2025-04-11T03:52:12.7997284Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.7997445Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.7997522Z     
2025-04-11T03:52:12.7997604Z         Args:
2025-04-11T03:52:12.7997776Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.7997947Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.7998058Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.7998135Z         """
2025-04-11T03:52:12.7998221Z         _lazy_init()
2025-04-11T03:52:12.7998319Z         with torch.cuda.device(device):
2025-04-11T03:52:12.7998427Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.7998535Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.7998823Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.7998967Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.7999126Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.7999240Z 
2025-04-11T03:52:12.7999482Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.7999637Z _______________ test_copy_kv_to_caches[False-1-False-16-16-16-7] _______________
2025-04-11T03:52:12.7999641Z 
2025-04-11T03:52:12.7999791Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.7999947Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.7999952Z 
2025-04-11T03:52:12.8000156Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8000313Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8000436Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8000571Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8000688Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8000832Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8000939Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8001140Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8001229Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8001306Z         bsz: int,
2025-04-11T03:52:12.8001392Z         block_size: int,
2025-04-11T03:52:12.8001482Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8001568Z         num_kv_heads: int,
2025-04-11T03:52:12.8001655Z         same_context_len: bool,
2025-04-11T03:52:12.8001735Z         n_tokens: int,
2025-04-11T03:52:12.8001829Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8001905Z     ):
2025-04-11T03:52:12.8001995Z         torch.manual_seed(123)
2025-04-11T03:52:12.8002083Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8002175Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8002179Z 
2025-04-11T03:52:12.8002328Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8002441Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8002447Z 
2025-04-11T03:52:12.8002526Z device = None
2025-04-11T03:52:12.8002530Z 
2025-04-11T03:52:12.8002644Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8002797Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8002867Z     
2025-04-11T03:52:12.8002945Z         Args:
2025-04-11T03:52:12.8003112Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8003276Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8003387Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8003460Z         """
2025-04-11T03:52:12.8003543Z         _lazy_init()
2025-04-11T03:52:12.8003638Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8003740Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8003847Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8004125Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8004264Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8004422Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8004426Z 
2025-04-11T03:52:12.8004659Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8004816Z ______________ test_copy_kv_to_caches[False-1-False-16-16-16-32] _______________
2025-04-11T03:52:12.8004821Z 
2025-04-11T03:52:12.8004970Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8005123Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.8005127Z 
2025-04-11T03:52:12.8005439Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8005543Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8005667Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8005803Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8005917Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8006060Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8006169Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8006318Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8006459Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8006537Z         bsz: int,
2025-04-11T03:52:12.8006624Z         block_size: int,
2025-04-11T03:52:12.8006714Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8006799Z         num_kv_heads: int,
2025-04-11T03:52:12.8006887Z         same_context_len: bool,
2025-04-11T03:52:12.8006965Z         n_tokens: int,
2025-04-11T03:52:12.8007060Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8007206Z     ):
2025-04-11T03:52:12.8007296Z         torch.manual_seed(123)
2025-04-11T03:52:12.8007385Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8007476Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8007480Z 
2025-04-11T03:52:12.8007633Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8007745Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8007750Z 
2025-04-11T03:52:12.8007832Z device = None
2025-04-11T03:52:12.8007839Z 
2025-04-11T03:52:12.8007955Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8008111Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8008181Z     
2025-04-11T03:52:12.8008255Z         Args:
2025-04-11T03:52:12.8008427Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8008596Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8008708Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8008780Z         """
2025-04-11T03:52:12.8008861Z         _lazy_init()
2025-04-11T03:52:12.8008958Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8009059Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8009170Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8009453Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8009594Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8009752Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8009756Z 
2025-04-11T03:52:12.8009997Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8010153Z _______________ test_copy_kv_to_caches[False-1-False-16-16-32-7] _______________
2025-04-11T03:52:12.8010159Z 
2025-04-11T03:52:12.8010310Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8010466Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.8010470Z 
2025-04-11T03:52:12.8010665Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8010773Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8010899Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8011044Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8011161Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8011308Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8011564Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8011714Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8011817Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8011898Z         bsz: int,
2025-04-11T03:52:12.8011995Z         block_size: int,
2025-04-11T03:52:12.8012088Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8012179Z         num_kv_heads: int,
2025-04-11T03:52:12.8012270Z         same_context_len: bool,
2025-04-11T03:52:12.8012351Z         n_tokens: int,
2025-04-11T03:52:12.8012452Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8012524Z     ):
2025-04-11T03:52:12.8012616Z         torch.manual_seed(123)
2025-04-11T03:52:12.8012771Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8012867Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8012871Z 
2025-04-11T03:52:12.8013030Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8013142Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8013149Z 
2025-04-11T03:52:12.8013238Z device = None
2025-04-11T03:52:12.8013242Z 
2025-04-11T03:52:12.8013408Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8013568Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8013639Z     
2025-04-11T03:52:12.8013718Z         Args:
2025-04-11T03:52:12.8013893Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8014063Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8014174Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8014248Z         """
2025-04-11T03:52:12.8014333Z         _lazy_init()
2025-04-11T03:52:12.8014430Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8014532Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8014646Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8014933Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8015075Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8015235Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8015239Z 
2025-04-11T03:52:12.8015481Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8015636Z ______________ test_copy_kv_to_caches[False-1-False-16-16-32-32] _______________
2025-04-11T03:52:12.8015642Z 
2025-04-11T03:52:12.8015792Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8015954Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.8015958Z 
2025-04-11T03:52:12.8016158Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8016272Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8016397Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8016537Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8016655Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8016802Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8016911Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8017063Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8017157Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8017236Z         bsz: int,
2025-04-11T03:52:12.8017323Z         block_size: int,
2025-04-11T03:52:12.8017411Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8017492Z         num_kv_heads: int,
2025-04-11T03:52:12.8017583Z         same_context_len: bool,
2025-04-11T03:52:12.8017660Z         n_tokens: int,
2025-04-11T03:52:12.8017870Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8017944Z     ):
2025-04-11T03:52:12.8018031Z         torch.manual_seed(123)
2025-04-11T03:52:12.8018129Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8018218Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8018222Z 
2025-04-11T03:52:12.8018379Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8018491Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8018496Z 
2025-04-11T03:52:12.8018580Z device = None
2025-04-11T03:52:12.8018584Z 
2025-04-11T03:52:12.8018702Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8018911Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8018986Z     
2025-04-11T03:52:12.8019061Z         Args:
2025-04-11T03:52:12.8019237Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8019405Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8019520Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8019639Z         """
2025-04-11T03:52:12.8019719Z         _lazy_init()
2025-04-11T03:52:12.8019818Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8019919Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8020035Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8020317Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8020455Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8020618Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8020623Z 
2025-04-11T03:52:12.8020860Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8021025Z _______________ test_copy_kv_to_caches[False-1-False-16-16-64-7] _______________
2025-04-11T03:52:12.8021029Z 
2025-04-11T03:52:12.8021175Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8021337Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.8021341Z 
2025-04-11T03:52:12.8021540Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8021649Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8021774Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8021912Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8022031Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8022171Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8022286Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8022436Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8022532Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8022609Z         bsz: int,
2025-04-11T03:52:12.8022698Z         block_size: int,
2025-04-11T03:52:12.8022790Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8022874Z         num_kv_heads: int,
2025-04-11T03:52:12.8022968Z         same_context_len: bool,
2025-04-11T03:52:12.8023046Z         n_tokens: int,
2025-04-11T03:52:12.8023146Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8023218Z     ):
2025-04-11T03:52:12.8023306Z         torch.manual_seed(123)
2025-04-11T03:52:12.8023402Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8023498Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8023502Z 
2025-04-11T03:52:12.8023656Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8023767Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8023771Z 
2025-04-11T03:52:12.8023964Z device = None
2025-04-11T03:52:12.8023968Z 
2025-04-11T03:52:12.8024088Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8024239Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8024318Z     
2025-04-11T03:52:12.8024393Z         Args:
2025-04-11T03:52:12.8024567Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8024734Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8024841Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8024916Z         """
2025-04-11T03:52:12.8025050Z         _lazy_init()
2025-04-11T03:52:12.8025154Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8025254Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8025362Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8025639Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8025778Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8025995Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8026000Z 
2025-04-11T03:52:12.8026236Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8026395Z ______________ test_copy_kv_to_caches[False-1-False-16-16-64-32] _______________
2025-04-11T03:52:12.8026399Z 
2025-04-11T03:52:12.8026546Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8026708Z same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T03:52:12.8026712Z 
2025-04-11T03:52:12.8026907Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8027016Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8027144Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8027299Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8027461Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8027605Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8027718Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8027867Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8027963Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8028039Z         bsz: int,
2025-04-11T03:52:12.8028124Z         block_size: int,
2025-04-11T03:52:12.8028222Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8028305Z         num_kv_heads: int,
2025-04-11T03:52:12.8028399Z         same_context_len: bool,
2025-04-11T03:52:12.8028516Z         n_tokens: int,
2025-04-11T03:52:12.8028614Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8028687Z     ):
2025-04-11T03:52:12.8028780Z         torch.manual_seed(123)
2025-04-11T03:52:12.8028879Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8028971Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8028978Z 
2025-04-11T03:52:12.8029133Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8029246Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8029251Z 
2025-04-11T03:52:12.8029332Z device = None
2025-04-11T03:52:12.8029337Z 
2025-04-11T03:52:12.8029455Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8029607Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8029687Z     
2025-04-11T03:52:12.8029761Z         Args:
2025-04-11T03:52:12.8029942Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8030107Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8030340Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8030418Z         """
2025-04-11T03:52:12.8030497Z         _lazy_init()
2025-04-11T03:52:12.8030603Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8030707Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8030818Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8031100Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8031235Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8031398Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8031472Z 
2025-04-11T03:52:12.8031713Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8031876Z _______________ test_copy_kv_to_caches[False-5-True-16-16-16-7] ________________
2025-04-11T03:52:12.8031884Z 
2025-04-11T03:52:12.8032031Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8032193Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8032256Z 
2025-04-11T03:52:12.8032458Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8032570Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8032696Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8032830Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8032950Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8033090Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8033206Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8033355Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8033452Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8033535Z         bsz: int,
2025-04-11T03:52:12.8033617Z         block_size: int,
2025-04-11T03:52:12.8033711Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8033794Z         num_kv_heads: int,
2025-04-11T03:52:12.8033886Z         same_context_len: bool,
2025-04-11T03:52:12.8033967Z         n_tokens: int,
2025-04-11T03:52:12.8034057Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8034134Z     ):
2025-04-11T03:52:12.8034221Z         torch.manual_seed(123)
2025-04-11T03:52:12.8034317Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8034406Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8034410Z 
2025-04-11T03:52:12.8034572Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8034686Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8034690Z 
2025-04-11T03:52:12.8034766Z device = None
2025-04-11T03:52:12.8034775Z 
2025-04-11T03:52:12.8034890Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8035045Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8035119Z     
2025-04-11T03:52:12.8035196Z         Args:
2025-04-11T03:52:12.8035368Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8035532Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8035636Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8035712Z         """
2025-04-11T03:52:12.8035789Z         _lazy_init()
2025-04-11T03:52:12.8035891Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8035992Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8036098Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8036380Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8036637Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8036801Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8036808Z 
2025-04-11T03:52:12.8037046Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8037203Z _______________ test_copy_kv_to_caches[False-5-True-16-16-16-32] _______________
2025-04-11T03:52:12.8037207Z 
2025-04-11T03:52:12.8037354Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8037508Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8037573Z 
2025-04-11T03:52:12.8037771Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8037878Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8037998Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8038129Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8038248Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8038388Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8038643Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8038793Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8038887Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8038963Z         bsz: int,
2025-04-11T03:52:12.8039044Z         block_size: int,
2025-04-11T03:52:12.8039140Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8039225Z         num_kv_heads: int,
2025-04-11T03:52:12.8039321Z         same_context_len: bool,
2025-04-11T03:52:12.8039402Z         n_tokens: int,
2025-04-11T03:52:12.8039491Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8039567Z     ):
2025-04-11T03:52:12.8039654Z         torch.manual_seed(123)
2025-04-11T03:52:12.8039745Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8039841Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8039845Z 
2025-04-11T03:52:12.8039991Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8040110Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8040114Z 
2025-04-11T03:52:12.8040190Z device = None
2025-04-11T03:52:12.8040194Z 
2025-04-11T03:52:12.8040317Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8040465Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8040536Z     
2025-04-11T03:52:12.8040610Z         Args:
2025-04-11T03:52:12.8040777Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8040948Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8041053Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8041131Z         """
2025-04-11T03:52:12.8041214Z         _lazy_init()
2025-04-11T03:52:12.8041314Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8041415Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8041524Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8041809Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8041944Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8042103Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8042107Z 
2025-04-11T03:52:12.8042342Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8042500Z _______________ test_copy_kv_to_caches[False-5-True-16-16-32-7] ________________
2025-04-11T03:52:12.8042504Z 
2025-04-11T03:52:12.8042651Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8042903Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8042907Z 
2025-04-11T03:52:12.8043108Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8043212Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8043339Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8043471Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8043587Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8043722Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8043889Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8044038Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8044129Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8044212Z         bsz: int,
2025-04-11T03:52:12.8044295Z         block_size: int,
2025-04-11T03:52:12.8044393Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8044475Z         num_kv_heads: int,
2025-04-11T03:52:12.8044561Z         same_context_len: bool,
2025-04-11T03:52:12.8044702Z         n_tokens: int,
2025-04-11T03:52:12.8044791Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8044865Z     ):
2025-04-11T03:52:12.8044953Z         torch.manual_seed(123)
2025-04-11T03:52:12.8045044Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8045135Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8045139Z 
2025-04-11T03:52:12.8045288Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8045403Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8045409Z 
2025-04-11T03:52:12.8045485Z device = None
2025-04-11T03:52:12.8045489Z 
2025-04-11T03:52:12.8045612Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8045759Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8045836Z     
2025-04-11T03:52:12.8045909Z         Args:
2025-04-11T03:52:12.8046075Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8046247Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8046353Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8046435Z         """
2025-04-11T03:52:12.8046512Z         _lazy_init()
2025-04-11T03:52:12.8046611Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8046712Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8046819Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8047107Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8047243Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8047408Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8047412Z 
2025-04-11T03:52:12.8047650Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8047807Z _______________ test_copy_kv_to_caches[False-5-True-16-16-32-32] _______________
2025-04-11T03:52:12.8047811Z 
2025-04-11T03:52:12.8047960Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8048110Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8048118Z 
2025-04-11T03:52:12.8048314Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8048423Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8048549Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8048682Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8048799Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8049052Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8049166Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8049317Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8049408Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8049490Z         bsz: int,
2025-04-11T03:52:12.8049571Z         block_size: int,
2025-04-11T03:52:12.8049665Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8049748Z         num_kv_heads: int,
2025-04-11T03:52:12.8049835Z         same_context_len: bool,
2025-04-11T03:52:12.8049917Z         n_tokens: int,
2025-04-11T03:52:12.8050058Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8050136Z     ):
2025-04-11T03:52:12.8050222Z         torch.manual_seed(123)
2025-04-11T03:52:12.8050310Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8050404Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8050408Z 
2025-04-11T03:52:12.8050558Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8050678Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8050736Z 
2025-04-11T03:52:12.8050816Z device = None
2025-04-11T03:52:12.8050820Z 
2025-04-11T03:52:12.8050941Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8051088Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8051164Z     
2025-04-11T03:52:12.8051240Z         Args:
2025-04-11T03:52:12.8051412Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8051584Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8051690Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8051769Z         """
2025-04-11T03:52:12.8051848Z         _lazy_init()
2025-04-11T03:52:12.8051943Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8052053Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8052159Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8052447Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8053983Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8054139Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8054148Z 
2025-04-11T03:52:12.8054388Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8054545Z _______________ test_copy_kv_to_caches[False-5-True-16-16-64-7] ________________
2025-04-11T03:52:12.8054549Z 
2025-04-11T03:52:12.8054699Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8054851Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8054857Z 
2025-04-11T03:52:12.8055059Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8055168Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8055294Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8055424Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8055561Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8055702Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8055808Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8055966Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8056056Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8056135Z         bsz: int,
2025-04-11T03:52:12.8056221Z         block_size: int,
2025-04-11T03:52:12.8056311Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8056397Z         num_kv_heads: int,
2025-04-11T03:52:12.8056546Z         same_context_len: bool,
2025-04-11T03:52:12.8056625Z         n_tokens: int,
2025-04-11T03:52:12.8056721Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8056794Z     ):
2025-04-11T03:52:12.8056885Z         torch.manual_seed(123)
2025-04-11T03:52:12.8056976Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8057069Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8057078Z 
2025-04-11T03:52:12.8057232Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8057343Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8057347Z 
2025-04-11T03:52:12.8057488Z device = None
2025-04-11T03:52:12.8057492Z 
2025-04-11T03:52:12.8057609Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8057766Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8057837Z     
2025-04-11T03:52:12.8057913Z         Args:
2025-04-11T03:52:12.8058081Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8058247Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8058430Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8058507Z         """
2025-04-11T03:52:12.8058591Z         _lazy_init()
2025-04-11T03:52:12.8058687Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8058789Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8058899Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8059183Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8059325Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8059479Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8059486Z 
2025-04-11T03:52:12.8059730Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8059881Z _______________ test_copy_kv_to_caches[False-5-True-16-16-64-32] _______________
2025-04-11T03:52:12.8059887Z 
2025-04-11T03:52:12.8060036Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8060276Z same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8060281Z 
2025-04-11T03:52:12.8060477Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8060591Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8060716Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8060856Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8060969Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8061113Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8061224Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8061376Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8061470Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8061547Z         bsz: int,
2025-04-11T03:52:12.8061634Z         block_size: int,
2025-04-11T03:52:12.8061725Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8061810Z         num_kv_heads: int,
2025-04-11T03:52:12.8061897Z         same_context_len: bool,
2025-04-11T03:52:12.8061976Z         n_tokens: int,
2025-04-11T03:52:12.8062071Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8062143Z     ):
2025-04-11T03:52:12.8062232Z         torch.manual_seed(123)
2025-04-11T03:52:12.8062319Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8062409Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8062413Z 
2025-04-11T03:52:12.8062563Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8062738Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8062742Z 
2025-04-11T03:52:12.8062825Z device = None
2025-04-11T03:52:12.8062831Z 
2025-04-11T03:52:12.8062948Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8063103Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8063176Z     
2025-04-11T03:52:12.8063250Z         Args:
2025-04-11T03:52:12.8063424Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8063593Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8063766Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8063841Z         """
2025-04-11T03:52:12.8063926Z         _lazy_init()
2025-04-11T03:52:12.8064021Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8064121Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8064237Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8064520Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8064717Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8064878Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8064883Z 
2025-04-11T03:52:12.8065127Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8065284Z _______________ test_copy_kv_to_caches[False-5-False-16-16-16-7] _______________
2025-04-11T03:52:12.8065290Z 
2025-04-11T03:52:12.8065443Z bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8065604Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8065608Z 
2025-04-11T03:52:12.8065807Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8065921Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8066046Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8066192Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8066309Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8066516Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8066623Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8066773Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8066870Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8066946Z         bsz: int,
2025-04-11T03:52:12.8067031Z         block_size: int,
2025-04-11T03:52:12.8067122Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8067205Z         num_kv_heads: int,
2025-04-11T03:52:12.8067292Z         same_context_len: bool,
2025-04-11T03:52:12.8067374Z         n_tokens: int,
2025-04-11T03:52:12.8067467Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8067540Z     ):
2025-04-11T03:52:12.8067630Z         torch.manual_seed(123)
2025-04-11T03:52:12.8067722Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8067813Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8067818Z 
2025-04-11T03:52:12.8067974Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8068086Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8068090Z 
2025-04-11T03:52:12.8068169Z device = None
2025-04-11T03:52:12.8068174Z 
2025-04-11T03:52:12.8068292Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8068511Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8068584Z     
2025-04-11T03:52:12.8068657Z         Args:
2025-04-11T03:52:12.8068830Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8069068Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8069180Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8069254Z         """
2025-04-11T03:52:12.8069332Z         _lazy_init()
2025-04-11T03:52:12.8069432Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8069535Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8069644Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8069924Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8070148Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8070305Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8070309Z 
2025-04-11T03:52:12.8070550Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8070707Z ______________ test_copy_kv_to_caches[False-5-False-16-16-16-32] _______________
2025-04-11T03:52:12.8070711Z 
2025-04-11T03:52:12.8070923Z bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8071084Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8071091Z 
2025-04-11T03:52:12.8071290Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8071402Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8071528Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8071669Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8071783Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8071922Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8072037Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8072189Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8072284Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8072364Z         bsz: int,
2025-04-11T03:52:12.8072451Z         block_size: int,
2025-04-11T03:52:12.8072545Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8072628Z         num_kv_heads: int,
2025-04-11T03:52:12.8072781Z         same_context_len: bool,
2025-04-11T03:52:12.8072860Z         n_tokens: int,
2025-04-11T03:52:12.8072955Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8073029Z     ):
2025-04-11T03:52:12.8073117Z         torch.manual_seed(123)
2025-04-11T03:52:12.8073209Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8073301Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8073305Z 
2025-04-11T03:52:12.8073458Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8073569Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8073576Z 
2025-04-11T03:52:12.8073656Z device = None
2025-04-11T03:52:12.8073661Z 
2025-04-11T03:52:12.8073778Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8073930Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8074004Z     
2025-04-11T03:52:12.8074077Z         Args:
2025-04-11T03:52:12.8074248Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8074416Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8074524Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8074600Z         """
2025-04-11T03:52:12.8074677Z         _lazy_init()
2025-04-11T03:52:12.8074774Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8074875Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8074982Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8075264Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8075458Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8075619Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8075624Z 
2025-04-11T03:52:12.8075861Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8076017Z _______________ test_copy_kv_to_caches[False-5-False-16-16-32-7] _______________
2025-04-11T03:52:12.8076022Z 
2025-04-11T03:52:12.8076169Z bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8076395Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8076400Z 
2025-04-11T03:52:12.8076598Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8076705Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8076829Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8076966Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8077140Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8077278Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8077393Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8077543Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8077636Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8077715Z         bsz: int,
2025-04-11T03:52:12.8077802Z         block_size: int,
2025-04-11T03:52:12.8077892Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8077974Z         num_kv_heads: int,
2025-04-11T03:52:12.8078067Z         same_context_len: bool,
2025-04-11T03:52:12.8078145Z         n_tokens: int,
2025-04-11T03:52:12.8078239Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8078314Z     ):
2025-04-11T03:52:12.8078401Z         torch.manual_seed(123)
2025-04-11T03:52:12.8078496Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8078587Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8078591Z 
2025-04-11T03:52:12.8078743Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8078852Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8078913Z 
2025-04-11T03:52:12.8078993Z device = None
2025-04-11T03:52:12.8078997Z 
2025-04-11T03:52:12.8079116Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8079264Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8079341Z     
2025-04-11T03:52:12.8079415Z         Args:
2025-04-11T03:52:12.8079583Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8079750Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8079861Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8079934Z         """
2025-04-11T03:52:12.8080012Z         _lazy_init()
2025-04-11T03:52:12.8080111Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8080211Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8080320Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8080600Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8080732Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8080896Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8080900Z 
2025-04-11T03:52:12.8081137Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8081296Z ______________ test_copy_kv_to_caches[False-5-False-16-16-32-32] _______________
2025-04-11T03:52:12.8081362Z 
2025-04-11T03:52:12.8081516Z bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8081681Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8081686Z 
2025-04-11T03:52:12.8081882Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8081992Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8082114Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8082247Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8082438Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8082578Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8082690Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8082839Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8082935Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8083011Z         bsz: int,
2025-04-11T03:52:12.8083098Z         block_size: int,
2025-04-11T03:52:12.8083254Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8083337Z         num_kv_heads: int,
2025-04-11T03:52:12.8083428Z         same_context_len: bool,
2025-04-11T03:52:12.8083506Z         n_tokens: int,
2025-04-11T03:52:12.8083601Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8083673Z     ):
2025-04-11T03:52:12.8083757Z         torch.manual_seed(123)
2025-04-11T03:52:12.8083849Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8083941Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8083947Z 
2025-04-11T03:52:12.8084097Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8084210Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8084214Z 
2025-04-11T03:52:12.8084291Z device = None
2025-04-11T03:52:12.8084295Z 
2025-04-11T03:52:12.8084415Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8084565Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8084641Z     
2025-04-11T03:52:12.8084715Z         Args:
2025-04-11T03:52:12.8084886Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8085109Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8085217Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8085290Z         """
2025-04-11T03:52:12.8085368Z         _lazy_init()
2025-04-11T03:52:12.8085469Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8085571Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8085682Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8085961Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8086095Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8086257Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8086264Z 
2025-04-11T03:52:12.8086500Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8086658Z _______________ test_copy_kv_to_caches[False-5-False-16-16-64-7] _______________
2025-04-11T03:52:12.8086662Z 
2025-04-11T03:52:12.8086808Z bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8086965Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8086972Z 
2025-04-11T03:52:12.8087167Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8087276Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8087397Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8087591Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8087714Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8087857Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8087973Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8088129Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8088225Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8088306Z         bsz: int,
2025-04-11T03:52:12.8088391Z         block_size: int,
2025-04-11T03:52:12.8088488Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8088628Z         num_kv_heads: int,
2025-04-11T03:52:12.8088717Z         same_context_len: bool,
2025-04-11T03:52:12.8088797Z         n_tokens: int,
2025-04-11T03:52:12.8088888Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8088966Z     ):
2025-04-11T03:52:12.8089052Z         torch.manual_seed(123)
2025-04-11T03:52:12.8089145Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8089235Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8089239Z 
2025-04-11T03:52:12.8089391Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8089557Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8089563Z 
2025-04-11T03:52:12.8089640Z device = None
2025-04-11T03:52:12.8089649Z 
2025-04-11T03:52:12.8089764Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8089914Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8089990Z     
2025-04-11T03:52:12.8090065Z         Args:
2025-04-11T03:52:12.8090234Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8090399Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8090505Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8090584Z         """
2025-04-11T03:52:12.8090662Z         _lazy_init()
2025-04-11T03:52:12.8090762Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8090864Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8090974Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8091252Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8091446Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8091610Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8091616Z 
2025-04-11T03:52:12.8091855Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8092017Z ______________ test_copy_kv_to_caches[False-5-False-16-16-64-32] _______________
2025-04-11T03:52:12.8092021Z 
2025-04-11T03:52:12.8092166Z bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
2025-04-11T03:52:12.8092325Z same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T03:52:12.8092331Z 
2025-04-11T03:52:12.8092529Z     @pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
2025-04-11T03:52:12.8092638Z     @pytest.mark.parametrize("bsz", [7, 32])
2025-04-11T03:52:12.8092763Z     @pytest.mark.parametrize("block_size", [16, 32, 64])
2025-04-11T03:52:12.8092896Z     @pytest.mark.parametrize("max_num_blocks_per_seq", [16])
2025-04-11T03:52:12.8093011Z     @pytest.mark.parametrize("num_kv_heads", [16])
2025-04-11T03:52:12.8093148Z     @pytest.mark.parametrize("same_context_len", [True, False])
2025-04-11T03:52:12.8093261Z     @pytest.mark.parametrize("n_tokens", [1, 5])
2025-04-11T03:52:12.8093410Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8093500Z     def test_copy_kv_to_caches(
2025-04-11T03:52:12.8093577Z         bsz: int,
2025-04-11T03:52:12.8093717Z         block_size: int,
2025-04-11T03:52:12.8093813Z         max_num_blocks_per_seq: int,
2025-04-11T03:52:12.8093894Z         num_kv_heads: int,
2025-04-11T03:52:12.8093983Z         same_context_len: bool,
2025-04-11T03:52:12.8094060Z         n_tokens: int,
2025-04-11T03:52:12.8094150Z         use_new_kcache_layout: bool,
2025-04-11T03:52:12.8094230Z     ):
2025-04-11T03:52:12.8094318Z         torch.manual_seed(123)
2025-04-11T03:52:12.8094410Z         torch.cuda.empty_cache()
2025-04-11T03:52:12.8094502Z >       torch.cuda.synchronize()
2025-04-11T03:52:12.8094506Z 
2025-04-11T03:52:12.8094662Z tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91: 
2025-04-11T03:52:12.8094832Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8094836Z 
2025-04-11T03:52:12.8094912Z device = None
2025-04-11T03:52:12.8094916Z 
2025-04-11T03:52:12.8095037Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8095184Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8095260Z     
2025-04-11T03:52:12.8095333Z         Args:
2025-04-11T03:52:12.8095548Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8095714Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8095821Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8095900Z         """
2025-04-11T03:52:12.8095980Z         _lazy_init()
2025-04-11T03:52:12.8096078Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8096181Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8096287Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8096570Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8096703Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8096868Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8096872Z 
2025-04-11T03:52:12.8097116Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8097257Z _______________________________ test_layer_norm ________________________________
2025-04-11T03:52:12.8097318Z 
2025-04-11T03:52:12.8097414Z kwargs = {}, val = 2, arg_map = {'M': 2}
2025-04-11T03:52:12.8097752Z partial_func = functools.partial(<function parameterize.<locals>._wrapper.<locals>._execute_function_by_param at 0x7f68f05e1750>, M=2)
2025-04-11T03:52:12.8097757Z 
2025-04-11T03:52:12.8097863Z     def _execute_function_by_param(**kwargs):
2025-04-11T03:52:12.8097951Z         for val in values:
2025-04-11T03:52:12.8098043Z             arg_map = {argument: val}
2025-04-11T03:52:12.8098151Z             partial_func = partial(func, **arg_map)
2025-04-11T03:52:12.8098247Z >           partial_func(**kwargs)
2025-04-11T03:52:12.8098253Z 
2025-04-11T03:52:12.8098349Z colossalai/testing/utils.py:64: 
2025-04-11T03:52:12.8098463Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8098620Z colossalai/testing/utils.py:64: in _execute_function_by_param
2025-04-11T03:52:12.8098709Z     partial_func(**kwargs)
2025-04-11T03:52:12.8098823Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8098828Z 
2025-04-11T03:52:12.8098902Z M = 2, N = 64
2025-04-11T03:52:12.8098907Z 
2025-04-11T03:52:12.8098999Z     @pytest.mark.skipif(
2025-04-11T03:52:12.8099241Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T03:52:12.8099320Z     )
2025-04-11T03:52:12.8099413Z     @parameterize("M", [2, 4, 8, 16])
2025-04-11T03:52:12.8099504Z     @parameterize("N", [64, 128])
2025-04-11T03:52:12.8099600Z     def test_layer_norm(M, N):
2025-04-11T03:52:12.8099689Z         dtype = torch.float16
2025-04-11T03:52:12.8099829Z         eps = 1e-5
2025-04-11T03:52:12.8099910Z         x_shape = (M, N)
2025-04-11T03:52:12.8100003Z         w_shape = (x_shape[-1],)
2025-04-11T03:52:12.8100146Z >       weight = torch.ones(w_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.8100252Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8100541Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8100676Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8100839Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8100905Z 
2025-04-11T03:52:12.8101085Z tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py:30: RuntimeError
2025-04-11T03:52:12.8101246Z ___________________ test_rotary_emb[True-dtype0-64-32-64-4] ____________________
2025-04-11T03:52:12.8101250Z 
2025-04-11T03:52:12.8101391Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, D = 64, dtype = torch.float32
2025-04-11T03:52:12.8101489Z use_new_kcache_layout = True
2025-04-11T03:52:12.8101494Z 
2025-04-11T03:52:12.8101628Z     @pytest.mark.skipif(
2025-04-11T03:52:12.8101869Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T03:52:12.8101949Z     )
2025-04-11T03:52:12.8102060Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.8102170Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T03:52:12.8102267Z     @pytest.mark.parametrize("H", [32])
2025-04-11T03:52:12.8102364Z     @pytest.mark.parametrize("D", [64])
2025-04-11T03:52:12.8102497Z     @pytest.mark.parametrize("dtype", [torch.float32])
2025-04-11T03:52:12.8102654Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8102833Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, D, dtype, use_new_kcache_layout):
2025-04-11T03:52:12.8102932Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T03:52:12.8103040Z         # our crafted op equals to Transformers
2025-04-11T03:52:12.8103170Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.8103300Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.8103404Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T03:52:12.8103632Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T03:52:12.8103735Z         cos, sin = emb(x0, position_ids)
2025-04-11T03:52:12.8103859Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T03:52:12.8103965Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.8104066Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.8104153Z         cos_2 = cos[:, :32]
2025-04-11T03:52:12.8104243Z         sin_2 = sin[:, :32]
2025-04-11T03:52:12.8104365Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.8104499Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T03:52:12.8104712Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T03:52:12.8104846Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T03:52:12.8104918Z     
2025-04-11T03:52:12.8104998Z         # create data
2025-04-11T03:52:12.8105086Z         block_size = 32
2025-04-11T03:52:12.8105184Z         max_num_blocks_per_seq = 4
2025-04-11T03:52:12.8105282Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.8105429Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.8105534Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8105827Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8105964Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8106127Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8106200Z 
2025-04-11T03:52:12.8106402Z tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py:65: RuntimeError
2025-04-11T03:52:12.8106564Z ___________________ test_rotary_emb[False-dtype0-64-32-64-4] ___________________
2025-04-11T03:52:12.8106568Z 
2025-04-11T03:52:12.8106707Z BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, D = 64, dtype = torch.float32
2025-04-11T03:52:12.8106806Z use_new_kcache_layout = False
2025-04-11T03:52:12.8106811Z 
2025-04-11T03:52:12.8106900Z     @pytest.mark.skipif(
2025-04-11T03:52:12.8107137Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T03:52:12.8107301Z     )
2025-04-11T03:52:12.8107412Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.8107522Z     @pytest.mark.parametrize("SEQ_LEN", [64])
2025-04-11T03:52:12.8107619Z     @pytest.mark.parametrize("H", [32])
2025-04-11T03:52:12.8107719Z     @pytest.mark.parametrize("D", [64])
2025-04-11T03:52:12.8107855Z     @pytest.mark.parametrize("dtype", [torch.float32])
2025-04-11T03:52:12.8108009Z     @pytest.mark.parametrize("use_new_kcache_layout", [True, False])
2025-04-11T03:52:12.8108249Z     def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, D, dtype, use_new_kcache_layout):
2025-04-11T03:52:12.8108346Z         TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
2025-04-11T03:52:12.8108496Z         # our crafted op equals to Transformers
2025-04-11T03:52:12.8108625Z         x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.8108747Z         x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T03:52:12.8108849Z         emb = LlamaRotaryEmbedding(D)
2025-04-11T03:52:12.8109020Z         position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T03:52:12.8109119Z         cos, sin = emb(x0, position_ids)
2025-04-11T03:52:12.8109240Z         embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
2025-04-11T03:52:12.8109342Z         cos = cos.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.8109439Z         sin = sin.reshape((TOTAL_TOKENS, -1))
2025-04-11T03:52:12.8109523Z         cos_2 = cos[:, :32]
2025-04-11T03:52:12.8109614Z         sin_2 = sin[:, :32]
2025-04-11T03:52:12.8109733Z         x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.8109862Z         embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
2025-04-11T03:52:12.8110133Z         embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
2025-04-11T03:52:12.8110263Z         assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T03:52:12.8110335Z     
2025-04-11T03:52:12.8110415Z         # create data
2025-04-11T03:52:12.8110505Z         block_size = 32
2025-04-11T03:52:12.8110600Z         max_num_blocks_per_seq = 4
2025-04-11T03:52:12.8110697Z         q_shape = (TOTAL_TOKENS, H, D)
2025-04-11T03:52:12.8110838Z >       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
2025-04-11T03:52:12.8110945Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8111234Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8111368Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8111529Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8111535Z 
2025-04-11T03:52:12.8111735Z tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py:65: RuntimeError
2025-04-11T03:52:12.8111884Z _____________________ test_get_xine_cache[dtype0-64-64-4] ______________________
2025-04-11T03:52:12.8111887Z 
2025-04-11T03:52:12.8112034Z BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float32
2025-04-11T03:52:12.8112039Z 
2025-04-11T03:52:12.8112135Z     @pytest.mark.skipif(
2025-04-11T03:52:12.8112368Z         not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
2025-04-11T03:52:12.8112440Z     )
2025-04-11T03:52:12.8112617Z     @pytest.mark.parametrize("BATCH_SIZE", [4])
2025-04-11T03:52:12.8112734Z     @pytest.mark.parametrize("MAX_SEQ_LEN", [64])
2025-04-11T03:52:12.8112848Z     @pytest.mark.parametrize("HEAD_DIM", [64])
2025-04-11T03:52:12.8112975Z     @pytest.mark.parametrize("dtype", [torch.float32])
2025-04-11T03:52:12.8113125Z     def test_get_xine_cache(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
2025-04-11T03:52:12.8113233Z         MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
2025-04-11T03:52:12.8113415Z >       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
2025-04-11T03:52:12.8113522Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8113881Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8114019Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8114179Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8114186Z 
2025-04-11T03:52:12.8114361Z tests/test_infer/test_kernels/triton/test_xine_copy.py:50: RuntimeError
2025-04-11T03:52:12.8114577Z _____________________ test_models_lazy_init[cuda-subset0] ______________________
2025-04-11T03:52:12.8114581Z 
2025-04-11T03:52:12.8114993Z subset = ['custom_hanging_param_model', 'custom_nested_model', 'custom_repeated_computed_layers', 'custom_simple_net', 'diffusers_clip_text_model', 'diffusers_auto_encoder_kl', ...]
2025-04-11T03:52:12.8115086Z default_device = 'cuda'
2025-04-11T03:52:12.8115090Z 
2025-04-11T03:52:12.8115258Z     @pytest.mark.skipif(not SUPPORT_LAZY, reason="requires torch >= 1.12.0")
2025-04-11T03:52:12.8115363Z     @pytest.mark.parametrize(
2025-04-11T03:52:12.8115439Z         "subset",
2025-04-11T03:52:12.8115518Z         (
2025-04-11T03:52:12.8115601Z             [COMMON_MODELS]
2025-04-11T03:52:12.8115685Z             if IS_FAST_TEST
2025-04-11T03:52:12.8115887Z             else ["torchvision", "diffusers", "timm", "transformers", "torchaudio", "deepfm", "dlrm"]
2025-04-11T03:52:12.8115964Z         ),
2025-04-11T03:52:12.8116040Z     )
2025-04-11T03:52:12.8116184Z     @pytest.mark.parametrize("default_device", ["cpu", "cuda"])
2025-04-11T03:52:12.8116308Z     def test_models_lazy_init(subset, default_device):
2025-04-11T03:52:12.8116466Z         sub_model_zoo = model_zoo.get_sub_registry(subset, allow_empty=True)
2025-04-11T03:52:12.8116630Z         for name, entry in sub_model_zoo.items():
2025-04-11T03:52:12.8116804Z             # TODO(ver217): lazy init does not support weight norm, skip these models
2025-04-11T03:52:12.8116885Z             if name in (
2025-04-11T03:52:12.8116986Z                 "torchaudio_wav2vec2_base",
2025-04-11T03:52:12.8117079Z                 "torchaudio_hubert_base",
2025-04-11T03:52:12.8117167Z                 "timm_beit",
2025-04-11T03:52:12.8117262Z                 "timm_vision_transformer",
2025-04-11T03:52:12.8117344Z                 "timm_deit",
2025-04-11T03:52:12.8117431Z                 "timm_beitv2",
2025-04-11T03:52:12.8117515Z                 "timm_deit3",
2025-04-11T03:52:12.8117599Z                 "timm_convit",
2025-04-11T03:52:12.8117691Z                 "timm_tnt_b_patch16_224",
2025-04-11T03:52:12.8117905Z             ) or name.startswith(("transformers_vit", "transformers_blip2", "transformers_whisper")):
2025-04-11T03:52:12.8117992Z                 continue
2025-04-11T03:52:12.8118151Z >           check_lazy_init(entry, verbose=True, default_device=default_device)
2025-04-11T03:52:12.8118156Z 
2025-04-11T03:52:12.8118257Z tests/test_lazy/test_models.py:33: 
2025-04-11T03:52:12.8118368Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8118511Z tests/test_lazy/lazy_init_utils.py:77: in check_lazy_init
2025-04-11T03:52:12.8118596Z     model = model_fn()
2025-04-11T03:52:12.8118760Z tests/kit/model_zoo/custom/hanging_param_model.py:17: in __init__
2025-04-11T03:52:12.8118858Z     self.proj1 = nn.Linear(4, 8)
2025-04-11T03:52:12.8119176Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/linear.py:98: in __init__
2025-04-11T03:52:12.8119380Z     self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
2025-04-11T03:52:12.8119491Z colossalai/lazy/lazy_init.py:506: in wrapper
2025-04-11T03:52:12.8119608Z     return self.tensor_cls(target, *args, **kwargs)
2025-04-11T03:52:12.8119718Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8119723Z 
2025-04-11T03:52:12.8119846Z cls = <class 'colossalai.lazy.lazy_init._MyTensor'>
2025-04-11T03:52:12.8119998Z func = <built-in method empty of type object at 0x7f6bf0606840>
2025-04-11T03:52:12.8120159Z concrete_data = None, args = ((8, 4),)
2025-04-11T03:52:12.8120267Z kwargs = {'device': 'cuda', 'dtype': None}
2025-04-11T03:52:12.8120272Z 
2025-04-11T03:52:12.8120437Z     def __new__(cls, func, *args, concrete_data=None, **kwargs) -> "_MyTensor":
2025-04-11T03:52:12.8120525Z         cls._pre_op_fn()
2025-04-11T03:52:12.8120619Z         if concrete_data is not None:
2025-04-11T03:52:12.8120714Z             # uniform api as LazyTensor
2025-04-11T03:52:12.8120855Z             data = concrete_data
2025-04-11T03:52:12.8120932Z         else:
2025-04-11T03:52:12.8121041Z             kwargs["device"] = cls.default_device
2025-04-11T03:52:12.8121133Z >           data = func(*args, **kwargs)
2025-04-11T03:52:12.8121247Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8121534Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8121672Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8121839Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8121843Z 
2025-04-11T03:52:12.8121952Z colossalai/lazy/lazy_init.py:93: RuntimeError
2025-04-11T03:52:12.8122089Z ________________________________ test_lazy_ops _________________________________
2025-04-11T03:52:12.8122095Z 
2025-04-11T03:52:12.8122263Z     @pytest.mark.skipif(not SUPPORT_LAZY, reason="requires torch >= 1.12.0")
2025-04-11T03:52:12.8122355Z     def test_lazy_ops():
2025-04-11T03:52:12.8122445Z         with LazyInitContext():
2025-04-11T03:52:12.8122534Z             x = torch.rand(2, 3)
2025-04-11T03:52:12.8122684Z             assert tuple(x.shape) == (2, 3)
2025-04-11T03:52:12.8122779Z             assert x.device.type == "cpu"
2025-04-11T03:52:12.8122874Z             x.requires_grad is False
2025-04-11T03:52:12.8122954Z             y = x.cuda()
2025-04-11T03:52:12.8123051Z             assert tuple(y.shape) == (2, 3)
2025-04-11T03:52:12.8123147Z             assert y.device.type == "cuda"
2025-04-11T03:52:12.8123240Z             assert y.requires_grad is False
2025-04-11T03:52:12.8123331Z             assert x.cpu() is x
2025-04-11T03:52:12.8123432Z             p = Parameter(torch.empty(2, 3))
2025-04-11T03:52:12.8123529Z             assert tuple(p.shape) == (2, 3)
2025-04-11T03:52:12.8123624Z             assert p.device.type == "cpu"
2025-04-11T03:52:12.8123721Z             assert p.requires_grad is True
2025-04-11T03:52:12.8123819Z             assert isinstance(p, Parameter)
2025-04-11T03:52:12.8123902Z         x.materialize()
2025-04-11T03:52:12.8123998Z         assert tuple(x.shape) == (2, 3)
2025-04-11T03:52:12.8124093Z         assert x.device.type == "cpu"
2025-04-11T03:52:12.8124192Z         assert x.requires_grad is False
2025-04-11T03:52:12.8124273Z >       y.materialize()
2025-04-11T03:52:12.8124278Z 
2025-04-11T03:52:12.8124369Z tests/test_lazy/test_ops.py:33: 
2025-04-11T03:52:12.8124486Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8124605Z colossalai/lazy/lazy_init.py:217: in materialize
2025-04-11T03:52:12.8124704Z     target = self._materialize_data()
2025-04-11T03:52:12.8124834Z colossalai/lazy/lazy_init.py:242: in _materialize_data
2025-04-11T03:52:12.8124918Z     init_val = func(
2025-04-11T03:52:12.8125086Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8125091Z 
2025-04-11T03:52:12.8125178Z t = tensor([[0.8823, 0.9150, 0.3829],
2025-04-11T03:52:12.8125267Z         [0.9593, 0.3904, 0.6009]])
2025-04-11T03:52:12.8125363Z kw = {'device': device(type='cuda')}
2025-04-11T03:52:12.8125367Z 
2025-04-11T03:52:12.8125477Z     def factory_fn(t: torch.Tensor, **kw):
2025-04-11T03:52:12.8125568Z >       return t.to(*args, **kwargs)
2025-04-11T03:52:12.8125683Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8125977Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8126173Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8126335Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8126340Z 
2025-04-11T03:52:12.8126451Z colossalai/lazy/lazy_init.py:380: RuntimeError
2025-04-11T03:52:12.8126593Z _____________________________ test_torch_ddp_lora ______________________________
2025-04-11T03:52:12.8126598Z 
2025-04-11T03:52:12.8126734Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8127343Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8127353Z 
2025-04-11T03:52:12.8127453Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8127539Z         try_count = 0
2025-04-11T03:52:12.8127641Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8127720Z             max_try, int
2025-04-11T03:52:12.8127887Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8127985Z     
2025-04-11T03:52:12.8128132Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8128217Z             try:
2025-04-11T03:52:12.8128312Z                 try_count += 1
2025-04-11T03:52:12.8128404Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8128487Z                 return ret
2025-04-11T03:52:12.8128586Z             except exception_type as e:
2025-04-11T03:52:12.8128685Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8128947Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8129068Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8129213Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8129373Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8129455Z                     continue
2025-04-11T03:52:12.8129535Z                 else:
2025-04-11T03:52:12.8129756Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8129841Z >                   raise e
2025-04-11T03:52:12.8129846Z 
2025-04-11T03:52:12.8129942Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8130052Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8130186Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8130272Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8130411Z tests/test_lora/test_lora.py:108: in test_torch_ddp_lora
2025-04-11T03:52:12.8130493Z     spawn(run_dist, 2)
2025-04-11T03:52:12.8130595Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8130698Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8130956Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8131138Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8131423Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8131591Z     while not context.join():
2025-04-11T03:52:12.8131699Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8131706Z 
2025-04-11T03:52:12.8131903Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be3c5210>
2025-04-11T03:52:12.8131986Z timeout = None
2025-04-11T03:52:12.8131990Z 
2025-04-11T03:52:12.8132084Z     def join(self, timeout=None):
2025-04-11T03:52:12.8132209Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8132282Z     
2025-04-11T03:52:12.8132430Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8132638Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8132805Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8132898Z         of the first process exiting.
2025-04-11T03:52:12.8132969Z     
2025-04-11T03:52:12.8133120Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8133256Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8133390Z     
2025-04-11T03:52:12.8133466Z         Args:
2025-04-11T03:52:12.8133604Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8133681Z         """
2025-04-11T03:52:12.8133821Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8133918Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8133999Z             return True
2025-04-11T03:52:12.8134077Z     
2025-04-11T03:52:12.8134210Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8134329Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8134426Z             self.sentinels.keys(),
2025-04-11T03:52:12.8134511Z             timeout=timeout,
2025-04-11T03:52:12.8134586Z         )
2025-04-11T03:52:12.8134659Z     
2025-04-11T03:52:12.8134744Z         error_index = None
2025-04-11T03:52:12.8134833Z         for sentinel in ready:
2025-04-11T03:52:12.8134942Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8135045Z             process = self.processes[index]
2025-04-11T03:52:12.8135132Z             process.join()
2025-04-11T03:52:12.8135283Z             if process.exitcode != 0:
2025-04-11T03:52:12.8135372Z                 error_index = index
2025-04-11T03:52:12.8135448Z                 break
2025-04-11T03:52:12.8135525Z     
2025-04-11T03:52:12.8135614Z         # Return if there was no error.
2025-04-11T03:52:12.8135703Z         if error_index is None:
2025-04-11T03:52:12.8135839Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8135937Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8136012Z     
2025-04-11T03:52:12.8136151Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8136258Z         for process in self.processes:
2025-04-11T03:52:12.8136348Z             if process.is_alive():
2025-04-11T03:52:12.8136446Z                 process.terminate()
2025-04-11T03:52:12.8136532Z             process.join()
2025-04-11T03:52:12.8136602Z     
2025-04-11T03:52:12.8136744Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8136862Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8136974Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8137095Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8137177Z             if exitcode < 0:
2025-04-11T03:52:12.8137290Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8137396Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8137552Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8137650Z                     error_index=error_index,
2025-04-11T03:52:12.8137819Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8137908Z                     exit_code=exitcode,
2025-04-11T03:52:12.8137998Z                     signal_name=name,
2025-04-11T03:52:12.8138077Z                 )
2025-04-11T03:52:12.8138151Z             else:
2025-04-11T03:52:12.8138258Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8138425Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8138522Z                     error_index=error_index,
2025-04-11T03:52:12.8138624Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8138851Z                     exit_code=exitcode,
2025-04-11T03:52:12.8138935Z                 )
2025-04-11T03:52:12.8139006Z     
2025-04-11T03:52:12.8139145Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8139318Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8139406Z         msg += original_trace
2025-04-11T03:52:12.8139582Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8139797Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8139875Z E       
2025-04-11T03:52:12.8140004Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8140107Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8140417Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8140500Z E           fn(i, *args)
2025-04-11T03:52:12.8140721Z E         File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 103, in run_dist
2025-04-11T03:52:12.8140807Z E           run_lora_test()
2025-04-11T03:52:12.8141029Z E         File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 98, in run_lora_test
2025-04-11T03:52:12.8141208Z E           check_fwd_bwd(model_fn, data_gen_fn, output_transform_fn, loss_fn, task_type)
2025-04-11T03:52:12.8141430Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.8141532Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.8141788Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.8141954Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.8142241Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.8142350Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8142457Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8142744Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8142880Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8143042Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8143046Z 
2025-04-11T03:52:12.8143356Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8143515Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8143679Z [04/11/25 03:46:35] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8143808Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8143919Z                              :75 launch                                         
2025-04-11T03:52:12.8144058Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8144187Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8144382Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8144587Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8145727Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8145904Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8147063Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8147234Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8147989Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8148079Z   warnings.warn(
2025-04-11T03:52:12.8148800Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8148889Z   warnings.warn(
2025-04-11T03:52:12.8149715Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8149799Z   warnings.warn(
2025-04-11T03:52:12.8150596Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8150750Z   warnings.warn(
2025-04-11T03:52:12.8151557Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8151640Z   warnings.warn(
2025-04-11T03:52:12.8152433Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8152516Z   warnings.warn(
2025-04-11T03:52:12.8153312Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8153393Z   warnings.warn(
2025-04-11T03:52:12.8154199Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8154340Z   warnings.warn(
2025-04-11T03:52:12.8154893Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8154978Z   warnings.warn(
2025-04-11T03:52:12.8155509Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8155657Z   warnings.warn(
2025-04-11T03:52:12.8156205Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8156290Z   warnings.warn(
2025-04-11T03:52:12.8156826Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8156989Z   warnings.warn(
2025-04-11T03:52:12.8157137Z _________________________ test_moe_kernel[data_type0] __________________________
2025-04-11T03:52:12.8157145Z 
2025-04-11T03:52:12.8157240Z data_type = torch.float32
2025-04-11T03:52:12.8157245Z 
2025-04-11T03:52:12.8157418Z     @pytest.mark.parametrize("data_type", [torch.float32, torch.float16])
2025-04-11T03:52:12.8157514Z     def test_moe_kernel(data_type):
2025-04-11T03:52:12.8157612Z         torch.manual_seed(1024)
2025-04-11T03:52:12.8157696Z >       run_moe_cumsum()
2025-04-11T03:52:12.8157701Z 
2025-04-11T03:52:12.8157802Z tests/test_moe/test_kernel.py:93: 
2025-04-11T03:52:12.8157915Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8157922Z 
2025-04-11T03:52:12.8158009Z     def run_moe_cumsum():
2025-04-11T03:52:12.8158099Z         test_mask = torch.tensor(
2025-04-11T03:52:12.8158174Z             [
2025-04-11T03:52:12.8158260Z                 [0, 1, 0, 0],
2025-04-11T03:52:12.8158340Z                 [1, 0, 0, 0],
2025-04-11T03:52:12.8158419Z                 [0, 1, 0, 0],
2025-04-11T03:52:12.8158552Z                 [1, 0, 0, 0],
2025-04-11T03:52:12.8158631Z             ],
2025-04-11T03:52:12.8158722Z             dtype=torch.int32,
2025-04-11T03:52:12.8158803Z >       ).to("cuda")
2025-04-11T03:52:12.8158917Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8159209Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8159352Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8159518Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8159525Z 
2025-04-11T03:52:12.8159641Z tests/test_moe/test_kernel.py:29: RuntimeError
2025-04-11T03:52:12.8159782Z _________________________ test_moe_kernel[data_type1] __________________________
2025-04-11T03:52:12.8159789Z 
2025-04-11T03:52:12.8159877Z data_type = torch.float16
2025-04-11T03:52:12.8159881Z 
2025-04-11T03:52:12.8160058Z     @pytest.mark.parametrize("data_type", [torch.float32, torch.float16])
2025-04-11T03:52:12.8160154Z     def test_moe_kernel(data_type):
2025-04-11T03:52:12.8160247Z         torch.manual_seed(1024)
2025-04-11T03:52:12.8160330Z >       run_moe_cumsum()
2025-04-11T03:52:12.8160334Z 
2025-04-11T03:52:12.8160436Z tests/test_moe/test_kernel.py:93: 
2025-04-11T03:52:12.8160549Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8160553Z 
2025-04-11T03:52:12.8160635Z     def run_moe_cumsum():
2025-04-11T03:52:12.8160728Z         test_mask = torch.tensor(
2025-04-11T03:52:12.8160804Z             [
2025-04-11T03:52:12.8160898Z                 [0, 1, 0, 0],
2025-04-11T03:52:12.8161032Z                 [1, 0, 0, 0],
2025-04-11T03:52:12.8161115Z                 [0, 1, 0, 0],
2025-04-11T03:52:12.8161195Z                 [1, 0, 0, 0],
2025-04-11T03:52:12.8161274Z             ],
2025-04-11T03:52:12.8161373Z             dtype=torch.int32,
2025-04-11T03:52:12.8161454Z >       ).to("cuda")
2025-04-11T03:52:12.8161572Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8161855Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8161997Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8162226Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8162231Z 
2025-04-11T03:52:12.8162342Z tests/test_moe/test_kernel.py:29: RuntimeError
2025-04-11T03:52:12.8162489Z __________________________ test_mixtral_moe_layer[4] ___________________________
2025-04-11T03:52:12.8162495Z 
2025-04-11T03:52:12.8162571Z world_size = 4
2025-04-11T03:52:12.8162575Z 
2025-04-11T03:52:12.8162691Z     @pytest.mark.parametrize("world_size", [4])
2025-04-11T03:52:12.8162849Z     def test_mixtral_moe_layer(world_size: int):
2025-04-11T03:52:12.8162943Z >       spawn(run_dist, world_size)
2025-04-11T03:52:12.8162948Z 
2025-04-11T03:52:12.8163054Z tests/test_moe/test_moe_checkpoint.py:171: 
2025-04-11T03:52:12.8163166Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8163270Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8163372Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8163635Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8163814Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8164104Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8164196Z     while not context.join():
2025-04-11T03:52:12.8164303Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8164309Z 
2025-04-11T03:52:12.8164512Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be4edd20>
2025-04-11T03:52:12.8164590Z timeout = None
2025-04-11T03:52:12.8164652Z 
2025-04-11T03:52:12.8164747Z     def join(self, timeout=None):
2025-04-11T03:52:12.8164875Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8164949Z     
2025-04-11T03:52:12.8165093Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8165241Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8165408Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8165502Z         of the first process exiting.
2025-04-11T03:52:12.8165578Z     
2025-04-11T03:52:12.8165726Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8165867Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8165941Z     
2025-04-11T03:52:12.8166016Z         Args:
2025-04-11T03:52:12.8166158Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8166232Z         """
2025-04-11T03:52:12.8166379Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8166473Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8166553Z             return True
2025-04-11T03:52:12.8166627Z     
2025-04-11T03:52:12.8166758Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8166887Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8166979Z             self.sentinels.keys(),
2025-04-11T03:52:12.8167068Z             timeout=timeout,
2025-04-11T03:52:12.8167142Z         )
2025-04-11T03:52:12.8167213Z     
2025-04-11T03:52:12.8167300Z         error_index = None
2025-04-11T03:52:12.8167447Z         for sentinel in ready:
2025-04-11T03:52:12.8167563Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8167664Z             process = self.processes[index]
2025-04-11T03:52:12.8167750Z             process.join()
2025-04-11T03:52:12.8167849Z             if process.exitcode != 0:
2025-04-11T03:52:12.8167937Z                 error_index = index
2025-04-11T03:52:12.8168018Z                 break
2025-04-11T03:52:12.8168089Z     
2025-04-11T03:52:12.8168180Z         # Return if there was no error.
2025-04-11T03:52:12.8168271Z         if error_index is None:
2025-04-11T03:52:12.8168403Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8168562Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8168634Z     
2025-04-11T03:52:12.8168777Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8168880Z         for process in self.processes:
2025-04-11T03:52:12.8168975Z             if process.is_alive():
2025-04-11T03:52:12.8169069Z                 process.terminate()
2025-04-11T03:52:12.8169154Z             process.join()
2025-04-11T03:52:12.8169285Z     
2025-04-11T03:52:12.8169430Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8169550Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8169668Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8169791Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8169879Z             if exitcode < 0:
2025-04-11T03:52:12.8169991Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8170104Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8170261Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8170360Z                     error_index=error_index,
2025-04-11T03:52:12.8170466Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8170559Z                     exit_code=exitcode,
2025-04-11T03:52:12.8170654Z                     signal_name=name,
2025-04-11T03:52:12.8170729Z                 )
2025-04-11T03:52:12.8170805Z             else:
2025-04-11T03:52:12.8170915Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8171081Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8171237Z                     error_index=error_index,
2025-04-11T03:52:12.8171337Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8171427Z                     exit_code=exitcode,
2025-04-11T03:52:12.8171503Z                 )
2025-04-11T03:52:12.8171577Z     
2025-04-11T03:52:12.8171715Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8171888Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8171978Z         msg += original_trace
2025-04-11T03:52:12.8172153Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8172318Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8172399Z E       
2025-04-11T03:52:12.8172525Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8172627Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8172935Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8173022Z E           fn(i, *args)
2025-04-11T03:52:12.8173256Z E         File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 165, in run_dist
2025-04-11T03:52:12.8173352Z E           check_moe_checkpoint()
2025-04-11T03:52:12.8173613Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8173701Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8173967Z E         File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 101, in check_moe_checkpoint
2025-04-11T03:52:12.8174158Z E           dist.broadcast_object_list(broadcast_objects, src=0)
2025-04-11T03:52:12.8174462Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:12.8174557Z E           return func(*args, **kwargs)
2025-04-11T03:52:12.8174917Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in broadcast_object_list
2025-04-11T03:52:12.8175136Z E           tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T03:52:12.8175531Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in <listcomp>
2025-04-11T03:52:12.8175744Z E           tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T03:52:12.8176094Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2115, in _object_to_tensor
2025-04-11T03:52:12.8176293Z E           byte_tensor = torch.ByteTensor(byte_storage).to(device)
2025-04-11T03:52:12.8176404Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8176687Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8176828Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8176987Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8176997Z 
2025-04-11T03:52:12.8177300Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8177455Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8177620Z [04/11/25 03:46:41] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8177750Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8177863Z                              :75 launch                                         
2025-04-11T03:52:12.8178001Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8178189Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8178338Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8178635Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:27296 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8178925Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:27296 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8179097Z _____________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False] ______________
2025-04-11T03:52:12.8179103Z 
2025-04-11T03:52:12.8179250Z adamw = False, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T03:52:12.8179342Z g_dtype = torch.float16
2025-04-11T03:52:12.8179348Z 
2025-04-11T03:52:12.8179479Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8179613Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8179794Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8179956Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8180051Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8180193Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8180289Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8180437Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8180527Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8180620Z >       check_adam_kernel(
2025-04-11T03:52:12.8180937Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8181013Z         )
2025-04-11T03:52:12.8181018Z 
2025-04-11T03:52:12.8181137Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8181247Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8181413Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8181564Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8181679Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8181756Z 
2025-04-11T03:52:12.8181940Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be3c54b0>, lr = 0.001
2025-04-11T03:52:12.8182100Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T03:52:12.8182108Z 
2025-04-11T03:52:12.8182349Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8182499Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8182727Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8182799Z     
2025-04-11T03:52:12.8182920Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8183039Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8183289Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8183401Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8183681Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8183819Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8183975Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8183981Z 
2025-04-11T03:52:12.8184125Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8184299Z ______________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True] ______________
2025-04-11T03:52:12.8184303Z 
2025-04-11T03:52:12.8184439Z adamw = True, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T03:52:12.8184585Z g_dtype = torch.float16
2025-04-11T03:52:12.8184590Z 
2025-04-11T03:52:12.8184718Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8184846Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8185016Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8185174Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8185265Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8185400Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8185489Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8185629Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8185716Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8185801Z >       check_adam_kernel(
2025-04-11T03:52:12.8186061Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8186137Z         )
2025-04-11T03:52:12.8186141Z 
2025-04-11T03:52:12.8186258Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8186371Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8186532Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8186680Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8186789Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8186849Z 
2025-04-11T03:52:12.8187033Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f688e8e6650>, lr = 0.001
2025-04-11T03:52:12.8187188Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T03:52:12.8187194Z 
2025-04-11T03:52:12.8187437Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8187579Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8187748Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8187821Z     
2025-04-11T03:52:12.8187933Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8188120Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8188366Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8188524Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8188810Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8189014Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8189170Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8189177Z 
2025-04-11T03:52:12.8189315Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8189478Z _____________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False] ______________
2025-04-11T03:52:12.8189482Z 
2025-04-11T03:52:12.8189611Z adamw = False, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T03:52:12.8189702Z g_dtype = torch.float16
2025-04-11T03:52:12.8189707Z 
2025-04-11T03:52:12.8189826Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8189955Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8190122Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8190276Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8190364Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8190497Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8190586Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8190784Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8190873Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8190957Z >       check_adam_kernel(
2025-04-11T03:52:12.8191217Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8191292Z         )
2025-04-11T03:52:12.8191297Z 
2025-04-11T03:52:12.8191408Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8191524Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8191682Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8191834Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8191943Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8191948Z 
2025-04-11T03:52:12.8192126Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be78d090>, lr = 0.001
2025-04-11T03:52:12.8192278Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T03:52:12.8192282Z 
2025-04-11T03:52:12.8192525Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8192666Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8192831Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8192907Z     
2025-04-11T03:52:12.8193017Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8193210Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8193454Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8193567Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8193848Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8193983Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8194142Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8194147Z 
2025-04-11T03:52:12.8194345Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8194514Z ______________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True] ______________
2025-04-11T03:52:12.8194519Z 
2025-04-11T03:52:12.8194648Z adamw = True, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T03:52:12.8194738Z g_dtype = torch.float16
2025-04-11T03:52:12.8194744Z 
2025-04-11T03:52:12.8194865Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8194996Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8195228Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8195383Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8195474Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8195604Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8195693Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8195826Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8195914Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8195997Z >       check_adam_kernel(
2025-04-11T03:52:12.8196250Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8196330Z         )
2025-04-11T03:52:12.8196335Z 
2025-04-11T03:52:12.8196450Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8196567Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8196724Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8196875Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8197056Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8197061Z 
2025-04-11T03:52:12.8197237Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be3374f0>, lr = 0.001
2025-04-11T03:52:12.8197395Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T03:52:12.8197400Z 
2025-04-11T03:52:12.8197637Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8197781Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8197948Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8198022Z     
2025-04-11T03:52:12.8198134Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8198253Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8198499Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8198606Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8198893Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8199028Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8199190Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8199194Z 
2025-04-11T03:52:12.8199330Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8199550Z _____________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False] ______________
2025-04-11T03:52:12.8199554Z 
2025-04-11T03:52:12.8199687Z adamw = False, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T03:52:12.8199774Z g_dtype = torch.float32
2025-04-11T03:52:12.8199783Z 
2025-04-11T03:52:12.8199903Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8200031Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8200205Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8200357Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8200518Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8200649Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8200739Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8200874Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8200969Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8201059Z >       check_adam_kernel(
2025-04-11T03:52:12.8201312Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8201445Z         )
2025-04-11T03:52:12.8201450Z 
2025-04-11T03:52:12.8201564Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8201676Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8201835Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8201980Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8202094Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8202099Z 
2025-04-11T03:52:12.8202274Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be7cdb40>, lr = 0.001
2025-04-11T03:52:12.8202425Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T03:52:12.8202432Z 
2025-04-11T03:52:12.8202666Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8202812Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8202973Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8203101Z     
2025-04-11T03:52:12.8203215Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8203331Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8203572Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8203682Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8203966Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8204102Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8204258Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8204269Z 
2025-04-11T03:52:12.8204403Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8204568Z ______________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True] ______________
2025-04-11T03:52:12.8204574Z 
2025-04-11T03:52:12.8204704Z adamw = True, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T03:52:12.8204792Z g_dtype = torch.float32
2025-04-11T03:52:12.8204797Z 
2025-04-11T03:52:12.8204923Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8205051Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8205222Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8205373Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8205458Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8205662Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8205748Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8205888Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8205975Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8206058Z >       check_adam_kernel(
2025-04-11T03:52:12.8206315Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8206389Z         )
2025-04-11T03:52:12.8206393Z 
2025-04-11T03:52:12.8206508Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8206685Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8206845Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8206991Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8207104Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8207110Z 
2025-04-11T03:52:12.8207283Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f688ee27d30>, lr = 0.001
2025-04-11T03:52:12.8207492Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T03:52:12.8207500Z 
2025-04-11T03:52:12.8207740Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8207878Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8208047Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8208120Z     
2025-04-11T03:52:12.8208233Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8208350Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8208590Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8208699Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8208980Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8209118Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8209334Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8209338Z 
2025-04-11T03:52:12.8209480Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8209643Z _____________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False] ______________
2025-04-11T03:52:12.8209649Z 
2025-04-11T03:52:12.8209784Z adamw = False, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T03:52:12.8209873Z g_dtype = torch.float32
2025-04-11T03:52:12.8209877Z 
2025-04-11T03:52:12.8209997Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8210126Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8210294Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8210450Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8210538Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8210673Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8210762Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8210896Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8210986Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8211071Z >       check_adam_kernel(
2025-04-11T03:52:12.8211329Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8211403Z         )
2025-04-11T03:52:12.8211407Z 
2025-04-11T03:52:12.8211523Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8211634Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8211849Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8212000Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8212110Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8212116Z 
2025-04-11T03:52:12.8212298Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be71b100>, lr = 0.001
2025-04-11T03:52:12.8212449Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T03:52:12.8212453Z 
2025-04-11T03:52:12.8212691Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8212890Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8213060Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8213132Z     
2025-04-11T03:52:12.8213245Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8213366Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8213658Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8213772Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8214054Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8214194Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8214350Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8214357Z 
2025-04-11T03:52:12.8214495Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8214660Z ______________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True] ______________
2025-04-11T03:52:12.8214664Z 
2025-04-11T03:52:12.8214791Z adamw = True, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T03:52:12.8214885Z g_dtype = torch.float32
2025-04-11T03:52:12.8214890Z 
2025-04-11T03:52:12.8215011Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8215138Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8215302Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8215517Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8215604Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8215734Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8215825Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8215961Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8216049Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8216132Z >       check_adam_kernel(
2025-04-11T03:52:12.8216382Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8216461Z         )
2025-04-11T03:52:12.8216466Z 
2025-04-11T03:52:12.8216577Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8216693Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8216848Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8217001Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8217108Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8217112Z 
2025-04-11T03:52:12.8217292Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f688e8e6320>, lr = 0.001
2025-04-11T03:52:12.8217442Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T03:52:12.8217447Z 
2025-04-11T03:52:12.8217683Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8217885Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8218051Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8218130Z     
2025-04-11T03:52:12.8218244Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8218370Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8218614Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8218723Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8219011Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8219206Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8219370Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8219377Z 
2025-04-11T03:52:12.8219513Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8219681Z _____________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False] ______________
2025-04-11T03:52:12.8219729Z 
2025-04-11T03:52:12.8219862Z adamw = False, weight_decay = 0.0, p_dtype = torch.float16
2025-04-11T03:52:12.8219954Z g_dtype = torch.float16
2025-04-11T03:52:12.8219960Z 
2025-04-11T03:52:12.8220080Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8220206Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8220373Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8220523Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8220616Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8220745Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8220837Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8220972Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8221056Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8221147Z >       check_adam_kernel(
2025-04-11T03:52:12.8221397Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8221532Z         )
2025-04-11T03:52:12.8221537Z 
2025-04-11T03:52:12.8221653Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8221766Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8221920Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8222071Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8222183Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8222187Z 
2025-04-11T03:52:12.8222366Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be35cb20>, lr = 0.001
2025-04-11T03:52:12.8222520Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T03:52:12.8222525Z 
2025-04-11T03:52:12.8222761Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8222902Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8223067Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8223144Z     
2025-04-11T03:52:12.8223255Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8223374Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8223620Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8223727Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8224011Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8224207Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8224372Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8224377Z 
2025-04-11T03:52:12.8224511Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8224674Z ______________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True] ______________
2025-04-11T03:52:12.8224682Z 
2025-04-11T03:52:12.8224808Z adamw = True, weight_decay = 0.0, p_dtype = torch.float16
2025-04-11T03:52:12.8224896Z g_dtype = torch.float16
2025-04-11T03:52:12.8224962Z 
2025-04-11T03:52:12.8225087Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8225212Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8225378Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8225526Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8225619Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8225748Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8225889Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8226026Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8226114Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8226201Z >       check_adam_kernel(
2025-04-11T03:52:12.8226454Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8226532Z         )
2025-04-11T03:52:12.8226538Z 
2025-04-11T03:52:12.8226652Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8226760Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8226917Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8227060Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8227171Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8227177Z 
2025-04-11T03:52:12.8227349Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be334520>, lr = 0.001
2025-04-11T03:52:12.8227503Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T03:52:12.8227580Z 
2025-04-11T03:52:12.8227819Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8227961Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8228126Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8228200Z     
2025-04-11T03:52:12.8228316Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8228479Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8228785Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8228899Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8229181Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8229320Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8229477Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8229481Z 
2025-04-11T03:52:12.8229624Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8229786Z _____________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False] ______________
2025-04-11T03:52:12.8229790Z 
2025-04-11T03:52:12.8229924Z adamw = False, weight_decay = 0.1, p_dtype = torch.float16
2025-04-11T03:52:12.8230012Z g_dtype = torch.float16
2025-04-11T03:52:12.8230017Z 
2025-04-11T03:52:12.8230140Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8230341Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8230507Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8230661Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8230749Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8230884Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8230969Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8231104Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8231189Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8231342Z >       check_adam_kernel(
2025-04-11T03:52:12.8231601Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8231676Z         )
2025-04-11T03:52:12.8231680Z 
2025-04-11T03:52:12.8231800Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8231914Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8232074Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8232279Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8232385Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8232394Z 
2025-04-11T03:52:12.8232567Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be30af50>, lr = 0.001
2025-04-11T03:52:12.8232715Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T03:52:12.8232720Z 
2025-04-11T03:52:12.8232958Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8233096Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8233259Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8233332Z     
2025-04-11T03:52:12.8233450Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8233569Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8233808Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8233987Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8234265Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8234401Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8234562Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8234567Z 
2025-04-11T03:52:12.8234705Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8234867Z ______________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True] ______________
2025-04-11T03:52:12.8234873Z 
2025-04-11T03:52:12.8235004Z adamw = True, weight_decay = 0.1, p_dtype = torch.float16
2025-04-11T03:52:12.8235092Z g_dtype = torch.float16
2025-04-11T03:52:12.8235096Z 
2025-04-11T03:52:12.8235214Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8235344Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8235513Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8235668Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8235754Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8235890Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8235978Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8236109Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8236199Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8236282Z >       check_adam_kernel(
2025-04-11T03:52:12.8236598Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8236676Z         )
2025-04-11T03:52:12.8236680Z 
2025-04-11T03:52:12.8236799Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8236907Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8237063Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8237211Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8237319Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8237382Z 
2025-04-11T03:52:12.8237560Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be71b100>, lr = 0.001
2025-04-11T03:52:12.8237716Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T03:52:12.8237721Z 
2025-04-11T03:52:12.8237964Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8238103Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8238325Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8238397Z     
2025-04-11T03:52:12.8238510Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8238631Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8238873Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8238984Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8239259Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8239394Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8239553Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8239557Z 
2025-04-11T03:52:12.8239692Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8239858Z _____________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False] ______________
2025-04-11T03:52:12.8239862Z 
2025-04-11T03:52:12.8240092Z adamw = False, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T03:52:12.8240185Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8240190Z 
2025-04-11T03:52:12.8240309Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8240438Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8240604Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8240753Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8240846Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8240974Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8241066Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8241200Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8241290Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8241373Z >       check_adam_kernel(
2025-04-11T03:52:12.8241620Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8241700Z         )
2025-04-11T03:52:12.8241703Z 
2025-04-11T03:52:12.8241816Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8241929Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8242083Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8242232Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8242337Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8242484Z 
2025-04-11T03:52:12.8242665Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be30a350>, lr = 0.001
2025-04-11T03:52:12.8242814Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T03:52:12.8242820Z 
2025-04-11T03:52:12.8243059Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8243200Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8243359Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8243436Z     
2025-04-11T03:52:12.8243545Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8243732Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8243974Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8244083Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8244367Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8244556Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8244718Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8244725Z 
2025-04-11T03:52:12.8244865Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8245027Z ______________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True] ______________
2025-04-11T03:52:12.8245032Z 
2025-04-11T03:52:12.8245162Z adamw = True, weight_decay = 0.0, p_dtype = torch.float32
2025-04-11T03:52:12.8245253Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8245257Z 
2025-04-11T03:52:12.8245380Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8245506Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8245677Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8245829Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8245921Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8246051Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8246140Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8246332Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8246417Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8246505Z >       check_adam_kernel(
2025-04-11T03:52:12.8246755Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8246833Z         )
2025-04-11T03:52:12.8246837Z 
2025-04-11T03:52:12.8246949Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8247063Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8247219Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8247368Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8247481Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8247485Z 
2025-04-11T03:52:12.8247658Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be719030>, lr = 0.001
2025-04-11T03:52:12.8247816Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T03:52:12.8247820Z 
2025-04-11T03:52:12.8248053Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8248194Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8248356Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8248431Z     
2025-04-11T03:52:12.8248541Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8248711Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8248956Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8249065Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8249350Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8249489Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8249653Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8249658Z 
2025-04-11T03:52:12.8249853Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8250014Z _____________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False] ______________
2025-04-11T03:52:12.8250022Z 
2025-04-11T03:52:12.8250149Z adamw = False, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T03:52:12.8250237Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8250244Z 
2025-04-11T03:52:12.8250367Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8250490Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8250726Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8250880Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8250969Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8251103Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8251189Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8251324Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8251411Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8251495Z >       check_adam_kernel(
2025-04-11T03:52:12.8251743Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8251817Z         )
2025-04-11T03:52:12.8251826Z 
2025-04-11T03:52:12.8251940Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8252050Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8252211Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8252355Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8252526Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8252531Z 
2025-04-11T03:52:12.8252704Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be308370>, lr = 0.001
2025-04-11T03:52:12.8252858Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T03:52:12.8252862Z 
2025-04-11T03:52:12.8253097Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8253235Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8253400Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8253472Z     
2025-04-11T03:52:12.8253586Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8253703Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8253946Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8254055Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8254331Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8254468Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8254623Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8254628Z 
2025-04-11T03:52:12.8254769Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8254987Z ______________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True] ______________
2025-04-11T03:52:12.8254991Z 
2025-04-11T03:52:12.8255127Z adamw = True, weight_decay = 0.1, p_dtype = torch.float32
2025-04-11T03:52:12.8255217Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8255221Z 
2025-04-11T03:52:12.8255344Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8255470Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8255636Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8255788Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8255938Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8256073Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8256157Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8256295Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8256380Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8256465Z >       check_adam_kernel(
2025-04-11T03:52:12.8256724Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8256871Z         )
2025-04-11T03:52:12.8256876Z 
2025-04-11T03:52:12.8256993Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8257104Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8257260Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8257408Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8257517Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8257521Z 
2025-04-11T03:52:12.8257700Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68f04e4eb0>, lr = 0.001
2025-04-11T03:52:12.8257850Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T03:52:12.8257856Z 
2025-04-11T03:52:12.8258096Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8258236Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8258399Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8258527Z     
2025-04-11T03:52:12.8258640Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8258761Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8258999Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8259110Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8259389Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8259528Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8259686Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8259692Z 
2025-04-11T03:52:12.8259832Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8259993Z _____________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False] ______________
2025-04-11T03:52:12.8259999Z 
2025-04-11T03:52:12.8260136Z adamw = False, weight_decay = 0.0, p_dtype = torch.bfloat16
2025-04-11T03:52:12.8260227Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8260232Z 
2025-04-11T03:52:12.8260352Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8260483Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8260649Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8260807Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8260895Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8261082Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8261174Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8261309Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8261399Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8261487Z >       check_adam_kernel(
2025-04-11T03:52:12.8261740Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8261815Z         )
2025-04-11T03:52:12.8261820Z 
2025-04-11T03:52:12.8261931Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8262105Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8262263Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8262414Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8262525Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8262529Z 
2025-04-11T03:52:12.8262711Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be30a3e0>, lr = 0.001
2025-04-11T03:52:12.8262918Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T03:52:12.8262923Z 
2025-04-11T03:52:12.8263164Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8263300Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8263461Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8263539Z     
2025-04-11T03:52:12.8263648Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8263770Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8264011Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8264128Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8264406Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8264541Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8264759Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8264764Z 
2025-04-11T03:52:12.8264899Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8265066Z ______________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True] ______________
2025-04-11T03:52:12.8265072Z 
2025-04-11T03:52:12.8265205Z adamw = True, weight_decay = 0.0, p_dtype = torch.bfloat16
2025-04-11T03:52:12.8265297Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8265301Z 
2025-04-11T03:52:12.8265420Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8265549Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8265717Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8265869Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8265959Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8266086Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8266179Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8266313Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8266401Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8266487Z >       check_adam_kernel(
2025-04-11T03:52:12.8266737Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8266815Z         )
2025-04-11T03:52:12.8266819Z 
2025-04-11T03:52:12.8266929Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8267042Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8267252Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8267402Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8267508Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8267515Z 
2025-04-11T03:52:12.8267689Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f688ee24400>, lr = 0.001
2025-04-11T03:52:12.8267846Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T03:52:12.8267851Z 
2025-04-11T03:52:12.8268084Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8268290Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8268514Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8268589Z     
2025-04-11T03:52:12.8268704Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8268828Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8269133Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8269239Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8269527Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8269659Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8269817Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8269824Z 
2025-04-11T03:52:12.8269958Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8270122Z _____________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False] ______________
2025-04-11T03:52:12.8270126Z 
2025-04-11T03:52:12.8270266Z adamw = False, weight_decay = 0.1, p_dtype = torch.bfloat16
2025-04-11T03:52:12.8270353Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8270362Z 
2025-04-11T03:52:12.8270486Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8270611Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8270783Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8270995Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8271088Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8271217Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8271308Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8271446Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8271531Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8271621Z >       check_adam_kernel(
2025-04-11T03:52:12.8271869Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8271949Z         )
2025-04-11T03:52:12.8271953Z 
2025-04-11T03:52:12.8272069Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8272177Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8272336Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8272480Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8272592Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8272596Z 
2025-04-11T03:52:12.8272772Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be7ccc40>, lr = 0.001
2025-04-11T03:52:12.8272923Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T03:52:12.8272928Z 
2025-04-11T03:52:12.8273161Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8273366Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8273529Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8273603Z     
2025-04-11T03:52:12.8273721Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8273838Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8274084Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8274189Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8274551Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8274683Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8274838Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8274848Z 
2025-04-11T03:52:12.8274988Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8275150Z ______________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True] ______________
2025-04-11T03:52:12.8275206Z 
2025-04-11T03:52:12.8275342Z adamw = True, weight_decay = 0.1, p_dtype = torch.bfloat16
2025-04-11T03:52:12.8275430Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8275434Z 
2025-04-11T03:52:12.8275557Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8275682Z     @pytest.mark.parametrize("weight_decay", [0.0, 0.1])
2025-04-11T03:52:12.8275849Z     @pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8276002Z     def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
2025-04-11T03:52:12.8276089Z         rtol, atol = 1e-5, 1e-8
2025-04-11T03:52:12.8276222Z         if p_dtype is torch.float16 or g_dtype is torch.float16:
2025-04-11T03:52:12.8276309Z             rtol, atol = 1e-3, 1e-3
2025-04-11T03:52:12.8276449Z         if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
2025-04-11T03:52:12.8276535Z             rtol, atol = 4e-3, 4e-3
2025-04-11T03:52:12.8276624Z >       check_adam_kernel(
2025-04-11T03:52:12.8276874Z             FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
2025-04-11T03:52:12.8277006Z         )
2025-04-11T03:52:12.8277011Z 
2025-04-11T03:52:12.8277132Z tests/test_optimizer/test_adam_kernel.py:159: 
2025-04-11T03:52:12.8277244Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8277407Z tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
2025-04-11T03:52:12.8277555Z     adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
2025-04-11T03:52:12.8277664Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8277668Z 
2025-04-11T03:52:12.8277843Z self = <test_adam_kernel.FusedAdamKernel object at 0x7f68be4a4430>, lr = 0.001
2025-04-11T03:52:12.8277995Z beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T03:52:12.8278004Z 
2025-04-11T03:52:12.8278242Z     def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
2025-04-11T03:52:12.8278384Z         super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
2025-04-11T03:52:12.8278552Z         from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T03:52:12.8278623Z     
2025-04-11T03:52:12.8278736Z         fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8278853Z         self.fused_adam = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8279099Z >       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8279206Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8279488Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8279687Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8279845Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8279850Z 
2025-04-11T03:52:12.8279991Z tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
2025-04-11T03:52:12.8280175Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0] ______
2025-04-11T03:52:12.8280179Z 
2025-04-11T03:52:12.8280340Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T03:52:12.8280506Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8280660Z g_dtype = torch.float32
2025-04-11T03:52:12.8280665Z 
2025-04-11T03:52:12.8280831Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8280951Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8281118Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8281212Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8281430Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8281520Z         device: torch.device,
2025-04-11T03:52:12.8281601Z         adamw: bool,
2025-04-11T03:52:12.8281695Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8281781Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8281863Z     ) -> None:
2025-04-11T03:52:12.8282116Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8282221Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8282226Z 
2025-04-11T03:52:12.8282335Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8282443Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8282705Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8282806Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8283046Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8283141Z     return self._apply(convert)
2025-04-11T03:52:12.8283380Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8283521Z     module._apply(fn)
2025-04-11T03:52:12.8283755Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8283844Z     module._apply(fn)
2025-04-11T03:52:12.8284075Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8284160Z     module._apply(fn)
2025-04-11T03:52:12.8284390Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8284486Z     param_applied = fn(param)
2025-04-11T03:52:12.8284601Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8284605Z 
2025-04-11T03:52:12.8284695Z t = Parameter containing:
2025-04-11T03:52:12.8284841Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8284940Z         [ 0.0028, -0.0014,...0,  0.0213, -0.0091],
2025-04-11T03:52:12.8285074Z         [-0.0226, -0.0230, -0.0057,  ..., -0.0094, -0.0239, -0.0399]],
2025-04-11T03:52:12.8285161Z        requires_grad=True)
2025-04-11T03:52:12.8285165Z 
2025-04-11T03:52:12.8285248Z     def convert(t):
2025-04-11T03:52:12.8285380Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8285566Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8285690Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8285897Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8286066Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8286346Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8286488Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8286645Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8286650Z 
2025-04-11T03:52:12.8286907Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8287087Z _______ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2] _______
2025-04-11T03:52:12.8287146Z 
2025-04-11T03:52:12.8287299Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T03:52:12.8287472Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8287559Z g_dtype = torch.float32
2025-04-11T03:52:12.8287565Z 
2025-04-11T03:52:12.8287737Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8287861Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8288070Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8288164Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8288325Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8288412Z         device: torch.device,
2025-04-11T03:52:12.8288493Z         adamw: bool,
2025-04-11T03:52:12.8288587Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8288670Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8288754Z     ) -> None:
2025-04-11T03:52:12.8289011Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8289108Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8289113Z 
2025-04-11T03:52:12.8289228Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8289339Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8289588Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8289686Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8289973Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8290066Z     return self._apply(convert)
2025-04-11T03:52:12.8290297Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8290383Z     module._apply(fn)
2025-04-11T03:52:12.8290615Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8290703Z     module._apply(fn)
2025-04-11T03:52:12.8290930Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8291015Z     module._apply(fn)
2025-04-11T03:52:12.8291243Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8291336Z     param_applied = fn(param)
2025-04-11T03:52:12.8291451Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8291455Z 
2025-04-11T03:52:12.8291547Z t = Parameter containing:
2025-04-11T03:52:12.8291686Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8291784Z         [-0.0048,  0.0237,...2, -0.0204,  0.0268],
2025-04-11T03:52:12.8291909Z         [ 0.0211,  0.0139,  0.0082,  ...,  0.0303, -0.0201, -0.0544]],
2025-04-11T03:52:12.8291997Z        requires_grad=True)
2025-04-11T03:52:12.8292002Z 
2025-04-11T03:52:12.8292081Z     def convert(t):
2025-04-11T03:52:12.8292216Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8292392Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8292576Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8292783Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8292894Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8293177Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8293319Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8293476Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8293542Z 
2025-04-11T03:52:12.8293794Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8293980Z _____ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3] ______
2025-04-11T03:52:12.8293985Z 
2025-04-11T03:52:12.8294151Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8294312Z device = device(type='cpu'), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8294400Z g_dtype = torch.float32
2025-04-11T03:52:12.8294459Z 
2025-04-11T03:52:12.8294633Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8294755Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8294913Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8295007Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8295162Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8295256Z         device: torch.device,
2025-04-11T03:52:12.8295338Z         adamw: bool,
2025-04-11T03:52:12.8295430Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8295514Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8295592Z     ) -> None:
2025-04-11T03:52:12.8295850Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8295948Z         torch_model = model_fn().to(device)
2025-04-11T03:52:12.8296062Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T03:52:12.8296144Z         lr = 1e-3
2025-04-11T03:52:12.8296236Z         beta1, beta2 = 0.9, 0.999
2025-04-11T03:52:12.8296317Z         eps = 1e-8
2025-04-11T03:52:12.8296476Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T03:52:12.8296709Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T03:52:12.8296923Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T03:52:12.8296930Z 
2025-04-11T03:52:12.8297043Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T03:52:12.8297155Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8297159Z 
2025-04-11T03:52:12.8297246Z self = HybridAdam (
2025-04-11T03:52:12.8297330Z Parameter Group 0
2025-04-11T03:52:12.8297411Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8297502Z     bias_correction: True
2025-04-11T03:52:12.8297584Z     eps: 1e-08
2025-04-11T03:52:12.8297665Z     lr: 0.001
2025-04-11T03:52:12.8297752Z     weig...arameter Group 1
2025-04-11T03:52:12.8297832Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8297924Z     bias_correction: True
2025-04-11T03:52:12.8298002Z     eps: 1e-08
2025-04-11T03:52:12.8298081Z     lr: 0.001
2025-04-11T03:52:12.8298163Z     weight_decay: 0.0
2025-04-11T03:52:12.8298242Z )
2025-04-11T03:52:12.8298564Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T03:52:12.8298717Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T03:52:12.8298875Z weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
2025-04-11T03:52:12.8298972Z nvme_offload_dir = None, defaults = {}
2025-04-11T03:52:12.8299423Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T03:52:12.8299432Z 
2025-04-11T03:52:12.8299508Z     def __init__(
2025-04-11T03:52:12.8299587Z         self,
2025-04-11T03:52:12.8299665Z         model_params,
2025-04-11T03:52:12.8299741Z         lr=1e-3,
2025-04-11T03:52:12.8299831Z         bias_correction=True,
2025-04-11T03:52:12.8299913Z         betas=(0.9, 0.999),
2025-04-11T03:52:12.8299996Z         eps=1e-8,
2025-04-11T03:52:12.8300075Z         weight_decay=0,
2025-04-11T03:52:12.8300158Z         adamw_mode=True,
2025-04-11T03:52:12.8300321Z         nvme_offload_fraction: float = 0.0,
2025-04-11T03:52:12.8300429Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T03:52:12.8300514Z         **defaults: Any,
2025-04-11T03:52:12.8300584Z     ):
2025-04-11T03:52:12.8300667Z         super().__init__(
2025-04-11T03:52:12.8300754Z             model_params,
2025-04-11T03:52:12.8300830Z             lr,
2025-04-11T03:52:12.8300919Z             bias_correction,
2025-04-11T03:52:12.8300994Z             betas,
2025-04-11T03:52:12.8301135Z             eps,
2025-04-11T03:52:12.8301216Z             weight_decay,
2025-04-11T03:52:12.8301293Z             adamw_mode,
2025-04-11T03:52:12.8301389Z             nvme_offload_fraction,
2025-04-11T03:52:12.8301474Z             nvme_offload_dir,
2025-04-11T03:52:12.8301550Z         )
2025-04-11T03:52:12.8301644Z         if torch.cuda.is_available():
2025-04-11T03:52:12.8301765Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8301897Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8302111Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T03:52:12.8302224Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8302516Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8302664Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8302825Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8302830Z 
2025-04-11T03:52:12.8302967Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T03:52:12.8303214Z _____ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4] ______
2025-04-11T03:52:12.8303219Z 
2025-04-11T03:52:12.8303384Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8303560Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8303649Z g_dtype = torch.float32
2025-04-11T03:52:12.8303653Z 
2025-04-11T03:52:12.8303821Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8303943Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8304098Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8304193Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8304350Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8304442Z         device: torch.device,
2025-04-11T03:52:12.8304521Z         adamw: bool,
2025-04-11T03:52:12.8304611Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8304696Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8304774Z     ) -> None:
2025-04-11T03:52:12.8305027Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8305126Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8305131Z 
2025-04-11T03:52:12.8305244Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8305356Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8305607Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8305759Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8305987Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8306084Z     return self._apply(convert)
2025-04-11T03:52:12.8306317Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8306405Z     module._apply(fn)
2025-04-11T03:52:12.8306639Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8306722Z     module._apply(fn)
2025-04-11T03:52:12.8307013Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8307093Z     module._apply(fn)
2025-04-11T03:52:12.8307322Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8307411Z     param_applied = fn(param)
2025-04-11T03:52:12.8307527Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8307531Z 
2025-04-11T03:52:12.8307619Z t = Parameter containing:
2025-04-11T03:52:12.8307816Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8307917Z         [ 0.0210, -0.0131,...8, -0.0156, -0.0054],
2025-04-11T03:52:12.8308039Z         [ 0.0148,  0.0292,  0.0008,  ...,  0.0355, -0.0048, -0.0186]],
2025-04-11T03:52:12.8308128Z        requires_grad=True)
2025-04-11T03:52:12.8308133Z 
2025-04-11T03:52:12.8308213Z     def convert(t):
2025-04-11T03:52:12.8308348Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8308573Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8308698Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8308903Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8309014Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8309298Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8309434Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8309664Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8309669Z 
2025-04-11T03:52:12.8309922Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8310107Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0] _______
2025-04-11T03:52:12.8310113Z 
2025-04-11T03:52:12.8310277Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T03:52:12.8310447Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8310531Z g_dtype = torch.float32
2025-04-11T03:52:12.8310537Z 
2025-04-11T03:52:12.8310704Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8310828Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8310986Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8311084Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8311242Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8311335Z         device: torch.device,
2025-04-11T03:52:12.8311413Z         adamw: bool,
2025-04-11T03:52:12.8311501Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8311589Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8311669Z     ) -> None:
2025-04-11T03:52:12.8311925Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8312020Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8312024Z 
2025-04-11T03:52:12.8312135Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8312313Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8312555Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8312653Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8312882Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8312980Z     return self._apply(convert)
2025-04-11T03:52:12.8313213Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8313298Z     module._apply(fn)
2025-04-11T03:52:12.8313603Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8313685Z     module._apply(fn)
2025-04-11T03:52:12.8313918Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8313997Z     module._apply(fn)
2025-04-11T03:52:12.8314234Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8314392Z     param_applied = fn(param)
2025-04-11T03:52:12.8314509Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8314513Z 
2025-04-11T03:52:12.8314603Z t = Parameter containing:
2025-04-11T03:52:12.8314739Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8314840Z         [ 0.0168, -0.0116,...1,  0.0247, -0.0168],
2025-04-11T03:52:12.8314961Z         [ 0.0078, -0.0201,  0.0158,  ..., -0.0204,  0.0234,  0.0068]],
2025-04-11T03:52:12.8315053Z        requires_grad=True)
2025-04-11T03:52:12.8315057Z 
2025-04-11T03:52:12.8315136Z     def convert(t):
2025-04-11T03:52:12.8315271Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8315447Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8315568Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8315774Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8315886Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8316176Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8316373Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8316535Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8316542Z 
2025-04-11T03:52:12.8316795Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8316977Z _______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2] ________
2025-04-11T03:52:12.8316981Z 
2025-04-11T03:52:12.8317134Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T03:52:12.8317304Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8317393Z g_dtype = torch.float32
2025-04-11T03:52:12.8317399Z 
2025-04-11T03:52:12.8317565Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8317694Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8317850Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8317944Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8318097Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8318185Z         device: torch.device,
2025-04-11T03:52:12.8318270Z         adamw: bool,
2025-04-11T03:52:12.8318357Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8318445Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8318522Z     ) -> None:
2025-04-11T03:52:12.8318774Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8318927Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8318932Z 
2025-04-11T03:52:12.8319046Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8319161Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8319403Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8319501Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8319727Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8319898Z     return self._apply(convert)
2025-04-11T03:52:12.8320132Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8320213Z     module._apply(fn)
2025-04-11T03:52:12.8320448Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8320531Z     module._apply(fn)
2025-04-11T03:52:12.8320767Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8320906Z     module._apply(fn)
2025-04-11T03:52:12.8321137Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8321230Z     param_applied = fn(param)
2025-04-11T03:52:12.8321341Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8321345Z 
2025-04-11T03:52:12.8321439Z t = Parameter containing:
2025-04-11T03:52:12.8321572Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8321675Z         [ 0.0171, -0.0037,...8, -0.0068,  0.0037],
2025-04-11T03:52:12.8321797Z         [ 0.0260, -0.0271, -0.0247,  ...,  0.0262,  0.0078,  0.0236]],
2025-04-11T03:52:12.8321885Z        requires_grad=True)
2025-04-11T03:52:12.8321890Z 
2025-04-11T03:52:12.8321969Z     def convert(t):
2025-04-11T03:52:12.8322102Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8322285Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8322404Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8322616Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8322792Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8323078Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8323212Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8323372Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8323380Z 
2025-04-11T03:52:12.8323630Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8323813Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3] ______
2025-04-11T03:52:12.8323817Z 
2025-04-11T03:52:12.8323985Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8324135Z device = device(type='cpu'), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8324231Z g_dtype = torch.float32
2025-04-11T03:52:12.8324236Z 
2025-04-11T03:52:12.8324401Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8324524Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8324674Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8324769Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8324926Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8325012Z         device: torch.device,
2025-04-11T03:52:12.8325094Z         adamw: bool,
2025-04-11T03:52:12.8325180Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8325324Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8325403Z     ) -> None:
2025-04-11T03:52:12.8325650Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8325753Z         torch_model = model_fn().to(device)
2025-04-11T03:52:12.8325863Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T03:52:12.8325944Z         lr = 1e-3
2025-04-11T03:52:12.8326032Z         beta1, beta2 = 0.9, 0.999
2025-04-11T03:52:12.8326112Z         eps = 1e-8
2025-04-11T03:52:12.8326222Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T03:52:12.8326507Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T03:52:12.8326727Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T03:52:12.8326732Z 
2025-04-11T03:52:12.8326842Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T03:52:12.8326959Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8326964Z 
2025-04-11T03:52:12.8327101Z self = HybridAdam (
2025-04-11T03:52:12.8327189Z Parameter Group 0
2025-04-11T03:52:12.8327271Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8327357Z     bias_correction: True
2025-04-11T03:52:12.8327441Z     eps: 1e-08
2025-04-11T03:52:12.8327517Z     lr: 0.001
2025-04-11T03:52:12.8327607Z     weig...arameter Group 1
2025-04-11T03:52:12.8327687Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8327770Z     bias_correction: True
2025-04-11T03:52:12.8327856Z     eps: 1e-08
2025-04-11T03:52:12.8327930Z     lr: 0.001
2025-04-11T03:52:12.8328016Z     weight_decay: 0.0
2025-04-11T03:52:12.8328087Z )
2025-04-11T03:52:12.8328400Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T03:52:12.8328551Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T03:52:12.8328702Z weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
2025-04-11T03:52:12.8328808Z nvme_offload_dir = None, defaults = {}
2025-04-11T03:52:12.8329191Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T03:52:12.8329275Z 
2025-04-11T03:52:12.8329364Z     def __init__(
2025-04-11T03:52:12.8329437Z         self,
2025-04-11T03:52:12.8329520Z         model_params,
2025-04-11T03:52:12.8329596Z         lr=1e-3,
2025-04-11T03:52:12.8329680Z         bias_correction=True,
2025-04-11T03:52:12.8329768Z         betas=(0.9, 0.999),
2025-04-11T03:52:12.8329845Z         eps=1e-8,
2025-04-11T03:52:12.8329930Z         weight_decay=0,
2025-04-11T03:52:12.8330011Z         adamw_mode=True,
2025-04-11T03:52:12.8330106Z         nvme_offload_fraction: float = 0.0,
2025-04-11T03:52:12.8330218Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T03:52:12.8330303Z         **defaults: Any,
2025-04-11T03:52:12.8330377Z     ):
2025-04-11T03:52:12.8330458Z         super().__init__(
2025-04-11T03:52:12.8330541Z             model_params,
2025-04-11T03:52:12.8330622Z             lr,
2025-04-11T03:52:12.8330706Z             bias_correction,
2025-04-11T03:52:12.8330785Z             betas,
2025-04-11T03:52:12.8330863Z             eps,
2025-04-11T03:52:12.8330943Z             weight_decay,
2025-04-11T03:52:12.8331025Z             adamw_mode,
2025-04-11T03:52:12.8331114Z             nvme_offload_fraction,
2025-04-11T03:52:12.8331200Z             nvme_offload_dir,
2025-04-11T03:52:12.8331274Z         )
2025-04-11T03:52:12.8331370Z         if torch.cuda.is_available():
2025-04-11T03:52:12.8331494Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8331617Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8331832Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T03:52:12.8331999Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8332291Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8332431Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8332596Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8332601Z 
2025-04-11T03:52:12.8332738Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T03:52:12.8332922Z ______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4] ______
2025-04-11T03:52:12.8332988Z 
2025-04-11T03:52:12.8333158Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8333322Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8333412Z g_dtype = torch.float32
2025-04-11T03:52:12.8333419Z 
2025-04-11T03:52:12.8333584Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8333710Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8333923Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8334016Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8334178Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8334265Z         device: torch.device,
2025-04-11T03:52:12.8334351Z         adamw: bool,
2025-04-11T03:52:12.8334437Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8334525Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8334604Z     ) -> None:
2025-04-11T03:52:12.8334853Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8334954Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8334959Z 
2025-04-11T03:52:12.8335068Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8335187Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8335429Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8335532Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8335759Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8335908Z     return self._apply(convert)
2025-04-11T03:52:12.8336152Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8336234Z     module._apply(fn)
2025-04-11T03:52:12.8336476Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8336556Z     module._apply(fn)
2025-04-11T03:52:12.8336787Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8336866Z     module._apply(fn)
2025-04-11T03:52:12.8337095Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8337190Z     param_applied = fn(param)
2025-04-11T03:52:12.8337304Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8337308Z 
2025-04-11T03:52:12.8337398Z t = Parameter containing:
2025-04-11T03:52:12.8337538Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8337642Z         [-0.0301, -0.0063,...5, -0.0105,  0.0078],
2025-04-11T03:52:12.8337766Z         [-0.0225,  0.0108,  0.0321,  ..., -0.0056, -0.0089, -0.0360]],
2025-04-11T03:52:12.8337852Z        requires_grad=True)
2025-04-11T03:52:12.8337857Z 
2025-04-11T03:52:12.8337940Z     def convert(t):
2025-04-11T03:52:12.8338071Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8338253Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8338429Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8338643Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8338754Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8339037Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8339178Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8339337Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8339400Z 
2025-04-11T03:52:12.8339654Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8339835Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0] ______
2025-04-11T03:52:12.8339840Z 
2025-04-11T03:52:12.8340003Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T03:52:12.8340174Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8340263Z g_dtype = torch.float16
2025-04-11T03:52:12.8340322Z 
2025-04-11T03:52:12.8340488Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8340610Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8340767Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8340861Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8341019Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8341111Z         device: torch.device,
2025-04-11T03:52:12.8341192Z         adamw: bool,
2025-04-11T03:52:12.8341281Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8341362Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8341443Z     ) -> None:
2025-04-11T03:52:12.8341692Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8341792Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8341797Z 
2025-04-11T03:52:12.8341908Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8342021Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8342399Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8342493Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8342718Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8342812Z     return self._apply(convert)
2025-04-11T03:52:12.8343043Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8343121Z     module._apply(fn)
2025-04-11T03:52:12.8343353Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8343435Z     module._apply(fn)
2025-04-11T03:52:12.8343659Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8343745Z     module._apply(fn)
2025-04-11T03:52:12.8343971Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8344064Z     param_applied = fn(param)
2025-04-11T03:52:12.8344173Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8344178Z 
2025-04-11T03:52:12.8344268Z t = Parameter containing:
2025-04-11T03:52:12.8344399Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8344495Z         [-0.0063,  0.0127,...8,  0.0139, -0.0372],
2025-04-11T03:52:12.8344617Z         [-0.0001,  0.0211,  0.0425,  ..., -0.0074,  0.0182,  0.0033]],
2025-04-11T03:52:12.8344702Z        requires_grad=True)
2025-04-11T03:52:12.8344706Z 
2025-04-11T03:52:12.8344787Z     def convert(t):
2025-04-11T03:52:12.8344978Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8345161Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8345284Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8345490Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8345605Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8345892Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8346100Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8346258Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8346263Z 
2025-04-11T03:52:12.8346514Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8346696Z _______ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2] _______
2025-04-11T03:52:12.8346701Z 
2025-04-11T03:52:12.8346924Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T03:52:12.8347093Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8347179Z g_dtype = torch.float16
2025-04-11T03:52:12.8347187Z 
2025-04-11T03:52:12.8347355Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8347475Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8347632Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8347727Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8347885Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8347971Z         device: torch.device,
2025-04-11T03:52:12.8348050Z         adamw: bool,
2025-04-11T03:52:12.8348139Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8348227Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8348305Z     ) -> None:
2025-04-11T03:52:12.8348605Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8348709Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8348782Z 
2025-04-11T03:52:12.8348892Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8349003Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8349246Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8349343Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8349565Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8349657Z     return self._apply(convert)
2025-04-11T03:52:12.8349890Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8349976Z     module._apply(fn)
2025-04-11T03:52:12.8350202Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8350291Z     module._apply(fn)
2025-04-11T03:52:12.8350516Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8350604Z     module._apply(fn)
2025-04-11T03:52:12.8350829Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8350925Z     param_applied = fn(param)
2025-04-11T03:52:12.8351035Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8351042Z 
2025-04-11T03:52:12.8351129Z t = Parameter containing:
2025-04-11T03:52:12.8351264Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8351362Z         [ 0.0058,  0.0119,...4, -0.0198,  0.0151],
2025-04-11T03:52:12.8351489Z         [-0.0479,  0.0136, -0.0425,  ..., -0.0021, -0.0081,  0.0171]],
2025-04-11T03:52:12.8351637Z        requires_grad=True)
2025-04-11T03:52:12.8351642Z 
2025-04-11T03:52:12.8351725Z     def convert(t):
2025-04-11T03:52:12.8351854Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8352029Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8352155Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8352358Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8352467Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8352815Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8352951Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8353110Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8353117Z 
2025-04-11T03:52:12.8353371Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8353619Z _____ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3] ______
2025-04-11T03:52:12.8353623Z 
2025-04-11T03:52:12.8353791Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8353948Z device = device(type='cpu'), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8354034Z g_dtype = torch.float16
2025-04-11T03:52:12.8354038Z 
2025-04-11T03:52:12.8354208Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8354332Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8354489Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8354584Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8354739Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8354832Z         device: torch.device,
2025-04-11T03:52:12.8354909Z         adamw: bool,
2025-04-11T03:52:12.8355004Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8355088Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8355168Z     ) -> None:
2025-04-11T03:52:12.8355417Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8355575Z         torch_model = model_fn().to(device)
2025-04-11T03:52:12.8355690Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T03:52:12.8355770Z         lr = 1e-3
2025-04-11T03:52:12.8355864Z         beta1, beta2 = 0.9, 0.999
2025-04-11T03:52:12.8355945Z         eps = 1e-8
2025-04-11T03:52:12.8356049Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T03:52:12.8356279Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T03:52:12.8356495Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T03:52:12.8356500Z 
2025-04-11T03:52:12.8356613Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T03:52:12.8356725Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8356729Z 
2025-04-11T03:52:12.8356815Z self = HybridAdam (
2025-04-11T03:52:12.8356897Z Parameter Group 0
2025-04-11T03:52:12.8356981Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8357067Z     bias_correction: True
2025-04-11T03:52:12.8357144Z     eps: 1e-08
2025-04-11T03:52:12.8357222Z     lr: 0.001
2025-04-11T03:52:12.8357310Z     weig...arameter Group 1
2025-04-11T03:52:12.8357393Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8357478Z     bias_correction: True
2025-04-11T03:52:12.8357554Z     eps: 1e-08
2025-04-11T03:52:12.8357629Z     lr: 0.001
2025-04-11T03:52:12.8357709Z     weight_decay: 0.0
2025-04-11T03:52:12.8357786Z )
2025-04-11T03:52:12.8358100Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T03:52:12.8358307Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T03:52:12.8358465Z weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
2025-04-11T03:52:12.8358565Z nvme_offload_dir = None, defaults = {}
2025-04-11T03:52:12.8358937Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T03:52:12.8358943Z 
2025-04-11T03:52:12.8359017Z     def __init__(
2025-04-11T03:52:12.8359154Z         self,
2025-04-11T03:52:12.8359234Z         model_params,
2025-04-11T03:52:12.8359308Z         lr=1e-3,
2025-04-11T03:52:12.8359396Z         bias_correction=True,
2025-04-11T03:52:12.8359478Z         betas=(0.9, 0.999),
2025-04-11T03:52:12.8359561Z         eps=1e-8,
2025-04-11T03:52:12.8359644Z         weight_decay=0,
2025-04-11T03:52:12.8359729Z         adamw_mode=True,
2025-04-11T03:52:12.8359825Z         nvme_offload_fraction: float = 0.0,
2025-04-11T03:52:12.8359985Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T03:52:12.8360072Z         **defaults: Any,
2025-04-11T03:52:12.8360145Z     ):
2025-04-11T03:52:12.8360233Z         super().__init__(
2025-04-11T03:52:12.8360316Z             model_params,
2025-04-11T03:52:12.8360391Z             lr,
2025-04-11T03:52:12.8360480Z             bias_correction,
2025-04-11T03:52:12.8360556Z             betas,
2025-04-11T03:52:12.8360634Z             eps,
2025-04-11T03:52:12.8360715Z             weight_decay,
2025-04-11T03:52:12.8360795Z             adamw_mode,
2025-04-11T03:52:12.8360888Z             nvme_offload_fraction,
2025-04-11T03:52:12.8360970Z             nvme_offload_dir,
2025-04-11T03:52:12.8361047Z         )
2025-04-11T03:52:12.8361143Z         if torch.cuda.is_available():
2025-04-11T03:52:12.8361264Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8361395Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8361605Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T03:52:12.8361719Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8362007Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8362215Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8362380Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8362386Z 
2025-04-11T03:52:12.8362533Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T03:52:12.8362722Z _____ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4] ______
2025-04-11T03:52:12.8362727Z 
2025-04-11T03:52:12.8362895Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8363071Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8363160Z g_dtype = torch.float16
2025-04-11T03:52:12.8363167Z 
2025-04-11T03:52:12.8363340Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8363467Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8363628Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8363726Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8363884Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8363979Z         device: torch.device,
2025-04-11T03:52:12.8364060Z         adamw: bool,
2025-04-11T03:52:12.8364155Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8364242Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8364323Z     ) -> None:
2025-04-11T03:52:12.8364575Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8364728Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8364733Z 
2025-04-11T03:52:12.8364848Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8364961Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8365214Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8365311Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8365543Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8365704Z     return self._apply(convert)
2025-04-11T03:52:12.8365940Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8366027Z     module._apply(fn)
2025-04-11T03:52:12.8366260Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8366347Z     module._apply(fn)
2025-04-11T03:52:12.8366578Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8366719Z     module._apply(fn)
2025-04-11T03:52:12.8366949Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8367039Z     param_applied = fn(param)
2025-04-11T03:52:12.8367154Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8367158Z 
2025-04-11T03:52:12.8367246Z t = Parameter containing:
2025-04-11T03:52:12.8367385Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8367486Z         [ 0.0127, -0.0053,...6, -0.0203,  0.0294],
2025-04-11T03:52:12.8367609Z         [ 0.0315,  0.0270, -0.0379,  ...,  0.0044, -0.0077,  0.0209]],
2025-04-11T03:52:12.8367694Z        requires_grad=True)
2025-04-11T03:52:12.8367699Z 
2025-04-11T03:52:12.8367778Z     def convert(t):
2025-04-11T03:52:12.8367915Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8368096Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8368220Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8368428Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8368600Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8368882Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8369017Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8369179Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8369183Z 
2025-04-11T03:52:12.8369436Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8369621Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0] _______
2025-04-11T03:52:12.8369625Z 
2025-04-11T03:52:12.8369787Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T03:52:12.8369960Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8370051Z g_dtype = torch.float16
2025-04-11T03:52:12.8370055Z 
2025-04-11T03:52:12.8370228Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8370349Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8370501Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8370600Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8370761Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8370850Z         device: torch.device,
2025-04-11T03:52:12.8370930Z         adamw: bool,
2025-04-11T03:52:12.8371020Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8371164Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8371240Z     ) -> None:
2025-04-11T03:52:12.8371498Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8371595Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8371602Z 
2025-04-11T03:52:12.8371717Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8371830Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8372074Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8372250Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8372477Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8372572Z     return self._apply(convert)
2025-04-11T03:52:12.8372807Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8372895Z     module._apply(fn)
2025-04-11T03:52:12.8373122Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8373263Z     module._apply(fn)
2025-04-11T03:52:12.8373496Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8373578Z     module._apply(fn)
2025-04-11T03:52:12.8373809Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8373899Z     param_applied = fn(param)
2025-04-11T03:52:12.8374014Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8374021Z 
2025-04-11T03:52:12.8374110Z t = Parameter containing:
2025-04-11T03:52:12.8374247Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8374346Z         [ 0.0126,  0.0307,...5,  0.0153,  0.0116],
2025-04-11T03:52:12.8374468Z         [-0.0007,  0.0044, -0.0020,  ..., -0.0033,  0.0164, -0.0073]],
2025-04-11T03:52:12.8374561Z        requires_grad=True)
2025-04-11T03:52:12.8374565Z 
2025-04-11T03:52:12.8374645Z     def convert(t):
2025-04-11T03:52:12.8374778Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8374954Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8375135Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8375342Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8375449Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8375740Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8375876Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8376038Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8376044Z 
2025-04-11T03:52:12.8376293Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8376479Z _______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2] ________
2025-04-11T03:52:12.8376483Z 
2025-04-11T03:52:12.8376643Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T03:52:12.8376813Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8376901Z g_dtype = torch.float16
2025-04-11T03:52:12.8376905Z 
2025-04-11T03:52:12.8377072Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8377198Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8377348Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8377444Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8377599Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8377751Z         device: torch.device,
2025-04-11T03:52:12.8377833Z         adamw: bool,
2025-04-11T03:52:12.8377920Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8378010Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8378084Z     ) -> None:
2025-04-11T03:52:12.8378334Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8378432Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8378438Z 
2025-04-11T03:52:12.8378550Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8378660Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8378965Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8379063Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8379289Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8379386Z     return self._apply(convert)
2025-04-11T03:52:12.8379616Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8379758Z     module._apply(fn)
2025-04-11T03:52:12.8379989Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8380072Z     module._apply(fn)
2025-04-11T03:52:12.8380306Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8380386Z     module._apply(fn)
2025-04-11T03:52:12.8380617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8380706Z     param_applied = fn(param)
2025-04-11T03:52:12.8380819Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8380823Z 
2025-04-11T03:52:12.8380912Z t = Parameter containing:
2025-04-11T03:52:12.8381046Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8381148Z         [ 0.0062,  0.0098,...3, -0.0036,  0.0170],
2025-04-11T03:52:12.8381268Z         [ 0.0053,  0.0281, -0.0163,  ..., -0.0098, -0.0364,  0.0040]],
2025-04-11T03:52:12.8381357Z        requires_grad=True)
2025-04-11T03:52:12.8381362Z 
2025-04-11T03:52:12.8381502Z     def convert(t):
2025-04-11T03:52:12.8381638Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8381814Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8381933Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8382140Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8382248Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8382535Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8382670Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8382835Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8382840Z 
2025-04-11T03:52:12.8383089Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8383275Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3] ______
2025-04-11T03:52:12.8383279Z 
2025-04-11T03:52:12.8383442Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8383600Z device = device(type='cpu'), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8383691Z g_dtype = torch.float16
2025-04-11T03:52:12.8383696Z 
2025-04-11T03:52:12.8383861Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8383987Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8384214Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8384310Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8384466Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8384552Z         device: torch.device,
2025-04-11T03:52:12.8384637Z         adamw: bool,
2025-04-11T03:52:12.8384725Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8384811Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8384889Z     ) -> None:
2025-04-11T03:52:12.8385136Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8385297Z         torch_model = model_fn().to(device)
2025-04-11T03:52:12.8385408Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T03:52:12.8385490Z         lr = 1e-3
2025-04-11T03:52:12.8385578Z         beta1, beta2 = 0.9, 0.999
2025-04-11T03:52:12.8385662Z         eps = 1e-8
2025-04-11T03:52:12.8385769Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T03:52:12.8385998Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T03:52:12.8386269Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T03:52:12.8386273Z 
2025-04-11T03:52:12.8386386Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T03:52:12.8386503Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8386508Z 
2025-04-11T03:52:12.8386592Z self = HybridAdam (
2025-04-11T03:52:12.8386677Z Parameter Group 0
2025-04-11T03:52:12.8386760Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8386846Z     bias_correction: True
2025-04-11T03:52:12.8386929Z     eps: 1e-08
2025-04-11T03:52:12.8387007Z     lr: 0.001
2025-04-11T03:52:12.8387098Z     weig...arameter Group 1
2025-04-11T03:52:12.8387177Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8387264Z     bias_correction: True
2025-04-11T03:52:12.8387344Z     eps: 1e-08
2025-04-11T03:52:12.8387419Z     lr: 0.001
2025-04-11T03:52:12.8387504Z     weight_decay: 0.0
2025-04-11T03:52:12.8387576Z )
2025-04-11T03:52:12.8387890Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T03:52:12.8388098Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T03:52:12.8388247Z weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
2025-04-11T03:52:12.8388349Z nvme_offload_dir = None, defaults = {}
2025-04-11T03:52:12.8388772Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T03:52:12.8388780Z 
2025-04-11T03:52:12.8388861Z     def __init__(
2025-04-11T03:52:12.8388936Z         self,
2025-04-11T03:52:12.8389023Z         model_params,
2025-04-11T03:52:12.8389099Z         lr=1e-3,
2025-04-11T03:52:12.8389186Z         bias_correction=True,
2025-04-11T03:52:12.8389273Z         betas=(0.9, 0.999),
2025-04-11T03:52:12.8389350Z         eps=1e-8,
2025-04-11T03:52:12.8389437Z         weight_decay=0,
2025-04-11T03:52:12.8389518Z         adamw_mode=True,
2025-04-11T03:52:12.8389615Z         nvme_offload_fraction: float = 0.0,
2025-04-11T03:52:12.8389728Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T03:52:12.8389811Z         **defaults: Any,
2025-04-11T03:52:12.8389888Z     ):
2025-04-11T03:52:12.8389971Z         super().__init__(
2025-04-11T03:52:12.8390057Z             model_params,
2025-04-11T03:52:12.8390132Z             lr,
2025-04-11T03:52:12.8390220Z             bias_correction,
2025-04-11T03:52:12.8390298Z             betas,
2025-04-11T03:52:12.8390375Z             eps,
2025-04-11T03:52:12.8390459Z             weight_decay,
2025-04-11T03:52:12.8390537Z             adamw_mode,
2025-04-11T03:52:12.8390628Z             nvme_offload_fraction,
2025-04-11T03:52:12.8390777Z             nvme_offload_dir,
2025-04-11T03:52:12.8390848Z         )
2025-04-11T03:52:12.8390945Z         if torch.cuda.is_available():
2025-04-11T03:52:12.8391067Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8391191Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8391406Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T03:52:12.8391517Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8391817Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8392023Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8392185Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8392190Z 
2025-04-11T03:52:12.8392326Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T03:52:12.8392514Z ______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4] ______
2025-04-11T03:52:12.8392518Z 
2025-04-11T03:52:12.8392683Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8392907Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8392998Z g_dtype = torch.float16
2025-04-11T03:52:12.8393006Z 
2025-04-11T03:52:12.8393171Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8393298Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8393450Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8393546Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8393702Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8393788Z         device: torch.device,
2025-04-11T03:52:12.8393870Z         adamw: bool,
2025-04-11T03:52:12.8393957Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8394048Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8394126Z     ) -> None:
2025-04-11T03:52:12.8394373Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8394477Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8394482Z 
2025-04-11T03:52:12.8394652Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8394770Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8395016Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8395115Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8395343Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8395437Z     return self._apply(convert)
2025-04-11T03:52:12.8395667Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8395752Z     module._apply(fn)
2025-04-11T03:52:12.8395989Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8396073Z     module._apply(fn)
2025-04-11T03:52:12.8396303Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8396385Z     module._apply(fn)
2025-04-11T03:52:12.8396611Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8396701Z     param_applied = fn(param)
2025-04-11T03:52:12.8396813Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8396819Z 
2025-04-11T03:52:12.8396911Z t = Parameter containing:
2025-04-11T03:52:12.8397043Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8397143Z         [ 0.0145, -0.0268,...4,  0.0235, -0.0067],
2025-04-11T03:52:12.8397263Z         [-0.0276, -0.0061,  0.0080,  ...,  0.0096,  0.0016, -0.0028]],
2025-04-11T03:52:12.8397418Z        requires_grad=True)
2025-04-11T03:52:12.8397424Z 
2025-04-11T03:52:12.8397505Z     def convert(t):
2025-04-11T03:52:12.8397633Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8397817Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8397938Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8398146Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8398255Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8398656Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8398792Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8398949Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8398959Z 
2025-04-11T03:52:12.8399208Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8399450Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0] ______
2025-04-11T03:52:12.8399455Z 
2025-04-11T03:52:12.8399623Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T03:52:12.8399790Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8399882Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8399886Z 
2025-04-11T03:52:12.8400053Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8400178Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8400332Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8400425Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8400583Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8400672Z         device: torch.device,
2025-04-11T03:52:12.8400753Z         adamw: bool,
2025-04-11T03:52:12.8400840Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8400926Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8401006Z     ) -> None:
2025-04-11T03:52:12.8401252Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8401414Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8401419Z 
2025-04-11T03:52:12.8401529Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8401645Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8401896Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8401992Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8402220Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8402313Z     return self._apply(convert)
2025-04-11T03:52:12.8402551Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8402635Z     module._apply(fn)
2025-04-11T03:52:12.8402874Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8402955Z     module._apply(fn)
2025-04-11T03:52:12.8403192Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8403269Z     module._apply(fn)
2025-04-11T03:52:12.8403493Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8403590Z     param_applied = fn(param)
2025-04-11T03:52:12.8403699Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8403703Z 
2025-04-11T03:52:12.8403795Z t = Parameter containing:
2025-04-11T03:52:12.8403986Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8404087Z         [ 0.0360, -0.0060,...2,  0.0336, -0.0315],
2025-04-11T03:52:12.8404206Z         [ 0.0418,  0.0034,  0.0053,  ...,  0.0279, -0.0100,  0.0020]],
2025-04-11T03:52:12.8404295Z        requires_grad=True)
2025-04-11T03:52:12.8404299Z 
2025-04-11T03:52:12.8404384Z     def convert(t):
2025-04-11T03:52:12.8404513Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8404692Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8404809Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8405080Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8405190Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8405476Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8405620Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8405835Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8405840Z 
2025-04-11T03:52:12.8406095Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8406279Z _______ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2] _______
2025-04-11T03:52:12.8406284Z 
2025-04-11T03:52:12.8406440Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T03:52:12.8406608Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8406701Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8406706Z 
2025-04-11T03:52:12.8406877Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8406998Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8407160Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8407251Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8407412Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8407498Z         device: torch.device,
2025-04-11T03:52:12.8407580Z         adamw: bool,
2025-04-11T03:52:12.8407729Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8407814Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8407893Z     ) -> None:
2025-04-11T03:52:12.8408144Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8408247Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8408251Z 
2025-04-11T03:52:12.8408359Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8408474Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8408715Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8408809Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8409036Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8409126Z     return self._apply(convert)
2025-04-11T03:52:12.8409358Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8409441Z     module._apply(fn)
2025-04-11T03:52:12.8409677Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8409759Z     module._apply(fn)
2025-04-11T03:52:12.8409988Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8410072Z     module._apply(fn)
2025-04-11T03:52:12.8410302Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8410392Z     param_applied = fn(param)
2025-04-11T03:52:12.8410559Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8410563Z 
2025-04-11T03:52:12.8410656Z t = Parameter containing:
2025-04-11T03:52:12.8410786Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8410884Z         [-0.0029, -0.0003,...8,  0.0132,  0.0134],
2025-04-11T03:52:12.8411013Z         [-0.0017, -0.0011, -0.0088,  ...,  0.0178,  0.0258,  0.0116]],
2025-04-11T03:52:12.8411098Z        requires_grad=True)
2025-04-11T03:52:12.8411102Z 
2025-04-11T03:52:12.8411186Z     def convert(t):
2025-04-11T03:52:12.8411316Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8411559Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8411676Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8411879Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8411993Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8412275Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8412469Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8412630Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8412635Z 
2025-04-11T03:52:12.8412889Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8413074Z _____ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3] ______
2025-04-11T03:52:12.8413080Z 
2025-04-11T03:52:12.8413251Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8413406Z device = device(type='cpu'), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8413493Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8413499Z 
2025-04-11T03:52:12.8413673Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8413795Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8413953Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8414046Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8414268Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8414356Z         device: torch.device,
2025-04-11T03:52:12.8414435Z         adamw: bool,
2025-04-11T03:52:12.8414529Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8414613Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8414696Z     ) -> None:
2025-04-11T03:52:12.8414944Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8415040Z         torch_model = model_fn().to(device)
2025-04-11T03:52:12.8415151Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T03:52:12.8415232Z         lr = 1e-3
2025-04-11T03:52:12.8415323Z         beta1, beta2 = 0.9, 0.999
2025-04-11T03:52:12.8415403Z         eps = 1e-8
2025-04-11T03:52:12.8415516Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T03:52:12.8415741Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T03:52:12.8415954Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T03:52:12.8415964Z 
2025-04-11T03:52:12.8416071Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T03:52:12.8416183Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8416190Z 
2025-04-11T03:52:12.8416279Z self = HybridAdam (
2025-04-11T03:52:12.8416361Z Parameter Group 0
2025-04-11T03:52:12.8416446Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8416531Z     bias_correction: True
2025-04-11T03:52:12.8416611Z     eps: 1e-08
2025-04-11T03:52:12.8416751Z     lr: 0.001
2025-04-11T03:52:12.8416841Z     weig...arameter Group 1
2025-04-11T03:52:12.8416926Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8417013Z     bias_correction: True
2025-04-11T03:52:12.8417090Z     eps: 1e-08
2025-04-11T03:52:12.8417169Z     lr: 0.001
2025-04-11T03:52:12.8417249Z     weight_decay: 0.0
2025-04-11T03:52:12.8417325Z )
2025-04-11T03:52:12.8417635Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T03:52:12.8417783Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T03:52:12.8418003Z weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
2025-04-11T03:52:12.8418102Z nvme_offload_dir = None, defaults = {}
2025-04-11T03:52:12.8418492Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T03:52:12.8418499Z 
2025-04-11T03:52:12.8418576Z     def __init__(
2025-04-11T03:52:12.8418657Z         self,
2025-04-11T03:52:12.8418736Z         model_params,
2025-04-11T03:52:12.8418873Z         lr=1e-3,
2025-04-11T03:52:12.8418959Z         bias_correction=True,
2025-04-11T03:52:12.8419040Z         betas=(0.9, 0.999),
2025-04-11T03:52:12.8419126Z         eps=1e-8,
2025-04-11T03:52:12.8419206Z         weight_decay=0,
2025-04-11T03:52:12.8419294Z         adamw_mode=True,
2025-04-11T03:52:12.8419391Z         nvme_offload_fraction: float = 0.0,
2025-04-11T03:52:12.8419498Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T03:52:12.8419583Z         **defaults: Any,
2025-04-11T03:52:12.8419659Z     ):
2025-04-11T03:52:12.8419743Z         super().__init__(
2025-04-11T03:52:12.8419825Z             model_params,
2025-04-11T03:52:12.8419899Z             lr,
2025-04-11T03:52:12.8419987Z             bias_correction,
2025-04-11T03:52:12.8420062Z             betas,
2025-04-11T03:52:12.8420142Z             eps,
2025-04-11T03:52:12.8420222Z             weight_decay,
2025-04-11T03:52:12.8420303Z             adamw_mode,
2025-04-11T03:52:12.8420395Z             nvme_offload_fraction,
2025-04-11T03:52:12.8420480Z             nvme_offload_dir,
2025-04-11T03:52:12.8420554Z         )
2025-04-11T03:52:12.8420648Z         if torch.cuda.is_available():
2025-04-11T03:52:12.8420909Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8421039Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8421249Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T03:52:12.8421365Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8421653Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8421798Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8421963Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8421968Z 
2025-04-11T03:52:12.8422105Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T03:52:12.8422291Z _____ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4] ______
2025-04-11T03:52:12.8422295Z 
2025-04-11T03:52:12.8422467Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8422634Z device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
2025-04-11T03:52:12.8422722Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8422727Z 
2025-04-11T03:52:12.8422898Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8423023Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8423184Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8423277Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8423439Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8423586Z         device: torch.device,
2025-04-11T03:52:12.8423666Z         adamw: bool,
2025-04-11T03:52:12.8423760Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8423845Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8423923Z     ) -> None:
2025-04-11T03:52:12.8424175Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8424273Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8424282Z 
2025-04-11T03:52:12.8424390Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8424502Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8424822Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8424918Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8425153Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8425249Z     return self._apply(convert)
2025-04-11T03:52:12.8425488Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8425631Z     module._apply(fn)
2025-04-11T03:52:12.8425867Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8425953Z     module._apply(fn)
2025-04-11T03:52:12.8426184Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8426266Z     module._apply(fn)
2025-04-11T03:52:12.8426496Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8426591Z     param_applied = fn(param)
2025-04-11T03:52:12.8426702Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8426706Z 
2025-04-11T03:52:12.8426794Z t = Parameter containing:
2025-04-11T03:52:12.8426936Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8427035Z         [ 0.0281,  0.0026,...4, -0.0037,  0.0294],
2025-04-11T03:52:12.8427159Z         [ 0.0003,  0.0104, -0.0075,  ...,  0.0078,  0.0005, -0.0179]],
2025-04-11T03:52:12.8427245Z        requires_grad=True)
2025-04-11T03:52:12.8427249Z 
2025-04-11T03:52:12.8427388Z     def convert(t):
2025-04-11T03:52:12.8427519Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8427697Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8427819Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8428025Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8428139Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8428469Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8428611Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8428772Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8428777Z 
2025-04-11T03:52:12.8429026Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8429215Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0] _______
2025-04-11T03:52:12.8429219Z 
2025-04-11T03:52:12.8429384Z optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
2025-04-11T03:52:12.8429555Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8429646Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8429651Z 
2025-04-11T03:52:12.8429869Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8429996Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8430227Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8430319Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8430477Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8430567Z         device: torch.device,
2025-04-11T03:52:12.8430645Z         adamw: bool,
2025-04-11T03:52:12.8430737Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8430822Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8430898Z     ) -> None:
2025-04-11T03:52:12.8431151Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8431312Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8431317Z 
2025-04-11T03:52:12.8431431Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8431544Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8431792Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8431888Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8432175Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8432267Z     return self._apply(convert)
2025-04-11T03:52:12.8432498Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8432586Z     module._apply(fn)
2025-04-11T03:52:12.8432812Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8432895Z     module._apply(fn)
2025-04-11T03:52:12.8433123Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8433207Z     module._apply(fn)
2025-04-11T03:52:12.8433431Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8433520Z     param_applied = fn(param)
2025-04-11T03:52:12.8433635Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8433639Z 
2025-04-11T03:52:12.8433729Z t = Parameter containing:
2025-04-11T03:52:12.8433863Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8433963Z         [ 0.0240, -0.0152,...6, -0.0175, -0.0244],
2025-04-11T03:52:12.8434154Z         [-0.0064, -0.0248,  0.0195,  ..., -0.0030, -0.0263,  0.0248]],
2025-04-11T03:52:12.8434241Z        requires_grad=True)
2025-04-11T03:52:12.8434245Z 
2025-04-11T03:52:12.8434325Z     def convert(t):
2025-04-11T03:52:12.8434463Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8434640Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8434761Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8434964Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8435076Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8435356Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8435494Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8435657Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8435662Z 
2025-04-11T03:52:12.8435910Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8436090Z _______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2] ________
2025-04-11T03:52:12.8436096Z 
2025-04-11T03:52:12.8436249Z optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
2025-04-11T03:52:12.8436419Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8436506Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8436567Z 
2025-04-11T03:52:12.8436740Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8436862Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8437020Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8437117Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8437274Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8437363Z         device: torch.device,
2025-04-11T03:52:12.8437442Z         adamw: bool,
2025-04-11T03:52:12.8437535Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8437619Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8437752Z     ) -> None:
2025-04-11T03:52:12.8438008Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8438104Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8438109Z 
2025-04-11T03:52:12.8438222Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8438332Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8438633Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8438728Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8438953Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8439051Z     return self._apply(convert)
2025-04-11T03:52:12.8439280Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8439366Z     module._apply(fn)
2025-04-11T03:52:12.8439592Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8439674Z     module._apply(fn)
2025-04-11T03:52:12.8439898Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8439978Z     module._apply(fn)
2025-04-11T03:52:12.8440206Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8440296Z     param_applied = fn(param)
2025-04-11T03:52:12.8440406Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8440410Z 
2025-04-11T03:52:12.8440553Z t = Parameter containing:
2025-04-11T03:52:12.8440690Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8440787Z         [ 0.0105,  0.0235,...3,  0.0204, -0.0137],
2025-04-11T03:52:12.8440906Z         [ 0.0001, -0.0009, -0.0197,  ...,  0.0352, -0.0017,  0.0075]],
2025-04-11T03:52:12.8440999Z        requires_grad=True)
2025-04-11T03:52:12.8441003Z 
2025-04-11T03:52:12.8441082Z     def convert(t):
2025-04-11T03:52:12.8441215Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8441390Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8441514Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8441715Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8441826Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8442108Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8442244Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8442405Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8442411Z 
2025-04-11T03:52:12.8442661Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8442845Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3] ______
2025-04-11T03:52:12.8442849Z 
2025-04-11T03:52:12.8443014Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8443222Z device = device(type='cpu'), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8443313Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8443317Z 
2025-04-11T03:52:12.8443484Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8443607Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8443764Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8443861Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8444015Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8444167Z         device: torch.device,
2025-04-11T03:52:12.8444248Z         adamw: bool,
2025-04-11T03:52:12.8444336Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8444426Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8444503Z     ) -> None:
2025-04-11T03:52:12.8444754Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8444851Z         torch_model = model_fn().to(device)
2025-04-11T03:52:12.8445103Z         model = deepcopy(torch_model).to(p_dtype)
2025-04-11T03:52:12.8445184Z         lr = 1e-3
2025-04-11T03:52:12.8445272Z         beta1, beta2 = 0.9, 0.999
2025-04-11T03:52:12.8445360Z         eps = 1e-8
2025-04-11T03:52:12.8445464Z         torch_optim_cls = AdamW if adamw else Adam
2025-04-11T03:52:12.8445694Z         torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
2025-04-11T03:52:12.8445906Z >       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T03:52:12.8445912Z 
2025-04-11T03:52:12.8446023Z tests/test_optimizer/test_adam_optim.py:55: 
2025-04-11T03:52:12.8446138Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8446143Z 
2025-04-11T03:52:12.8446224Z self = HybridAdam (
2025-04-11T03:52:12.8446315Z Parameter Group 0
2025-04-11T03:52:12.8446398Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8446485Z     bias_correction: True
2025-04-11T03:52:12.8446565Z     eps: 1e-08
2025-04-11T03:52:12.8446641Z     lr: 0.001
2025-04-11T03:52:12.8446732Z     weig...arameter Group 1
2025-04-11T03:52:12.8446811Z     betas: (0.9, 0.999)
2025-04-11T03:52:12.8446973Z     bias_correction: True
2025-04-11T03:52:12.8447051Z     eps: 1e-08
2025-04-11T03:52:12.8447126Z     lr: 0.001
2025-04-11T03:52:12.8447211Z     weight_decay: 0.0
2025-04-11T03:52:12.8447281Z )
2025-04-11T03:52:12.8447597Z model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
2025-04-11T03:52:12.8447746Z lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
2025-04-11T03:52:12.8447899Z weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
2025-04-11T03:52:12.8447998Z nvme_offload_dir = None, defaults = {}
2025-04-11T03:52:12.8448372Z fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T03:52:12.8448382Z 
2025-04-11T03:52:12.8448461Z     def __init__(
2025-04-11T03:52:12.8448534Z         self,
2025-04-11T03:52:12.8448618Z         model_params,
2025-04-11T03:52:12.8448693Z         lr=1e-3,
2025-04-11T03:52:12.8448783Z         bias_correction=True,
2025-04-11T03:52:12.8448863Z         betas=(0.9, 0.999),
2025-04-11T03:52:12.8448939Z         eps=1e-8,
2025-04-11T03:52:12.8449025Z         weight_decay=0,
2025-04-11T03:52:12.8449105Z         adamw_mode=True,
2025-04-11T03:52:12.8449207Z         nvme_offload_fraction: float = 0.0,
2025-04-11T03:52:12.8449313Z         nvme_offload_dir: Optional[str] = None,
2025-04-11T03:52:12.8449395Z         **defaults: Any,
2025-04-11T03:52:12.8449474Z     ):
2025-04-11T03:52:12.8449555Z         super().__init__(
2025-04-11T03:52:12.8449703Z             model_params,
2025-04-11T03:52:12.8449779Z             lr,
2025-04-11T03:52:12.8449864Z             bias_correction,
2025-04-11T03:52:12.8449945Z             betas,
2025-04-11T03:52:12.8450021Z             eps,
2025-04-11T03:52:12.8450103Z             weight_decay,
2025-04-11T03:52:12.8450183Z             adamw_mode,
2025-04-11T03:52:12.8450276Z             nvme_offload_fraction,
2025-04-11T03:52:12.8450363Z             nvme_offload_dir,
2025-04-11T03:52:12.8450436Z         )
2025-04-11T03:52:12.8450538Z         if torch.cuda.is_available():
2025-04-11T03:52:12.8450661Z             fused_optim = FusedOptimizerLoader().load()
2025-04-11T03:52:12.8450790Z             self.gpu_adam_op = fused_optim.multi_tensor_adam
2025-04-11T03:52:12.8451066Z >           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
2025-04-11T03:52:12.8451176Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8451468Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8451610Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8451832Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8451837Z 
2025-04-11T03:52:12.8451973Z colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
2025-04-11T03:52:12.8452158Z ______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4] ______
2025-04-11T03:52:12.8452162Z 
2025-04-11T03:52:12.8452327Z optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
2025-04-11T03:52:12.8452494Z device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
2025-04-11T03:52:12.8452584Z g_dtype = torch.bfloat16
2025-04-11T03:52:12.8452588Z 
2025-04-11T03:52:12.8452756Z     @pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
2025-04-11T03:52:12.8452884Z     @pytest.mark.parametrize("adamw", [False, True])
2025-04-11T03:52:12.8453041Z     @pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
2025-04-11T03:52:12.8453139Z     def test_adam_optim_on_bert(
2025-04-11T03:52:12.8453298Z         optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
2025-04-11T03:52:12.8453389Z         device: torch.device,
2025-04-11T03:52:12.8453467Z         adamw: bool,
2025-04-11T03:52:12.8453615Z         p_dtype: torch.dtype,
2025-04-11T03:52:12.8453706Z         g_dtype: torch.dtype,
2025-04-11T03:52:12.8453783Z     ) -> None:
2025-04-11T03:52:12.8454037Z         model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
2025-04-11T03:52:12.8454135Z >       torch_model = model_fn().to(device)
2025-04-11T03:52:12.8454140Z 
2025-04-11T03:52:12.8454252Z tests/test_optimizer/test_adam_optim.py:48: 
2025-04-11T03:52:12.8454366Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8454613Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
2025-04-11T03:52:12.8454713Z     return super().to(*args, **kwargs)
2025-04-11T03:52:12.8454940Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
2025-04-11T03:52:12.8455036Z     return self._apply(convert)
2025-04-11T03:52:12.8455270Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8455355Z     module._apply(fn)
2025-04-11T03:52:12.8455587Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8455667Z     module._apply(fn)
2025-04-11T03:52:12.8455904Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8455985Z     module._apply(fn)
2025-04-11T03:52:12.8456220Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8456309Z     param_applied = fn(param)
2025-04-11T03:52:12.8456474Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8456481Z 
2025-04-11T03:52:12.8456572Z t = Parameter containing:
2025-04-11T03:52:12.8456709Z tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
2025-04-11T03:52:12.8456816Z         [ 0.0140, -0.0115,...1,  0.0094,  0.0310],
2025-04-11T03:52:12.8456941Z         [ 0.0050,  0.0139, -0.0004,  ...,  0.0203, -0.0216, -0.0075]],
2025-04-11T03:52:12.8457032Z        requires_grad=True)
2025-04-11T03:52:12.8457038Z 
2025-04-11T03:52:12.8457121Z     def convert(t):
2025-04-11T03:52:12.8457256Z         if convert_to_format is not None and t.dim() in (4, 5):
2025-04-11T03:52:12.8457502Z             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
2025-04-11T03:52:12.8457623Z                         non_blocking, memory_format=convert_to_format)
2025-04-11T03:52:12.8457830Z >       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8457940Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8458228Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8458419Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8458584Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8458589Z 
2025-04-11T03:52:12.8458846Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
2025-04-11T03:52:12.8458987Z _____________________________ test_dist_adafactor ______________________________
2025-04-11T03:52:12.8458998Z 
2025-04-11T03:52:12.8459091Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8459705Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8459717Z 
2025-04-11T03:52:12.8459820Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8459901Z         try_count = 0
2025-04-11T03:52:12.8460003Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8460086Z             max_try, int
2025-04-11T03:52:12.8460296Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8460366Z     
2025-04-11T03:52:12.8460484Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8460564Z             try:
2025-04-11T03:52:12.8460650Z                 try_count += 1
2025-04-11T03:52:12.8460744Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8460825Z                 return ret
2025-04-11T03:52:12.8460924Z             except exception_type as e:
2025-04-11T03:52:12.8461030Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8461221Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8461349Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8461500Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8461660Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8461744Z                     continue
2025-04-11T03:52:12.8461821Z                 else:
2025-04-11T03:52:12.8462043Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8462124Z >                   raise e
2025-04-11T03:52:12.8462130Z 
2025-04-11T03:52:12.8462234Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8462345Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8462482Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8462624Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8462797Z tests/test_optimizer/test_dist_adafactor.py:468: in test_dist_adafactor
2025-04-11T03:52:12.8462891Z     spawn(run_dist, nprocs=4)
2025-04-11T03:52:12.8462994Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8463098Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8463350Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8463532Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8463816Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8464016Z     while not context.join():
2025-04-11T03:52:12.8464131Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8464135Z 
2025-04-11T03:52:12.8464336Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f023e410>
2025-04-11T03:52:12.8464423Z timeout = None
2025-04-11T03:52:12.8464427Z 
2025-04-11T03:52:12.8464516Z     def join(self, timeout=None):
2025-04-11T03:52:12.8464646Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8464772Z     
2025-04-11T03:52:12.8464920Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8465072Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8465236Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8465335Z         of the first process exiting.
2025-04-11T03:52:12.8465406Z     
2025-04-11T03:52:12.8465561Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8465701Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8465771Z     
2025-04-11T03:52:12.8465853Z         Args:
2025-04-11T03:52:12.8465990Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8466068Z         """
2025-04-11T03:52:12.8466208Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8466307Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8466386Z             return True
2025-04-11T03:52:12.8466458Z     
2025-04-11T03:52:12.8466597Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8466779Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8466878Z             self.sentinels.keys(),
2025-04-11T03:52:12.8466965Z             timeout=timeout,
2025-04-11T03:52:12.8467037Z         )
2025-04-11T03:52:12.8467111Z     
2025-04-11T03:52:12.8467196Z         error_index = None
2025-04-11T03:52:12.8467287Z         for sentinel in ready:
2025-04-11T03:52:12.8467395Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8467494Z             process = self.processes[index]
2025-04-11T03:52:12.8467583Z             process.join()
2025-04-11T03:52:12.8467677Z             if process.exitcode != 0:
2025-04-11T03:52:12.8467769Z                 error_index = index
2025-04-11T03:52:12.8467845Z                 break
2025-04-11T03:52:12.8467919Z     
2025-04-11T03:52:12.8468012Z         # Return if there was no error.
2025-04-11T03:52:12.8468098Z         if error_index is None:
2025-04-11T03:52:12.8468236Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8468335Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8468445Z     
2025-04-11T03:52:12.8468588Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8468686Z         for process in self.processes:
2025-04-11T03:52:12.8468781Z             if process.is_alive():
2025-04-11T03:52:12.8468875Z                 process.terminate()
2025-04-11T03:52:12.8468963Z             process.join()
2025-04-11T03:52:12.8469035Z     
2025-04-11T03:52:12.8469175Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8469362Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8469468Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8469597Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8469681Z             if exitcode < 0:
2025-04-11T03:52:12.8469792Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8469903Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8470057Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8470160Z                     error_index=error_index,
2025-04-11T03:52:12.8470264Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8470439Z                     exit_code=exitcode,
2025-04-11T03:52:12.8470525Z                     signal_name=name,
2025-04-11T03:52:12.8470606Z                 )
2025-04-11T03:52:12.8470681Z             else:
2025-04-11T03:52:12.8470786Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8470956Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8471048Z                     error_index=error_index,
2025-04-11T03:52:12.8471215Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8471302Z                     exit_code=exitcode,
2025-04-11T03:52:12.8471377Z                 )
2025-04-11T03:52:12.8471456Z     
2025-04-11T03:52:12.8471588Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8471766Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8471852Z         msg += original_trace
2025-04-11T03:52:12.8472034Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8472198Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8472272Z E       
2025-04-11T03:52:12.8472407Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8472510Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8472812Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8472897Z E           fn(i, *args)
2025-04-11T03:52:12.8473149Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 459, in run_dist
2025-04-11T03:52:12.8473311Z E           exam_dist_adafactor_base()
2025-04-11T03:52:12.8473569Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8473664Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8473916Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8474007Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8474291Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 111, in exam_dist_adafactor_base
2025-04-11T03:52:12.8474453Z E           model_col = nn.Linear(H, W).to(local_rank)  # Col parallel weight
2025-04-11T03:52:12.8474719Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:12.8474816Z E           return self._apply(convert)
2025-04-11T03:52:12.8475090Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8475185Z E           param_applied = fn(param)
2025-04-11T03:52:12.8475463Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:12.8475677Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.8475790Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8476069Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8476262Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8476427Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8476434Z 
2025-04-11T03:52:12.8476734Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8476894Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8477052Z [04/11/25 03:47:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8477188Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8477355Z                              :75 launch                                         
2025-04-11T03:52:12.8477494Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8477620Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8477815Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8478014Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8479143Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8479323Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8480413Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8480588Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8481733Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8481904Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8482983Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8483151Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8483829Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8483918Z   warnings.warn(
2025-04-11T03:52:12.8484592Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8484730Z   warnings.warn(
2025-04-11T03:52:12.8485384Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8485471Z   warnings.warn(
2025-04-11T03:52:12.8486126Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8486273Z   warnings.warn(
2025-04-11T03:52:12.8487088Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8487230Z   warnings.warn(
2025-04-11T03:52:12.8488031Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8488119Z   warnings.warn(
2025-04-11T03:52:12.8488910Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8488995Z   warnings.warn(
2025-04-11T03:52:12.8489784Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8489871Z   warnings.warn(
2025-04-11T03:52:12.8490654Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8490797Z   warnings.warn(
2025-04-11T03:52:12.8491586Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8491670Z   warnings.warn(
2025-04-11T03:52:12.8492457Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8492540Z   warnings.warn(
2025-04-11T03:52:12.8493334Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8493416Z   warnings.warn(
2025-04-11T03:52:12.8494206Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8494341Z   warnings.warn(
2025-04-11T03:52:12.8495122Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8495202Z   warnings.warn(
2025-04-11T03:52:12.8495981Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8496126Z   warnings.warn(
2025-04-11T03:52:12.8496931Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8497067Z   warnings.warn(
2025-04-11T03:52:12.8497208Z ________________________________ test_dist_came ________________________________
2025-04-11T03:52:12.8497215Z 
2025-04-11T03:52:12.8497309Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8497910Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8497918Z 
2025-04-11T03:52:12.8498025Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8498111Z         try_count = 0
2025-04-11T03:52:12.8498211Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8498295Z             max_try, int
2025-04-11T03:52:12.8498453Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8498526Z     
2025-04-11T03:52:12.8498643Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8498718Z             try:
2025-04-11T03:52:12.8498863Z                 try_count += 1
2025-04-11T03:52:12.8498963Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8499045Z                 return ret
2025-04-11T03:52:12.8499150Z             except exception_type as e:
2025-04-11T03:52:12.8499256Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8499450Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8499577Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8499723Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8499885Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8499968Z                     continue
2025-04-11T03:52:12.8500053Z                 else:
2025-04-11T03:52:12.8500274Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8500362Z >                   raise e
2025-04-11T03:52:12.8500367Z 
2025-04-11T03:52:12.8500461Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8500571Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8500710Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8500800Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8500959Z tests/test_optimizer/test_dist_came.py:357: in test_dist_came
2025-04-11T03:52:12.8501050Z     spawn(run_dist, nprocs=4)
2025-04-11T03:52:12.8501154Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8501259Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8501584Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8501766Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8502049Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8502147Z     while not context.join():
2025-04-11T03:52:12.8502259Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8502263Z 
2025-04-11T03:52:12.8502461Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f023e3b0>
2025-04-11T03:52:12.8502606Z timeout = None
2025-04-11T03:52:12.8502611Z 
2025-04-11T03:52:12.8502703Z     def join(self, timeout=None):
2025-04-11T03:52:12.8502832Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8502903Z     
2025-04-11T03:52:12.8503053Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8503204Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8503427Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8503522Z         of the first process exiting.
2025-04-11T03:52:12.8503594Z     
2025-04-11T03:52:12.8503747Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8503889Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8503962Z     
2025-04-11T03:52:12.8504036Z         Args:
2025-04-11T03:52:12.8504175Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8504257Z         """
2025-04-11T03:52:12.8504396Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8504491Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8504572Z             return True
2025-04-11T03:52:12.8504648Z     
2025-04-11T03:52:12.8504784Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8504907Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8505006Z             self.sentinels.keys(),
2025-04-11T03:52:12.8505090Z             timeout=timeout,
2025-04-11T03:52:12.8505164Z         )
2025-04-11T03:52:12.8505235Z     
2025-04-11T03:52:12.8505379Z         error_index = None
2025-04-11T03:52:12.8505472Z         for sentinel in ready:
2025-04-11T03:52:12.8505581Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8505685Z             process = self.processes[index]
2025-04-11T03:52:12.8505772Z             process.join()
2025-04-11T03:52:12.8505869Z             if process.exitcode != 0:
2025-04-11T03:52:12.8505960Z                 error_index = index
2025-04-11T03:52:12.8506036Z                 break
2025-04-11T03:52:12.8506111Z     
2025-04-11T03:52:12.8506205Z         # Return if there was no error.
2025-04-11T03:52:12.8506292Z         if error_index is None:
2025-04-11T03:52:12.8506431Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8506527Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8506604Z     
2025-04-11T03:52:12.8506742Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8506846Z         for process in self.processes:
2025-04-11T03:52:12.8506937Z             if process.is_alive():
2025-04-11T03:52:12.8507030Z                 process.terminate()
2025-04-11T03:52:12.8507119Z             process.join()
2025-04-11T03:52:12.8507190Z     
2025-04-11T03:52:12.8507332Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8507453Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8507561Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8507686Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8507771Z             if exitcode < 0:
2025-04-11T03:52:12.8507937Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8508045Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8508202Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8508301Z                     error_index=error_index,
2025-04-11T03:52:12.8508405Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8508554Z                     exit_code=exitcode,
2025-04-11T03:52:12.8508643Z                     signal_name=name,
2025-04-11T03:52:12.8508728Z                 )
2025-04-11T03:52:12.8508807Z             else:
2025-04-11T03:52:12.8508918Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8509153Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8509248Z                     error_index=error_index,
2025-04-11T03:52:12.8509353Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8509440Z                     exit_code=exitcode,
2025-04-11T03:52:12.8509521Z                 )
2025-04-11T03:52:12.8509591Z     
2025-04-11T03:52:12.8509723Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8509956Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8510043Z         msg += original_trace
2025-04-11T03:52:12.8510221Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8510381Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8510457Z E       
2025-04-11T03:52:12.8510583Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8510683Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8510986Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8511069Z E           fn(i, *args)
2025-04-11T03:52:12.8511305Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 349, in run_dist
2025-04-11T03:52:12.8511451Z E           exam_bert_test_on_lowlevelzero_plugin()  # err in TODO layer
2025-04-11T03:52:12.8511713Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8511807Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8512176Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 206, in exam_bert_test_on_lowlevelzero_plugin
2025-04-11T03:52:12.8512404Z E           ) = build_model_from_low_level_zero_plugin(model_fn, loss_fn, test_config, CAME, DistributedCAME)
2025-04-11T03:52:12.8512722Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 188, in build_model_from_low_level_zero_plugin
2025-04-11T03:52:12.8512826Z E           org_model = org_model.cuda()
2025-04-11T03:52:12.8513111Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.8513220Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.8513485Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8513608Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8513878Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8513966Z E           module._apply(fn)
2025-04-11T03:52:12.8514232Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8514320Z E           module._apply(fn)
2025-04-11T03:52:12.8514586Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8514681Z E           param_applied = fn(param)
2025-04-11T03:52:12.8514956Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8515136Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8515245Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8515532Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8515672Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8515837Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8515841Z 
2025-04-11T03:52:12.8516200Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8516357Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8516513Z [04/11/25 03:47:13] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8516650Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8516759Z                              :75 launch                                         
2025-04-11T03:52:12.8516966Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8517096Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8517292Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8517445Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8518567Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8518744Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8519850Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8520077Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8521155Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8521326Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8522396Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8522565Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8523239Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8523385Z   warnings.warn(
2025-04-11T03:52:12.8524046Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8524135Z   warnings.warn(
2025-04-11T03:52:12.8524806Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8524954Z   warnings.warn(
2025-04-11T03:52:12.8525609Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8525758Z   warnings.warn(
2025-04-11T03:52:12.8526577Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8526662Z   warnings.warn(
2025-04-11T03:52:12.8527449Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8527534Z   warnings.warn(
2025-04-11T03:52:12.8528333Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8528415Z   warnings.warn(
2025-04-11T03:52:12.8529266Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8529348Z   warnings.warn(
2025-04-11T03:52:12.8530128Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8530208Z   warnings.warn(
2025-04-11T03:52:12.8531055Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8531141Z   warnings.warn(
2025-04-11T03:52:12.8531923Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8532004Z   warnings.warn(
2025-04-11T03:52:12.8532774Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8532913Z   warnings.warn(
2025-04-11T03:52:12.8533701Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8533785Z   warnings.warn(
2025-04-11T03:52:12.8534570Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8534713Z   warnings.warn(
2025-04-11T03:52:12.8535491Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8535630Z   warnings.warn(
2025-04-11T03:52:12.8536414Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8536494Z   warnings.warn(
2025-04-11T03:52:12.8536792Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8537074Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8537216Z _______________________________ test_dist_galore _______________________________
2025-04-11T03:52:12.8537222Z 
2025-04-11T03:52:12.8537318Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8537922Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8537983Z 
2025-04-11T03:52:12.8538093Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8538177Z         try_count = 0
2025-04-11T03:52:12.8538284Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8538366Z             max_try, int
2025-04-11T03:52:12.8538514Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8538588Z     
2025-04-11T03:52:12.8538701Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8538787Z             try:
2025-04-11T03:52:12.8538871Z                 try_count += 1
2025-04-11T03:52:12.8538966Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8539049Z                 return ret
2025-04-11T03:52:12.8539146Z             except exception_type as e:
2025-04-11T03:52:12.8539250Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8539438Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8539562Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8539710Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8539871Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8539954Z                     continue
2025-04-11T03:52:12.8540033Z                 else:
2025-04-11T03:52:12.8540258Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8540401Z >                   raise e
2025-04-11T03:52:12.8540406Z 
2025-04-11T03:52:12.8540512Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8540625Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8540768Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8540858Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8541020Z tests/test_optimizer/test_dist_galore.py:298: in test_dist_galore
2025-04-11T03:52:12.8541124Z     spawn(check_dist_galore, nprocs=4)
2025-04-11T03:52:12.8541224Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8541391Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8541651Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8541830Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8542118Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8542207Z     while not context.join():
2025-04-11T03:52:12.8542379Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8542383Z 
2025-04-11T03:52:12.8542581Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ee240a0>
2025-04-11T03:52:12.8542666Z timeout = None
2025-04-11T03:52:12.8542671Z 
2025-04-11T03:52:12.8542762Z     def join(self, timeout=None):
2025-04-11T03:52:12.8542890Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8542964Z     
2025-04-11T03:52:12.8543111Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8543258Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8543416Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8543513Z         of the first process exiting.
2025-04-11T03:52:12.8543587Z     
2025-04-11T03:52:12.8543736Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8543874Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8543946Z     
2025-04-11T03:52:12.8544027Z         Args:
2025-04-11T03:52:12.8544220Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8544299Z         """
2025-04-11T03:52:12.8544437Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8544530Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8544616Z             return True
2025-04-11T03:52:12.8544689Z     
2025-04-11T03:52:12.8544822Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8544939Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8545035Z             self.sentinels.keys(),
2025-04-11T03:52:12.8545119Z             timeout=timeout,
2025-04-11T03:52:12.8545194Z         )
2025-04-11T03:52:12.8545269Z     
2025-04-11T03:52:12.8545354Z         error_index = None
2025-04-11T03:52:12.8545446Z         for sentinel in ready:
2025-04-11T03:52:12.8545551Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8545651Z             process = self.processes[index]
2025-04-11T03:52:12.8545742Z             process.join()
2025-04-11T03:52:12.8545836Z             if process.exitcode != 0:
2025-04-11T03:52:12.8545929Z                 error_index = index
2025-04-11T03:52:12.8546004Z                 break
2025-04-11T03:52:12.8546075Z     
2025-04-11T03:52:12.8546174Z         # Return if there was no error.
2025-04-11T03:52:12.8546262Z         if error_index is None:
2025-04-11T03:52:12.8546401Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8546500Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8546575Z     
2025-04-11T03:52:12.8546716Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8546874Z         for process in self.processes:
2025-04-11T03:52:12.8546967Z             if process.is_alive():
2025-04-11T03:52:12.8547063Z                 process.terminate()
2025-04-11T03:52:12.8547149Z             process.join()
2025-04-11T03:52:12.8547220Z     
2025-04-11T03:52:12.8547359Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8547485Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8547595Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8547721Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8547868Z             if exitcode < 0:
2025-04-11T03:52:12.8547978Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8548091Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8548243Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8548348Z                     error_index=error_index,
2025-04-11T03:52:12.8548490Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8548585Z                     exit_code=exitcode,
2025-04-11T03:52:12.8548738Z                     signal_name=name,
2025-04-11T03:52:12.8548815Z                 )
2025-04-11T03:52:12.8548894Z             else:
2025-04-11T03:52:12.8548998Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8549167Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8549261Z                     error_index=error_index,
2025-04-11T03:52:12.8549365Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8549454Z                     exit_code=exitcode,
2025-04-11T03:52:12.8549528Z                 )
2025-04-11T03:52:12.8549604Z     
2025-04-11T03:52:12.8549734Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8549912Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8550001Z         msg += original_trace
2025-04-11T03:52:12.8550172Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8550342Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8550416Z E       
2025-04-11T03:52:12.8550547Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8550709Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8551014Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8551095Z E           fn(i, *args)
2025-04-11T03:52:12.8551368Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_galore.py", line 291, in check_dist_galore
2025-04-11T03:52:12.8551456Z E           dist.barrier()
2025-04-11T03:52:12.8551752Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:12.8551855Z E           return func(*args, **kwargs)
2025-04-11T03:52:12.8552178Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T03:52:12.8552290Z E           work = default_pg.barrier(opts=opts)
2025-04-11T03:52:12.8552396Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8552694Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8552834Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8552998Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8553005Z 
2025-04-11T03:52:12.8553311Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8553464Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8553718Z [04/11/25 03:47:21] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8553848Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8553961Z                              :75 launch                                         
2025-04-11T03:52:12.8554096Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8554219Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8554360Z Skipping forward-backward tests due to SVD instability
2025-04-11T03:52:12.8554770Z Running bert tests, which are expected to produce minor errors due to instability in SVD convergence.             For example, a 1e-9 grad diff causes drastic difference in SVD output.
2025-04-11T03:52:12.8554933Z CUDA error: out of memory
2025-04-11T03:52:12.8555212Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8555347Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8555507Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8555577Z 
2025-04-11T03:52:12.8555671Z CUDA error: out of memory
2025-04-11T03:52:12.8555944Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8556069Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8556229Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8556234Z 
2025-04-11T03:52:12.8556321Z CUDA error: out of memory
2025-04-11T03:52:12.8556593Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8556715Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8556869Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8556875Z 
2025-04-11T03:52:12.8557256Z [3] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
2025-04-11T03:52:12.8557670Z Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
2025-04-11T03:52:12.8558188Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f5059469d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T03:52:12.8558534Z frame #1: <unknown function> + 0x5522c2e (0x7f509dd52c2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8559043Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f509dd4d440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8559427Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f509dd4d782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8559796Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f509dd4e5b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8560165Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8560534Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8560878Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8561246Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8561818Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f505a62ea59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8562430Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f505a635a4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8563254Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f505a64be4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8563738Z frame #12: <unknown function> + 0x54c7dbd (0x7f509dcf7dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8564100Z frame #13: <unknown function> + 0x54d1cb8 (0x7f509dd01cb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8564423Z frame #14: <unknown function> + 0x4b16e6c (0x7f509d346e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8564736Z frame #15: <unknown function> + 0x1696528 (0x7f5099ec6528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8565051Z frame #16: <unknown function> + 0x54d94d3 (0x7f509dd094d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8565363Z frame #17: <unknown function> + 0x54e48bf (0x7f509dd148bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8566314Z frame #18: c10d::verify_params_across_processes(c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::optional<std::weak_ptr<c10d::Logger> > const&) + 0x26f (0x7f509dd7af2f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8566705Z frame #19: <unknown function> + 0xc55ad1 (0x7f50a5925ad1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8567032Z frame #20: <unknown function> + 0x413ea4 (0x7f50a50e3ea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8567171Z frame #21: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T03:52:12.8567374Z frame #22: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8567588Z frame #23: _PyEval_EvalFrameDefault + 0x53d6 (0x4f34c6 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8567783Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8567990Z frame #25: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8568180Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8568399Z frame #27: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8568532Z frame #28: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8568671Z frame #29: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T03:52:12.8568848Z frame #30: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8569057Z frame #31: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8569248Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8569522Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8569661Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8569845Z frame #35: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8570052Z frame #36: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8570242Z frame #37: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8570505Z frame #38: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8570693Z frame #39: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8570894Z frame #40: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8571083Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8571278Z frame #42: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8571523Z frame #43: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8571718Z frame #44: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8571906Z frame #45: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8572078Z frame #46: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8572216Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T03:52:12.8572399Z frame #48: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8572566Z frame #49: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8572771Z frame #50: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8572955Z frame #51: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8573160Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8573402Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8573570Z frame #54: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8573698Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T03:52:12.8573887Z frame #56: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8574083Z frame #57: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8574266Z frame #58: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8574469Z frame #59: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8574655Z frame #60: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8574853Z frame #61: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8575039Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8575237Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8575440Z . This may indicate a possible application crash on rank 0 or a network set up issue.
2025-04-11T03:52:12.8575807Z [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
2025-04-11T03:52:12.8576199Z Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
2025-04-11T03:52:12.8576624Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f60e3718d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T03:52:12.8576950Z frame #1: <unknown function> + 0x5522c2e (0x7f6128001c2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8577435Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f6127ffc440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8577911Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f6127ffc782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8578252Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f6127ffd5b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8578609Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f6127fb2a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8579006Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f6127fb2a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8579352Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f6127fb2a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8579778Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f6127fb2a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8580296Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f60e48dda59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8580888Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f60e48e4a4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8581692Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f60e48fae4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8582077Z frame #12: <unknown function> + 0x54c7dbd (0x7f6127fa6dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8582395Z frame #13: <unknown function> + 0x54d1cb8 (0x7f6127fb0cb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8582715Z frame #14: <unknown function> + 0x4b16e6c (0x7f61275f5e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8583032Z frame #15: <unknown function> + 0x1696528 (0x7f6124175528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8583342Z frame #16: <unknown function> + 0x54d94d3 (0x7f6127fb84d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8583657Z frame #17: <unknown function> + 0x54e48bf (0x7f6127fc38bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8584500Z frame #18: c10d::verify_params_across_processes(c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::optional<std::weak_ptr<c10d::Logger> > const&) + 0x26f (0x7f6128029f2f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8584898Z frame #19: <unknown function> + 0xc55ad1 (0x7f612fbd4ad1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8585224Z frame #20: <unknown function> + 0x413ea4 (0x7f612f392ea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8585359Z frame #21: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T03:52:12.8585551Z frame #22: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8585758Z frame #23: _PyEval_EvalFrameDefault + 0x53d6 (0x4f34c6 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8586026Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8586223Z frame #25: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8586416Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8586626Z frame #27: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8586819Z frame #28: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8586948Z frame #29: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T03:52:12.8587125Z frame #30: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8587326Z frame #31: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8587513Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8587724Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8587849Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8588036Z frame #35: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8588238Z frame #36: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8588481Z frame #37: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8588679Z frame #38: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8588940Z frame #39: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8589136Z frame #40: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8589324Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8589523Z frame #42: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8589708Z frame #43: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8589909Z frame #44: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8590093Z frame #45: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8590265Z frame #46: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8590394Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T03:52:12.8590576Z frame #48: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8590750Z frame #49: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8590955Z frame #50: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8591145Z frame #51: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8591347Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8591602Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8591767Z frame #54: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8591897Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
2025-04-11T03:52:12.8592084Z frame #56: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8592280Z frame #57: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8592471Z frame #58: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8592746Z frame #59: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8592936Z frame #60: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8593132Z frame #61: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8593322Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8593579Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8593783Z . This may indicate a possible application crash on rank 0 or a network set up issue.
2025-04-11T03:52:12.8594380Z Failed to replace attention.self.query of type Linear with Linear1D_Col with the exception: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
2025-04-11T03:52:12.8594771Z Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
2025-04-11T03:52:12.8595151Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f5059469d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T03:52:12.8595477Z frame #1: <unknown function> + 0x5522c2e (0x7f509dd52c2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8595967Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f509dd4d440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8596378Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f509dd4d782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8596724Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f509dd4e5b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8597071Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8597421Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8597766Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8598115Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8598608Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f505a62ea59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8599208Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f505a635a4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8600069Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f505a64be4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8600408Z frame #12: <unknown function> + 0x54c7dbd (0x7f509dcf7dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8600727Z frame #13: <unknown function> + 0x54d1cb8 (0x7f509dd01cb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8601113Z frame #14: <unknown function> + 0x4b16e6c (0x7f509d346e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8601426Z frame #15: <unknown function> + 0x1696528 (0x7f5099ec6528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8601747Z frame #16: <unknown function> + 0x54d94d3 (0x7f509dd094d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8602122Z frame #17: <unknown function> + 0x54e48bf (0x7f509dd148bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8602457Z frame #18: <unknown function> + 0xca3fae (0x7f50a5973fae in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8602781Z frame #19: <unknown function> + 0x413ea4 (0x7f50a50e3ea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8602915Z frame #20: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T03:52:12.8603105Z frame #21: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8603234Z frame #22: /opt/conda/envs/pytorch/bin/python() [0x509cbf]
2025-04-11T03:52:12.8603442Z frame #23: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8603631Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8603837Z frame #25: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8604084Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8604288Z frame #27: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8604477Z frame #28: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8604672Z frame #29: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8604861Z frame #30: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8605059Z frame #31: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8605250Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8605462Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8605593Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8605716Z frame #35: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T03:52:12.8605894Z frame #36: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8606092Z frame #37: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8606279Z frame #38: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8606450Z frame #39: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8606701Z frame #40: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8606890Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8607087Z frame #42: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8607219Z frame #43: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8607414Z frame #44: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8607540Z frame #45: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8607808Z frame #46: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8607932Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8608131Z frame #48: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8608257Z frame #49: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8608462Z frame #50: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8608645Z frame #51: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8608846Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8609035Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8609227Z frame #54: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8609361Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8609557Z frame #56: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8609751Z frame #57: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8609958Z frame #58: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8610092Z frame #59: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8610278Z frame #60: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8610477Z frame #61: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8610727Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8610921Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8611372Z . This may indicate a possible application crash on rank 0 or a network set up issue.. Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
2025-04-11T03:52:12.8611932Z Failed to replace attention.self.query of type Linear with Linear1D_Col with the exception: [3] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Broken pipe
2025-04-11T03:52:12.8612321Z Exception raised from sendBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:643 (most recent call first):
2025-04-11T03:52:12.8612688Z frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f5059469d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
2025-04-11T03:52:12.8613017Z frame #1: <unknown function> + 0x5521d1f (0x7f509dd51d1f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8613508Z frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x23f (0x7f509dd4d31f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8613928Z frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f509dd4d782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8614346Z frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f509dd4e5b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8614698Z frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8615044Z frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8615386Z frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8615791Z frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f509dd03a21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8616285Z frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f505a62ea59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8616941Z frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f505a635a4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8617763Z frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f505a64be4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
2025-04-11T03:52:12.8618093Z frame #12: <unknown function> + 0x54c7dbd (0x7f509dcf7dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8618415Z frame #13: <unknown function> + 0x54d1cb8 (0x7f509dd01cb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8618732Z frame #14: <unknown function> + 0x4b16e6c (0x7f509d346e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8619107Z frame #15: <unknown function> + 0x1696528 (0x7f5099ec6528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8619418Z frame #16: <unknown function> + 0x54d94d3 (0x7f509dd094d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8619731Z frame #17: <unknown function> + 0x54e48bf (0x7f509dd148bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
2025-04-11T03:52:12.8620060Z frame #18: <unknown function> + 0xca3fae (0x7f50a5973fae in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8620380Z frame #19: <unknown function> + 0x413ea4 (0x7f50a50e3ea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
2025-04-11T03:52:12.8620516Z frame #20: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
2025-04-11T03:52:12.8620709Z frame #21: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8620841Z frame #22: /opt/conda/envs/pytorch/bin/python() [0x509cbf]
2025-04-11T03:52:12.8621050Z frame #23: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8621242Z frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8621445Z frame #25: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8621633Z frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8621898Z frame #27: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8622084Z frame #28: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8622287Z frame #29: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8622475Z frame #30: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8622678Z frame #31: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8622864Z frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8623140Z frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8623268Z frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8623393Z frame #35: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
2025-04-11T03:52:12.8623567Z frame #36: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8623765Z frame #37: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8624013Z frame #38: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8624183Z frame #39: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8624384Z frame #40: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8624573Z frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8624778Z frame #42: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8624908Z frame #43: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8625106Z frame #44: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8625243Z frame #45: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8625442Z frame #46: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8625575Z frame #47: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8625776Z frame #48: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8625958Z frame #49: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8626153Z frame #50: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8626275Z frame #51: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8626476Z frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8626664Z frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8626862Z frame #54: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8626988Z frame #55: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
2025-04-11T03:52:12.8627193Z frame #56: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8627379Z frame #57: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8627587Z frame #58: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8627718Z frame #59: /opt/conda/envs/pytorch/bin/python() [0x507588]
2025-04-11T03:52:12.8627902Z frame #60: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8628106Z frame #61: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8628290Z frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8628525Z frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
2025-04-11T03:52:12.8629038Z . This may indicate a possible application crash on rank 0 or a network set up issue.. Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
2025-04-11T03:52:12.8629242Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8629399Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8630542Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8630782Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8631953Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8632187Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8633286Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8633452Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8634541Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8634767Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8635452Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8635540Z   warnings.warn(
2025-04-11T03:52:12.8636204Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8636289Z   warnings.warn(
2025-04-11T03:52:12.8636959Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8637042Z   warnings.warn(
2025-04-11T03:52:12.8637693Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8637838Z   warnings.warn(
2025-04-11T03:52:12.8638655Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8638741Z   warnings.warn(
2025-04-11T03:52:12.8639539Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8639689Z   warnings.warn(
2025-04-11T03:52:12.8640470Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8640616Z   warnings.warn(
2025-04-11T03:52:12.8641390Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8641479Z   warnings.warn(
2025-04-11T03:52:12.8642249Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8642335Z   warnings.warn(
2025-04-11T03:52:12.8643108Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8643194Z   warnings.warn(
2025-04-11T03:52:12.8643969Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8644124Z   warnings.warn(
2025-04-11T03:52:12.8644900Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8644983Z   warnings.warn(
2025-04-11T03:52:12.8645765Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8645854Z   warnings.warn(
2025-04-11T03:52:12.8646635Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8646715Z   warnings.warn(
2025-04-11T03:52:12.8647491Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8647631Z   warnings.warn(
2025-04-11T03:52:12.8648421Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8648501Z   warnings.warn(
2025-04-11T03:52:12.8648645Z ________________________________ test_dist_lamb ________________________________
2025-04-11T03:52:12.8648717Z 
2025-04-11T03:52:12.8648814Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8649429Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8649437Z 
2025-04-11T03:52:12.8649544Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8649687Z         try_count = 0
2025-04-11T03:52:12.8649794Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8649884Z             max_try, int
2025-04-11T03:52:12.8650039Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8650111Z     
2025-04-11T03:52:12.8650232Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8650307Z             try:
2025-04-11T03:52:12.8650400Z                 try_count += 1
2025-04-11T03:52:12.8650496Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8650578Z                 return ret
2025-04-11T03:52:12.8650680Z             except exception_type as e:
2025-04-11T03:52:12.8650783Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8650976Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8651099Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8651253Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8651410Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8651551Z                     continue
2025-04-11T03:52:12.8651633Z                 else:
2025-04-11T03:52:12.8651853Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8651937Z >                   raise e
2025-04-11T03:52:12.8651944Z 
2025-04-11T03:52:12.8652040Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8652157Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8652292Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8652381Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8652536Z tests/test_optimizer/test_dist_lamb.py:276: in test_dist_lamb
2025-04-11T03:52:12.8652637Z     spawn(check_dist_lamb, nprocs=4)
2025-04-11T03:52:12.8652748Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8652849Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8653107Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8653297Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8653590Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8653689Z     while not context.join():
2025-04-11T03:52:12.8653802Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8653807Z 
2025-04-11T03:52:12.8654009Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ee27f40>
2025-04-11T03:52:12.8654091Z timeout = None
2025-04-11T03:52:12.8654153Z 
2025-04-11T03:52:12.8654252Z     def join(self, timeout=None):
2025-04-11T03:52:12.8654378Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8654452Z     
2025-04-11T03:52:12.8654605Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8654750Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8654918Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8655012Z         of the first process exiting.
2025-04-11T03:52:12.8655089Z     
2025-04-11T03:52:12.8655234Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8655429Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8655505Z     
2025-04-11T03:52:12.8655582Z         Args:
2025-04-11T03:52:12.8655722Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8655800Z         """
2025-04-11T03:52:12.8655938Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8656035Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8656174Z             return True
2025-04-11T03:52:12.8656247Z     
2025-04-11T03:52:12.8656380Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8656504Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8656602Z             self.sentinels.keys(),
2025-04-11T03:52:12.8656689Z             timeout=timeout,
2025-04-11T03:52:12.8656767Z         )
2025-04-11T03:52:12.8656839Z     
2025-04-11T03:52:12.8656930Z         error_index = None
2025-04-11T03:52:12.8657018Z         for sentinel in ready:
2025-04-11T03:52:12.8657128Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8657235Z             process = self.processes[index]
2025-04-11T03:52:12.8657322Z             process.join()
2025-04-11T03:52:12.8657421Z             if process.exitcode != 0:
2025-04-11T03:52:12.8657515Z                 error_index = index
2025-04-11T03:52:12.8657592Z                 break
2025-04-11T03:52:12.8657666Z     
2025-04-11T03:52:12.8657761Z         # Return if there was no error.
2025-04-11T03:52:12.8657854Z         if error_index is None:
2025-04-11T03:52:12.8657989Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8658144Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8658219Z     
2025-04-11T03:52:12.8658362Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8658464Z         for process in self.processes:
2025-04-11T03:52:12.8658553Z             if process.is_alive():
2025-04-11T03:52:12.8658654Z                 process.terminate()
2025-04-11T03:52:12.8658741Z             process.join()
2025-04-11T03:52:12.8658811Z     
2025-04-11T03:52:12.8658958Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8659073Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8659187Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8659311Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8659400Z             if exitcode < 0:
2025-04-11T03:52:12.8659511Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8659620Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8659780Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8659884Z                     error_index=error_index,
2025-04-11T03:52:12.8659991Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8660084Z                     exit_code=exitcode,
2025-04-11T03:52:12.8660171Z                     signal_name=name,
2025-04-11T03:52:12.8660248Z                 )
2025-04-11T03:52:12.8660325Z             else:
2025-04-11T03:52:12.8660435Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8660601Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8660754Z                     error_index=error_index,
2025-04-11T03:52:12.8660859Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8660947Z                     exit_code=exitcode,
2025-04-11T03:52:12.8661025Z                 )
2025-04-11T03:52:12.8661098Z     
2025-04-11T03:52:12.8661236Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8661407Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8661494Z         msg += original_trace
2025-04-11T03:52:12.8661671Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8661895Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8661977Z E       
2025-04-11T03:52:12.8662104Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8662210Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8662512Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8662651Z E           fn(i, *args)
2025-04-11T03:52:12.8662908Z E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_lamb.py", line 263, in check_dist_lamb
2025-04-11T03:52:12.8663001Z E           run_dist_lamb_basic()
2025-04-11T03:52:12.8663259Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8663351Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8663604Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8663694Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8663938Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8664029Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8664247Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:12.8664352Z E           get_accelerator().synchronize()
2025-04-11T03:52:12.8664607Z E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:12.8664771Z E           torch.cuda.synchronize(device)
2025-04-11T03:52:12.8665049Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:12.8665156Z E           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8665270Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8665556Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8665700Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8665864Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8665871Z 
2025-04-11T03:52:12.8666179Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8666341Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8666504Z [04/11/25 03:47:29] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8666636Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8666747Z                              :75 launch                                         
2025-04-11T03:52:12.8683454Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8683674Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8683888Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8684310Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8685472Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8685658Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8686777Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8687028Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8688185Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8688351Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8689448Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.8689614Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.8690305Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8690461Z   warnings.warn(
2025-04-11T03:52:12.8691123Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8691209Z   warnings.warn(
2025-04-11T03:52:12.8691894Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8691985Z   warnings.warn(
2025-04-11T03:52:12.8692636Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.8692724Z   warnings.warn(
2025-04-11T03:52:12.8693534Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8693683Z   warnings.warn(
2025-04-11T03:52:12.8694489Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8694577Z   warnings.warn(
2025-04-11T03:52:12.8695370Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8695521Z   warnings.warn(
2025-04-11T03:52:12.8696314Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8696399Z   warnings.warn(
2025-04-11T03:52:12.8697320Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8697406Z   warnings.warn(
2025-04-11T03:52:12.8698189Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8698272Z   warnings.warn(
2025-04-11T03:52:12.8699053Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8699138Z   warnings.warn(
2025-04-11T03:52:12.8699921Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8700064Z   warnings.warn(
2025-04-11T03:52:12.8700866Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8700950Z   warnings.warn(
2025-04-11T03:52:12.8701725Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8701808Z   warnings.warn(
2025-04-11T03:52:12.8702608Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8702690Z   warnings.warn(
2025-04-11T03:52:12.8703492Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.8703631Z   warnings.warn(
2025-04-11T03:52:12.8704187Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8704269Z   warnings.warn(
2025-04-11T03:52:12.8704813Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8704955Z   warnings.warn(
2025-04-11T03:52:12.8705487Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8705564Z   warnings.warn(
2025-04-11T03:52:12.8706102Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8706236Z   warnings.warn(
2025-04-11T03:52:12.8706762Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8706846Z   warnings.warn(
2025-04-11T03:52:12.8707373Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8707459Z   warnings.warn(
2025-04-11T03:52:12.8707982Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8708066Z   warnings.warn(
2025-04-11T03:52:12.8708623Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:12.8708774Z   warnings.warn(
2025-04-11T03:52:12.8708922Z ______________________________ test_pipeline_p2p _______________________________
2025-04-11T03:52:12.8708928Z 
2025-04-11T03:52:12.8709031Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8709633Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8709642Z 
2025-04-11T03:52:12.8709751Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8709842Z         try_count = 0
2025-04-11T03:52:12.8709945Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8710033Z             max_try, int
2025-04-11T03:52:12.8710186Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8710265Z     
2025-04-11T03:52:12.8710380Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8710459Z             try:
2025-04-11T03:52:12.8710550Z                 try_count += 1
2025-04-11T03:52:12.8710645Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8710735Z                 return ret
2025-04-11T03:52:12.8710834Z             except exception_type as e:
2025-04-11T03:52:12.8710938Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8711130Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8711252Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8711482Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8711637Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8711728Z                     continue
2025-04-11T03:52:12.8711807Z                 else:
2025-04-11T03:52:12.8712025Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8712111Z >                   raise e
2025-04-11T03:52:12.8712116Z 
2025-04-11T03:52:12.8712213Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8712334Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8712540Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8712633Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8712813Z tests/test_pipeline/test_p2p_communication.py:79: in test_pipeline_p2p
2025-04-11T03:52:12.8712904Z     spawn(run_dist, WORLD_SIZE)
2025-04-11T03:52:12.8713011Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8713114Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8713376Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8713617Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8713910Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8714003Z     while not context.join():
2025-04-11T03:52:12.8714121Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8714131Z 
2025-04-11T03:52:12.8714335Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ee25510>
2025-04-11T03:52:12.8714414Z timeout = None
2025-04-11T03:52:12.8714419Z 
2025-04-11T03:52:12.8714514Z     def join(self, timeout=None):
2025-04-11T03:52:12.8714644Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8714721Z     
2025-04-11T03:52:12.8714867Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8715014Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8715184Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8715342Z         of the first process exiting.
2025-04-11T03:52:12.8715418Z     
2025-04-11T03:52:12.8715567Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8715710Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8715784Z     
2025-04-11T03:52:12.8715860Z         Args:
2025-04-11T03:52:12.8716001Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8716076Z         """
2025-04-11T03:52:12.8716221Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8716314Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8716396Z             return True
2025-04-11T03:52:12.8716473Z     
2025-04-11T03:52:12.8716607Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8716735Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8716831Z             self.sentinels.keys(),
2025-04-11T03:52:12.8716920Z             timeout=timeout,
2025-04-11T03:52:12.8716995Z         )
2025-04-11T03:52:12.8717065Z     
2025-04-11T03:52:12.8717153Z         error_index = None
2025-04-11T03:52:12.8717238Z         for sentinel in ready:
2025-04-11T03:52:12.8717349Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8717450Z             process = self.processes[index]
2025-04-11T03:52:12.8717535Z             process.join()
2025-04-11T03:52:12.8717635Z             if process.exitcode != 0:
2025-04-11T03:52:12.8717723Z                 error_index = index
2025-04-11T03:52:12.8717805Z                 break
2025-04-11T03:52:12.8717877Z     
2025-04-11T03:52:12.8718030Z         # Return if there was no error.
2025-04-11T03:52:12.8718121Z         if error_index is None:
2025-04-11T03:52:12.8718258Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8718363Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8718436Z     
2025-04-11T03:52:12.8718580Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8718681Z         for process in self.processes:
2025-04-11T03:52:12.8718771Z             if process.is_alive():
2025-04-11T03:52:12.8718867Z                 process.terminate()
2025-04-11T03:52:12.8718953Z             process.join()
2025-04-11T03:52:12.8719092Z     
2025-04-11T03:52:12.8719236Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8719354Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8719466Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8719589Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8719680Z             if exitcode < 0:
2025-04-11T03:52:12.8719789Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8719958Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8720111Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8720209Z                     error_index=error_index,
2025-04-11T03:52:12.8720319Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8720408Z                     exit_code=exitcode,
2025-04-11T03:52:12.8720499Z                     signal_name=name,
2025-04-11T03:52:12.8720574Z                 )
2025-04-11T03:52:12.8720652Z             else:
2025-04-11T03:52:12.8720759Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8720926Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8721024Z                     error_index=error_index,
2025-04-11T03:52:12.8721127Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8721215Z                     exit_code=exitcode,
2025-04-11T03:52:12.8721290Z                 )
2025-04-11T03:52:12.8721363Z     
2025-04-11T03:52:12.8721502Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8721675Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8721827Z         msg += original_trace
2025-04-11T03:52:12.8722000Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8722166Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8722243Z E       
2025-04-11T03:52:12.8722369Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8722475Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8722776Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8722867Z E           fn(i, *args)
2025-04-11T03:52:12.8723125Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 73, in run_dist
2025-04-11T03:52:12.8723220Z E           check_p2p_communication()
2025-04-11T03:52:12.8723514Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 21, in check_p2p_communication
2025-04-11T03:52:12.8723681Z E           tensor = torch.ones(1, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8723794Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8724081Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8724224Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8724385Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8724391Z 
2025-04-11T03:52:12.8724701Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8724914Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8725075Z [04/11/25 03:47:34] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8725207Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8725316Z                              :75 launch                                         
2025-04-11T03:52:12.8725459Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8725583Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8725848Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8725990Z _________________________ test_pipeline_stage_manager __________________________
2025-04-11T03:52:12.8725995Z 
2025-04-11T03:52:12.8726093Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8726681Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8726744Z 
2025-04-11T03:52:12.8726855Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8726939Z         try_count = 0
2025-04-11T03:52:12.8727043Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8727130Z             max_try, int
2025-04-11T03:52:12.8727276Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8727351Z     
2025-04-11T03:52:12.8727465Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8727539Z             try:
2025-04-11T03:52:12.8727627Z                 try_count += 1
2025-04-11T03:52:12.8727718Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8727802Z                 return ret
2025-04-11T03:52:12.8727897Z             except exception_type as e:
2025-04-11T03:52:12.8727999Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8728188Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8728309Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8728516Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8728669Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8728754Z                     continue
2025-04-11T03:52:12.8728835Z                 else:
2025-04-11T03:52:12.8729055Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8729138Z >                   raise e
2025-04-11T03:52:12.8729143Z 
2025-04-11T03:52:12.8729237Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8729356Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8729491Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8729583Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8729768Z tests/test_pipeline/test_stage_manager.py:74: in test_pipeline_stage_manager
2025-04-11T03:52:12.8729857Z     spawn(run_dist, 4)
2025-04-11T03:52:12.8729960Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8730060Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8730321Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8730500Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8730785Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8730876Z     while not context.join():
2025-04-11T03:52:12.8731042Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8731051Z 
2025-04-11T03:52:12.8731247Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ecbf8e0>
2025-04-11T03:52:12.8731327Z timeout = None
2025-04-11T03:52:12.8731332Z 
2025-04-11T03:52:12.8731428Z     def join(self, timeout=None):
2025-04-11T03:52:12.8731582Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8731684Z     
2025-04-11T03:52:12.8731836Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8731985Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8732214Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8732309Z         of the first process exiting.
2025-04-11T03:52:12.8732386Z     
2025-04-11T03:52:12.8732534Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8732677Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8732750Z     
2025-04-11T03:52:12.8732828Z         Args:
2025-04-11T03:52:12.8733029Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8733105Z         """
2025-04-11T03:52:12.8733248Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8733344Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8733430Z             return True
2025-04-11T03:52:12.8733503Z     
2025-04-11T03:52:12.8733638Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8733761Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8733856Z             self.sentinels.keys(),
2025-04-11T03:52:12.8733944Z             timeout=timeout,
2025-04-11T03:52:12.8734019Z         )
2025-04-11T03:52:12.8734092Z     
2025-04-11T03:52:12.8734181Z         error_index = None
2025-04-11T03:52:12.8734266Z         for sentinel in ready:
2025-04-11T03:52:12.8734376Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8734475Z             process = self.processes[index]
2025-04-11T03:52:12.8734562Z             process.join()
2025-04-11T03:52:12.8734662Z             if process.exitcode != 0:
2025-04-11T03:52:12.8734751Z                 error_index = index
2025-04-11T03:52:12.8734889Z                 break
2025-04-11T03:52:12.8734959Z     
2025-04-11T03:52:12.8735057Z         # Return if there was no error.
2025-04-11T03:52:12.8735144Z         if error_index is None:
2025-04-11T03:52:12.8735277Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8735381Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8735451Z     
2025-04-11T03:52:12.8735593Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8735688Z         for process in self.processes:
2025-04-11T03:52:12.8735777Z             if process.is_alive():
2025-04-11T03:52:12.8735876Z                 process.terminate()
2025-04-11T03:52:12.8735959Z             process.join()
2025-04-11T03:52:12.8736032Z     
2025-04-11T03:52:12.8736173Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8736295Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8736402Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8736525Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8736614Z             if exitcode < 0:
2025-04-11T03:52:12.8736720Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8736830Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8736980Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8737075Z                     error_index=error_index,
2025-04-11T03:52:12.8737180Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8737268Z                     exit_code=exitcode,
2025-04-11T03:52:12.8737413Z                     signal_name=name,
2025-04-11T03:52:12.8737487Z                 )
2025-04-11T03:52:12.8737566Z             else:
2025-04-11T03:52:12.8737673Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8737837Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8737935Z                     error_index=error_index,
2025-04-11T03:52:12.8738035Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8738127Z                     exit_code=exitcode,
2025-04-11T03:52:12.8738200Z                 )
2025-04-11T03:52:12.8738271Z     
2025-04-11T03:52:12.8738471Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8738642Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8738734Z         msg += original_trace
2025-04-11T03:52:12.8738911Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8739074Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8739149Z E       
2025-04-11T03:52:12.8739279Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8739456Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8739755Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8739843Z E           fn(i, *args)
2025-04-11T03:52:12.8740085Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 68, in run_dist
2025-04-11T03:52:12.8740177Z E           check_stage_manager()
2025-04-11T03:52:12.8740450Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 56, in check_stage_manager
2025-04-11T03:52:12.8740544Z E           dist.barrier(group=group)
2025-04-11T03:52:12.8740849Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:12.8740945Z E           return func(*args, **kwargs)
2025-04-11T03:52:12.8741270Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3441, in barrier
2025-04-11T03:52:12.8741375Z E           work = group.barrier(opts=opts)
2025-04-11T03:52:12.8741542Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8741829Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8741966Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8742135Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8742140Z 
2025-04-11T03:52:12.8742441Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8742598Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8742757Z [04/11/25 03:47:39] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8742890Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8742999Z                              :75 launch                                         
2025-04-11T03:52:12.8743145Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8743269Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8743463Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8743603Z _______________________________ test_pp[2-12-4] ________________________________
2025-04-11T03:52:12.8743607Z 
2025-04-11T03:52:12.8743684Z args = ()
2025-04-11T03:52:12.8743846Z kwargs = {'batch_size': 12, 'num_microbatch': 4, 'num_model_chunk': 2}
2025-04-11T03:52:12.8743922Z try_count = 1
2025-04-11T03:52:12.8744582Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8744590Z 
2025-04-11T03:52:12.8744691Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8744777Z         try_count = 0
2025-04-11T03:52:12.8744880Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8744962Z             max_try, int
2025-04-11T03:52:12.8745116Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8745248Z     
2025-04-11T03:52:12.8745370Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8745445Z             try:
2025-04-11T03:52:12.8745530Z                 try_count += 1
2025-04-11T03:52:12.8745626Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8745707Z                 return ret
2025-04-11T03:52:12.8745810Z             except exception_type as e:
2025-04-11T03:52:12.8745908Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8746155Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8746274Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8746424Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8746583Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8746664Z                     continue
2025-04-11T03:52:12.8746746Z                 else:
2025-04-11T03:52:12.8746966Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8747049Z >                   raise e
2025-04-11T03:52:12.8747054Z 
2025-04-11T03:52:12.8747148Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8747258Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8747398Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8747486Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8747660Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T03:52:12.8747737Z     spawn(
2025-04-11T03:52:12.8747907Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8748005Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8748262Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8748475Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8748757Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8748848Z     while not context.join():
2025-04-11T03:52:12.8748956Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8748963Z 
2025-04-11T03:52:12.8749166Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f023e2f0>
2025-04-11T03:52:12.8749247Z timeout = None
2025-04-11T03:52:12.8749251Z 
2025-04-11T03:52:12.8749342Z     def join(self, timeout=None):
2025-04-11T03:52:12.8749470Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8749544Z     
2025-04-11T03:52:12.8749693Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8749837Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8750000Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8750096Z         of the first process exiting.
2025-04-11T03:52:12.8750168Z     
2025-04-11T03:52:12.8750318Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8750452Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8750595Z     
2025-04-11T03:52:12.8750670Z         Args:
2025-04-11T03:52:12.8750808Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8750888Z         """
2025-04-11T03:52:12.8751025Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8751122Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8751205Z             return True
2025-04-11T03:52:12.8751280Z     
2025-04-11T03:52:12.8751411Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8751528Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8751626Z             self.sentinels.keys(),
2025-04-11T03:52:12.8751775Z             timeout=timeout,
2025-04-11T03:52:12.8751853Z         )
2025-04-11T03:52:12.8751926Z     
2025-04-11T03:52:12.8752010Z         error_index = None
2025-04-11T03:52:12.8752099Z         for sentinel in ready:
2025-04-11T03:52:12.8752207Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8752312Z             process = self.processes[index]
2025-04-11T03:52:12.8752399Z             process.join()
2025-04-11T03:52:12.8752493Z             if process.exitcode != 0:
2025-04-11T03:52:12.8752650Z                 error_index = index
2025-04-11T03:52:12.8752730Z                 break
2025-04-11T03:52:12.8752807Z     
2025-04-11T03:52:12.8752901Z         # Return if there was no error.
2025-04-11T03:52:12.8752989Z         if error_index is None:
2025-04-11T03:52:12.8753123Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8753220Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8753294Z     
2025-04-11T03:52:12.8753436Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8753538Z         for process in self.processes:
2025-04-11T03:52:12.8753628Z             if process.is_alive():
2025-04-11T03:52:12.8753720Z                 process.terminate()
2025-04-11T03:52:12.8753810Z             process.join()
2025-04-11T03:52:12.8753883Z     
2025-04-11T03:52:12.8754027Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8754145Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8754256Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8754379Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8754545Z             if exitcode < 0:
2025-04-11T03:52:12.8754661Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8754769Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8754925Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8755023Z                     error_index=error_index,
2025-04-11T03:52:12.8755123Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8755216Z                     exit_code=exitcode,
2025-04-11T03:52:12.8755304Z                     signal_name=name,
2025-04-11T03:52:12.8755383Z                 )
2025-04-11T03:52:12.8755459Z             else:
2025-04-11T03:52:12.8755565Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8755728Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8755819Z                     error_index=error_index,
2025-04-11T03:52:12.8755926Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8756014Z                     exit_code=exitcode,
2025-04-11T03:52:12.8756088Z                 )
2025-04-11T03:52:12.8756158Z     
2025-04-11T03:52:12.8756289Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8756468Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8756557Z         msg += original_trace
2025-04-11T03:52:12.8756732Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8756890Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8757028Z E       
2025-04-11T03:52:12.8757156Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8757255Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8757560Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8757644Z E           fn(i, *args)
2025-04-11T03:52:12.8757924Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:12.8758022Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8758295Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8758477Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8758748Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8758844Z E           module._apply(fn)
2025-04-11T03:52:12.8759107Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8759255Z E           module._apply(fn)
2025-04-11T03:52:12.8759525Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8759626Z E           param_applied = fn(param)
2025-04-11T03:52:12.8759899Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8760022Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8760132Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8760422Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8760567Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8760730Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8760736Z 
2025-04-11T03:52:12.8761052Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8761206Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8761423Z [04/11/25 03:47:45] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8761552Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8761661Z                              :75 launch                                         
2025-04-11T03:52:12.8761803Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8761929Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8762128Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8762277Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8762580Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38231 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8762713Z _______________________________ test_pp[2-12-12] _______________________________
2025-04-11T03:52:12.8762719Z 
2025-04-11T03:52:12.8762800Z args = ()
2025-04-11T03:52:12.8762956Z kwargs = {'batch_size': 12, 'num_microbatch': 12, 'num_model_chunk': 2}
2025-04-11T03:52:12.8763036Z try_count = 1
2025-04-11T03:52:12.8763625Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8763633Z 
2025-04-11T03:52:12.8763737Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8763881Z         try_count = 0
2025-04-11T03:52:12.8763980Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8764067Z             max_try, int
2025-04-11T03:52:12.8764216Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8764288Z     
2025-04-11T03:52:12.8764402Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8764480Z             try:
2025-04-11T03:52:12.8764570Z                 try_count += 1
2025-04-11T03:52:12.8764659Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8764742Z                 return ret
2025-04-11T03:52:12.8764834Z             except exception_type as e:
2025-04-11T03:52:12.8765005Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8765199Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8765319Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8765470Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8765623Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8765764Z                     continue
2025-04-11T03:52:12.8765843Z                 else:
2025-04-11T03:52:12.8766062Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8766148Z >                   raise e
2025-04-11T03:52:12.8766153Z 
2025-04-11T03:52:12.8766248Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8766362Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8766498Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8766590Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8766763Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T03:52:12.8766839Z     spawn(
2025-04-11T03:52:12.8766944Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8767046Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8767305Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8767481Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8767824Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8767919Z     while not context.join():
2025-04-11T03:52:12.8768031Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8768035Z 
2025-04-11T03:52:12.8768241Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be334ac0>
2025-04-11T03:52:12.8768320Z timeout = None
2025-04-11T03:52:12.8768324Z 
2025-04-11T03:52:12.8768418Z     def join(self, timeout=None):
2025-04-11T03:52:12.8768547Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8768622Z     
2025-04-11T03:52:12.8768767Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8768913Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8769079Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8769172Z         of the first process exiting.
2025-04-11T03:52:12.8769245Z     
2025-04-11T03:52:12.8769393Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8769527Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8769601Z     
2025-04-11T03:52:12.8769679Z         Args:
2025-04-11T03:52:12.8769820Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8769894Z         """
2025-04-11T03:52:12.8770036Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8770129Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8770264Z             return True
2025-04-11T03:52:12.8770339Z     
2025-04-11T03:52:12.8770472Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8770599Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8770691Z             self.sentinels.keys(),
2025-04-11T03:52:12.8770774Z             timeout=timeout,
2025-04-11T03:52:12.8770855Z         )
2025-04-11T03:52:12.8770927Z     
2025-04-11T03:52:12.8771013Z         error_index = None
2025-04-11T03:52:12.8771098Z         for sentinel in ready:
2025-04-11T03:52:12.8771205Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8771308Z             process = self.processes[index]
2025-04-11T03:52:12.8771459Z             process.join()
2025-04-11T03:52:12.8771557Z             if process.exitcode != 0:
2025-04-11T03:52:12.8771646Z                 error_index = index
2025-04-11T03:52:12.8771728Z                 break
2025-04-11T03:52:12.8771798Z     
2025-04-11T03:52:12.8771890Z         # Return if there was no error.
2025-04-11T03:52:12.8771983Z         if error_index is None:
2025-04-11T03:52:12.8772116Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8772278Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8772350Z     
2025-04-11T03:52:12.8772491Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8772593Z         for process in self.processes:
2025-04-11T03:52:12.8772683Z             if process.is_alive():
2025-04-11T03:52:12.8772777Z                 process.terminate()
2025-04-11T03:52:12.8772861Z             process.join()
2025-04-11T03:52:12.8772935Z     
2025-04-11T03:52:12.8773077Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8773194Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8773303Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8773423Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8773514Z             if exitcode < 0:
2025-04-11T03:52:12.8773620Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8773726Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8773881Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8773974Z                     error_index=error_index,
2025-04-11T03:52:12.8774141Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8774230Z                     exit_code=exitcode,
2025-04-11T03:52:12.8774320Z                     signal_name=name,
2025-04-11T03:52:12.8774394Z                 )
2025-04-11T03:52:12.8774470Z             else:
2025-04-11T03:52:12.8774578Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8774737Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8774835Z                     error_index=error_index,
2025-04-11T03:52:12.8774934Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8775021Z                     exit_code=exitcode,
2025-04-11T03:52:12.8775097Z                 )
2025-04-11T03:52:12.8775168Z     
2025-04-11T03:52:12.8775303Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8775470Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8775561Z         msg += original_trace
2025-04-11T03:52:12.8775734Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8775891Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8775969Z E       
2025-04-11T03:52:12.8776097Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8776198Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8776490Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8776639Z E           fn(i, *args)
2025-04-11T03:52:12.8776909Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:12.8777010Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8777281Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8777401Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8777670Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8777757Z E           module._apply(fn)
2025-04-11T03:52:12.8778091Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8778175Z E           module._apply(fn)
2025-04-11T03:52:12.8778437Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8778535Z E           param_applied = fn(param)
2025-04-11T03:52:12.8778807Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8778985Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8779093Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8779383Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8779518Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8779680Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8779691Z 
2025-04-11T03:52:12.8779993Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8780143Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8780302Z [04/11/25 03:47:51] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8780432Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8780544Z                              :75 launch                                         
2025-04-11T03:52:12.8780679Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8780865Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8781062Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8781211Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8781510Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:20425 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8781647Z _______________________________ test_pp[4-12-4] ________________________________
2025-04-11T03:52:12.8781653Z 
2025-04-11T03:52:12.8781735Z args = ()
2025-04-11T03:52:12.8781889Z kwargs = {'batch_size': 12, 'num_microbatch': 4, 'num_model_chunk': 4}
2025-04-11T03:52:12.8781972Z try_count = 1
2025-04-11T03:52:12.8782575Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8782583Z 
2025-04-11T03:52:12.8782689Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8782769Z         try_count = 0
2025-04-11T03:52:12.8782871Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8782958Z             max_try, int
2025-04-11T03:52:12.8783104Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8783179Z     
2025-04-11T03:52:12.8783290Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8783430Z             try:
2025-04-11T03:52:12.8783516Z                 try_count += 1
2025-04-11T03:52:12.8783608Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8783696Z                 return ret
2025-04-11T03:52:12.8783790Z             except exception_type as e:
2025-04-11T03:52:12.8783893Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8784080Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8784197Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8784346Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8784567Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8784654Z                     continue
2025-04-11T03:52:12.8784733Z                 else:
2025-04-11T03:52:12.8784952Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8785037Z >                   raise e
2025-04-11T03:52:12.8785042Z 
2025-04-11T03:52:12.8785201Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8785317Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8785451Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8785543Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8785714Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T03:52:12.8785794Z     spawn(
2025-04-11T03:52:12.8785894Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8785995Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8786254Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8786433Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8786717Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8786809Z     while not context.join():
2025-04-11T03:52:12.8786922Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8786926Z 
2025-04-11T03:52:12.8787125Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f023dcf0>
2025-04-11T03:52:12.8787263Z timeout = None
2025-04-11T03:52:12.8787271Z 
2025-04-11T03:52:12.8787361Z     def join(self, timeout=None):
2025-04-11T03:52:12.8787486Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8787562Z     
2025-04-11T03:52:12.8787713Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8787863Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8788026Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8788122Z         of the first process exiting.
2025-04-11T03:52:12.8788198Z     
2025-04-11T03:52:12.8788343Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8788519Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8788591Z     
2025-04-11T03:52:12.8788667Z         Args:
2025-04-11T03:52:12.8788804Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8788878Z         """
2025-04-11T03:52:12.8789023Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8789114Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8789195Z             return True
2025-04-11T03:52:12.8789267Z     
2025-04-11T03:52:12.8789397Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8789518Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8789609Z             self.sentinels.keys(),
2025-04-11T03:52:12.8789697Z             timeout=timeout,
2025-04-11T03:52:12.8789853Z         )
2025-04-11T03:52:12.8789922Z     
2025-04-11T03:52:12.8790010Z         error_index = None
2025-04-11T03:52:12.8790097Z         for sentinel in ready:
2025-04-11T03:52:12.8790212Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8790316Z             process = self.processes[index]
2025-04-11T03:52:12.8790405Z             process.join()
2025-04-11T03:52:12.8790501Z             if process.exitcode != 0:
2025-04-11T03:52:12.8790586Z                 error_index = index
2025-04-11T03:52:12.8790666Z                 break
2025-04-11T03:52:12.8790737Z     
2025-04-11T03:52:12.8790833Z         # Return if there was no error.
2025-04-11T03:52:12.8790990Z         if error_index is None:
2025-04-11T03:52:12.8791125Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8791225Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8791297Z     
2025-04-11T03:52:12.8791439Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8791539Z         for process in self.processes:
2025-04-11T03:52:12.8791634Z             if process.is_alive():
2025-04-11T03:52:12.8791724Z                 process.terminate()
2025-04-11T03:52:12.8791871Z             process.join()
2025-04-11T03:52:12.8791947Z     
2025-04-11T03:52:12.8792086Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8792207Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8792316Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8792439Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8792526Z             if exitcode < 0:
2025-04-11T03:52:12.8792637Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8792746Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8792895Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8792994Z                     error_index=error_index,
2025-04-11T03:52:12.8793097Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8793184Z                     exit_code=exitcode,
2025-04-11T03:52:12.8793279Z                     signal_name=name,
2025-04-11T03:52:12.8793354Z                 )
2025-04-11T03:52:12.8793433Z             else:
2025-04-11T03:52:12.8793536Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8793760Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8793854Z                     error_index=error_index,
2025-04-11T03:52:12.8793954Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8794049Z                     exit_code=exitcode,
2025-04-11T03:52:12.8794124Z                 )
2025-04-11T03:52:12.8794197Z     
2025-04-11T03:52:12.8794328Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8794504Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8794597Z         msg += original_trace
2025-04-11T03:52:12.8794773Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8794938Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8795012Z E       
2025-04-11T03:52:12.8795139Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8795243Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8795539Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8795627Z E           fn(i, *args)
2025-04-11T03:52:12.8795899Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:12.8796001Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8796268Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8796446Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8796710Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8796799Z E           module._apply(fn)
2025-04-11T03:52:12.8797067Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8797154Z E           module._apply(fn)
2025-04-11T03:52:12.8797419Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8797512Z E           param_applied = fn(param)
2025-04-11T03:52:12.8797850Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8797968Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8798076Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8798361Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8798551Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8798719Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8798725Z 
2025-04-11T03:52:12.8799025Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8799178Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8799332Z [04/11/25 03:47:55] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8799468Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8799573Z                              :75 launch                                         
2025-04-11T03:52:12.8799706Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8799835Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8800032Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8800165Z _______________________________ test_pp[4-12-12] _______________________________
2025-04-11T03:52:12.8800222Z 
2025-04-11T03:52:12.8800303Z args = ()
2025-04-11T03:52:12.8800463Z kwargs = {'batch_size': 12, 'num_microbatch': 12, 'num_model_chunk': 4}
2025-04-11T03:52:12.8800542Z try_count = 1
2025-04-11T03:52:12.8801128Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8801138Z 
2025-04-11T03:52:12.8801240Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8801321Z         try_count = 0
2025-04-11T03:52:12.8801429Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8801511Z             max_try, int
2025-04-11T03:52:12.8801661Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8801733Z     
2025-04-11T03:52:12.8801845Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8801927Z             try:
2025-04-11T03:52:12.8802011Z                 try_count += 1
2025-04-11T03:52:12.8802103Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8802186Z                 return ret
2025-04-11T03:52:12.8802284Z             except exception_type as e:
2025-04-11T03:52:12.8802385Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8802571Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8802691Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8802835Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8803053Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8803136Z                     continue
2025-04-11T03:52:12.8803215Z                 else:
2025-04-11T03:52:12.8803432Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8803513Z >                   raise e
2025-04-11T03:52:12.8803518Z 
2025-04-11T03:52:12.8803619Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8803728Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8804012Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8804101Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8804277Z tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
2025-04-11T03:52:12.8804353Z     spawn(
2025-04-11T03:52:12.8804452Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8804556Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8804813Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8805049Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8805331Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8805424Z     while not context.join():
2025-04-11T03:52:12.8805533Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8805537Z 
2025-04-11T03:52:12.8805733Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be3353c0>
2025-04-11T03:52:12.8805819Z timeout = None
2025-04-11T03:52:12.8805823Z 
2025-04-11T03:52:12.8805914Z     def join(self, timeout=None):
2025-04-11T03:52:12.8806040Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8806110Z     
2025-04-11T03:52:12.8806260Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8806403Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8806563Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8806659Z         of the first process exiting.
2025-04-11T03:52:12.8806789Z     
2025-04-11T03:52:12.8806938Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8807077Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8807149Z     
2025-04-11T03:52:12.8807229Z         Args:
2025-04-11T03:52:12.8807369Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8807449Z         """
2025-04-11T03:52:12.8807587Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8807686Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8807767Z             return True
2025-04-11T03:52:12.8807840Z     
2025-04-11T03:52:12.8807974Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8808093Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8808187Z             self.sentinels.keys(),
2025-04-11T03:52:12.8808271Z             timeout=timeout,
2025-04-11T03:52:12.8808345Z         )
2025-04-11T03:52:12.8808419Z     
2025-04-11T03:52:12.8808501Z         error_index = None
2025-04-11T03:52:12.8808587Z         for sentinel in ready:
2025-04-11T03:52:12.8808694Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8808792Z             process = self.processes[index]
2025-04-11T03:52:12.8808883Z             process.join()
2025-04-11T03:52:12.8808978Z             if process.exitcode != 0:
2025-04-11T03:52:12.8809068Z                 error_index = index
2025-04-11T03:52:12.8809144Z                 break
2025-04-11T03:52:12.8809218Z     
2025-04-11T03:52:12.8809309Z         # Return if there was no error.
2025-04-11T03:52:12.8809448Z         if error_index is None:
2025-04-11T03:52:12.8809586Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8809685Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8809759Z     
2025-04-11T03:52:12.8809898Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8809995Z         for process in self.processes:
2025-04-11T03:52:12.8810090Z             if process.is_alive():
2025-04-11T03:52:12.8810182Z                 process.terminate()
2025-04-11T03:52:12.8810268Z             process.join()
2025-04-11T03:52:12.8810339Z     
2025-04-11T03:52:12.8810478Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8810660Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8810769Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8810896Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8810981Z             if exitcode < 0:
2025-04-11T03:52:12.8811094Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8811202Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8811406Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8811503Z                     error_index=error_index,
2025-04-11T03:52:12.8811607Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8811697Z                     exit_code=exitcode,
2025-04-11T03:52:12.8811785Z                     signal_name=name,
2025-04-11T03:52:12.8811862Z                 )
2025-04-11T03:52:12.8811934Z             else:
2025-04-11T03:52:12.8812038Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8812209Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8812301Z                     error_index=error_index,
2025-04-11T03:52:12.8812405Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8812493Z                     exit_code=exitcode,
2025-04-11T03:52:12.8812564Z                 )
2025-04-11T03:52:12.8812640Z     
2025-04-11T03:52:12.8812773Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8812946Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8813103Z         msg += original_trace
2025-04-11T03:52:12.8813275Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8813432Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8813505Z E       
2025-04-11T03:52:12.8813640Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8813739Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8814036Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8814118Z E           fn(i, *args)
2025-04-11T03:52:12.8814391Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:12.8814489Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8814758Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8814881Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8815149Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8815240Z E           module._apply(fn)
2025-04-11T03:52:12.8815508Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8815597Z E           module._apply(fn)
2025-04-11T03:52:12.8815860Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8816019Z E           param_applied = fn(param)
2025-04-11T03:52:12.8816301Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8816419Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8816531Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8816820Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8816960Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8817125Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8817191Z 
2025-04-11T03:52:12.8817499Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8817650Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8817807Z [04/11/25 03:48:00] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8817939Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8818118Z                              :75 launch                                         
2025-04-11T03:52:12.8818259Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8818384Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8818582Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8818727Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8819023Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:47696 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8819158Z _______________________________ test_pp[2-12-4] ________________________________
2025-04-11T03:52:12.8819164Z 
2025-04-11T03:52:12.8819319Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 4, 'world_size': 2}
2025-04-11T03:52:12.8819402Z try_count = 1
2025-04-11T03:52:12.8820014Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8820077Z 
2025-04-11T03:52:12.8820185Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8820264Z         try_count = 0
2025-04-11T03:52:12.8820371Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8820456Z             max_try, int
2025-04-11T03:52:12.8820598Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8820679Z     
2025-04-11T03:52:12.8820791Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8820870Z             try:
2025-04-11T03:52:12.8820956Z                 try_count += 1
2025-04-11T03:52:12.8821049Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8821129Z                 return ret
2025-04-11T03:52:12.8821227Z             except exception_type as e:
2025-04-11T03:52:12.8821330Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8821513Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8821635Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8821776Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8821927Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8822016Z                     continue
2025-04-11T03:52:12.8822095Z                 else:
2025-04-11T03:52:12.8822315Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8822457Z >                   raise e
2025-04-11T03:52:12.8822462Z 
2025-04-11T03:52:12.8822560Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8822673Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8822805Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8822892Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8823060Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T03:52:12.8823139Z     spawn(
2025-04-11T03:52:12.8823243Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8823346Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8823669Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8823848Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8824136Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8824227Z     while not context.join():
2025-04-11T03:52:12.8824340Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8824402Z 
2025-04-11T03:52:12.8824608Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ee27fd0>
2025-04-11T03:52:12.8824689Z timeout = None
2025-04-11T03:52:12.8824698Z 
2025-04-11T03:52:12.8824790Z     def join(self, timeout=None):
2025-04-11T03:52:12.8824918Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8824991Z     
2025-04-11T03:52:12.8825137Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8825285Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8825447Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8825543Z         of the first process exiting.
2025-04-11T03:52:12.8825613Z     
2025-04-11T03:52:12.8825762Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8825903Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8825975Z     
2025-04-11T03:52:12.8826056Z         Args:
2025-04-11T03:52:12.8826195Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8826332Z         """
2025-04-11T03:52:12.8826478Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8826575Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8826664Z             return True
2025-04-11T03:52:12.8826739Z     
2025-04-11T03:52:12.8826878Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8827003Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8827100Z             self.sentinels.keys(),
2025-04-11T03:52:12.8827193Z             timeout=timeout,
2025-04-11T03:52:12.8827269Z         )
2025-04-11T03:52:12.8827346Z     
2025-04-11T03:52:12.8827439Z         error_index = None
2025-04-11T03:52:12.8827528Z         for sentinel in ready:
2025-04-11T03:52:12.8827645Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8827751Z             process = self.processes[index]
2025-04-11T03:52:12.8827846Z             process.join()
2025-04-11T03:52:12.8827943Z             if process.exitcode != 0:
2025-04-11T03:52:12.8828042Z                 error_index = index
2025-04-11T03:52:12.8828124Z                 break
2025-04-11T03:52:12.8828196Z     
2025-04-11T03:52:12.8828295Z         # Return if there was no error.
2025-04-11T03:52:12.8828382Z         if error_index is None:
2025-04-11T03:52:12.8828559Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8828656Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8828728Z     
2025-04-11T03:52:12.8828874Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8828971Z         for process in self.processes:
2025-04-11T03:52:12.8829135Z             if process.is_alive():
2025-04-11T03:52:12.8829230Z                 process.terminate()
2025-04-11T03:52:12.8829319Z             process.join()
2025-04-11T03:52:12.8829392Z     
2025-04-11T03:52:12.8829531Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8829652Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8829760Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8829887Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8829970Z             if exitcode < 0:
2025-04-11T03:52:12.8830077Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8830263Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8830416Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8830516Z                     error_index=error_index,
2025-04-11T03:52:12.8830617Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8830709Z                     exit_code=exitcode,
2025-04-11T03:52:12.8830794Z                     signal_name=name,
2025-04-11T03:52:12.8830930Z                 )
2025-04-11T03:52:12.8831011Z             else:
2025-04-11T03:52:12.8831115Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8831281Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8831375Z                     error_index=error_index,
2025-04-11T03:52:12.8831476Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8831565Z                     exit_code=exitcode,
2025-04-11T03:52:12.8831639Z                 )
2025-04-11T03:52:12.8831717Z     
2025-04-11T03:52:12.8831852Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8832026Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8832117Z         msg += original_trace
2025-04-11T03:52:12.8832349Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8832517Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8832593Z E       
2025-04-11T03:52:12.8832728Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8832827Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8833183Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8833267Z E           fn(i, *args)
2025-04-11T03:52:12.8833535Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:12.8833651Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:12.8833915Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:12.8834012Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8834282Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8834406Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8834672Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8834761Z E           module._apply(fn)
2025-04-11T03:52:12.8835027Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8835113Z E           module._apply(fn)
2025-04-11T03:52:12.8835378Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8835472Z E           param_applied = fn(param)
2025-04-11T03:52:12.8835744Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8835918Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8836028Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8836316Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8836451Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8836620Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8836625Z 
2025-04-11T03:52:12.8836922Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8837142Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8837297Z [04/11/25 03:48:06] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8837430Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8837540Z                              :75 launch                                         
2025-04-11T03:52:12.8837678Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8837876Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8838069Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8838218Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8838509Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:42298 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8838650Z _______________________________ test_pp[2-12-6] ________________________________
2025-04-11T03:52:12.8838654Z 
2025-04-11T03:52:12.8838812Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 6, 'world_size': 2}
2025-04-11T03:52:12.8838894Z try_count = 1
2025-04-11T03:52:12.8839488Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8839495Z 
2025-04-11T03:52:12.8839599Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8839744Z         try_count = 0
2025-04-11T03:52:12.8839842Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8839929Z             max_try, int
2025-04-11T03:52:12.8840090Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8840163Z     
2025-04-11T03:52:12.8840276Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8840351Z             try:
2025-04-11T03:52:12.8840437Z                 try_count += 1
2025-04-11T03:52:12.8840528Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8840610Z                 return ret
2025-04-11T03:52:12.8840708Z             except exception_type as e:
2025-04-11T03:52:12.8840807Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8840995Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8841115Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8841265Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8841419Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8841506Z                     continue
2025-04-11T03:52:12.8841585Z                 else:
2025-04-11T03:52:12.8841803Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8841886Z >                   raise e
2025-04-11T03:52:12.8841891Z 
2025-04-11T03:52:12.8841985Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8842099Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8842290Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8842384Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8842551Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T03:52:12.8842628Z     spawn(
2025-04-11T03:52:12.8842733Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8842838Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8843101Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8843279Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8843626Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8843717Z     while not context.join():
2025-04-11T03:52:12.8843824Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8843834Z 
2025-04-11T03:52:12.8844030Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be337550>
2025-04-11T03:52:12.8844110Z timeout = None
2025-04-11T03:52:12.8844170Z 
2025-04-11T03:52:12.8844273Z     def join(self, timeout=None):
2025-04-11T03:52:12.8844400Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8844474Z     
2025-04-11T03:52:12.8844621Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8844763Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8844926Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8845022Z         of the first process exiting.
2025-04-11T03:52:12.8845098Z     
2025-04-11T03:52:12.8845242Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8845379Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8845451Z     
2025-04-11T03:52:12.8845525Z         Args:
2025-04-11T03:52:12.8845668Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8845745Z         """
2025-04-11T03:52:12.8845886Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8845977Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8846119Z             return True
2025-04-11T03:52:12.8846191Z     
2025-04-11T03:52:12.8846324Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8846447Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8846540Z             self.sentinels.keys(),
2025-04-11T03:52:12.8846628Z             timeout=timeout,
2025-04-11T03:52:12.8846701Z         )
2025-04-11T03:52:12.8846771Z     
2025-04-11T03:52:12.8846858Z         error_index = None
2025-04-11T03:52:12.8846944Z         for sentinel in ready:
2025-04-11T03:52:12.8847052Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8847152Z             process = self.processes[index]
2025-04-11T03:52:12.8847236Z             process.join()
2025-04-11T03:52:12.8847337Z             if process.exitcode != 0:
2025-04-11T03:52:12.8847427Z                 error_index = index
2025-04-11T03:52:12.8847508Z                 break
2025-04-11T03:52:12.8847581Z     
2025-04-11T03:52:12.8847670Z         # Return if there was no error.
2025-04-11T03:52:12.8847763Z         if error_index is None:
2025-04-11T03:52:12.8847895Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8847999Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8848069Z     
2025-04-11T03:52:12.8848215Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8848312Z         for process in self.processes:
2025-04-11T03:52:12.8848400Z             if process.is_alive():
2025-04-11T03:52:12.8848494Z                 process.terminate()
2025-04-11T03:52:12.8848578Z             process.join()
2025-04-11T03:52:12.8848712Z     
2025-04-11T03:52:12.8848854Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8848969Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8849081Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8849203Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8849293Z             if exitcode < 0:
2025-04-11T03:52:12.8849397Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8849508Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8849655Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8849819Z                     error_index=error_index,
2025-04-11T03:52:12.8849924Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8850014Z                     exit_code=exitcode,
2025-04-11T03:52:12.8850108Z                     signal_name=name,
2025-04-11T03:52:12.8850184Z                 )
2025-04-11T03:52:12.8850265Z             else:
2025-04-11T03:52:12.8850371Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8850536Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8850690Z                     error_index=error_index,
2025-04-11T03:52:12.8850791Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8850883Z                     exit_code=exitcode,
2025-04-11T03:52:12.8850957Z                 )
2025-04-11T03:52:12.8851030Z     
2025-04-11T03:52:12.8851167Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8851338Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8851429Z         msg += original_trace
2025-04-11T03:52:12.8851598Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8851762Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8851836Z E       
2025-04-11T03:52:12.8851961Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8852066Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8852371Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8852515Z E           fn(i, *args)
2025-04-11T03:52:12.8852786Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:12.8852897Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:12.8853165Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:12.8853264Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8853537Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8853654Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8853929Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8854017Z E           module._apply(fn)
2025-04-11T03:52:12.8854283Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8854369Z E           module._apply(fn)
2025-04-11T03:52:12.8854628Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8854726Z E           param_applied = fn(param)
2025-04-11T03:52:12.8854999Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8855119Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8855227Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8855510Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8855704Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8855866Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8855874Z 
2025-04-11T03:52:12.8856179Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8856329Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8856487Z [04/11/25 03:48:10] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8856678Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8856789Z                              :75 launch                                         
2025-04-11T03:52:12.8856927Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8857059Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8857254Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8857443Z _______________________________ test_pp[4-12-4] ________________________________
2025-04-11T03:52:12.8857452Z 
2025-04-11T03:52:12.8857608Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 4, 'world_size': 4}
2025-04-11T03:52:12.8857685Z try_count = 1
2025-04-11T03:52:12.8858283Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8858291Z 
2025-04-11T03:52:12.8858392Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8858476Z         try_count = 0
2025-04-11T03:52:12.8858576Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8858662Z             max_try, int
2025-04-11T03:52:12.8858807Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8858881Z     
2025-04-11T03:52:12.8858995Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8859070Z             try:
2025-04-11T03:52:12.8859219Z                 try_count += 1
2025-04-11T03:52:12.8859309Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8859389Z                 return ret
2025-04-11T03:52:12.8859486Z             except exception_type as e:
2025-04-11T03:52:12.8859584Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8859775Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8859891Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8860035Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8860189Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8860273Z                     continue
2025-04-11T03:52:12.8860352Z                 else:
2025-04-11T03:52:12.8860568Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8860652Z >                   raise e
2025-04-11T03:52:12.8860657Z 
2025-04-11T03:52:12.8860751Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8860864Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8860996Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8861084Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8861250Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T03:52:12.8861326Z     spawn(
2025-04-11T03:52:12.8861429Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8861529Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8861845Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8862018Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8862300Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8862394Z     while not context.join():
2025-04-11T03:52:12.8862505Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8862509Z 
2025-04-11T03:52:12.8862708Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ee276d0>
2025-04-11T03:52:12.8862864Z timeout = None
2025-04-11T03:52:12.8862869Z 
2025-04-11T03:52:12.8862963Z     def join(self, timeout=None):
2025-04-11T03:52:12.8863092Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8863161Z     
2025-04-11T03:52:12.8863313Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8863458Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8863624Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8863776Z         of the first process exiting.
2025-04-11T03:52:12.8863853Z     
2025-04-11T03:52:12.8864000Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8864142Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8864218Z     
2025-04-11T03:52:12.8864292Z         Args:
2025-04-11T03:52:12.8864431Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8864508Z         """
2025-04-11T03:52:12.8864646Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8864742Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8864821Z             return True
2025-04-11T03:52:12.8864895Z     
2025-04-11T03:52:12.8865029Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8865150Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8865248Z             self.sentinels.keys(),
2025-04-11T03:52:12.8865332Z             timeout=timeout,
2025-04-11T03:52:12.8865411Z         )
2025-04-11T03:52:12.8865481Z     
2025-04-11T03:52:12.8865623Z         error_index = None
2025-04-11T03:52:12.8865711Z         for sentinel in ready:
2025-04-11T03:52:12.8865817Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8865921Z             process = self.processes[index]
2025-04-11T03:52:12.8866005Z             process.join()
2025-04-11T03:52:12.8866103Z             if process.exitcode != 0:
2025-04-11T03:52:12.8866192Z                 error_index = index
2025-04-11T03:52:12.8866269Z                 break
2025-04-11T03:52:12.8866344Z     
2025-04-11T03:52:12.8866436Z         # Return if there was no error.
2025-04-11T03:52:12.8866525Z         if error_index is None:
2025-04-11T03:52:12.8866659Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8866758Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8866830Z     
2025-04-11T03:52:12.8866970Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8867072Z         for process in self.processes:
2025-04-11T03:52:12.8867160Z             if process.is_alive():
2025-04-11T03:52:12.8867255Z                 process.terminate()
2025-04-11T03:52:12.8867339Z             process.join()
2025-04-11T03:52:12.8867410Z     
2025-04-11T03:52:12.8867553Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8867671Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8867780Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8867902Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8867988Z             if exitcode < 0:
2025-04-11T03:52:12.8868093Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8868258Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8868454Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8868555Z                     error_index=error_index,
2025-04-11T03:52:12.8868659Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8868749Z                     exit_code=exitcode,
2025-04-11T03:52:12.8868836Z                     signal_name=name,
2025-04-11T03:52:12.8868914Z                 )
2025-04-11T03:52:12.8868988Z             else:
2025-04-11T03:52:12.8869093Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8869330Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8869426Z                     error_index=error_index,
2025-04-11T03:52:12.8869525Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8869612Z                     exit_code=exitcode,
2025-04-11T03:52:12.8869690Z                 )
2025-04-11T03:52:12.8869758Z     
2025-04-11T03:52:12.8869893Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8870129Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8870219Z         msg += original_trace
2025-04-11T03:52:12.8870394Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8870552Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8870629Z E       
2025-04-11T03:52:12.8870754Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8870856Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8871146Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8871229Z E           fn(i, *args)
2025-04-11T03:52:12.8871499Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:12.8871609Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:12.8871880Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:12.8871974Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8872301Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8872418Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8872688Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8872777Z E           module._apply(fn)
2025-04-11T03:52:12.8873043Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8873134Z E           module._apply(fn)
2025-04-11T03:52:12.8873397Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8873496Z E           param_applied = fn(param)
2025-04-11T03:52:12.8873767Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8873886Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8873994Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8874280Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8874423Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8874585Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8874590Z 
2025-04-11T03:52:12.8874894Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8875112Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8875269Z [04/11/25 03:48:16] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8875396Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8875510Z                              :75 launch                                         
2025-04-11T03:52:12.8875645Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8875769Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8876025Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8876172Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8876466Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30189 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8876603Z _______________________________ test_pp[4-12-6] ________________________________
2025-04-11T03:52:12.8876669Z 
2025-04-11T03:52:12.8876828Z args = (), kwargs = {'batch_size': 12, 'num_microbatch': 6, 'world_size': 4}
2025-04-11T03:52:12.8876906Z try_count = 1
2025-04-11T03:52:12.8877499Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8877511Z 
2025-04-11T03:52:12.8877614Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8877696Z         try_count = 0
2025-04-11T03:52:12.8877801Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8877882Z             max_try, int
2025-04-11T03:52:12.8878030Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8878104Z     
2025-04-11T03:52:12.8878221Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8878296Z             try:
2025-04-11T03:52:12.8878383Z                 try_count += 1
2025-04-11T03:52:12.8878481Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8878562Z                 return ret
2025-04-11T03:52:12.8878660Z             except exception_type as e:
2025-04-11T03:52:12.8878819Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8879006Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8879127Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8879273Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8879429Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8879510Z                     continue
2025-04-11T03:52:12.8879590Z                 else:
2025-04-11T03:52:12.8879810Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8879893Z >                   raise e
2025-04-11T03:52:12.8879898Z 
2025-04-11T03:52:12.8879996Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8880105Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8880243Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8880329Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8880497Z tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
2025-04-11T03:52:12.8880573Z     spawn(
2025-04-11T03:52:12.8880677Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8880780Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8881032Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8881211Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8881547Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8881641Z     while not context.join():
2025-04-11T03:52:12.8881748Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8881754Z 
2025-04-11T03:52:12.8881954Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be3341c0>
2025-04-11T03:52:12.8882036Z timeout = None
2025-04-11T03:52:12.8882040Z 
2025-04-11T03:52:12.8882130Z     def join(self, timeout=None):
2025-04-11T03:52:12.8882259Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8882409Z     
2025-04-11T03:52:12.8882563Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8882706Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8882869Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8882969Z         of the first process exiting.
2025-04-11T03:52:12.8883040Z     
2025-04-11T03:52:12.8883190Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8883386Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8883461Z     
2025-04-11T03:52:12.8883538Z         Args:
2025-04-11T03:52:12.8883677Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8883752Z         """
2025-04-11T03:52:12.8883890Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8883988Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8884070Z             return True
2025-04-11T03:52:12.8884140Z     
2025-04-11T03:52:12.8884275Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8884393Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8884491Z             self.sentinels.keys(),
2025-04-11T03:52:12.8884577Z             timeout=timeout,
2025-04-11T03:52:12.8884651Z         )
2025-04-11T03:52:12.8884721Z     
2025-04-11T03:52:12.8884805Z         error_index = None
2025-04-11T03:52:12.8884896Z         for sentinel in ready:
2025-04-11T03:52:12.8885001Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8885158Z             process = self.processes[index]
2025-04-11T03:52:12.8885243Z             process.join()
2025-04-11T03:52:12.8885337Z             if process.exitcode != 0:
2025-04-11T03:52:12.8885430Z                 error_index = index
2025-04-11T03:52:12.8885505Z                 break
2025-04-11T03:52:12.8885579Z     
2025-04-11T03:52:12.8885670Z         # Return if there was no error.
2025-04-11T03:52:12.8885754Z         if error_index is None:
2025-04-11T03:52:12.8885893Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8885988Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8886063Z     
2025-04-11T03:52:12.8886205Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8886306Z         for process in self.processes:
2025-04-11T03:52:12.8886396Z             if process.is_alive():
2025-04-11T03:52:12.8886486Z                 process.terminate()
2025-04-11T03:52:12.8886575Z             process.join()
2025-04-11T03:52:12.8886647Z     
2025-04-11T03:52:12.8886791Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8886905Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8887010Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8887136Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8887220Z             if exitcode < 0:
2025-04-11T03:52:12.8887331Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8887436Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8887588Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8887760Z                     error_index=error_index,
2025-04-11T03:52:12.8887865Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8887960Z                     exit_code=exitcode,
2025-04-11T03:52:12.8888049Z                     signal_name=name,
2025-04-11T03:52:12.8888125Z                 )
2025-04-11T03:52:12.8888204Z             else:
2025-04-11T03:52:12.8888308Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8888474Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8888566Z                     error_index=error_index,
2025-04-11T03:52:12.8888736Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8888823Z                     exit_code=exitcode,
2025-04-11T03:52:12.8888900Z                 )
2025-04-11T03:52:12.8888970Z     
2025-04-11T03:52:12.8889101Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8889275Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8889364Z         msg += original_trace
2025-04-11T03:52:12.8889593Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8889760Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8889835Z E       
2025-04-11T03:52:12.8889966Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8890064Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8890363Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8890452Z E           fn(i, *args)
2025-04-11T03:52:12.8890725Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:12.8890832Z E           examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:12.8891097Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:12.8891198Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:12.8891468Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8891590Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8891915Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8892005Z E           module._apply(fn)
2025-04-11T03:52:12.8892269Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8892359Z E           module._apply(fn)
2025-04-11T03:52:12.8892622Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8892715Z E           param_applied = fn(param)
2025-04-11T03:52:12.8892994Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8893110Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8893222Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8893510Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8893651Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8893813Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8893819Z 
2025-04-11T03:52:12.8894130Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8894284Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8894439Z [04/11/25 03:48:21] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8894626Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8894736Z                              :75 launch                                         
2025-04-11T03:52:12.8894879Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8895008Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8895208Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8895355Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8895710Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8895998Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8896132Z ___________________________________ test_pp ____________________________________
2025-04-11T03:52:12.8896136Z 
2025-04-11T03:52:12.8896287Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8896883Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8896891Z 
2025-04-11T03:52:12.8896995Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8897076Z         try_count = 0
2025-04-11T03:52:12.8897177Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8897260Z             max_try, int
2025-04-11T03:52:12.8897406Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8897482Z     
2025-04-11T03:52:12.8897592Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8897674Z             try:
2025-04-11T03:52:12.8897756Z                 try_count += 1
2025-04-11T03:52:12.8897851Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8897933Z                 return ret
2025-04-11T03:52:12.8898026Z             except exception_type as e:
2025-04-11T03:52:12.8898128Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8898371Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8898489Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8898635Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8898790Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8898875Z                     continue
2025-04-11T03:52:12.8898951Z                 else:
2025-04-11T03:52:12.8899171Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8899256Z >                   raise e
2025-04-11T03:52:12.8899261Z 
2025-04-11T03:52:12.8899357Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8899469Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8899601Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8899694Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8899869Z tests/test_pipeline/test_schedule/test_zerobubble_pp.py:1077: in test_pp
2025-04-11T03:52:12.8899951Z     spawn(
2025-04-11T03:52:12.8900052Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8900155Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8900415Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8900589Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8900876Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8901028Z     while not context.join():
2025-04-11T03:52:12.8901138Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8901143Z 
2025-04-11T03:52:12.8901341Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be335f00>
2025-04-11T03:52:12.8901424Z timeout = None
2025-04-11T03:52:12.8901428Z 
2025-04-11T03:52:12.8901518Z     def join(self, timeout=None):
2025-04-11T03:52:12.8901646Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8901717Z     
2025-04-11T03:52:12.8901863Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8902072Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8902233Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8902329Z         of the first process exiting.
2025-04-11T03:52:12.8902402Z     
2025-04-11T03:52:12.8902547Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8902687Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8902813Z     
2025-04-11T03:52:12.8902893Z         Args:
2025-04-11T03:52:12.8903030Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8903109Z         """
2025-04-11T03:52:12.8903247Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8903339Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8903426Z             return True
2025-04-11T03:52:12.8903499Z     
2025-04-11T03:52:12.8903635Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8903752Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8903845Z             self.sentinels.keys(),
2025-04-11T03:52:12.8903936Z             timeout=timeout,
2025-04-11T03:52:12.8904009Z         )
2025-04-11T03:52:12.8904086Z     
2025-04-11T03:52:12.8904168Z         error_index = None
2025-04-11T03:52:12.8904253Z         for sentinel in ready:
2025-04-11T03:52:12.8904366Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8904464Z             process = self.processes[index]
2025-04-11T03:52:12.8904553Z             process.join()
2025-04-11T03:52:12.8904704Z             if process.exitcode != 0:
2025-04-11T03:52:12.8904795Z                 error_index = index
2025-04-11T03:52:12.8904873Z                 break
2025-04-11T03:52:12.8904944Z     
2025-04-11T03:52:12.8905041Z         # Return if there was no error.
2025-04-11T03:52:12.8905128Z         if error_index is None:
2025-04-11T03:52:12.8905267Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8905363Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8905435Z     
2025-04-11T03:52:12.8905577Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8905675Z         for process in self.processes:
2025-04-11T03:52:12.8905769Z             if process.is_alive():
2025-04-11T03:52:12.8905860Z                 process.terminate()
2025-04-11T03:52:12.8905950Z             process.join()
2025-04-11T03:52:12.8906021Z     
2025-04-11T03:52:12.8906159Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8906281Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8906386Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8906512Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8906594Z             if exitcode < 0:
2025-04-11T03:52:12.8906702Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8906808Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8906957Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8907055Z                     error_index=error_index,
2025-04-11T03:52:12.8907303Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8907393Z                     exit_code=exitcode,
2025-04-11T03:52:12.8907484Z                     signal_name=name,
2025-04-11T03:52:12.8907558Z                 )
2025-04-11T03:52:12.8907636Z             else:
2025-04-11T03:52:12.8907737Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8907903Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8907996Z                     error_index=error_index,
2025-04-11T03:52:12.8908096Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8908186Z                     exit_code=exitcode,
2025-04-11T03:52:12.8908323Z                 )
2025-04-11T03:52:12.8908402Z     
2025-04-11T03:52:12.8908580Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8908752Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8908839Z         msg += original_trace
2025-04-11T03:52:12.8909013Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8909242Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8909316Z E       
2025-04-11T03:52:12.8909447Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8909547Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8909838Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8909921Z E           fn(i, *args)
2025-04-11T03:52:12.8910208Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 1070, in run_dist
2025-04-11T03:52:12.8910321Z E           run_with_booster_moehybridplugin()
2025-04-11T03:52:12.8910576Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8910674Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8911010Z E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 788, in run_with_booster_moehybridplugin
2025-04-11T03:52:12.8911145Z E           torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T03:52:12.8911428Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.8911611Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.8911877Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8911996Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8912269Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.8912355Z E           module._apply(fn)
2025-04-11T03:52:12.8912624Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8912720Z E           param_applied = fn(param)
2025-04-11T03:52:12.8912997Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8913117Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8913224Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8913518Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8913656Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8913825Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8913829Z 
2025-04-11T03:52:12.8914130Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8914350Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8914504Z [04/11/25 03:48:27] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8914633Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8914743Z                              :75 launch                                         
2025-04-11T03:52:12.8914880Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8915011Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.8915207Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8915420Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8916526Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.8917670Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.8918748Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.8919813Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.8920018Z _____________________________ test_flash_attn_func _____________________________
2025-04-11T03:52:12.8920022Z 
2025-04-11T03:52:12.8920107Z args = (), kwargs = {}
2025-04-11T03:52:12.8920112Z 
2025-04-11T03:52:12.8920208Z     def _clear_cache(*args, **kwargs):
2025-04-11T03:52:12.8920308Z         get_accelerator().empty_cache()
2025-04-11T03:52:12.8920418Z         get_accelerator().reset_peak_memory_stats()
2025-04-11T03:52:12.8920539Z         get_accelerator().reset_max_memory_allocated()
2025-04-11T03:52:12.8920644Z         get_accelerator().reset_max_memory_cached()
2025-04-11T03:52:12.8920741Z >       get_accelerator().synchronize()
2025-04-11T03:52:12.8920746Z 
2025-04-11T03:52:12.8920842Z colossalai/testing/utils.py:271: 
2025-04-11T03:52:12.8920954Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8921115Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.8921217Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.8921330Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8921334Z 
2025-04-11T03:52:12.8921411Z device = None
2025-04-11T03:52:12.8921416Z 
2025-04-11T03:52:12.8921540Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8921762Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8921835Z     
2025-04-11T03:52:12.8921913Z         Args:
2025-04-11T03:52:12.8922086Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8922262Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8922374Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8922453Z         """
2025-04-11T03:52:12.8922534Z         _lazy_init()
2025-04-11T03:52:12.8922630Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8922802Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8922909Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8923203Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8923344Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8923506Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8923566Z 
2025-04-11T03:52:12.8923814Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8923952Z ______________________________ test_release_layer ______________________________
2025-04-11T03:52:12.8923960Z 
2025-04-11T03:52:12.8924046Z     def test_release_layer():
2025-04-11T03:52:12.8924172Z         orig_cuda_allocated = torch.cuda.memory_allocated()
2025-04-11T03:52:12.8924264Z >       model = Net().cuda()
2025-04-11T03:52:12.8924271Z 
2025-04-11T03:52:12.8924389Z tests/test_shardformer/test_shard_utils.py:16: 
2025-04-11T03:52:12.8924501Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8924732Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: in cuda
2025-04-11T03:52:12.8924849Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8925085Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8925170Z     module._apply(fn)
2025-04-11T03:52:12.8925404Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
2025-04-11T03:52:12.8925549Z     module._apply(fn)
2025-04-11T03:52:12.8925788Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
2025-04-11T03:52:12.8925880Z     param_applied = fn(param)
2025-04-11T03:52:12.8925991Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8925998Z 
2025-04-11T03:52:12.8926087Z t = Parameter containing:
2025-04-11T03:52:12.8926169Z tensor([[-0.8151],
2025-04-11T03:52:12.8926260Z         [ 0.1839]], requires_grad=True)
2025-04-11T03:52:12.8926264Z 
2025-04-11T03:52:12.8926371Z >   return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8926479Z E   RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8926767Z E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8926909Z E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8927070Z E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8927075Z 
2025-04-11T03:52:12.8927325Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: RuntimeError
2025-04-11T03:52:12.8927459Z __________________________________ test_gpt2 ___________________________________
2025-04-11T03:52:12.8927465Z 
2025-04-11T03:52:12.8927557Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8927561Z 
2025-04-11T03:52:12.8927665Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8927743Z         try_count = 0
2025-04-11T03:52:12.8927844Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8927989Z             max_try, int
2025-04-11T03:52:12.8928136Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8928212Z     
2025-04-11T03:52:12.8928323Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8928403Z             try:
2025-04-11T03:52:12.8928487Z                 try_count += 1
2025-04-11T03:52:12.8928583Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.8928588Z 
2025-04-11T03:52:12.8928683Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.8928793Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8928971Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.8929065Z     get_accelerator().synchronize()
2025-04-11T03:52:12.8929227Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.8929324Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.8929567Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
2025-04-11T03:52:12.8929665Z     with torch.cuda.device(device):
2025-04-11T03:52:12.8929773Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8929835Z 
2025-04-11T03:52:12.8929962Z self = <torch.cuda.device object at 0x7f68be35f2e0>
2025-04-11T03:52:12.8929969Z 
2025-04-11T03:52:12.8930051Z     def __enter__(self):
2025-04-11T03:52:12.8930187Z >       self.prev_idx = torch.cuda._exchange_device(self.idx)
2025-04-11T03:52:12.8930292Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8930582Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8930720Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8930876Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8930884Z 
2025-04-11T03:52:12.8931118Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
2025-04-11T03:52:12.8931255Z _____________________________ test_grad_clip_norm ______________________________
2025-04-11T03:52:12.8931261Z 
2025-04-11T03:52:12.8931356Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8931360Z 
2025-04-11T03:52:12.8931461Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8931605Z         try_count = 0
2025-04-11T03:52:12.8931706Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8931788Z             max_try, int
2025-04-11T03:52:12.8931931Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8932005Z     
2025-04-11T03:52:12.8932121Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8932197Z             try:
2025-04-11T03:52:12.8932282Z                 try_count += 1
2025-04-11T03:52:12.8932372Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.8932377Z 
2025-04-11T03:52:12.8932477Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.8932584Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8932699Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.8932824Z     get_accelerator().synchronize()
2025-04-11T03:52:12.8932996Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.8933099Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.8933205Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8933210Z 
2025-04-11T03:52:12.8933290Z device = None
2025-04-11T03:52:12.8933294Z 
2025-04-11T03:52:12.8933419Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8933573Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8933649Z     
2025-04-11T03:52:12.8933723Z         Args:
2025-04-11T03:52:12.8933895Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8934127Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8934239Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8934316Z         """
2025-04-11T03:52:12.8934396Z         _lazy_init()
2025-04-11T03:52:12.8934496Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8934601Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8934713Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8935002Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8935202Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8935362Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8935367Z 
2025-04-11T03:52:12.8935603Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8935742Z _____________________________ test_grad_clip_norm ______________________________
2025-04-11T03:52:12.8935813Z 
2025-04-11T03:52:12.8935906Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8935910Z 
2025-04-11T03:52:12.8936014Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8936095Z         try_count = 0
2025-04-11T03:52:12.8936195Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8936275Z             max_try, int
2025-04-11T03:52:12.8936417Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8936492Z     
2025-04-11T03:52:12.8936605Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8936684Z             try:
2025-04-11T03:52:12.8936767Z                 try_count += 1
2025-04-11T03:52:12.8936858Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.8936868Z 
2025-04-11T03:52:12.8936959Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.8937068Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8937184Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.8937278Z     get_accelerator().synchronize()
2025-04-11T03:52:12.8937434Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.8937585Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.8937693Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8937700Z 
2025-04-11T03:52:12.8937778Z device = None
2025-04-11T03:52:12.8937782Z 
2025-04-11T03:52:12.8937901Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8938056Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8938128Z     
2025-04-11T03:52:12.8938207Z         Args:
2025-04-11T03:52:12.8938370Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8938537Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8938648Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8938724Z         """
2025-04-11T03:52:12.8938807Z         _lazy_init()
2025-04-11T03:52:12.8938900Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8939007Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8939111Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8939392Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8939534Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8939690Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8939694Z 
2025-04-11T03:52:12.8939930Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8940122Z _____________________________ test_grad_clip_norm ______________________________
2025-04-11T03:52:12.8940126Z 
2025-04-11T03:52:12.8940220Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8940227Z 
2025-04-11T03:52:12.8940326Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8940408Z         try_count = 0
2025-04-11T03:52:12.8940508Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8940590Z             max_try, int
2025-04-11T03:52:12.8940738Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8940808Z     
2025-04-11T03:52:12.8940922Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8941059Z             try:
2025-04-11T03:52:12.8941144Z                 try_count += 1
2025-04-11T03:52:12.8941239Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.8941244Z 
2025-04-11T03:52:12.8941338Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.8941451Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8941566Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.8941710Z     get_accelerator().synchronize()
2025-04-11T03:52:12.8941865Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.8941960Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.8942076Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8942080Z 
2025-04-11T03:52:12.8942159Z device = None
2025-04-11T03:52:12.8942164Z 
2025-04-11T03:52:12.8942287Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.8942439Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.8942511Z     
2025-04-11T03:52:12.8942585Z         Args:
2025-04-11T03:52:12.8942755Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.8942930Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.8943041Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.8943119Z         """
2025-04-11T03:52:12.8943200Z         _lazy_init()
2025-04-11T03:52:12.8943299Z         with torch.cuda.device(device):
2025-04-11T03:52:12.8943397Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.8943565Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8943852Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8943987Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8944152Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8944157Z 
2025-04-11T03:52:12.8944393Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.8944537Z ____________________________ test_dist_crossentropy ____________________________
2025-04-11T03:52:12.8944543Z 
2025-04-11T03:52:12.8944632Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8945245Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8945255Z 
2025-04-11T03:52:12.8945357Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8945435Z         try_count = 0
2025-04-11T03:52:12.8945539Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8945620Z             max_try, int
2025-04-11T03:52:12.8945766Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8945837Z     
2025-04-11T03:52:12.8945953Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8946028Z             try:
2025-04-11T03:52:12.8946111Z                 try_count += 1
2025-04-11T03:52:12.8946279Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8946361Z                 return ret
2025-04-11T03:52:12.8946462Z             except exception_type as e:
2025-04-11T03:52:12.8946560Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8946746Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8946870Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8947013Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8947170Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8947309Z                     continue
2025-04-11T03:52:12.8947390Z                 else:
2025-04-11T03:52:12.8947615Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8947698Z >                   raise e
2025-04-11T03:52:12.8947703Z 
2025-04-11T03:52:12.8947801Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8947913Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8948106Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8948193Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8948461Z tests/test_shardformer/test_layer/test_dist_crossentropy.py:51: in test_dist_crossentropy
2025-04-11T03:52:12.8948611Z     spawn(check_dist_crossentropy, 2, ignore_index=ignore_index)
2025-04-11T03:52:12.8948710Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8948813Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8949063Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8949242Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8949525Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8949620Z     while not context.join():
2025-04-11T03:52:12.8949729Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8949733Z 
2025-04-11T03:52:12.8949931Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ecd9360>
2025-04-11T03:52:12.8950083Z timeout = None
2025-04-11T03:52:12.8950088Z 
2025-04-11T03:52:12.8950178Z     def join(self, timeout=None):
2025-04-11T03:52:12.8950309Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8950380Z     
2025-04-11T03:52:12.8950531Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8950676Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8950840Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8950936Z         of the first process exiting.
2025-04-11T03:52:12.8951008Z     
2025-04-11T03:52:12.8951158Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8951299Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8951373Z     
2025-04-11T03:52:12.8951447Z         Args:
2025-04-11T03:52:12.8951585Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8951664Z         """
2025-04-11T03:52:12.8951801Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8951895Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8951975Z             return True
2025-04-11T03:52:12.8952046Z     
2025-04-11T03:52:12.8952180Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8952301Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8952395Z             self.sentinels.keys(),
2025-04-11T03:52:12.8952480Z             timeout=timeout,
2025-04-11T03:52:12.8952618Z         )
2025-04-11T03:52:12.8952690Z     
2025-04-11T03:52:12.8952774Z         error_index = None
2025-04-11T03:52:12.8952862Z         for sentinel in ready:
2025-04-11T03:52:12.8952969Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8953072Z             process = self.processes[index]
2025-04-11T03:52:12.8953161Z             process.join()
2025-04-11T03:52:12.8953258Z             if process.exitcode != 0:
2025-04-11T03:52:12.8953349Z                 error_index = index
2025-04-11T03:52:12.8953426Z                 break
2025-04-11T03:52:12.8953500Z     
2025-04-11T03:52:12.8953591Z         # Return if there was no error.
2025-04-11T03:52:12.8953745Z         if error_index is None:
2025-04-11T03:52:12.8953885Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8953980Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8954053Z     
2025-04-11T03:52:12.8954192Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8954294Z         for process in self.processes:
2025-04-11T03:52:12.8954385Z             if process.is_alive():
2025-04-11T03:52:12.8954475Z                 process.terminate()
2025-04-11T03:52:12.8954625Z             process.join()
2025-04-11T03:52:12.8954694Z     
2025-04-11T03:52:12.8954837Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8954954Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8955066Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8955192Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8955276Z             if exitcode < 0:
2025-04-11T03:52:12.8955389Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8955495Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8955649Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8955746Z                     error_index=error_index,
2025-04-11T03:52:12.8955849Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8955941Z                     exit_code=exitcode,
2025-04-11T03:52:12.8956030Z                     signal_name=name,
2025-04-11T03:52:12.8956106Z                 )
2025-04-11T03:52:12.8956180Z             else:
2025-04-11T03:52:12.8956282Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8956505Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8956600Z                     error_index=error_index,
2025-04-11T03:52:12.8956706Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8956795Z                     exit_code=exitcode,
2025-04-11T03:52:12.8956870Z                 )
2025-04-11T03:52:12.8956940Z     
2025-04-11T03:52:12.8957071Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8957248Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8957337Z         msg += original_trace
2025-04-11T03:52:12.8957510Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8957675Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8957752Z E       
2025-04-11T03:52:12.8957876Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8957974Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8958277Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8958355Z E           fn(i, *args)
2025-04-11T03:52:12.8958692Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dist_crossentropy.py", line 20, in check_dist_crossentropy
2025-04-11T03:52:12.8958827Z E           pred = torch.randn(2, 4, 8, requires_grad=True).cuda()
2025-04-11T03:52:12.8958936Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8959278Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8959413Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8959577Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8959582Z 
2025-04-11T03:52:12.8959888Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8960040Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8960191Z [04/11/25 03:48:34] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8960407Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8960516Z                              :75 launch                                         
2025-04-11T03:52:12.8960651Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8960781Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8960972Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8961174Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8961470Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26698 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8961606Z _________________________________ test_dropout _________________________________
2025-04-11T03:52:12.8961610Z 
2025-04-11T03:52:12.8961705Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8962301Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8962309Z 
2025-04-11T03:52:12.8962410Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8962496Z         try_count = 0
2025-04-11T03:52:12.8962598Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8962678Z             max_try, int
2025-04-11T03:52:12.8962825Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8962951Z     
2025-04-11T03:52:12.8963068Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8963144Z             try:
2025-04-11T03:52:12.8963228Z                 try_count += 1
2025-04-11T03:52:12.8963323Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8963404Z                 return ret
2025-04-11T03:52:12.8963503Z             except exception_type as e:
2025-04-11T03:52:12.8963603Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8963793Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8963911Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8964058Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8964222Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8964306Z                     continue
2025-04-11T03:52:12.8964388Z                 else:
2025-04-11T03:52:12.8964607Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8964689Z >                   raise e
2025-04-11T03:52:12.8964694Z 
2025-04-11T03:52:12.8964789Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8964901Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8965037Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8965124Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8965298Z tests/test_shardformer/test_layer/test_dropout.py:66: in test_dropout
2025-04-11T03:52:12.8965448Z     spawn(run_dist, nprocs=2)
2025-04-11T03:52:12.8965554Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8965655Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8965908Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8966089Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8966379Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8966471Z     while not context.join():
2025-04-11T03:52:12.8966640Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8966645Z 
2025-04-11T03:52:12.8966851Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be35eb00>
2025-04-11T03:52:12.8966930Z timeout = None
2025-04-11T03:52:12.8966934Z 
2025-04-11T03:52:12.8967024Z     def join(self, timeout=None):
2025-04-11T03:52:12.8967155Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8967228Z     
2025-04-11T03:52:12.8967436Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8967581Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8967750Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8967844Z         of the first process exiting.
2025-04-11T03:52:12.8967917Z     
2025-04-11T03:52:12.8968065Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8968199Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8968274Z     
2025-04-11T03:52:12.8968348Z         Args:
2025-04-11T03:52:12.8968484Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8968560Z         """
2025-04-11T03:52:12.8968698Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8968798Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8968878Z             return True
2025-04-11T03:52:12.8968953Z     
2025-04-11T03:52:12.8969084Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8969202Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8969355Z             self.sentinels.keys(),
2025-04-11T03:52:12.8969441Z             timeout=timeout,
2025-04-11T03:52:12.8969516Z         )
2025-04-11T03:52:12.8969586Z     
2025-04-11T03:52:12.8969668Z         error_index = None
2025-04-11T03:52:12.8969758Z         for sentinel in ready:
2025-04-11T03:52:12.8969866Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8969967Z             process = self.processes[index]
2025-04-11T03:52:12.8970052Z             process.join()
2025-04-11T03:52:12.8970145Z             if process.exitcode != 0:
2025-04-11T03:52:12.8970237Z                 error_index = index
2025-04-11T03:52:12.8970315Z                 break
2025-04-11T03:52:12.8970388Z     
2025-04-11T03:52:12.8970481Z         # Return if there was no error.
2025-04-11T03:52:12.8970573Z         if error_index is None:
2025-04-11T03:52:12.8970707Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8970804Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8970878Z     
2025-04-11T03:52:12.8971019Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8971120Z         for process in self.processes:
2025-04-11T03:52:12.8971206Z             if process.is_alive():
2025-04-11T03:52:12.8971296Z                 process.terminate()
2025-04-11T03:52:12.8971387Z             process.join()
2025-04-11T03:52:12.8971457Z     
2025-04-11T03:52:12.8971601Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8971716Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8971827Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8972007Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8972091Z             if exitcode < 0:
2025-04-11T03:52:12.8972203Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8972307Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8972462Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8972557Z                     error_index=error_index,
2025-04-11T03:52:12.8972655Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8972747Z                     exit_code=exitcode,
2025-04-11T03:52:12.8972893Z                     signal_name=name,
2025-04-11T03:52:12.8972971Z                 )
2025-04-11T03:52:12.8973045Z             else:
2025-04-11T03:52:12.8973150Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8973314Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8973408Z                     error_index=error_index,
2025-04-11T03:52:12.8973510Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8973655Z                     exit_code=exitcode,
2025-04-11T03:52:12.8973731Z                 )
2025-04-11T03:52:12.8973803Z     
2025-04-11T03:52:12.8973935Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8974113Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8974198Z         msg += original_trace
2025-04-11T03:52:12.8974375Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8974537Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8974616Z E       
2025-04-11T03:52:12.8974741Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8974839Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8975143Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8975226Z E           fn(i, *args)
2025-04-11T03:52:12.8975496Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 60, in run_dist
2025-04-11T03:52:12.8975595Z E           check_dropout_parallel_input()
2025-04-11T03:52:12.8975970Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 12, in check_dropout_parallel_input
2025-04-11T03:52:12.8976188Z E           dropout_1d = DropoutForParallelInput.from_native_module(dropout, process_group=None)
2025-04-11T03:52:12.8976460Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 42, in from_native_module
2025-04-11T03:52:12.8976682Z E           return DropoutForParallelInput(p=p, inplace=inplace, process_group=process_group)
2025-04-11T03:52:12.8976923Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 31, in __init__
2025-04-11T03:52:12.8977129Z E           self.randomizer = create_randomizer_with_offset(seed, process_group=process_group)
2025-04-11T03:52:12.8977430Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 318, in create_randomizer_with_offset
2025-04-11T03:52:12.8977618Z E           is_synchronized = Randomizer.is_randomizer_index_synchronized(process_group)
2025-04-11T03:52:12.8977922Z E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 258, in is_randomizer_index_synchronized
2025-04-11T03:52:12.8978158Z E           index_tensor = torch.tensor(index, dtype=torch.int32, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.8978266Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8978552Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8978686Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8978905Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8978910Z 
2025-04-11T03:52:12.8979222Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8979376Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8979540Z [04/11/25 03:48:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8979669Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8979779Z                              :75 launch                                         
2025-04-11T03:52:12.8979973Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8980095Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8980292Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8980425Z ______________________________ test_embedding_1d _______________________________
2025-04-11T03:52:12.8980474Z 
2025-04-11T03:52:12.8980572Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8981163Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8981171Z 
2025-04-11T03:52:12.8981276Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.8981355Z         try_count = 0
2025-04-11T03:52:12.8981459Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.8981539Z             max_try, int
2025-04-11T03:52:12.8981681Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.8981755Z     
2025-04-11T03:52:12.8981865Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.8981944Z             try:
2025-04-11T03:52:12.8982029Z                 try_count += 1
2025-04-11T03:52:12.8982124Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.8982207Z                 return ret
2025-04-11T03:52:12.8982300Z             except exception_type as e:
2025-04-11T03:52:12.8982403Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.8982645Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.8982766Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.8982913Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.8983068Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.8983149Z                     continue
2025-04-11T03:52:12.8983227Z                 else:
2025-04-11T03:52:12.8983444Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.8983526Z >                   raise e
2025-04-11T03:52:12.8983532Z 
2025-04-11T03:52:12.8983630Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.8983738Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8983875Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.8983962Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.8984147Z tests/test_shardformer/test_layer/test_embedding.py:52: in test_embedding_1d
2025-04-11T03:52:12.8984240Z     spawn(run_dist, nprocs=2)
2025-04-11T03:52:12.8984341Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.8984443Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.8984696Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.8984868Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.8985225Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.8985316Z     while not context.join():
2025-04-11T03:52:12.8985430Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.8985435Z 
2025-04-11T03:52:12.8985630Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfa6e0>
2025-04-11T03:52:12.8985712Z timeout = None
2025-04-11T03:52:12.8985716Z 
2025-04-11T03:52:12.8985807Z     def join(self, timeout=None):
2025-04-11T03:52:12.8985938Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.8986069Z     
2025-04-11T03:52:12.8986213Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.8986358Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.8986520Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.8986618Z         of the first process exiting.
2025-04-11T03:52:12.8986690Z     
2025-04-11T03:52:12.8986837Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.8987033Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.8987105Z     
2025-04-11T03:52:12.8987184Z         Args:
2025-04-11T03:52:12.8987324Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.8987399Z         """
2025-04-11T03:52:12.8987537Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.8987628Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.8987715Z             return True
2025-04-11T03:52:12.8987786Z     
2025-04-11T03:52:12.8987922Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.8988042Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.8988133Z             self.sentinels.keys(),
2025-04-11T03:52:12.8988223Z             timeout=timeout,
2025-04-11T03:52:12.8988295Z         )
2025-04-11T03:52:12.8988371Z     
2025-04-11T03:52:12.8988491Z         error_index = None
2025-04-11T03:52:12.8988585Z         for sentinel in ready:
2025-04-11T03:52:12.8988690Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.8988788Z             process = self.processes[index]
2025-04-11T03:52:12.8988947Z             process.join()
2025-04-11T03:52:12.8989040Z             if process.exitcode != 0:
2025-04-11T03:52:12.8989132Z                 error_index = index
2025-04-11T03:52:12.8989210Z                 break
2025-04-11T03:52:12.8989279Z     
2025-04-11T03:52:12.8989376Z         # Return if there was no error.
2025-04-11T03:52:12.8989467Z         if error_index is None:
2025-04-11T03:52:12.8989606Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.8989704Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.8989773Z     
2025-04-11T03:52:12.8989915Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.8990027Z         for process in self.processes:
2025-04-11T03:52:12.8990118Z             if process.is_alive():
2025-04-11T03:52:12.8990211Z                 process.terminate()
2025-04-11T03:52:12.8990300Z             process.join()
2025-04-11T03:52:12.8990370Z     
2025-04-11T03:52:12.8990512Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.8990634Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.8990741Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.8990868Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.8990952Z             if exitcode < 0:
2025-04-11T03:52:12.8991059Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.8991168Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8991317Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.8991475Z                     error_index=error_index,
2025-04-11T03:52:12.8991577Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8991671Z                     exit_code=exitcode,
2025-04-11T03:52:12.8991760Z                     signal_name=name,
2025-04-11T03:52:12.8991834Z                 )
2025-04-11T03:52:12.8991912Z             else:
2025-04-11T03:52:12.8992015Z                 raise ProcessExitedException(
2025-04-11T03:52:12.8992181Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.8992273Z                     error_index=error_index,
2025-04-11T03:52:12.8992374Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.8992529Z                     exit_code=exitcode,
2025-04-11T03:52:12.8992603Z                 )
2025-04-11T03:52:12.8992679Z     
2025-04-11T03:52:12.8992810Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.8992983Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.8993071Z         msg += original_trace
2025-04-11T03:52:12.8993241Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.8993469Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.8993541Z E       
2025-04-11T03:52:12.8993672Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.8993771Z E       Traceback (most recent call last):
2025-04-11T03:52:12.8994071Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.8994153Z E           fn(i, *args)
2025-04-11T03:52:12.8994423Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 47, in run_dist
2025-04-11T03:52:12.8994516Z E           check_embedding_1d()
2025-04-11T03:52:12.8994769Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.8994864Z E           partial_func(**kwargs)
2025-04-11T03:52:12.8995154Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 18, in check_embedding_1d
2025-04-11T03:52:12.8995267Z E           embedding = nn.Embedding(32, 128).cuda()
2025-04-11T03:52:12.8995596Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.8995713Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8995988Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.8996083Z E           param_applied = fn(param)
2025-04-11T03:52:12.8996360Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.8996476Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.8996587Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.8996865Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.8997003Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.8997165Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.8997171Z 
2025-04-11T03:52:12.8997474Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.8997629Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.8997784Z [04/11/25 03:48:43] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.8997913Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.8998019Z                              :75 launch                                         
2025-04-11T03:52:12.8998218Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.8998342Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.8998537Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.8998686Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.8998982Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:44898 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.8999121Z _______________________________ test_linearconv ________________________________
2025-04-11T03:52:12.8999189Z 
2025-04-11T03:52:12.8999284Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.8999886Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.8999893Z 
2025-04-11T03:52:12.8999993Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9000136Z         try_count = 0
2025-04-11T03:52:12.9000236Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9000319Z             max_try, int
2025-04-11T03:52:12.9000470Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9000542Z     
2025-04-11T03:52:12.9000658Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9000733Z             try:
2025-04-11T03:52:12.9000821Z                 try_count += 1
2025-04-11T03:52:12.9000913Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9000993Z                 return ret
2025-04-11T03:52:12.9001092Z             except exception_type as e:
2025-04-11T03:52:12.9001190Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9001378Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9001498Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9001647Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9001799Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9001942Z                     continue
2025-04-11T03:52:12.9002025Z                 else:
2025-04-11T03:52:12.9002246Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9002331Z >                   raise e
2025-04-11T03:52:12.9002338Z 
2025-04-11T03:52:12.9002431Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9002546Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9002679Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9002768Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9002992Z tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py:209: in test_linearconv
2025-04-11T03:52:12.9003083Z     spawn(run_dist, nprocs=2)
2025-04-11T03:52:12.9003186Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9003284Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9003540Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9003718Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9004002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9004096Z     while not context.join():
2025-04-11T03:52:12.9004204Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9004208Z 
2025-04-11T03:52:12.9004408Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0157970>
2025-04-11T03:52:12.9004548Z timeout = None
2025-04-11T03:52:12.9004552Z 
2025-04-11T03:52:12.9004646Z     def join(self, timeout=None):
2025-04-11T03:52:12.9004774Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9004846Z     
2025-04-11T03:52:12.9004994Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9005138Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9005303Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9005395Z         of the first process exiting.
2025-04-11T03:52:12.9005467Z     
2025-04-11T03:52:12.9005672Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9005809Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9005883Z     
2025-04-11T03:52:12.9005958Z         Args:
2025-04-11T03:52:12.9006098Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9006173Z         """
2025-04-11T03:52:12.9006312Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9006463Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9006547Z             return True
2025-04-11T03:52:12.9006623Z     
2025-04-11T03:52:12.9006757Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9006878Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9006979Z             self.sentinels.keys(),
2025-04-11T03:52:12.9007066Z             timeout=timeout,
2025-04-11T03:52:12.9007145Z         )
2025-04-11T03:52:12.9007218Z     
2025-04-11T03:52:12.9007310Z         error_index = None
2025-04-11T03:52:12.9007398Z         for sentinel in ready:
2025-04-11T03:52:12.9007509Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9007617Z             process = self.processes[index]
2025-04-11T03:52:12.9007706Z             process.join()
2025-04-11T03:52:12.9007809Z             if process.exitcode != 0:
2025-04-11T03:52:12.9007900Z                 error_index = index
2025-04-11T03:52:12.9007978Z                 break
2025-04-11T03:52:12.9008057Z     
2025-04-11T03:52:12.9008151Z         # Return if there was no error.
2025-04-11T03:52:12.9008244Z         if error_index is None:
2025-04-11T03:52:12.9008379Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9008661Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9008735Z     
2025-04-11T03:52:12.9008875Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9008974Z         for process in self.processes:
2025-04-11T03:52:12.9009064Z             if process.is_alive():
2025-04-11T03:52:12.9009160Z                 process.terminate()
2025-04-11T03:52:12.9009242Z             process.join()
2025-04-11T03:52:12.9009311Z     
2025-04-11T03:52:12.9009455Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9009571Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9009681Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9009809Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9009894Z             if exitcode < 0:
2025-04-11T03:52:12.9010002Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9010110Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9010264Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9010359Z                     error_index=error_index,
2025-04-11T03:52:12.9010463Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9010553Z                     exit_code=exitcode,
2025-04-11T03:52:12.9010639Z                     signal_name=name,
2025-04-11T03:52:12.9010717Z                 )
2025-04-11T03:52:12.9010791Z             else:
2025-04-11T03:52:12.9010900Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9011121Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9011217Z                     error_index=error_index,
2025-04-11T03:52:12.9011320Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9011408Z                     exit_code=exitcode,
2025-04-11T03:52:12.9011489Z                 )
2025-04-11T03:52:12.9011558Z     
2025-04-11T03:52:12.9011692Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9011860Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9011945Z         msg += original_trace
2025-04-11T03:52:12.9012174Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9012336Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9012415Z E       
2025-04-11T03:52:12.9012541Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9012642Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9012934Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9013072Z E           fn(i, *args)
2025-04-11T03:52:12.9013384Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 204, in run_dist
2025-04-11T03:52:12.9013484Z E           check_gpt2_qkv_fused_linear_1d()
2025-04-11T03:52:12.9013742Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9013832Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9014086Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9014173Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9014526Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 194, in check_gpt2_qkv_fused_linear_1d
2025-04-11T03:52:12.9014666Z E           check_linear_conv_1d_col(lazy_init, seq_parallel_mode)
2025-04-11T03:52:12.9015002Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 47, in check_linear_conv_1d_col
2025-04-11T03:52:12.9015152Z E           linear = Conv1D(192, 48).cuda()
2025-04-11T03:52:12.9015423Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9015544Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9015823Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9015921Z E           param_applied = fn(param)
2025-04-11T03:52:12.9016203Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9016320Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9016433Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9016722Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9016861Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9017024Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9017028Z 
2025-04-11T03:52:12.9017344Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9017499Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9017657Z [04/11/25 03:48:47] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9017786Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9017948Z                              :75 launch                                         
2025-04-11T03:52:12.9018088Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9018214Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9018409Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9018541Z ________________________________ test_layernorm ________________________________
2025-04-11T03:52:12.9018546Z 
2025-04-11T03:52:12.9018641Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9019231Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9019296Z 
2025-04-11T03:52:12.9019404Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9019487Z         try_count = 0
2025-04-11T03:52:12.9019588Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9019674Z             max_try, int
2025-04-11T03:52:12.9019864Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9019942Z     
2025-04-11T03:52:12.9020057Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9020140Z             try:
2025-04-11T03:52:12.9020233Z                 try_count += 1
2025-04-11T03:52:12.9020327Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9020416Z                 return ret
2025-04-11T03:52:12.9020516Z             except exception_type as e:
2025-04-11T03:52:12.9020622Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9020811Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9020933Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9021084Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9021243Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9021336Z                     continue
2025-04-11T03:52:12.9021417Z                 else:
2025-04-11T03:52:12.9021645Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9021787Z >                   raise e
2025-04-11T03:52:12.9021792Z 
2025-04-11T03:52:12.9021888Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9022001Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9022134Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9022225Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9022403Z tests/test_shardformer/test_layer/test_layernorm.py:50: in test_layernorm
2025-04-11T03:52:12.9022501Z     spawn(run_dist, nprocs=2)
2025-04-11T03:52:12.9022603Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9022701Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9022959Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9023137Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9023425Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9023515Z     while not context.join():
2025-04-11T03:52:12.9023629Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9023635Z 
2025-04-11T03:52:12.9023835Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be336b30>
2025-04-11T03:52:12.9023915Z timeout = None
2025-04-11T03:52:12.9023920Z 
2025-04-11T03:52:12.9024013Z     def join(self, timeout=None):
2025-04-11T03:52:12.9024140Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9024275Z     
2025-04-11T03:52:12.9024424Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9024573Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9024733Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9024827Z         of the first process exiting.
2025-04-11T03:52:12.9024901Z     
2025-04-11T03:52:12.9025047Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9025186Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9025324Z     
2025-04-11T03:52:12.9025403Z         Args:
2025-04-11T03:52:12.9025545Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9025618Z         """
2025-04-11T03:52:12.9025761Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9025852Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9025938Z             return True
2025-04-11T03:52:12.9026008Z     
2025-04-11T03:52:12.9026143Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9026322Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9026416Z             self.sentinels.keys(),
2025-04-11T03:52:12.9026509Z             timeout=timeout,
2025-04-11T03:52:12.9026582Z         )
2025-04-11T03:52:12.9026652Z     
2025-04-11T03:52:12.9026737Z         error_index = None
2025-04-11T03:52:12.9026821Z         for sentinel in ready:
2025-04-11T03:52:12.9026933Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9027035Z             process = self.processes[index]
2025-04-11T03:52:12.9027122Z             process.join()
2025-04-11T03:52:12.9027217Z             if process.exitcode != 0:
2025-04-11T03:52:12.9027306Z                 error_index = index
2025-04-11T03:52:12.9027387Z                 break
2025-04-11T03:52:12.9027457Z     
2025-04-11T03:52:12.9027555Z         # Return if there was no error.
2025-04-11T03:52:12.9027641Z         if error_index is None:
2025-04-11T03:52:12.9027775Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9027877Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9027948Z     
2025-04-11T03:52:12.9028091Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9028246Z         for process in self.processes:
2025-04-11T03:52:12.9028338Z             if process.is_alive():
2025-04-11T03:52:12.9028485Z                 process.terminate()
2025-04-11T03:52:12.9028572Z             process.join()
2025-04-11T03:52:12.9028649Z     
2025-04-11T03:52:12.9028791Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9028912Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9029021Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9029144Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9029236Z             if exitcode < 0:
2025-04-11T03:52:12.9029342Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9029452Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9029602Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9029701Z                     error_index=error_index,
2025-04-11T03:52:12.9029801Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9029888Z                     exit_code=exitcode,
2025-04-11T03:52:12.9029980Z                     signal_name=name,
2025-04-11T03:52:12.9030054Z                 )
2025-04-11T03:52:12.9030134Z             else:
2025-04-11T03:52:12.9030238Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9030402Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9030498Z                     error_index=error_index,
2025-04-11T03:52:12.9030662Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9030752Z                     exit_code=exitcode,
2025-04-11T03:52:12.9030826Z                 )
2025-04-11T03:52:12.9030902Z     
2025-04-11T03:52:12.9031037Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9031204Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9031295Z         msg += original_trace
2025-04-11T03:52:12.9031466Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9031625Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9031766Z E       
2025-04-11T03:52:12.9031894Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9031998Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9032292Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9032380Z E           fn(i, *args)
2025-04-11T03:52:12.9032648Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 45, in run_dist
2025-04-11T03:52:12.9032808Z E           check_layernorm()
2025-04-11T03:52:12.9033065Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9033156Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9033493Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 17, in check_layernorm
2025-04-11T03:52:12.9033603Z E           norm = nn.LayerNorm(128, 0.00001).cuda()
2025-04-11T03:52:12.9033878Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9033995Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9034264Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9034361Z E           param_applied = fn(param)
2025-04-11T03:52:12.9034629Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9034748Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9034925Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9035209Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9035344Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9035510Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9035515Z 
2025-04-11T03:52:12.9035817Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9035973Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9036131Z [04/11/25 03:48:52] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9036263Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9036374Z                              :75 launch                                         
2025-04-11T03:52:12.9036512Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9036639Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9036832Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9036980Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9037274Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43526 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9037410Z _________________________________ test_linear __________________________________
2025-04-11T03:52:12.9037468Z 
2025-04-11T03:52:12.9037564Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9038157Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9038169Z 
2025-04-11T03:52:12.9038272Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9038352Z         try_count = 0
2025-04-11T03:52:12.9038454Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9038601Z             max_try, int
2025-04-11T03:52:12.9038755Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9038827Z     
2025-04-11T03:52:12.9038939Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9039018Z             try:
2025-04-11T03:52:12.9039104Z                 try_count += 1
2025-04-11T03:52:12.9039199Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9039279Z                 return ret
2025-04-11T03:52:12.9039436Z             except exception_type as e:
2025-04-11T03:52:12.9039537Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9039723Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9039845Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9039993Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9040151Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9040236Z                     continue
2025-04-11T03:52:12.9040317Z                 else:
2025-04-11T03:52:12.9040534Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9040616Z >                   raise e
2025-04-11T03:52:12.9040621Z 
2025-04-11T03:52:12.9040719Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9040831Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9040971Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9041057Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9041291Z tests/test_shardformer/test_layer/test_linear_1d.py:284: in test_linear
2025-04-11T03:52:12.9041391Z     spawn(check_dist_linear, nprocs=2)
2025-04-11T03:52:12.9041492Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9041596Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9041852Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9042030Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9042311Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9042405Z     while not context.join():
2025-04-11T03:52:12.9042515Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9042521Z 
2025-04-11T03:52:12.9042719Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be309f30>
2025-04-11T03:52:12.9042804Z timeout = None
2025-04-11T03:52:12.9042808Z 
2025-04-11T03:52:12.9042901Z     def join(self, timeout=None):
2025-04-11T03:52:12.9043028Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9043099Z     
2025-04-11T03:52:12.9043245Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9043389Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9043552Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9043649Z         of the first process exiting.
2025-04-11T03:52:12.9043720Z     
2025-04-11T03:52:12.9043921Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9044056Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9044130Z     
2025-04-11T03:52:12.9044204Z         Args:
2025-04-11T03:52:12.9044341Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9044420Z         """
2025-04-11T03:52:12.9044558Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9044656Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9044737Z             return True
2025-04-11T03:52:12.9044805Z     
2025-04-11T03:52:12.9045009Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9045126Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9045220Z             self.sentinels.keys(),
2025-04-11T03:52:12.9045304Z             timeout=timeout,
2025-04-11T03:52:12.9045375Z         )
2025-04-11T03:52:12.9045450Z     
2025-04-11T03:52:12.9045532Z         error_index = None
2025-04-11T03:52:12.9045620Z         for sentinel in ready:
2025-04-11T03:52:12.9045727Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9045886Z             process = self.processes[index]
2025-04-11T03:52:12.9045971Z             process.join()
2025-04-11T03:52:12.9046069Z             if process.exitcode != 0:
2025-04-11T03:52:12.9046160Z                 error_index = index
2025-04-11T03:52:12.9046236Z                 break
2025-04-11T03:52:12.9046308Z     
2025-04-11T03:52:12.9046399Z         # Return if there was no error.
2025-04-11T03:52:12.9046484Z         if error_index is None:
2025-04-11T03:52:12.9046625Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9046723Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9046795Z     
2025-04-11T03:52:12.9046934Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9047030Z         for process in self.processes:
2025-04-11T03:52:12.9047122Z             if process.is_alive():
2025-04-11T03:52:12.9047212Z                 process.terminate()
2025-04-11T03:52:12.9047301Z             process.join()
2025-04-11T03:52:12.9047370Z     
2025-04-11T03:52:12.9047512Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9047687Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9047796Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9047920Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9048004Z             if exitcode < 0:
2025-04-11T03:52:12.9048118Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9048222Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9048371Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9048469Z                     error_index=error_index,
2025-04-11T03:52:12.9048574Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9048665Z                     exit_code=exitcode,
2025-04-11T03:52:12.9048753Z                     signal_name=name,
2025-04-11T03:52:12.9048833Z                 )
2025-04-11T03:52:12.9048907Z             else:
2025-04-11T03:52:12.9049007Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9049176Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9049267Z                     error_index=error_index,
2025-04-11T03:52:12.9049371Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9049455Z                     exit_code=exitcode,
2025-04-11T03:52:12.9049531Z                 )
2025-04-11T03:52:12.9049602Z     
2025-04-11T03:52:12.9049731Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9049903Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9049989Z         msg += original_trace
2025-04-11T03:52:12.9050216Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9050378Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9050454Z E       
2025-04-11T03:52:12.9050586Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9050686Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9050982Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9051062Z E           fn(i, *args)
2025-04-11T03:52:12.9051359Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 279, in check_dist_linear
2025-04-11T03:52:12.9051513Z E           run_dist_linear_test()
2025-04-11T03:52:12.9051773Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9051865Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9052124Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9052277Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9052525Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9052616Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9052914Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 270, in run_dist_linear_test
2025-04-11T03:52:12.9053061Z E           check_linear_1d_col(lazy_init, seq_parallel_mode, overlap)
2025-04-11T03:52:12.9053349Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T03:52:12.9053449Z E           linear = nn.Linear(32, 128).cuda()
2025-04-11T03:52:12.9053722Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9053842Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9054120Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9054217Z E           param_applied = fn(param)
2025-04-11T03:52:12.9054552Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9054670Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9054778Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9055064Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9055200Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9055366Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9055373Z 
2025-04-11T03:52:12.9055669Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9055825Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9055978Z [04/11/25 03:48:56] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9056111Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9056217Z                              :75 launch                                         
2025-04-11T03:52:12.9056353Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9056483Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9056676Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9056815Z _______________________________ test_linearconv ________________________________
2025-04-11T03:52:12.9056872Z 
2025-04-11T03:52:12.9056966Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9057558Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9057567Z 
2025-04-11T03:52:12.9057668Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9057750Z         try_count = 0
2025-04-11T03:52:12.9057848Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9057929Z             max_try, int
2025-04-11T03:52:12.9058152Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9058223Z     
2025-04-11T03:52:12.9058337Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9058411Z             try:
2025-04-11T03:52:12.9058497Z                 try_count += 1
2025-04-11T03:52:12.9058591Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9058671Z                 return ret
2025-04-11T03:52:12.9058770Z             except exception_type as e:
2025-04-11T03:52:12.9058926Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9059115Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9059233Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9059376Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9059532Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9059615Z                     continue
2025-04-11T03:52:12.9059695Z                 else:
2025-04-11T03:52:12.9059913Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9059996Z >                   raise e
2025-04-11T03:52:12.9060002Z 
2025-04-11T03:52:12.9060097Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9060209Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9060347Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9060433Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9060694Z tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py:166: in test_linearconv
2025-04-11T03:52:12.9060788Z     spawn(run_dist, nprocs=2)
2025-04-11T03:52:12.9060891Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9060992Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9061244Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9061423Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9061706Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9061801Z     while not context.join():
2025-04-11T03:52:12.9061908Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9061914Z 
2025-04-11T03:52:12.9062117Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be334940>
2025-04-11T03:52:12.9062196Z timeout = None
2025-04-11T03:52:12.9062203Z 
2025-04-11T03:52:12.9062293Z     def join(self, timeout=None):
2025-04-11T03:52:12.9062420Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9062491Z     
2025-04-11T03:52:12.9062638Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9062781Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9062948Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9063041Z         of the first process exiting.
2025-04-11T03:52:12.9063113Z     
2025-04-11T03:52:12.9063260Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9063454Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9063534Z     
2025-04-11T03:52:12.9063612Z         Args:
2025-04-11T03:52:12.9063755Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9063831Z         """
2025-04-11T03:52:12.9063976Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9064078Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9064164Z             return True
2025-04-11T03:52:12.9064246Z     
2025-04-11T03:52:12.9064379Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9064561Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9064656Z             self.sentinels.keys(),
2025-04-11T03:52:12.9064740Z             timeout=timeout,
2025-04-11T03:52:12.9064816Z         )
2025-04-11T03:52:12.9064888Z     
2025-04-11T03:52:12.9064971Z         error_index = None
2025-04-11T03:52:12.9065063Z         for sentinel in ready:
2025-04-11T03:52:12.9065171Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9065336Z             process = self.processes[index]
2025-04-11T03:52:12.9065420Z             process.join()
2025-04-11T03:52:12.9065518Z             if process.exitcode != 0:
2025-04-11T03:52:12.9065609Z                 error_index = index
2025-04-11T03:52:12.9065686Z                 break
2025-04-11T03:52:12.9065760Z     
2025-04-11T03:52:12.9065855Z         # Return if there was no error.
2025-04-11T03:52:12.9065944Z         if error_index is None:
2025-04-11T03:52:12.9066080Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9066177Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9066250Z     
2025-04-11T03:52:12.9066395Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9066494Z         for process in self.processes:
2025-04-11T03:52:12.9066585Z             if process.is_alive():
2025-04-11T03:52:12.9066679Z                 process.terminate()
2025-04-11T03:52:12.9066766Z             process.join()
2025-04-11T03:52:12.9066839Z     
2025-04-11T03:52:12.9066986Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9067103Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9067267Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9067389Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9067473Z             if exitcode < 0:
2025-04-11T03:52:12.9067584Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9067690Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9067839Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9067934Z                     error_index=error_index,
2025-04-11T03:52:12.9068035Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9068124Z                     exit_code=exitcode,
2025-04-11T03:52:12.9068211Z                     signal_name=name,
2025-04-11T03:52:12.9068295Z                 )
2025-04-11T03:52:12.9068370Z             else:
2025-04-11T03:52:12.9068526Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9068690Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9068785Z                     error_index=error_index,
2025-04-11T03:52:12.9068887Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9068976Z                     exit_code=exitcode,
2025-04-11T03:52:12.9069053Z                 )
2025-04-11T03:52:12.9069128Z     
2025-04-11T03:52:12.9069268Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9069434Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9069520Z         msg += original_trace
2025-04-11T03:52:12.9069765Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9069926Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9070007Z E       
2025-04-11T03:52:12.9070133Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9070232Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9070532Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9070612Z E           fn(i, *args)
2025-04-11T03:52:12.9070912Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 158, in run_dist
2025-04-11T03:52:12.9071068Z E           check_linear_1d_col()
2025-04-11T03:52:12.9071332Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9071425Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9071745Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T03:52:12.9071926Z E           linear = nn.Linear(8, 80).cuda()
2025-04-11T03:52:12.9072201Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9072324Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9072590Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9072688Z E           param_applied = fn(param)
2025-04-11T03:52:12.9072963Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9073082Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9073186Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9073471Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9073613Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9073776Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9073782Z 
2025-04-11T03:52:12.9074087Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9074299Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9074460Z [04/11/25 03:49:01] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9074590Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9074701Z                              :75 launch                                         
2025-04-11T03:52:12.9074837Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9074962Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9075158Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9075304Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9075605Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:59565 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9075738Z ________________________________ test_ring_attn ________________________________
2025-04-11T03:52:12.9075742Z 
2025-04-11T03:52:12.9075839Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9076431Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9076437Z 
2025-04-11T03:52:12.9076601Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9076682Z         try_count = 0
2025-04-11T03:52:12.9076783Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9076873Z             max_try, int
2025-04-11T03:52:12.9077017Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9077093Z     
2025-04-11T03:52:12.9077204Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9077282Z             try:
2025-04-11T03:52:12.9077365Z                 try_count += 1
2025-04-11T03:52:12.9077456Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9077540Z                 return ret
2025-04-11T03:52:12.9077700Z             except exception_type as e:
2025-04-11T03:52:12.9077801Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9077988Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9078106Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9078257Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9078467Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9078555Z                     continue
2025-04-11T03:52:12.9078636Z                 else:
2025-04-11T03:52:12.9078860Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9078943Z >                   raise e
2025-04-11T03:52:12.9078948Z 
2025-04-11T03:52:12.9079045Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9079162Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9079299Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9079393Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9079552Z colossalai/testing/utils.py:64: in _execute_function_by_param
2025-04-11T03:52:12.9079647Z     partial_func(**kwargs)
2025-04-11T03:52:12.9079827Z tests/test_shardformer/test_layer/test_ring_attn.py:181: in test_ring_attn
2025-04-11T03:52:12.9079945Z     spawn(launch_single_ring, nprocs=world_size)
2025-04-11T03:52:12.9080053Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9080155Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9080474Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9080647Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9080934Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9081028Z     while not context.join():
2025-04-11T03:52:12.9081135Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9081143Z 
2025-04-11T03:52:12.9081336Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f05ddba0>
2025-04-11T03:52:12.9081419Z timeout = None
2025-04-11T03:52:12.9081423Z 
2025-04-11T03:52:12.9081518Z     def join(self, timeout=None):
2025-04-11T03:52:12.9081643Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9081718Z     
2025-04-11T03:52:12.9081864Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9082007Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9082170Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9082263Z         of the first process exiting.
2025-04-11T03:52:12.9082341Z     
2025-04-11T03:52:12.9082486Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9082624Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9082694Z     
2025-04-11T03:52:12.9082767Z         Args:
2025-04-11T03:52:12.9082909Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9083057Z         """
2025-04-11T03:52:12.9083200Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9083295Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9083376Z             return True
2025-04-11T03:52:12.9083451Z     
2025-04-11T03:52:12.9083585Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9083706Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9083800Z             self.sentinels.keys(),
2025-04-11T03:52:12.9083887Z             timeout=timeout,
2025-04-11T03:52:12.9083959Z         )
2025-04-11T03:52:12.9084093Z     
2025-04-11T03:52:12.9084182Z         error_index = None
2025-04-11T03:52:12.9084268Z         for sentinel in ready:
2025-04-11T03:52:12.9084377Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9084476Z             process = self.processes[index]
2025-04-11T03:52:12.9084561Z             process.join()
2025-04-11T03:52:12.9084661Z             if process.exitcode != 0:
2025-04-11T03:52:12.9084749Z                 error_index = index
2025-04-11T03:52:12.9084884Z                 break
2025-04-11T03:52:12.9084955Z     
2025-04-11T03:52:12.9085047Z         # Return if there was no error.
2025-04-11T03:52:12.9085136Z         if error_index is None:
2025-04-11T03:52:12.9085273Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9085373Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9085444Z     
2025-04-11T03:52:12.9085587Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9085688Z         for process in self.processes:
2025-04-11T03:52:12.9085775Z             if process.is_alive():
2025-04-11T03:52:12.9085872Z                 process.terminate()
2025-04-11T03:52:12.9085955Z             process.join()
2025-04-11T03:52:12.9086028Z     
2025-04-11T03:52:12.9086169Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9086285Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9086397Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9086526Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9086613Z             if exitcode < 0:
2025-04-11T03:52:12.9086718Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9086884Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9087036Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9087131Z                     error_index=error_index,
2025-04-11T03:52:12.9087239Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9087329Z                     exit_code=exitcode,
2025-04-11T03:52:12.9087418Z                     signal_name=name,
2025-04-11T03:52:12.9087491Z                 )
2025-04-11T03:52:12.9087566Z             else:
2025-04-11T03:52:12.9087670Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9087838Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9087934Z                     error_index=error_index,
2025-04-11T03:52:12.9088032Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9088121Z                     exit_code=exitcode,
2025-04-11T03:52:12.9088196Z                 )
2025-04-11T03:52:12.9088266Z     
2025-04-11T03:52:12.9088400Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9088567Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9088658Z         msg += original_trace
2025-04-11T03:52:12.9088832Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9088990Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9089065Z E       
2025-04-11T03:52:12.9089192Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9089355Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9089649Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9089736Z E           fn(i, *args)
2025-04-11T03:52:12.9090039Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 169, in launch_single_ring
2025-04-11T03:52:12.9090127Z E           check_packed_seq()
2025-04-11T03:52:12.9090388Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9090476Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9090791Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9090879Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9091125Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9091214Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9091319Z E         [Previous line repeated 2 more times]
2025-04-11T03:52:12.9091662Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 94, in check_packed_seq
2025-04-11T03:52:12.9091838Z E           padding_mask = torch.ones((bs, seqlen), dtype=torch.int, device=device)
2025-04-11T03:52:12.9091949Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9092232Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9092377Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9092539Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9092544Z 
2025-04-11T03:52:12.9092852Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9093007Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9093160Z [04/11/25 03:49:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9093292Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9093453Z                              :75 launch                                         
2025-04-11T03:52:12.9093593Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9093716Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9093917Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9094053Z _______________________________ test_double_ring _______________________________
2025-04-11T03:52:12.9094057Z 
2025-04-11T03:52:12.9094151Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9094752Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9094759Z 
2025-04-11T03:52:12.9094864Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9094946Z         try_count = 0
2025-04-11T03:52:12.9095047Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9095134Z             max_try, int
2025-04-11T03:52:12.9095279Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9095354Z     
2025-04-11T03:52:12.9095466Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9095542Z             try:
2025-04-11T03:52:12.9095629Z                 try_count += 1
2025-04-11T03:52:12.9095719Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9095803Z                 return ret
2025-04-11T03:52:12.9095952Z             except exception_type as e:
2025-04-11T03:52:12.9096054Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9096241Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9096362Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9096513Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9096663Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9096750Z                     continue
2025-04-11T03:52:12.9096827Z                 else:
2025-04-11T03:52:12.9097106Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9097190Z >                   raise e
2025-04-11T03:52:12.9097195Z 
2025-04-11T03:52:12.9097289Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9097401Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9097538Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9097689Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9097840Z colossalai/testing/utils.py:64: in _execute_function_by_param
2025-04-11T03:52:12.9097929Z     partial_func(**kwargs)
2025-04-11T03:52:12.9098113Z tests/test_shardformer/test_layer/test_ring_attn.py:187: in test_double_ring
2025-04-11T03:52:12.9098226Z     spawn(launch_double_ring, nprocs=world_size)
2025-04-11T03:52:12.9098330Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9098428Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9098688Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9098861Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9099140Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9099234Z     while not context.join():
2025-04-11T03:52:12.9099343Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9099349Z 
2025-04-11T03:52:12.9099550Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be336b30>
2025-04-11T03:52:12.9099628Z timeout = None
2025-04-11T03:52:12.9099692Z 
2025-04-11T03:52:12.9099789Z     def join(self, timeout=None):
2025-04-11T03:52:12.9099912Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9099986Z     
2025-04-11T03:52:12.9100131Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9100272Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9100431Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9100524Z         of the first process exiting.
2025-04-11T03:52:12.9100599Z     
2025-04-11T03:52:12.9100744Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9100881Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9100956Z     
2025-04-11T03:52:12.9101029Z         Args:
2025-04-11T03:52:12.9101173Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9101247Z         """
2025-04-11T03:52:12.9101390Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9101485Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9101565Z             return True
2025-04-11T03:52:12.9101639Z     
2025-04-11T03:52:12.9101769Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9101891Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9101982Z             self.sentinels.keys(),
2025-04-11T03:52:12.9102065Z             timeout=timeout,
2025-04-11T03:52:12.9102144Z         )
2025-04-11T03:52:12.9102214Z     
2025-04-11T03:52:12.9102359Z         error_index = None
2025-04-11T03:52:12.9102444Z         for sentinel in ready:
2025-04-11T03:52:12.9102552Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9102656Z             process = self.processes[index]
2025-04-11T03:52:12.9102740Z             process.join()
2025-04-11T03:52:12.9102839Z             if process.exitcode != 0:
2025-04-11T03:52:12.9102927Z                 error_index = index
2025-04-11T03:52:12.9103007Z                 break
2025-04-11T03:52:12.9103076Z     
2025-04-11T03:52:12.9103167Z         # Return if there was no error.
2025-04-11T03:52:12.9103257Z         if error_index is None:
2025-04-11T03:52:12.9103453Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9103554Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9103625Z     
2025-04-11T03:52:12.9103764Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9103865Z         for process in self.processes:
2025-04-11T03:52:12.9103956Z             if process.is_alive():
2025-04-11T03:52:12.9104053Z                 process.terminate()
2025-04-11T03:52:12.9104135Z             process.join()
2025-04-11T03:52:12.9104270Z     
2025-04-11T03:52:12.9104410Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9104526Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9104639Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9104760Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9104849Z             if exitcode < 0:
2025-04-11T03:52:12.9104956Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9105064Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9105216Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9105311Z                     error_index=error_index,
2025-04-11T03:52:12.9105415Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9105505Z                     exit_code=exitcode,
2025-04-11T03:52:12.9105596Z                     signal_name=name,
2025-04-11T03:52:12.9105671Z                 )
2025-04-11T03:52:12.9105747Z             else:
2025-04-11T03:52:12.9105856Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9106020Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9106192Z                     error_index=error_index,
2025-04-11T03:52:12.9106297Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9106385Z                     exit_code=exitcode,
2025-04-11T03:52:12.9106463Z                 )
2025-04-11T03:52:12.9106532Z     
2025-04-11T03:52:12.9106669Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9106840Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9106928Z         msg += original_trace
2025-04-11T03:52:12.9107105Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9107265Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9107346Z E       
2025-04-11T03:52:12.9107472Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9107574Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9107877Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9107963Z E           fn(i, *args)
2025-04-11T03:52:12.9108263Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 175, in launch_double_ring
2025-04-11T03:52:12.9108367Z E           check_ring_attn(inner_ring_size=2)
2025-04-11T03:52:12.9108663Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9108755Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9109077Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9109167Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9109418Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9109509Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9109615Z E         [Previous line repeated 2 more times]
2025-04-11T03:52:12.9109896Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 36, in check_ring_attn
2025-04-11T03:52:12.9110099Z E           qkv = torch.randn(bs, seq_len, 3, nheads, d, device=device, dtype=dtype, requires_grad=True)
2025-04-11T03:52:12.9110278Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9110564Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9110707Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9110867Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9111014Z 
2025-04-11T03:52:12.9111327Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9111480Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9111633Z [04/11/25 03:49:11] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9111766Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9111876Z                              :75 launch                                         
2025-04-11T03:52:12.9112016Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9112140Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9112339Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9112481Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9112775Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:22847 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9113129Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:22847 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9113275Z __________________________ test_all_to_all_attention ___________________________
2025-04-11T03:52:12.9113279Z 
2025-04-11T03:52:12.9113379Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9113982Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9113989Z 
2025-04-11T03:52:12.9114096Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9114175Z         try_count = 0
2025-04-11T03:52:12.9114281Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9114363Z             max_try, int
2025-04-11T03:52:12.9114508Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9114584Z     
2025-04-11T03:52:12.9114698Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9114779Z             try:
2025-04-11T03:52:12.9114864Z                 try_count += 1
2025-04-11T03:52:12.9114960Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9115041Z                 return ret
2025-04-11T03:52:12.9115134Z             except exception_type as e:
2025-04-11T03:52:12.9115239Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9115426Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9115602Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9115746Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9115905Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9115989Z                     continue
2025-04-11T03:52:12.9116066Z                 else:
2025-04-11T03:52:12.9116286Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9116365Z >                   raise e
2025-04-11T03:52:12.9116369Z 
2025-04-11T03:52:12.9116532Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9116642Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9116777Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9116863Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9117094Z tests/test_shardformer/test_layer/test_sequence_parallel.py:174: in test_all_to_all_attention
2025-04-11T03:52:12.9117195Z     spawn(check_all2all_attn, nprocs=4)
2025-04-11T03:52:12.9117351Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9117453Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9117711Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9117889Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9118172Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9118264Z     while not context.join():
2025-04-11T03:52:12.9118378Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9118382Z 
2025-04-11T03:52:12.9118578Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be4a41c0>
2025-04-11T03:52:12.9118661Z timeout = None
2025-04-11T03:52:12.9118667Z 
2025-04-11T03:52:12.9118756Z     def join(self, timeout=None):
2025-04-11T03:52:12.9118885Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9118957Z     
2025-04-11T03:52:12.9119101Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9119248Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9119469Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9119565Z         of the first process exiting.
2025-04-11T03:52:12.9119637Z     
2025-04-11T03:52:12.9119785Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9119922Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9119992Z     
2025-04-11T03:52:12.9120071Z         Args:
2025-04-11T03:52:12.9120205Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9120281Z         """
2025-04-11T03:52:12.9120423Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9120516Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9120602Z             return True
2025-04-11T03:52:12.9120674Z     
2025-04-11T03:52:12.9120809Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9120927Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9121022Z             self.sentinels.keys(),
2025-04-11T03:52:12.9121106Z             timeout=timeout,
2025-04-11T03:52:12.9121177Z         )
2025-04-11T03:52:12.9121253Z     
2025-04-11T03:52:12.9121336Z         error_index = None
2025-04-11T03:52:12.9121425Z         for sentinel in ready:
2025-04-11T03:52:12.9121528Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9121626Z             process = self.processes[index]
2025-04-11T03:52:12.9121715Z             process.join()
2025-04-11T03:52:12.9121807Z             if process.exitcode != 0:
2025-04-11T03:52:12.9121956Z                 error_index = index
2025-04-11T03:52:12.9122032Z                 break
2025-04-11T03:52:12.9122102Z     
2025-04-11T03:52:12.9122199Z         # Return if there was no error.
2025-04-11T03:52:12.9122285Z         if error_index is None:
2025-04-11T03:52:12.9122421Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9122519Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9122592Z     
2025-04-11T03:52:12.9122731Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9122830Z         for process in self.processes:
2025-04-11T03:52:12.9122922Z             if process.is_alive():
2025-04-11T03:52:12.9123075Z                 process.terminate()
2025-04-11T03:52:12.9123163Z             process.join()
2025-04-11T03:52:12.9123233Z     
2025-04-11T03:52:12.9123374Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9123494Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9123604Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9123729Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9123869Z             if exitcode < 0:
2025-04-11T03:52:12.9123978Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9124086Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9124236Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9124334Z                     error_index=error_index,
2025-04-11T03:52:12.9124433Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9124526Z                     exit_code=exitcode,
2025-04-11T03:52:12.9124613Z                     signal_name=name,
2025-04-11T03:52:12.9124686Z                 )
2025-04-11T03:52:12.9124764Z             else:
2025-04-11T03:52:12.9124863Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9125026Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9125120Z                     error_index=error_index,
2025-04-11T03:52:12.9125223Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9125310Z                     exit_code=exitcode,
2025-04-11T03:52:12.9125381Z                 )
2025-04-11T03:52:12.9125457Z     
2025-04-11T03:52:12.9125643Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9125812Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9125897Z         msg += original_trace
2025-04-11T03:52:12.9126067Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9126229Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9126303Z E       
2025-04-11T03:52:12.9126432Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9126531Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9126831Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9126915Z E           fn(i, *args)
2025-04-11T03:52:12.9127237Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 169, in check_all2all_attn
2025-04-11T03:52:12.9127332Z E           run_seq_parallel_attn()
2025-04-11T03:52:12.9127594Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9127688Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9127936Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9128029Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9128276Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9128362Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9128537Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.9128862Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 164, in run_seq_parallel_attn
2025-04-11T03:52:12.9129013Z E           seq_parallel_attn(seq_len, hidden_dim, head_num, batch_size)
2025-04-11T03:52:12.9129327Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 101, in seq_parallel_attn
2025-04-11T03:52:12.9129462Z E           x = torch.randn(batch_size, seq_len, hidden_dim).cuda()
2025-04-11T03:52:12.9129568Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9129914Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9130050Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9130211Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9130217Z 
2025-04-11T03:52:12.9130525Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9130740Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9130897Z [04/11/25 03:49:16] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9131024Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9131132Z                              :75 launch                                         
2025-04-11T03:52:12.9131268Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9131400Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9131595Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9131731Z _____________________________ test_vocab_embedding _____________________________
2025-04-11T03:52:12.9131738Z 
2025-04-11T03:52:12.9131836Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9132428Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9132487Z 
2025-04-11T03:52:12.9132595Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9132677Z         try_count = 0
2025-04-11T03:52:12.9132782Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9132865Z             max_try, int
2025-04-11T03:52:12.9133009Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9133084Z     
2025-04-11T03:52:12.9133196Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9133276Z             try:
2025-04-11T03:52:12.9133364Z                 try_count += 1
2025-04-11T03:52:12.9133458Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9133537Z                 return ret
2025-04-11T03:52:12.9133632Z             except exception_type as e:
2025-04-11T03:52:12.9133738Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9133929Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9134091Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9134235Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9134391Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9134475Z                     continue
2025-04-11T03:52:12.9134550Z                 else:
2025-04-11T03:52:12.9134772Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9134910Z >                   raise e
2025-04-11T03:52:12.9134915Z 
2025-04-11T03:52:12.9135012Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9135124Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9135270Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9135357Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9135600Z tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py:54: in test_vocab_embedding
2025-04-11T03:52:12.9135695Z     spawn(run_dist, nprocs=2)
2025-04-11T03:52:12.9135798Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9135900Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9136234Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9136412Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9136697Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9136787Z     while not context.join():
2025-04-11T03:52:12.9136902Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9136964Z 
2025-04-11T03:52:12.9137164Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be337400>
2025-04-11T03:52:12.9137251Z timeout = None
2025-04-11T03:52:12.9137256Z 
2025-04-11T03:52:12.9137346Z     def join(self, timeout=None):
2025-04-11T03:52:12.9137475Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9137546Z     
2025-04-11T03:52:12.9137690Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9137837Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9138000Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9138095Z         of the first process exiting.
2025-04-11T03:52:12.9138166Z     
2025-04-11T03:52:12.9138316Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9138451Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9138522Z     
2025-04-11T03:52:12.9138601Z         Args:
2025-04-11T03:52:12.9138738Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9138872Z         """
2025-04-11T03:52:12.9139011Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9139105Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9139191Z             return True
2025-04-11T03:52:12.9139260Z     
2025-04-11T03:52:12.9139397Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9139515Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9139609Z             self.sentinels.keys(),
2025-04-11T03:52:12.9139692Z             timeout=timeout,
2025-04-11T03:52:12.9139764Z         )
2025-04-11T03:52:12.9139838Z     
2025-04-11T03:52:12.9139921Z         error_index = None
2025-04-11T03:52:12.9140008Z         for sentinel in ready:
2025-04-11T03:52:12.9140116Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9140213Z             process = self.processes[index]
2025-04-11T03:52:12.9140301Z             process.join()
2025-04-11T03:52:12.9140395Z             if process.exitcode != 0:
2025-04-11T03:52:12.9140485Z                 error_index = index
2025-04-11T03:52:12.9140560Z                 break
2025-04-11T03:52:12.9140629Z     
2025-04-11T03:52:12.9140723Z         # Return if there was no error.
2025-04-11T03:52:12.9140810Z         if error_index is None:
2025-04-11T03:52:12.9140948Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9141044Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9141116Z     
2025-04-11T03:52:12.9141253Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9141406Z         for process in self.processes:
2025-04-11T03:52:12.9141498Z             if process.is_alive():
2025-04-11T03:52:12.9141586Z                 process.terminate()
2025-04-11T03:52:12.9141675Z             process.join()
2025-04-11T03:52:12.9141746Z     
2025-04-11T03:52:12.9141887Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9142006Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9142112Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9142235Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9142318Z             if exitcode < 0:
2025-04-11T03:52:12.9142485Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9142593Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9142743Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9142841Z                     error_index=error_index,
2025-04-11T03:52:12.9142942Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9143033Z                     exit_code=exitcode,
2025-04-11T03:52:12.9143177Z                     signal_name=name,
2025-04-11T03:52:12.9143253Z                 )
2025-04-11T03:52:12.9143332Z             else:
2025-04-11T03:52:12.9143435Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9143606Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9143697Z                     error_index=error_index,
2025-04-11T03:52:12.9143800Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9143885Z                     exit_code=exitcode,
2025-04-11T03:52:12.9143958Z                 )
2025-04-11T03:52:12.9144033Z     
2025-04-11T03:52:12.9144165Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9144337Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9144423Z         msg += original_trace
2025-04-11T03:52:12.9144598Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9144766Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9144840Z E       
2025-04-11T03:52:12.9144969Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9145126Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9145423Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9145504Z E           fn(i, *args)
2025-04-11T03:52:12.9145821Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 49, in run_dist
2025-04-11T03:52:12.9145920Z E           check_vocab_embedding_1d()
2025-04-11T03:52:12.9146173Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9146271Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9146642Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 18, in check_vocab_embedding_1d
2025-04-11T03:52:12.9146761Z E           embedding = nn.Embedding(128, 32).to("cuda")
2025-04-11T03:52:12.9147026Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:12.9147122Z E           return self._apply(convert)
2025-04-11T03:52:12.9147395Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9147489Z E           param_applied = fn(param)
2025-04-11T03:52:12.9147762Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:12.9147970Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.9148142Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9148462Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9148608Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9148770Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9148777Z 
2025-04-11T03:52:12.9149078Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9149232Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9149463Z [04/11/25 03:49:20] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9149595Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9149701Z                              :75 launch                                         
2025-04-11T03:52:12.9149844Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9149966Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9150226Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9150355Z __________________________________ test_bert ___________________________________
2025-04-11T03:52:12.9150359Z 
2025-04-11T03:52:12.9150454Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9150461Z 
2025-04-11T03:52:12.9150559Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9150640Z         try_count = 0
2025-04-11T03:52:12.9150745Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9150826Z             max_try, int
2025-04-11T03:52:12.9150974Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9151044Z     
2025-04-11T03:52:12.9151157Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9151239Z             try:
2025-04-11T03:52:12.9151324Z                 try_count += 1
2025-04-11T03:52:12.9151422Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9151428Z 
2025-04-11T03:52:12.9151522Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9151632Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9151828Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9151927Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9152086Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9152186Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9152298Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9152302Z 
2025-04-11T03:52:12.9152379Z device = None
2025-04-11T03:52:12.9152384Z 
2025-04-11T03:52:12.9152511Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9152666Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9152735Z     
2025-04-11T03:52:12.9152817Z         Args:
2025-04-11T03:52:12.9152994Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9153167Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9153277Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9153350Z         """
2025-04-11T03:52:12.9153434Z         _lazy_init()
2025-04-11T03:52:12.9153529Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9153637Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9153745Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9154039Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9154177Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9154400Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9154410Z 
2025-04-11T03:52:12.9154654Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9154788Z __________________________________ test_blip2 __________________________________
2025-04-11T03:52:12.9154793Z 
2025-04-11T03:52:12.9154892Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9154896Z 
2025-04-11T03:52:12.9154994Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9155076Z         try_count = 0
2025-04-11T03:52:12.9155253Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9155337Z             max_try, int
2025-04-11T03:52:12.9155484Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9155557Z     
2025-04-11T03:52:12.9155673Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9155750Z             try:
2025-04-11T03:52:12.9155838Z                 try_count += 1
2025-04-11T03:52:12.9155930Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9155993Z 
2025-04-11T03:52:12.9156090Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9156204Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9156321Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9156421Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9156578Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9156678Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9156785Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9156789Z 
2025-04-11T03:52:12.9156866Z device = None
2025-04-11T03:52:12.9156873Z 
2025-04-11T03:52:12.9156994Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9157146Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9157227Z     
2025-04-11T03:52:12.9157301Z         Args:
2025-04-11T03:52:12.9157473Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9157644Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9157808Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9157889Z         """
2025-04-11T03:52:12.9157966Z         _lazy_init()
2025-04-11T03:52:12.9158065Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9158164Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9158278Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9158564Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9158701Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9158864Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9158868Z 
2025-04-11T03:52:12.9159107Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9159241Z __________________________________ test_bloom __________________________________
2025-04-11T03:52:12.9159246Z 
2025-04-11T03:52:12.9159337Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9159341Z 
2025-04-11T03:52:12.9159445Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9159525Z         try_count = 0
2025-04-11T03:52:12.9159627Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9159707Z             max_try, int
2025-04-11T03:52:12.9159849Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9159927Z     
2025-04-11T03:52:12.9160037Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9160118Z             try:
2025-04-11T03:52:12.9160262Z                 try_count += 1
2025-04-11T03:52:12.9160352Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9160357Z 
2025-04-11T03:52:12.9160456Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9160565Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9160683Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9160779Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9160938Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9161034Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9161142Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9161210Z 
2025-04-11T03:52:12.9161293Z device = None
2025-04-11T03:52:12.9161298Z 
2025-04-11T03:52:12.9161416Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9161573Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9161646Z     
2025-04-11T03:52:12.9161721Z         Args:
2025-04-11T03:52:12.9161886Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9162107Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9162220Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9162295Z         """
2025-04-11T03:52:12.9162378Z         _lazy_init()
2025-04-11T03:52:12.9162472Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9162577Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9162685Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9162971Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9163112Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9163270Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9163276Z 
2025-04-11T03:52:12.9163517Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9163649Z _________________________________ test_chatglm _________________________________
2025-04-11T03:52:12.9163653Z 
2025-04-11T03:52:12.9163807Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9163812Z 
2025-04-11T03:52:12.9163910Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9163991Z         try_count = 0
2025-04-11T03:52:12.9164088Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9164170Z             max_try, int
2025-04-11T03:52:12.9164317Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9164391Z     
2025-04-11T03:52:12.9164504Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9164579Z             try:
2025-04-11T03:52:12.9164662Z                 try_count += 1
2025-04-11T03:52:12.9164760Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9164765Z 
2025-04-11T03:52:12.9164859Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9164973Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9165086Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9165183Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9165338Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9165432Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9165543Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9165549Z 
2025-04-11T03:52:12.9165626Z device = None
2025-04-11T03:52:12.9165630Z 
2025-04-11T03:52:12.9165753Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9165904Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9165978Z     
2025-04-11T03:52:12.9166107Z         Args:
2025-04-11T03:52:12.9166276Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9166448Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9166556Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9166635Z         """
2025-04-11T03:52:12.9166713Z         _lazy_init()
2025-04-11T03:52:12.9166805Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9166910Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9167016Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9167371Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9167508Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9167668Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9167674Z 
2025-04-11T03:52:12.9167910Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9168103Z _________________________________ test_command _________________________________
2025-04-11T03:52:12.9168107Z 
2025-04-11T03:52:12.9168202Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9168208Z 
2025-04-11T03:52:12.9168307Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9168394Z         try_count = 0
2025-04-11T03:52:12.9168494Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9168577Z             max_try, int
2025-04-11T03:52:12.9168721Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9168797Z     
2025-04-11T03:52:12.9168905Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9168981Z             try:
2025-04-11T03:52:12.9169068Z                 try_count += 1
2025-04-11T03:52:12.9169157Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9169163Z 
2025-04-11T03:52:12.9169261Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9169368Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9169487Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9169577Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9169786Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9169885Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9169992Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9169996Z 
2025-04-11T03:52:12.9170081Z device = None
2025-04-11T03:52:12.9170085Z 
2025-04-11T03:52:12.9170205Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9170360Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9170432Z     
2025-04-11T03:52:12.9170505Z         Args:
2025-04-11T03:52:12.9170677Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9170840Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9170952Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9171024Z         """
2025-04-11T03:52:12.9171105Z         _lazy_init()
2025-04-11T03:52:12.9171204Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9171303Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9171412Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9171694Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9171834Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9171991Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9172053Z 
2025-04-11T03:52:12.9172294Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9172426Z _______________________________ test_deepseek[4] _______________________________
2025-04-11T03:52:12.9172431Z 
2025-04-11T03:52:12.9172548Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9173147Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9173153Z 
2025-04-11T03:52:12.9173314Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9173398Z         try_count = 0
2025-04-11T03:52:12.9173497Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9173582Z             max_try, int
2025-04-11T03:52:12.9173721Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9173792Z     
2025-04-11T03:52:12.9173904Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9173979Z             try:
2025-04-11T03:52:12.9174118Z                 try_count += 1
2025-04-11T03:52:12.9174208Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9174292Z                 return ret
2025-04-11T03:52:12.9174386Z             except exception_type as e:
2025-04-11T03:52:12.9174485Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9174675Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9174791Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9174940Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9175095Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9175181Z                     continue
2025-04-11T03:52:12.9175259Z                 else:
2025-04-11T03:52:12.9175480Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9175566Z >                   raise e
2025-04-11T03:52:12.9175572Z 
2025-04-11T03:52:12.9175665Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9175777Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9175963Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9176054Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9176243Z tests/test_shardformer/test_model/test_shard_deepseek.py:228: in test_deepseek
2025-04-11T03:52:12.9176341Z     spawn(check_deepseek, world_size)
2025-04-11T03:52:12.9176445Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9176544Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9176803Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9176981Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9177267Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9177358Z     while not context.join():
2025-04-11T03:52:12.9177465Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9177472Z 
2025-04-11T03:52:12.9177668Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ecb27d0>
2025-04-11T03:52:12.9177748Z timeout = None
2025-04-11T03:52:12.9177753Z 
2025-04-11T03:52:12.9177844Z     def join(self, timeout=None):
2025-04-11T03:52:12.9177967Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9178044Z     
2025-04-11T03:52:12.9178189Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9178336Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9178578Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9178673Z         of the first process exiting.
2025-04-11T03:52:12.9178749Z     
2025-04-11T03:52:12.9178898Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9179041Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9179114Z     
2025-04-11T03:52:12.9179189Z         Args:
2025-04-11T03:52:12.9179335Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9179408Z         """
2025-04-11T03:52:12.9179550Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9179703Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9179783Z             return True
2025-04-11T03:52:12.9179857Z     
2025-04-11T03:52:12.9179990Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9180112Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9180205Z             self.sentinels.keys(),
2025-04-11T03:52:12.9180291Z             timeout=timeout,
2025-04-11T03:52:12.9180370Z         )
2025-04-11T03:52:12.9180494Z     
2025-04-11T03:52:12.9180583Z         error_index = None
2025-04-11T03:52:12.9180667Z         for sentinel in ready:
2025-04-11T03:52:12.9180780Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9180884Z             process = self.processes[index]
2025-04-11T03:52:12.9180971Z             process.join()
2025-04-11T03:52:12.9181072Z             if process.exitcode != 0:
2025-04-11T03:52:12.9181160Z                 error_index = index
2025-04-11T03:52:12.9181241Z                 break
2025-04-11T03:52:12.9181311Z     
2025-04-11T03:52:12.9181405Z         # Return if there was no error.
2025-04-11T03:52:12.9181498Z         if error_index is None:
2025-04-11T03:52:12.9181632Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9181731Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9181803Z     
2025-04-11T03:52:12.9181941Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9182044Z         for process in self.processes:
2025-04-11T03:52:12.9182134Z             if process.is_alive():
2025-04-11T03:52:12.9182228Z                 process.terminate()
2025-04-11T03:52:12.9182368Z             process.join()
2025-04-11T03:52:12.9182442Z     
2025-04-11T03:52:12.9182583Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9182698Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9182809Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9182930Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9183020Z             if exitcode < 0:
2025-04-11T03:52:12.9183126Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9183231Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9183385Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9183479Z                     error_index=error_index,
2025-04-11T03:52:12.9183585Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9183672Z                     exit_code=exitcode,
2025-04-11T03:52:12.9183764Z                     signal_name=name,
2025-04-11T03:52:12.9183840Z                 )
2025-04-11T03:52:12.9183915Z             else:
2025-04-11T03:52:12.9184021Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9184181Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9184279Z                     error_index=error_index,
2025-04-11T03:52:12.9184377Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9184467Z                     exit_code=exitcode,
2025-04-11T03:52:12.9184540Z                 )
2025-04-11T03:52:12.9184609Z     
2025-04-11T03:52:12.9184742Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9184966Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9185056Z         msg += original_trace
2025-04-11T03:52:12.9185224Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9185383Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9185462Z E       
2025-04-11T03:52:12.9185587Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9185689Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9185983Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9186129Z E           fn(i, *args)
2025-04-11T03:52:12.9186435Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 216, in check_deepseek
2025-04-11T03:52:12.9186523Z E           run_deepseek_test()
2025-04-11T03:52:12.9186785Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9186933Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9187239Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 187, in run_deepseek_test
2025-04-11T03:52:12.9187335Z E           run_deepseek_commom(config)
2025-04-11T03:52:12.9187639Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 77, in run_deepseek_commom
2025-04-11T03:52:12.9187842Z E           torch_model = AutoModel.from_config(config, trust_remote_code=True).cuda().to(dtype)
2025-04-11T03:52:12.9188135Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9188236Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9188578Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9188705Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9188972Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9189063Z E           module._apply(fn)
2025-04-11T03:52:12.9189390Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9189488Z E           param_applied = fn(param)
2025-04-11T03:52:12.9189761Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9189878Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9189987Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9190268Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9190409Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9190569Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9190575Z 
2025-04-11T03:52:12.9190877Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9191031Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9191190Z [04/11/25 03:49:26] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9191321Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9191430Z                              :75 launch                                         
2025-04-11T03:52:12.9191571Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9191692Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9191846Z rank 0 testing (0, 1, 4, 1, 1)
2025-04-11T03:52:12.9192049Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9192198Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9192899Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9192991Z   warnings.warn(
2025-04-11T03:52:12.9193745Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9193829Z   warnings.warn(
2025-04-11T03:52:12.9194513Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9194656Z   warnings.warn(
2025-04-11T03:52:12.9195337Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9195417Z   warnings.warn(
2025-04-11T03:52:12.9196252Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9196333Z   warnings.warn(
2025-04-11T03:52:12.9197137Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9197276Z   warnings.warn(
2025-04-11T03:52:12.9198097Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9198179Z   warnings.warn(
2025-04-11T03:52:12.9199000Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9199082Z   warnings.warn(
2025-04-11T03:52:12.9199896Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9199976Z   warnings.warn(
2025-04-11T03:52:12.9200791Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9200872Z   warnings.warn(
2025-04-11T03:52:12.9201685Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9201844Z   warnings.warn(
2025-04-11T03:52:12.9202635Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9202786Z   warnings.warn(
2025-04-11T03:52:12.9203583Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9203667Z   warnings.warn(
2025-04-11T03:52:12.9204463Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9204603Z   warnings.warn(
2025-04-11T03:52:12.9204889Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9204988Z - configuration_deepseek.py
2025-04-11T03:52:12.9205339Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9206131Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9206213Z   warnings.warn(
2025-04-11T03:52:12.9206490Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9206649Z - configuration_deepseek.py
2025-04-11T03:52:12.9206989Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9207794Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9207875Z   warnings.warn(
2025-04-11T03:52:12.9208153Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9208249Z - configuration_deepseek.py
2025-04-11T03:52:12.9208588Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9209377Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9209463Z   warnings.warn(
2025-04-11T03:52:12.9209730Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9209823Z - configuration_deepseek.py
2025-04-11T03:52:12.9210153Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9210999Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9211083Z   warnings.warn(
2025-04-11T03:52:12.9211858Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9212007Z   warnings.warn(
2025-04-11T03:52:12.9212790Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9212934Z   warnings.warn(
2025-04-11T03:52:12.9213715Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9213801Z   warnings.warn(
2025-04-11T03:52:12.9214580Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9214666Z   warnings.warn(
2025-04-11T03:52:12.9215450Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9215536Z   warnings.warn(
2025-04-11T03:52:12.9215804Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9215945Z - modeling_deepseek.py
2025-04-11T03:52:12.9216282Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9216551Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9216642Z - modeling_deepseek.py
2025-04-11T03:52:12.9216978Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9217249Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9217333Z - modeling_deepseek.py
2025-04-11T03:52:12.9217666Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9217933Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
2025-04-11T03:52:12.9218021Z - modeling_deepseek.py
2025-04-11T03:52:12.9218349Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9219159Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9219300Z   warnings.warn(
2025-04-11T03:52:12.9220083Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9220168Z   warnings.warn(
2025-04-11T03:52:12.9220952Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9221184Z   warnings.warn(
2025-04-11T03:52:12.9221324Z _____________________________ test_deepseek_v3[4] ______________________________
2025-04-11T03:52:12.9221331Z 
2025-04-11T03:52:12.9221454Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9222058Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9222118Z 
2025-04-11T03:52:12.9222228Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9222312Z         try_count = 0
2025-04-11T03:52:12.9222419Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9222501Z             max_try, int
2025-04-11T03:52:12.9222652Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9222731Z     
2025-04-11T03:52:12.9222842Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9222923Z             try:
2025-04-11T03:52:12.9223007Z                 try_count += 1
2025-04-11T03:52:12.9223105Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9223185Z                 return ret
2025-04-11T03:52:12.9223280Z             except exception_type as e:
2025-04-11T03:52:12.9223390Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9223582Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9223760Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9223909Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9224069Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9224155Z                     continue
2025-04-11T03:52:12.9224232Z                 else:
2025-04-11T03:52:12.9224453Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9224531Z >                   raise e
2025-04-11T03:52:12.9224537Z 
2025-04-11T03:52:12.9224635Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9224746Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9224883Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9224971Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9225182Z tests/test_shardformer/test_model/test_shard_deepseek_v3.py:100: in test_deepseek_v3
2025-04-11T03:52:12.9225286Z     spawn(check_deepseek_v3, world_size)
2025-04-11T03:52:12.9225388Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9225493Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9225753Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9225930Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9226216Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9226365Z     while not context.join():
2025-04-11T03:52:12.9226477Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9226483Z 
2025-04-11T03:52:12.9226681Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ec753f0>
2025-04-11T03:52:12.9226764Z timeout = None
2025-04-11T03:52:12.9226768Z 
2025-04-11T03:52:12.9226861Z     def join(self, timeout=None):
2025-04-11T03:52:12.9226992Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9227063Z     
2025-04-11T03:52:12.9227207Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9227415Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9227578Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9227674Z         of the first process exiting.
2025-04-11T03:52:12.9227746Z     
2025-04-11T03:52:12.9227897Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9228039Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9228166Z     
2025-04-11T03:52:12.9228247Z         Args:
2025-04-11T03:52:12.9228384Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9228501Z         """
2025-04-11T03:52:12.9228641Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9228736Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9228821Z             return True
2025-04-11T03:52:12.9228892Z     
2025-04-11T03:52:12.9229026Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9229145Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9229238Z             self.sentinels.keys(),
2025-04-11T03:52:12.9229327Z             timeout=timeout,
2025-04-11T03:52:12.9229400Z         )
2025-04-11T03:52:12.9229474Z     
2025-04-11T03:52:12.9229559Z         error_index = None
2025-04-11T03:52:12.9229650Z         for sentinel in ready:
2025-04-11T03:52:12.9229757Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9229859Z             process = self.processes[index]
2025-04-11T03:52:12.9229947Z             process.join()
2025-04-11T03:52:12.9230040Z             if process.exitcode != 0:
2025-04-11T03:52:12.9230194Z                 error_index = index
2025-04-11T03:52:12.9230270Z                 break
2025-04-11T03:52:12.9230341Z     
2025-04-11T03:52:12.9230438Z         # Return if there was no error.
2025-04-11T03:52:12.9230522Z         if error_index is None:
2025-04-11T03:52:12.9230662Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9230757Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9230828Z     
2025-04-11T03:52:12.9230971Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9231068Z         for process in self.processes:
2025-04-11T03:52:12.9231164Z             if process.is_alive():
2025-04-11T03:52:12.9231255Z                 process.terminate()
2025-04-11T03:52:12.9231346Z             process.join()
2025-04-11T03:52:12.9231414Z     
2025-04-11T03:52:12.9231553Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9231673Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9231783Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9231906Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9231992Z             if exitcode < 0:
2025-04-11T03:52:12.9232101Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9232213Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9232364Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9232464Z                     error_index=error_index,
2025-04-11T03:52:12.9232565Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9232718Z                     exit_code=exitcode,
2025-04-11T03:52:12.9232808Z                     signal_name=name,
2025-04-11T03:52:12.9232885Z                 )
2025-04-11T03:52:12.9232963Z             else:
2025-04-11T03:52:12.9233065Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9233233Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9233326Z                     error_index=error_index,
2025-04-11T03:52:12.9233426Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9233516Z                     exit_code=exitcode,
2025-04-11T03:52:12.9233656Z                 )
2025-04-11T03:52:12.9233730Z     
2025-04-11T03:52:12.9233864Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9234039Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9234124Z         msg += original_trace
2025-04-11T03:52:12.9234298Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9234464Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9234654Z E       
2025-04-11T03:52:12.9234794Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9234894Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9235194Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9235275Z E           fn(i, *args)
2025-04-11T03:52:12.9235591Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 93, in check_deepseek_v3
2025-04-11T03:52:12.9235690Z E           run_deepseek_v3_test()
2025-04-11T03:52:12.9235945Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9236041Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9236356Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 82, in run_deepseek_v3_test
2025-04-11T03:52:12.9236453Z E           check_forward_backward(
2025-04-11T03:52:12.9236769Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 29, in check_forward_backward
2025-04-11T03:52:12.9237099Z E           org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
2025-04-11T03:52:12.9237405Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 138, in build_model_from_hybrid_plugin
2025-04-11T03:52:12.9237502Z E           org_model = org_model.cuda()
2025-04-11T03:52:12.9237794Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9237897Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9238171Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9238293Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9238567Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9238659Z E           module._apply(fn)
2025-04-11T03:52:12.9238926Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9239020Z E           module._apply(fn)
2025-04-11T03:52:12.9239287Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9239387Z E           param_applied = fn(param)
2025-04-11T03:52:12.9239662Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9239851Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9239962Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9240252Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9240391Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9240553Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9240558Z 
2025-04-11T03:52:12.9240861Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9241076Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9241235Z [04/11/25 03:49:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9241364Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9241479Z                              :75 launch                                         
2025-04-11T03:52:12.9241617Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9241803Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9242001Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9242148Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9243274Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9243446Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9244548Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9244779Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9245888Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9246053Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9247149Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9247310Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9248008Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9248151Z   warnings.warn(
2025-04-11T03:52:12.9248822Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9248906Z   warnings.warn(
2025-04-11T03:52:12.9249575Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9249722Z   warnings.warn(
2025-04-11T03:52:12.9250377Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9250459Z   warnings.warn(
2025-04-11T03:52:12.9251273Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9251411Z   warnings.warn(
2025-04-11T03:52:12.9252222Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9252303Z   warnings.warn(
2025-04-11T03:52:12.9253110Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9253191Z   warnings.warn(
2025-04-11T03:52:12.9253992Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9254130Z   warnings.warn(
2025-04-11T03:52:12.9254905Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9254989Z   warnings.warn(
2025-04-11T03:52:12.9255770Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9255857Z   warnings.warn(
2025-04-11T03:52:12.9256637Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9256721Z   warnings.warn(
2025-04-11T03:52:12.9257492Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9257635Z   warnings.warn(
2025-04-11T03:52:12.9258415Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9258503Z   warnings.warn(
2025-04-11T03:52:12.9259279Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9259426Z   warnings.warn(
2025-04-11T03:52:12.9260207Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9260350Z   warnings.warn(
2025-04-11T03:52:12.9261130Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9261213Z   warnings.warn(
2025-04-11T03:52:12.9261509Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:44807 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9261794Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:44807 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9262579Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9262664Z   warnings.warn(
2025-04-11T03:52:12.9263440Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9263574Z   warnings.warn(
2025-04-11T03:52:12.9264382Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9264463Z   warnings.warn(
2025-04-11T03:52:12.9265258Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9265340Z   warnings.warn(
2025-04-11T03:52:12.9266142Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9266221Z   warnings.warn(
2025-04-11T03:52:12.9267024Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9267156Z   warnings.warn(
2025-04-11T03:52:12.9267939Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9268023Z   warnings.warn(
2025-04-11T03:52:12.9268850Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9269011Z   warnings.warn(
2025-04-11T03:52:12.9269800Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9269942Z   warnings.warn(
2025-04-11T03:52:12.9270729Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9270812Z   warnings.warn(
2025-04-11T03:52:12.9271075Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9271171Z - configuration_deepseek.py
2025-04-11T03:52:12.9271513Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9272306Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9272453Z   warnings.warn(
2025-04-11T03:52:12.9272706Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9272801Z - configuration_deepseek.py
2025-04-11T03:52:12.9273148Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9273964Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9274045Z   warnings.warn(
2025-04-11T03:52:12.9274294Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9274388Z - configuration_deepseek.py
2025-04-11T03:52:12.9274723Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9275531Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9275617Z   warnings.warn(
2025-04-11T03:52:12.9275861Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9276014Z - configuration_deepseek.py
2025-04-11T03:52:12.9276343Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9277130Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9277211Z   warnings.warn(
2025-04-11T03:52:12.9277996Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9278142Z   warnings.warn(
2025-04-11T03:52:12.9278918Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9279055Z   warnings.warn(
2025-04-11T03:52:12.9279839Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9279918Z   warnings.warn(
2025-04-11T03:52:12.9280691Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9280776Z   warnings.warn(
2025-04-11T03:52:12.9281553Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9281713Z   warnings.warn(
2025-04-11T03:52:12.9281958Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9282048Z - modeling_deepseek.py
2025-04-11T03:52:12.9282386Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9282629Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9282718Z - modeling_deepseek.py
2025-04-11T03:52:12.9283053Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9283291Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9283379Z - modeling_deepseek.py
2025-04-11T03:52:12.9283719Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9283956Z A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
2025-04-11T03:52:12.9284050Z - modeling_deepseek.py
2025-04-11T03:52:12.9284382Z . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
2025-04-11T03:52:12.9284526Z _________________________________ test_falcon __________________________________
2025-04-11T03:52:12.9284589Z 
2025-04-11T03:52:12.9284687Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9284692Z 
2025-04-11T03:52:12.9284804Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9284889Z         try_count = 0
2025-04-11T03:52:12.9284994Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9285086Z             max_try, int
2025-04-11T03:52:12.9285236Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9285315Z     
2025-04-11T03:52:12.9285433Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9285515Z             try:
2025-04-11T03:52:12.9285665Z                 try_count += 1
2025-04-11T03:52:12.9285759Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9285764Z 
2025-04-11T03:52:12.9285865Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9285979Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9286105Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9286203Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9286361Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9286518Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9286631Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9286638Z 
2025-04-11T03:52:12.9286721Z device = None
2025-04-11T03:52:12.9286725Z 
2025-04-11T03:52:12.9286847Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9287005Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9287077Z     
2025-04-11T03:52:12.9287155Z         Args:
2025-04-11T03:52:12.9287326Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9287496Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9287613Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9287686Z         """
2025-04-11T03:52:12.9287766Z         _lazy_init()
2025-04-11T03:52:12.9287866Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9287970Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9288082Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9288431Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9288575Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9288736Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9288742Z 
2025-04-11T03:52:12.9288990Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9289123Z __________________________________ test_gpt2 ___________________________________
2025-04-11T03:52:12.9289129Z 
2025-04-11T03:52:12.9289227Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9289231Z 
2025-04-11T03:52:12.9289333Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9289414Z         try_count = 0
2025-04-11T03:52:12.9289518Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9289602Z             max_try, int
2025-04-11T03:52:12.9289750Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9289820Z     
2025-04-11T03:52:12.9289936Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9290012Z             try:
2025-04-11T03:52:12.9290097Z                 try_count += 1
2025-04-11T03:52:12.9290194Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9290198Z 
2025-04-11T03:52:12.9290292Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9290403Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9290520Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9290674Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9290832Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9290929Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9291042Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9291048Z 
2025-04-11T03:52:12.9291125Z device = None
2025-04-11T03:52:12.9291130Z 
2025-04-11T03:52:12.9291250Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9291402Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9291476Z     
2025-04-11T03:52:12.9291610Z         Args:
2025-04-11T03:52:12.9291779Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9291952Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9292060Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9292138Z         """
2025-04-11T03:52:12.9292216Z         _lazy_init()
2025-04-11T03:52:12.9292311Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9292476Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9292583Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9292871Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9293013Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9293173Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9293179Z 
2025-04-11T03:52:12.9293418Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9293547Z __________________________________ test_llama __________________________________
2025-04-11T03:52:12.9293555Z 
2025-04-11T03:52:12.9293648Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9293653Z 
2025-04-11T03:52:12.9293753Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9293839Z         try_count = 0
2025-04-11T03:52:12.9293938Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9294021Z             max_try, int
2025-04-11T03:52:12.9294164Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9294296Z     
2025-04-11T03:52:12.9294412Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9294489Z             try:
2025-04-11T03:52:12.9294576Z                 try_count += 1
2025-04-11T03:52:12.9294670Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9294676Z 
2025-04-11T03:52:12.9294771Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9294880Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9294994Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9295093Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9295247Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9295346Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9295454Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9295458Z 
2025-04-11T03:52:12.9295540Z device = None
2025-04-11T03:52:12.9295544Z 
2025-04-11T03:52:12.9295660Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9295811Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9295887Z     
2025-04-11T03:52:12.9295960Z         Args:
2025-04-11T03:52:12.9296135Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9296301Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9296413Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9296561Z         """
2025-04-11T03:52:12.9296641Z         _lazy_init()
2025-04-11T03:52:12.9296738Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9296840Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9296949Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9297233Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9297376Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9297534Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9297597Z 
2025-04-11T03:52:12.9297835Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9297968Z _________________________________ test_mistral _________________________________
2025-04-11T03:52:12.9297973Z 
2025-04-11T03:52:12.9298064Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9298070Z 
2025-04-11T03:52:12.9298173Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9298251Z         try_count = 0
2025-04-11T03:52:12.9298414Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9298494Z             max_try, int
2025-04-11T03:52:12.9298636Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9298712Z     
2025-04-11T03:52:12.9298823Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9298903Z             try:
2025-04-11T03:52:12.9298986Z                 try_count += 1
2025-04-11T03:52:12.9299079Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9299085Z 
2025-04-11T03:52:12.9299178Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9299284Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9299403Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9299496Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9299654Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9299748Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9299861Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9299866Z 
2025-04-11T03:52:12.9299941Z device = None
2025-04-11T03:52:12.9299946Z 
2025-04-11T03:52:12.9300120Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9300275Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9300345Z     
2025-04-11T03:52:12.9300424Z         Args:
2025-04-11T03:52:12.9300590Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9300758Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9300862Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9300935Z         """
2025-04-11T03:52:12.9301020Z         _lazy_init()
2025-04-11T03:52:12.9301118Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9301222Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9301330Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9301607Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9301750Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9301904Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9301909Z 
2025-04-11T03:52:12.9302151Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9302282Z _______________________________ test_mixtral[4] ________________________________
2025-04-11T03:52:12.9302286Z 
2025-04-11T03:52:12.9302403Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9303007Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9303067Z 
2025-04-11T03:52:12.9303173Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9303254Z         try_count = 0
2025-04-11T03:52:12.9303358Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9303438Z             max_try, int
2025-04-11T03:52:12.9303581Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9303655Z     
2025-04-11T03:52:12.9303829Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9303906Z             try:
2025-04-11T03:52:12.9303989Z                 try_count += 1
2025-04-11T03:52:12.9304078Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9304163Z                 return ret
2025-04-11T03:52:12.9304257Z             except exception_type as e:
2025-04-11T03:52:12.9304360Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9304546Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9304722Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9304870Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9305024Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9305110Z                     continue
2025-04-11T03:52:12.9305188Z                 else:
2025-04-11T03:52:12.9305413Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9305494Z >                   raise e
2025-04-11T03:52:12.9305499Z 
2025-04-11T03:52:12.9305595Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9305706Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9305839Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9305930Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9306116Z tests/test_shardformer/test_model/test_shard_mixtral.py:222: in test_mixtral
2025-04-11T03:52:12.9306213Z     spawn(check_mixtral, world_size)
2025-04-11T03:52:12.9306370Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9306476Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9306734Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9306912Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9307208Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9307298Z     while not context.join():
2025-04-11T03:52:12.9307412Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9307418Z 
2025-04-11T03:52:12.9307618Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ec76cb0>
2025-04-11T03:52:12.9307702Z timeout = None
2025-04-11T03:52:12.9307706Z 
2025-04-11T03:52:12.9307795Z     def join(self, timeout=None):
2025-04-11T03:52:12.9307922Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9308000Z     
2025-04-11T03:52:12.9308146Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9308295Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9308491Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9308591Z         of the first process exiting.
2025-04-11T03:52:12.9308661Z     
2025-04-11T03:52:12.9308809Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9308950Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9309086Z     
2025-04-11T03:52:12.9309167Z         Args:
2025-04-11T03:52:12.9309307Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9309384Z         """
2025-04-11T03:52:12.9309526Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9309618Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9309708Z             return True
2025-04-11T03:52:12.9309777Z     
2025-04-11T03:52:12.9309917Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9310034Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9310126Z             self.sentinels.keys(),
2025-04-11T03:52:12.9310289Z             timeout=timeout,
2025-04-11T03:52:12.9310362Z         )
2025-04-11T03:52:12.9310434Z     
2025-04-11T03:52:12.9310519Z         error_index = None
2025-04-11T03:52:12.9310605Z         for sentinel in ready:
2025-04-11T03:52:12.9310715Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9310816Z             process = self.processes[index]
2025-04-11T03:52:12.9310906Z             process.join()
2025-04-11T03:52:12.9311002Z             if process.exitcode != 0:
2025-04-11T03:52:12.9311155Z                 error_index = index
2025-04-11T03:52:12.9311234Z                 break
2025-04-11T03:52:12.9311305Z     
2025-04-11T03:52:12.9311404Z         # Return if there was no error.
2025-04-11T03:52:12.9311490Z         if error_index is None:
2025-04-11T03:52:12.9311629Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9311725Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9311798Z     
2025-04-11T03:52:12.9311945Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9312042Z         for process in self.processes:
2025-04-11T03:52:12.9312134Z             if process.is_alive():
2025-04-11T03:52:12.9312226Z                 process.terminate()
2025-04-11T03:52:12.9312311Z             process.join()
2025-04-11T03:52:12.9312387Z     
2025-04-11T03:52:12.9312528Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9312650Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9312757Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9312886Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9313035Z             if exitcode < 0:
2025-04-11T03:52:12.9313146Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9313259Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9313410Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9313511Z                     error_index=error_index,
2025-04-11T03:52:12.9313612Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9313702Z                     exit_code=exitcode,
2025-04-11T03:52:12.9313792Z                     signal_name=name,
2025-04-11T03:52:12.9313868Z                 )
2025-04-11T03:52:12.9313949Z             else:
2025-04-11T03:52:12.9314054Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9314225Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9314317Z                     error_index=error_index,
2025-04-11T03:52:12.9314420Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9314510Z                     exit_code=exitcode,
2025-04-11T03:52:12.9314581Z                 )
2025-04-11T03:52:12.9314656Z     
2025-04-11T03:52:12.9314788Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9314959Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9315049Z         msg += original_trace
2025-04-11T03:52:12.9315224Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9315388Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9315523Z E       
2025-04-11T03:52:12.9315656Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9315756Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9316056Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9316142Z E           fn(i, *args)
2025-04-11T03:52:12.9316438Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 210, in check_mixtral
2025-04-11T03:52:12.9316528Z E           run_mixtral_test()
2025-04-11T03:52:12.9316784Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9316942Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9317243Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 180, in run_mixtral_test
2025-04-11T03:52:12.9317339Z E           run_mixtral_commom(config)
2025-04-11T03:52:12.9317640Z E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 70, in run_mixtral_commom
2025-04-11T03:52:12.9317832Z E           torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T03:52:12.9318125Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9318227Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9318494Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9318614Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9318886Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9318974Z E           module._apply(fn)
2025-04-11T03:52:12.9319239Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9319339Z E           param_applied = fn(param)
2025-04-11T03:52:12.9319614Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9319735Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9319917Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9320203Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9320340Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9320503Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9320512Z 
2025-04-11T03:52:12.9320813Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9320972Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9321135Z [04/11/25 03:49:48] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9321267Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9321383Z                              :75 launch                                         
2025-04-11T03:52:12.9321522Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9321653Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9321845Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9321992Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9322288Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25164 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9322626Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25164 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9323734Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.9324807Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.9325946Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.9327066Z Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
2025-04-11T03:52:12.9327204Z ________________________________ test_OPTModel _________________________________
2025-04-11T03:52:12.9327210Z 
2025-04-11T03:52:12.9327307Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9327312Z 
2025-04-11T03:52:12.9327412Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9327553Z         try_count = 0
2025-04-11T03:52:12.9327653Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9327741Z             max_try, int
2025-04-11T03:52:12.9327889Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9327959Z     
2025-04-11T03:52:12.9328078Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9328152Z             try:
2025-04-11T03:52:12.9328241Z                 try_count += 1
2025-04-11T03:52:12.9328329Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9328334Z 
2025-04-11T03:52:12.9328429Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9328547Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9328661Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9328761Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9328914Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9329015Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9329122Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9329126Z 
2025-04-11T03:52:12.9329202Z device = None
2025-04-11T03:52:12.9329206Z 
2025-04-11T03:52:12.9329329Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9329483Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9329558Z     
2025-04-11T03:52:12.9329632Z         Args:
2025-04-11T03:52:12.9329806Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9330035Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9330141Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9330221Z         """
2025-04-11T03:52:12.9330301Z         _lazy_init()
2025-04-11T03:52:12.9330401Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9330505Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9330611Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9330901Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9331107Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9331270Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9331275Z 
2025-04-11T03:52:12.9331519Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9331656Z __________________________________ test_qwen2 __________________________________
2025-04-11T03:52:12.9331661Z 
2025-04-11T03:52:12.9331752Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9331818Z 
2025-04-11T03:52:12.9331925Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9332004Z         try_count = 0
2025-04-11T03:52:12.9332107Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9332190Z             max_try, int
2025-04-11T03:52:12.9332333Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9332408Z     
2025-04-11T03:52:12.9332517Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9332597Z             try:
2025-04-11T03:52:12.9332679Z                 try_count += 1
2025-04-11T03:52:12.9332768Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9332772Z 
2025-04-11T03:52:12.9332872Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9332981Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9333101Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9333196Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9333349Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9333450Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9333615Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9333619Z 
2025-04-11T03:52:12.9333701Z device = None
2025-04-11T03:52:12.9333706Z 
2025-04-11T03:52:12.9333824Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9333982Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9334055Z     
2025-04-11T03:52:12.9334130Z         Args:
2025-04-11T03:52:12.9334301Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9334466Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9334578Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9334650Z         """
2025-04-11T03:52:12.9334734Z         _lazy_init()
2025-04-11T03:52:12.9334829Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9334931Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9335042Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9335387Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9335530Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9335689Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9335694Z 
2025-04-11T03:52:12.9335942Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9336070Z ___________________________________ test_sam ___________________________________
2025-04-11T03:52:12.9336217Z 
2025-04-11T03:52:12.9336317Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9336323Z 
2025-04-11T03:52:12.9336421Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9336503Z         try_count = 0
2025-04-11T03:52:12.9336607Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9336690Z             max_try, int
2025-04-11T03:52:12.9336839Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9336911Z     
2025-04-11T03:52:12.9337021Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9337161Z             try:
2025-04-11T03:52:12.9337245Z                 try_count += 1
2025-04-11T03:52:12.9337339Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9337343Z 
2025-04-11T03:52:12.9337440Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9337554Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9337669Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9337760Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9337973Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9338069Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9338181Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9338188Z 
2025-04-11T03:52:12.9338266Z device = None
2025-04-11T03:52:12.9338271Z 
2025-04-11T03:52:12.9338390Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9338539Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9338612Z     
2025-04-11T03:52:12.9338690Z         Args:
2025-04-11T03:52:12.9338855Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9339022Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9339131Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9339207Z         """
2025-04-11T03:52:12.9339287Z         _lazy_init()
2025-04-11T03:52:12.9339384Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9339488Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9339595Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9339935Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9340073Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9340239Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9340243Z 
2025-04-11T03:52:12.9340480Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9340607Z ___________________________________ test_t5 ____________________________________
2025-04-11T03:52:12.9340616Z 
2025-04-11T03:52:12.9340707Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9340711Z 
2025-04-11T03:52:12.9340814Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9340897Z         try_count = 0
2025-04-11T03:52:12.9340997Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9341080Z             max_try, int
2025-04-11T03:52:12.9341221Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9341291Z     
2025-04-11T03:52:12.9341406Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9341482Z             try:
2025-04-11T03:52:12.9341571Z                 try_count += 1
2025-04-11T03:52:12.9341662Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9341667Z 
2025-04-11T03:52:12.9341763Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9341870Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9342041Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9342136Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9342292Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9342391Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9342497Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9342503Z 
2025-04-11T03:52:12.9342585Z device = None
2025-04-11T03:52:12.9342589Z 
2025-04-11T03:52:12.9342705Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9342855Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9342993Z     
2025-04-11T03:52:12.9343067Z         Args:
2025-04-11T03:52:12.9343237Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9343404Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9343515Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9343590Z         """
2025-04-11T03:52:12.9343667Z         _lazy_init()
2025-04-11T03:52:12.9343850Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9343952Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9344060Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9344341Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9344480Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9344636Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9344642Z 
2025-04-11T03:52:12.9344877Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9345007Z ___________________________________ test_vit ___________________________________
2025-04-11T03:52:12.9345013Z 
2025-04-11T03:52:12.9345104Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9345108Z 
2025-04-11T03:52:12.9345211Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9345293Z         try_count = 0
2025-04-11T03:52:12.9345394Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9345474Z             max_try, int
2025-04-11T03:52:12.9345671Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9345748Z     
2025-04-11T03:52:12.9345856Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9345937Z             try:
2025-04-11T03:52:12.9346020Z                 try_count += 1
2025-04-11T03:52:12.9346117Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9346122Z 
2025-04-11T03:52:12.9346217Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9346323Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9346440Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9346533Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9346691Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9346786Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9346896Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9346903Z 
2025-04-11T03:52:12.9346978Z device = None
2025-04-11T03:52:12.9346983Z 
2025-04-11T03:52:12.9347101Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9347253Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9347323Z     
2025-04-11T03:52:12.9347400Z         Args:
2025-04-11T03:52:12.9347566Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9347733Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9347840Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9347970Z         """
2025-04-11T03:52:12.9348054Z         _lazy_init()
2025-04-11T03:52:12.9348148Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9348254Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9348358Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9348687Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9348833Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9348989Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9349061Z 
2025-04-11T03:52:12.9349309Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9349445Z _________________________________ test_whisper _________________________________
2025-04-11T03:52:12.9349449Z 
2025-04-11T03:52:12.9349549Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9349556Z 
2025-04-11T03:52:12.9349659Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9349755Z         try_count = 0
2025-04-11T03:52:12.9349918Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9350005Z             max_try, int
2025-04-11T03:52:12.9350156Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9350232Z     
2025-04-11T03:52:12.9350351Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9350429Z             try:
2025-04-11T03:52:12.9350519Z                 try_count += 1
2025-04-11T03:52:12.9350613Z >               ret = func(*args, **kwargs)
2025-04-11T03:52:12.9350619Z 
2025-04-11T03:52:12.9350713Z colossalai/testing/utils.py:133: 
2025-04-11T03:52:12.9350831Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9350944Z colossalai/testing/utils.py:271: in _clear_cache
2025-04-11T03:52:12.9351045Z     get_accelerator().synchronize()
2025-04-11T03:52:12.9351210Z colossalai/accelerator/cuda_accelerator.py:62: in synchronize
2025-04-11T03:52:12.9351307Z     torch.cuda.synchronize(device)
2025-04-11T03:52:12.9351426Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9351430Z 
2025-04-11T03:52:12.9351506Z device = None
2025-04-11T03:52:12.9351570Z 
2025-04-11T03:52:12.9351699Z     def synchronize(device: _device_t = None) -> None:
2025-04-11T03:52:12.9351848Z         r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T03:52:12.9351924Z     
2025-04-11T03:52:12.9351999Z         Args:
2025-04-11T03:52:12.9352169Z             device (torch.device or int, optional): device for which to synchronize.
2025-04-11T03:52:12.9352337Z                 It uses the current device, given by :func:`~torch.cuda.current_device`,
2025-04-11T03:52:12.9352443Z                 if :attr:`device` is ``None`` (default).
2025-04-11T03:52:12.9352525Z         """
2025-04-11T03:52:12.9352606Z         _lazy_init()
2025-04-11T03:52:12.9352710Z         with torch.cuda.device(device):
2025-04-11T03:52:12.9352815Z >           return torch._C._cuda_synchronize()
2025-04-11T03:52:12.9352922Z E           RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9353207Z E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9353345Z E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9353506Z E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9353511Z 
2025-04-11T03:52:12.9353749Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
2025-04-11T03:52:12.9353884Z ________________________________ test_comm_spec ________________________________
2025-04-11T03:52:12.9353888Z 
2025-04-11T03:52:12.9353980Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9354662Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9354670Z 
2025-04-11T03:52:12.9354771Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9354857Z         try_count = 0
2025-04-11T03:52:12.9354962Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9355044Z             max_try, int
2025-04-11T03:52:12.9355194Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9355265Z     
2025-04-11T03:52:12.9355444Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9355518Z             try:
2025-04-11T03:52:12.9355600Z                 try_count += 1
2025-04-11T03:52:12.9355695Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9355776Z                 return ret
2025-04-11T03:52:12.9355877Z             except exception_type as e:
2025-04-11T03:52:12.9355980Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9356173Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9356350Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9356501Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9356658Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9356740Z                     continue
2025-04-11T03:52:12.9356821Z                 else:
2025-04-11T03:52:12.9357048Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9357133Z >                   raise e
2025-04-11T03:52:12.9357138Z 
2025-04-11T03:52:12.9357230Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9357345Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9357481Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9357569Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9357730Z tests/test_tensor/test_comm_spec_apply.py:211: in test_comm_spec
2025-04-11T03:52:12.9357824Z     spawn(check_comm, world_size)
2025-04-11T03:52:12.9357988Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9358091Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9358351Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9358533Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9358823Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9358919Z     while not context.join():
2025-04-11T03:52:12.9359027Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9359032Z 
2025-04-11T03:52:12.9359241Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfaef0>
2025-04-11T03:52:12.9359325Z timeout = None
2025-04-11T03:52:12.9359329Z 
2025-04-11T03:52:12.9359417Z     def join(self, timeout=None):
2025-04-11T03:52:12.9359547Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9359621Z     
2025-04-11T03:52:12.9359773Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9359916Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9360081Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9360177Z         of the first process exiting.
2025-04-11T03:52:12.9360253Z     
2025-04-11T03:52:12.9360407Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9360548Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9360681Z     
2025-04-11T03:52:12.9360756Z         Args:
2025-04-11T03:52:12.9360896Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9360973Z         """
2025-04-11T03:52:12.9361112Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9361210Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9361296Z             return True
2025-04-11T03:52:12.9361371Z     
2025-04-11T03:52:12.9361505Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9361625Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9361721Z             self.sentinels.keys(),
2025-04-11T03:52:12.9361875Z             timeout=timeout,
2025-04-11T03:52:12.9361952Z         )
2025-04-11T03:52:12.9362023Z     
2025-04-11T03:52:12.9362106Z         error_index = None
2025-04-11T03:52:12.9362201Z         for sentinel in ready:
2025-04-11T03:52:12.9362308Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9362415Z             process = self.processes[index]
2025-04-11T03:52:12.9362500Z             process.join()
2025-04-11T03:52:12.9362594Z             if process.exitcode != 0:
2025-04-11T03:52:12.9362745Z                 error_index = index
2025-04-11T03:52:12.9362822Z                 break
2025-04-11T03:52:12.9362896Z     
2025-04-11T03:52:12.9362992Z         # Return if there was no error.
2025-04-11T03:52:12.9363081Z         if error_index is None:
2025-04-11T03:52:12.9363218Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9363314Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9363388Z     
2025-04-11T03:52:12.9363528Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9363627Z         for process in self.processes:
2025-04-11T03:52:12.9363714Z             if process.is_alive():
2025-04-11T03:52:12.9363803Z                 process.terminate()
2025-04-11T03:52:12.9363893Z             process.join()
2025-04-11T03:52:12.9363964Z     
2025-04-11T03:52:12.9364108Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9364225Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9364337Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9364458Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9364598Z             if exitcode < 0:
2025-04-11T03:52:12.9364709Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9364814Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9364970Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9365067Z                     error_index=error_index,
2025-04-11T03:52:12.9365170Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9365262Z                     exit_code=exitcode,
2025-04-11T03:52:12.9365351Z                     signal_name=name,
2025-04-11T03:52:12.9365432Z                 )
2025-04-11T03:52:12.9365508Z             else:
2025-04-11T03:52:12.9365616Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9365783Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9365878Z                     error_index=error_index,
2025-04-11T03:52:12.9365985Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9366071Z                     exit_code=exitcode,
2025-04-11T03:52:12.9366149Z                 )
2025-04-11T03:52:12.9366221Z     
2025-04-11T03:52:12.9366355Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9366532Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9366620Z         msg += original_trace
2025-04-11T03:52:12.9366796Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9366959Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9367096Z E       
2025-04-11T03:52:12.9367222Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9367323Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9367635Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9367719Z E           fn(i, *args)
2025-04-11T03:52:12.9367979Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 191, in check_comm
2025-04-11T03:52:12.9368083Z E           check_all_gather(device_mesh, rank)
2025-04-11T03:52:12.9368347Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 16, in check_all_gather
2025-04-11T03:52:12.9368541Z E           sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T03:52:12.9368648Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9368942Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9369080Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9369299Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9369304Z 
2025-04-11T03:52:12.9369609Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9369770Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9369925Z [04/11/25 03:49:54] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9370061Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9370167Z                              :75 launch                                         
2025-04-11T03:52:12.9370306Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9370441Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9370636Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9370784Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9371074Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56178 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9371415Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56178 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9371551Z ______________________________ test_padded_tensor ______________________________
2025-04-11T03:52:12.9371557Z 
2025-04-11T03:52:12.9371655Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9372246Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9372254Z 
2025-04-11T03:52:12.9372358Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9372440Z         try_count = 0
2025-04-11T03:52:12.9372542Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9372632Z             max_try, int
2025-04-11T03:52:12.9372782Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9372857Z     
2025-04-11T03:52:12.9372967Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9373043Z             try:
2025-04-11T03:52:12.9373132Z                 try_count += 1
2025-04-11T03:52:12.9373224Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9373318Z                 return ret
2025-04-11T03:52:12.9373412Z             except exception_type as e:
2025-04-11T03:52:12.9373511Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9373700Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9373873Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9374031Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9374184Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9374275Z                     continue
2025-04-11T03:52:12.9374352Z                 else:
2025-04-11T03:52:12.9374574Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9374720Z >                   raise e
2025-04-11T03:52:12.9374725Z 
2025-04-11T03:52:12.9374822Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9374940Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9375073Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9375165Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9375328Z tests/test_tensor/test_padded_tensor.py:42: in test_padded_tensor
2025-04-11T03:52:12.9375434Z     spawn(check_padded_tensor, world_size)
2025-04-11T03:52:12.9375596Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9375696Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9375961Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9376140Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9376428Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9376522Z     while not context.join():
2025-04-11T03:52:12.9376629Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9376638Z 
2025-04-11T03:52:12.9376839Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be337ee0>
2025-04-11T03:52:12.9376924Z timeout = None
2025-04-11T03:52:12.9376928Z 
2025-04-11T03:52:12.9377025Z     def join(self, timeout=None):
2025-04-11T03:52:12.9377153Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9377230Z     
2025-04-11T03:52:12.9377377Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9377588Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9377749Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9377846Z         of the first process exiting.
2025-04-11T03:52:12.9377923Z     
2025-04-11T03:52:12.9378073Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9378222Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9378294Z     
2025-04-11T03:52:12.9378370Z         Args:
2025-04-11T03:52:12.9378515Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9378594Z         """
2025-04-11T03:52:12.9378742Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9378837Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9378923Z             return True
2025-04-11T03:52:12.9378996Z     
2025-04-11T03:52:12.9379129Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9384681Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9384828Z             self.sentinels.keys(),
2025-04-11T03:52:12.9384931Z             timeout=timeout,
2025-04-11T03:52:12.9385012Z         )
2025-04-11T03:52:12.9385108Z     
2025-04-11T03:52:12.9385225Z         error_index = None
2025-04-11T03:52:12.9385348Z         for sentinel in ready:
2025-04-11T03:52:12.9385471Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9385583Z             process = self.processes[index]
2025-04-11T03:52:12.9385676Z             process.join()
2025-04-11T03:52:12.9385858Z             if process.exitcode != 0:
2025-04-11T03:52:12.9385953Z                 error_index = index
2025-04-11T03:52:12.9386038Z                 break
2025-04-11T03:52:12.9386115Z     
2025-04-11T03:52:12.9386214Z         # Return if there was no error.
2025-04-11T03:52:12.9386301Z         if error_index is None:
2025-04-11T03:52:12.9386449Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9386555Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9386629Z     
2025-04-11T03:52:12.9386780Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9386883Z         for process in self.processes:
2025-04-11T03:52:12.9387046Z             if process.is_alive():
2025-04-11T03:52:12.9387140Z                 process.terminate()
2025-04-11T03:52:12.9387227Z             process.join()
2025-04-11T03:52:12.9387306Z     
2025-04-11T03:52:12.9387455Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9387585Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9387696Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9387880Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9387975Z             if exitcode < 0:
2025-04-11T03:52:12.9388087Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9388207Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9388374Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9388523Z                     error_index=error_index,
2025-04-11T03:52:12.9388636Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9388732Z                     exit_code=exitcode,
2025-04-11T03:52:12.9388833Z                     signal_name=name,
2025-04-11T03:52:12.9388911Z                 )
2025-04-11T03:52:12.9388993Z             else:
2025-04-11T03:52:12.9389105Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9389280Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9389382Z                     error_index=error_index,
2025-04-11T03:52:12.9389491Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9389582Z                     exit_code=exitcode,
2025-04-11T03:52:12.9389729Z                 )
2025-04-11T03:52:12.9389806Z     
2025-04-11T03:52:12.9389941Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9390115Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9390209Z         msg += original_trace
2025-04-11T03:52:12.9390392Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9390561Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9390640Z E       
2025-04-11T03:52:12.9390775Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9390878Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9391195Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9391286Z E           fn(i, *args)
2025-04-11T03:52:12.9391554Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_padded_tensor.py", line 14, in check_padded_tensor
2025-04-11T03:52:12.9391683Z E           original_tensor = torch.rand(32, 64).to("cuda")
2025-04-11T03:52:12.9391791Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9392086Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9392229Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9392396Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9392402Z 
2025-04-11T03:52:12.9392730Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9392959Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9393129Z [04/11/25 03:49:59] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9393262Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9393385Z                              :75 launch                                         
2025-04-11T03:52:12.9393527Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9393657Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9393927Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9394063Z __________________________________ test_apply __________________________________
2025-04-11T03:52:12.9394068Z 
2025-04-11T03:52:12.9394169Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9394779Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9394848Z 
2025-04-11T03:52:12.9394964Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9395048Z         try_count = 0
2025-04-11T03:52:12.9395156Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9395241Z             max_try, int
2025-04-11T03:52:12.9395392Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9395472Z     
2025-04-11T03:52:12.9395587Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9395672Z             try:
2025-04-11T03:52:12.9395759Z                 try_count += 1
2025-04-11T03:52:12.9395856Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9395942Z                 return ret
2025-04-11T03:52:12.9396039Z             except exception_type as e:
2025-04-11T03:52:12.9396146Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9396339Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9396462Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9396668Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9396829Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9396912Z                     continue
2025-04-11T03:52:12.9396992Z                 else:
2025-04-11T03:52:12.9397219Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9397304Z >                   raise e
2025-04-11T03:52:12.9397308Z 
2025-04-11T03:52:12.9397412Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9397531Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9397668Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9397759Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9418511Z tests/test_tensor/test_shape_consistency_apply.py:72: in test_apply
2025-04-11T03:52:12.9418961Z     spawn(check_apply, world_size)
2025-04-11T03:52:12.9419227Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9419497Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9419925Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9420444Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9420987Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9421439Z     while not context.join():
2025-04-11T03:52:12.9421775Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9421959Z 
2025-04-11T03:52:12.9422168Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cf8c70>
2025-04-11T03:52:12.9422521Z timeout = None
2025-04-11T03:52:12.9422637Z 
2025-04-11T03:52:12.9422733Z     def join(self, timeout=None):
2025-04-11T03:52:12.9423020Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9423284Z     
2025-04-11T03:52:12.9423528Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9423880Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9424312Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9424634Z         of the first process exiting.
2025-04-11T03:52:12.9424861Z     
2025-04-11T03:52:12.9425105Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9425459Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9425731Z     
2025-04-11T03:52:12.9425893Z         Args:
2025-04-11T03:52:12.9426202Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9426477Z         """
2025-04-11T03:52:12.9426716Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9427009Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9427247Z             return True
2025-04-11T03:52:12.9427445Z     
2025-04-11T03:52:12.9427667Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9427980Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9428257Z             self.sentinels.keys(),
2025-04-11T03:52:12.9428536Z             timeout=timeout,
2025-04-11T03:52:12.9428746Z         )
2025-04-11T03:52:12.9428923Z     
2025-04-11T03:52:12.9429103Z         error_index = None
2025-04-11T03:52:12.9449864Z         for sentinel in ready:
2025-04-11T03:52:12.9450128Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9450402Z             process = self.processes[index]
2025-04-11T03:52:12.9450657Z             process.join()
2025-04-11T03:52:12.9450937Z             if process.exitcode != 0:
2025-04-11T03:52:12.9451183Z                 error_index = index
2025-04-11T03:52:12.9451408Z                 break
2025-04-11T03:52:12.9451672Z     
2025-04-11T03:52:12.9451858Z         # Return if there was no error.
2025-04-11T03:52:12.9452094Z         if error_index is None:
2025-04-11T03:52:12.9452373Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9452662Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9452895Z     
2025-04-11T03:52:12.9453131Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9453426Z         for process in self.processes:
2025-04-11T03:52:12.9453671Z             if process.is_alive():
2025-04-11T03:52:12.9453905Z                 process.terminate()
2025-04-11T03:52:12.9454141Z             process.join()
2025-04-11T03:52:12.9454341Z     
2025-04-11T03:52:12.9454568Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9454888Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9455174Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9455473Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9455742Z             if exitcode < 0:
2025-04-11T03:52:12.9455982Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9456259Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9456578Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9456887Z                     error_index=error_index,
2025-04-11T03:52:12.9457144Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9457396Z                     exit_code=exitcode,
2025-04-11T03:52:12.9457699Z                     signal_name=name,
2025-04-11T03:52:12.9457920Z                 )
2025-04-11T03:52:12.9458101Z             else:
2025-04-11T03:52:12.9458312Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9458631Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9458954Z                     error_index=error_index,
2025-04-11T03:52:12.9459209Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9459463Z                     exit_code=exitcode,
2025-04-11T03:52:12.9459685Z                 )
2025-04-11T03:52:12.9459859Z     
2025-04-11T03:52:12.9460088Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9460531Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9460856Z         msg += original_trace
2025-04-11T03:52:12.9461169Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9461574Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9461863Z E       
2025-04-11T03:52:12.9462094Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9462462Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9462927Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9463372Z E           fn(i, *args)
2025-04-11T03:52:12.9463771Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_shape_consistency_apply.py", line 41, in check_apply
2025-04-11T03:52:12.9464281Z E           tensor_to_comm = torch.cat((sharded_tensor_0, sharded_tensor_1), 1).cuda()
2025-04-11T03:52:12.9464631Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9465089Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9465578Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9465941Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9466167Z 
2025-04-11T03:52:12.9466488Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9467021Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9467458Z [04/11/25 03:50:04] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9467810Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9468112Z                              :75 launch                                         
2025-04-11T03:52:12.9468459Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9468782Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9469169Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9469576Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9470090Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:61198 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9470584Z ________________________________ test_comm_spec ________________________________
2025-04-11T03:52:12.9470782Z 
2025-04-11T03:52:12.9470883Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9471634Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9472299Z 
2025-04-11T03:52:12.9472402Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9472649Z         try_count = 0
2025-04-11T03:52:12.9472874Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9473186Z             max_try, int
2025-04-11T03:52:12.9473461Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9473744Z     
2025-04-11T03:52:12.9473951Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9474204Z             try:
2025-04-11T03:52:12.9474406Z                 try_count += 1
2025-04-11T03:52:12.9474637Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9474871Z                 return ret
2025-04-11T03:52:12.9475096Z             except exception_type as e:
2025-04-11T03:52:12.9475357Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9475794Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9476163Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9476491Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9476855Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9477153Z                     continue
2025-04-11T03:52:12.9477432Z                 else:
2025-04-11T03:52:12.9477778Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9478147Z >                   raise e
2025-04-11T03:52:12.9478281Z 
2025-04-11T03:52:12.9478379Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9478651Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9478968Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9479260Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9479579Z tests/test_tensor/test_dtensor/test_comm_spec.py:157: in test_comm_spec
2025-04-11T03:52:12.9479905Z     spawn(check_comm, world_size)
2025-04-11T03:52:12.9480156Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9480426Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9480856Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9481364Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9481897Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9482403Z     while not context.join():
2025-04-11T03:52:12.9482662Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9482844Z 
2025-04-11T03:52:12.9483048Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cf8dc0>
2025-04-11T03:52:12.9483404Z timeout = None
2025-04-11T03:52:12.9483525Z 
2025-04-11T03:52:12.9483626Z     def join(self, timeout=None):
2025-04-11T03:52:12.9483914Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9484173Z     
2025-04-11T03:52:12.9484428Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9484788Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9485171Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9485498Z         of the first process exiting.
2025-04-11T03:52:12.9485727Z     
2025-04-11T03:52:12.9485975Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9486328Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9486602Z     
2025-04-11T03:52:12.9486778Z         Args:
2025-04-11T03:52:12.9487030Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9487315Z         """
2025-04-11T03:52:12.9487567Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9487865Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9488111Z             return True
2025-04-11T03:52:12.9488363Z     
2025-04-11T03:52:12.9488587Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9488897Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9489180Z             self.sentinels.keys(),
2025-04-11T03:52:12.9489418Z             timeout=timeout,
2025-04-11T03:52:12.9489627Z         )
2025-04-11T03:52:12.9489803Z     
2025-04-11T03:52:12.9489984Z         error_index = None
2025-04-11T03:52:12.9490214Z         for sentinel in ready:
2025-04-11T03:52:12.9490460Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9490727Z             process = self.processes[index]
2025-04-11T03:52:12.9491037Z             process.join()
2025-04-11T03:52:12.9491266Z             if process.exitcode != 0:
2025-04-11T03:52:12.9491503Z                 error_index = index
2025-04-11T03:52:12.9491728Z                 break
2025-04-11T03:52:12.9491916Z     
2025-04-11T03:52:12.9492100Z         # Return if there was no error.
2025-04-11T03:52:12.9492345Z         if error_index is None:
2025-04-11T03:52:12.9492621Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9492912Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9493200Z     
2025-04-11T03:52:12.9493443Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9493749Z         for process in self.processes:
2025-04-11T03:52:12.9494000Z             if process.is_alive():
2025-04-11T03:52:12.9494242Z                 process.terminate()
2025-04-11T03:52:12.9494475Z             process.join()
2025-04-11T03:52:12.9494682Z     
2025-04-11T03:52:12.9494920Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9495245Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9495536Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9495833Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9496106Z             if exitcode < 0:
2025-04-11T03:52:12.9496355Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9496636Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9496960Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9497278Z                     error_index=error_index,
2025-04-11T03:52:12.9497596Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9497853Z                     exit_code=exitcode,
2025-04-11T03:52:12.9498090Z                     signal_name=name,
2025-04-11T03:52:12.9498310Z                 )
2025-04-11T03:52:12.9498491Z             else:
2025-04-11T03:52:12.9498702Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9499029Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9499351Z                     error_index=error_index,
2025-04-11T03:52:12.9499605Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9499861Z                     exit_code=exitcode,
2025-04-11T03:52:12.9500079Z                 )
2025-04-11T03:52:12.9500256Z     
2025-04-11T03:52:12.9500477Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9500841Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9501159Z         msg += original_trace
2025-04-11T03:52:12.9501460Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9501855Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9502145Z E       
2025-04-11T03:52:12.9502370Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9502656Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9503114Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9503548Z E           fn(i, *args)
2025-04-11T03:52:12.9504003Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 140, in check_comm
2025-04-11T03:52:12.9504444Z E           check_all_gather(process_group_dict, rank)
2025-04-11T03:52:12.9504904Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 15, in check_all_gather
2025-04-11T03:52:12.9505365Z E           sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T03:52:12.9505653Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9506109Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9506659Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9507020Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9507246Z 
2025-04-11T03:52:12.9507558Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9508084Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9508498Z [04/11/25 03:50:09] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9508911Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9509216Z                              :75 launch                                         
2025-04-11T03:52:12.9509527Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9509853Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9510245Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9510644Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9511151Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60535 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9511785Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60535 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9512265Z _________________________________ test_dtensor _________________________________
2025-04-11T03:52:12.9512463Z 
2025-04-11T03:52:12.9512628Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9513382Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9514053Z 
2025-04-11T03:52:12.9514160Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9514405Z         try_count = 0
2025-04-11T03:52:12.9514632Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9514880Z             max_try, int
2025-04-11T03:52:12.9515154Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9515442Z     
2025-04-11T03:52:12.9515650Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9515906Z             try:
2025-04-11T03:52:12.9516104Z                 try_count += 1
2025-04-11T03:52:12.9516334Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9516574Z                 return ret
2025-04-11T03:52:12.9516797Z             except exception_type as e:
2025-04-11T03:52:12.9517056Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9517411Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9517783Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9518115Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9518483Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9518847Z                     continue
2025-04-11T03:52:12.9519057Z                 else:
2025-04-11T03:52:12.9519392Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9519753Z >                   raise e
2025-04-11T03:52:12.9519883Z 
2025-04-11T03:52:12.9519982Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9520245Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9520556Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9520834Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9521139Z tests/test_tensor/test_dtensor/test_dtensor.py:83: in test_dtensor
2025-04-11T03:52:12.9521533Z     spawn(check_dtensor, world_size)
2025-04-11T03:52:12.9521789Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9522046Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9522475Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9522975Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9523563Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9524024Z     while not context.join():
2025-04-11T03:52:12.9524273Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9524450Z 
2025-04-11T03:52:12.9524650Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ecd9240>
2025-04-11T03:52:12.9525009Z timeout = None
2025-04-11T03:52:12.9525124Z 
2025-04-11T03:52:12.9525217Z     def join(self, timeout=None):
2025-04-11T03:52:12.9525492Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9525750Z     
2025-04-11T03:52:12.9525987Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9526340Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9526709Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9527036Z         of the first process exiting.
2025-04-11T03:52:12.9527259Z     
2025-04-11T03:52:12.9527501Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9527909Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9528183Z     
2025-04-11T03:52:12.9528350Z         Args:
2025-04-11T03:52:12.9528597Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9528868Z         """
2025-04-11T03:52:12.9529115Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9529404Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9529639Z             return True
2025-04-11T03:52:12.9529830Z     
2025-04-11T03:52:12.9530051Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9530362Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9530641Z             self.sentinels.keys(),
2025-04-11T03:52:12.9530878Z             timeout=timeout,
2025-04-11T03:52:12.9531088Z         )
2025-04-11T03:52:12.9531254Z     
2025-04-11T03:52:12.9531431Z         error_index = None
2025-04-11T03:52:12.9531650Z         for sentinel in ready:
2025-04-11T03:52:12.9531905Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9532173Z             process = self.processes[index]
2025-04-11T03:52:12.9532414Z             process.join()
2025-04-11T03:52:12.9532642Z             if process.exitcode != 0:
2025-04-11T03:52:12.9532882Z                 error_index = index
2025-04-11T03:52:12.9533107Z                 break
2025-04-11T03:52:12.9533297Z     
2025-04-11T03:52:12.9533481Z         # Return if there was no error.
2025-04-11T03:52:12.9533722Z         if error_index is None:
2025-04-11T03:52:12.9533997Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9534347Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9534574Z     
2025-04-11T03:52:12.9534802Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9535100Z         for process in self.processes:
2025-04-11T03:52:12.9535346Z             if process.is_alive():
2025-04-11T03:52:12.9535587Z                 process.terminate()
2025-04-11T03:52:12.9535816Z             process.join()
2025-04-11T03:52:12.9536013Z     
2025-04-11T03:52:12.9536245Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9536629Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9536985Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9537278Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9537542Z             if exitcode < 0:
2025-04-11T03:52:12.9537783Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9538058Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9538374Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9538744Z                     error_index=error_index,
2025-04-11T03:52:12.9538999Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9539252Z                     exit_code=exitcode,
2025-04-11T03:52:12.9539491Z                     signal_name=name,
2025-04-11T03:52:12.9539712Z                 )
2025-04-11T03:52:12.9539893Z             else:
2025-04-11T03:52:12.9540099Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9540423Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9540747Z                     error_index=error_index,
2025-04-11T03:52:12.9541005Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9541253Z                     exit_code=exitcode,
2025-04-11T03:52:12.9541471Z                 )
2025-04-11T03:52:12.9541647Z     
2025-04-11T03:52:12.9541872Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9542237Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9542561Z         msg += original_trace
2025-04-11T03:52:12.9542868Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9543401Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9543695Z E       
2025-04-11T03:52:12.9543920Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9544204Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9544665Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9545101Z E           fn(i, *args)
2025-04-11T03:52:12.9545496Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py", line 25, in check_dtensor
2025-04-11T03:52:12.9545937Z E           test_model = TestModel(8, 8).to("cuda")
2025-04-11T03:52:12.9546376Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:12.9546797Z E           return self._apply(convert)
2025-04-11T03:52:12.9547220Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9547644Z E           module._apply(fn)
2025-04-11T03:52:12.9548044Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9548499Z E           param_applied = fn(param)
2025-04-11T03:52:12.9548936Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:12.9549488Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.9549869Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9550406Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9550891Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9551252Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9551476Z 
2025-04-11T03:52:12.9551787Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9552309Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9552766Z [04/11/25 03:50:13] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9553108Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9553412Z                              :75 launch                                         
2025-04-11T03:52:12.9553723Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9554050Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9554500Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9554891Z ____________________________ test_layout_converter _____________________________
2025-04-11T03:52:12.9555093Z 
2025-04-11T03:52:12.9555187Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9555929Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9556578Z 
2025-04-11T03:52:12.9556680Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9556924Z         try_count = 0
2025-04-11T03:52:12.9557145Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9557390Z             max_try, int
2025-04-11T03:52:12.9557659Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9557938Z     
2025-04-11T03:52:12.9558147Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9558397Z             try:
2025-04-11T03:52:12.9558590Z                 try_count += 1
2025-04-11T03:52:12.9558879Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9559117Z                 return ret
2025-04-11T03:52:12.9559337Z             except exception_type as e:
2025-04-11T03:52:12.9559596Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9559946Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9560305Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9560631Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9560994Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9561294Z                     continue
2025-04-11T03:52:12.9561505Z                 else:
2025-04-11T03:52:12.9561834Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9562195Z >                   raise e
2025-04-11T03:52:12.9562331Z 
2025-04-11T03:52:12.9562427Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9562692Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9563000Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9563280Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9563616Z tests/test_tensor/test_dtensor/test_layout_converter.py:180: in test_layout_converter
2025-04-11T03:52:12.9564001Z     spawn(check_layout_converting_apply, world_size)
2025-04-11T03:52:12.9564285Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9564607Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9565036Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9565536Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9566058Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9566523Z     while not context.join():
2025-04-11T03:52:12.9566772Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9566950Z 
2025-04-11T03:52:12.9567154Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfaf80>
2025-04-11T03:52:12.9567571Z timeout = None
2025-04-11T03:52:12.9567685Z 
2025-04-11T03:52:12.9567778Z     def join(self, timeout=None):
2025-04-11T03:52:12.9568052Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9568311Z     
2025-04-11T03:52:12.9568556Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9568900Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9569335Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9569655Z         of the first process exiting.
2025-04-11T03:52:12.9569883Z     
2025-04-11T03:52:12.9570122Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9570465Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9570728Z     
2025-04-11T03:52:12.9570891Z         Args:
2025-04-11T03:52:12.9571139Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9571416Z         """
2025-04-11T03:52:12.9571655Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9571946Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9572181Z             return True
2025-04-11T03:52:12.9572375Z     
2025-04-11T03:52:12.9572597Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9572905Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9573181Z             self.sentinels.keys(),
2025-04-11T03:52:12.9573415Z             timeout=timeout,
2025-04-11T03:52:12.9573625Z         )
2025-04-11T03:52:12.9573858Z     
2025-04-11T03:52:12.9574035Z         error_index = None
2025-04-11T03:52:12.9574257Z         for sentinel in ready:
2025-04-11T03:52:12.9574505Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9574776Z             process = self.processes[index]
2025-04-11T03:52:12.9575027Z             process.join()
2025-04-11T03:52:12.9575256Z             if process.exitcode != 0:
2025-04-11T03:52:12.9575499Z                 error_index = index
2025-04-11T03:52:12.9575723Z                 break
2025-04-11T03:52:12.9575909Z     
2025-04-11T03:52:12.9576093Z         # Return if there was no error.
2025-04-11T03:52:12.9576334Z         if error_index is None:
2025-04-11T03:52:12.9576601Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9576890Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9577119Z     
2025-04-11T03:52:12.9577352Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9577649Z         for process in self.processes:
2025-04-11T03:52:12.9577892Z             if process.is_alive():
2025-04-11T03:52:12.9578129Z                 process.terminate()
2025-04-11T03:52:12.9578365Z             process.join()
2025-04-11T03:52:12.9578563Z     
2025-04-11T03:52:12.9578798Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9579113Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9579395Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9579688Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9579954Z             if exitcode < 0:
2025-04-11T03:52:12.9580277Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9580557Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9580878Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9581193Z                     error_index=error_index,
2025-04-11T03:52:12.9581463Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9581724Z                     exit_code=exitcode,
2025-04-11T03:52:12.9581968Z                     signal_name=name,
2025-04-11T03:52:12.9582194Z                 )
2025-04-11T03:52:12.9582384Z             else:
2025-04-11T03:52:12.9582657Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9582983Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9583301Z                     error_index=error_index,
2025-04-11T03:52:12.9583550Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9583804Z                     exit_code=exitcode,
2025-04-11T03:52:12.9584024Z                 )
2025-04-11T03:52:12.9584203Z     
2025-04-11T03:52:12.9584428Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9584847Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9585172Z         msg += original_trace
2025-04-11T03:52:12.9585478Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9585869Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9586161Z E       
2025-04-11T03:52:12.9586382Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9586671Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9587134Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9587576Z E           fn(i, *args)
2025-04-11T03:52:12.9588044Z E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_layout_converter.py", line 162, in check_layout_converting_apply
2025-04-11T03:52:12.9588608Z E           original_tensor = torch.rand(global_shape).cuda()
2025-04-11T03:52:12.9588901Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9589354Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9589903Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9590268Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9590494Z 
2025-04-11T03:52:12.9590805Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9591328Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9591694Z [04/11/25 03:50:18] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9592039Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9592342Z                              :75 launch                                         
2025-04-11T03:52:12.9592650Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9592981Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9593312Z [04/11/25 03:50:22] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9593641Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9593937Z                              :75 launch                                         
2025-04-11T03:52:12.9594240Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9594363Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9594578Z [04/11/25 03:50:26] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9594703Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9594811Z                              :75 launch                                         
2025-04-11T03:52:12.9594943Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9595071Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9595268Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9595486Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9595785Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57837 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9596066Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57837 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9596209Z ____________________________ test_chunk_manager[2] _____________________________
2025-04-11T03:52:12.9596272Z 
2025-04-11T03:52:12.9596392Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T03:52:12.9596994Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9597002Z 
2025-04-11T03:52:12.9597107Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9597194Z         try_count = 0
2025-04-11T03:52:12.9597298Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9597381Z             max_try, int
2025-04-11T03:52:12.9597526Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9597596Z     
2025-04-11T03:52:12.9597713Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9597787Z             try:
2025-04-11T03:52:12.9597874Z                 try_count += 1
2025-04-11T03:52:12.9597967Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9598053Z                 return ret
2025-04-11T03:52:12.9598148Z             except exception_type as e:
2025-04-11T03:52:12.9598306Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9598497Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9598615Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9598764Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9598920Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9599002Z                     continue
2025-04-11T03:52:12.9599082Z                 else:
2025-04-11T03:52:12.9599299Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9599383Z >                   raise e
2025-04-11T03:52:12.9599389Z 
2025-04-11T03:52:12.9599486Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9599600Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9599732Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9599818Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9599996Z tests/test_zero/test_gemini/test_chunk_mgrv2.py:60: in test_chunk_manager
2025-04-11T03:52:12.9600089Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9600197Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9600297Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9600560Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9600736Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9601079Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9601176Z     while not context.join():
2025-04-11T03:52:12.9601286Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9601292Z 
2025-04-11T03:52:12.9601493Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ece1210>
2025-04-11T03:52:12.9601572Z timeout = None
2025-04-11T03:52:12.9601577Z 
2025-04-11T03:52:12.9601669Z     def join(self, timeout=None):
2025-04-11T03:52:12.9601792Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9601931Z     
2025-04-11T03:52:12.9602080Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9602225Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9602395Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9602490Z         of the first process exiting.
2025-04-11T03:52:12.9602565Z     
2025-04-11T03:52:12.9602714Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9602913Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9602988Z     
2025-04-11T03:52:12.9603067Z         Args:
2025-04-11T03:52:12.9603209Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9603286Z         """
2025-04-11T03:52:12.9603429Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9603526Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9603613Z             return True
2025-04-11T03:52:12.9603693Z     
2025-04-11T03:52:12.9603826Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9603954Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9604051Z             self.sentinels.keys(),
2025-04-11T03:52:12.9604141Z             timeout=timeout,
2025-04-11T03:52:12.9604220Z         )
2025-04-11T03:52:12.9604294Z     
2025-04-11T03:52:12.9604385Z         error_index = None
2025-04-11T03:52:12.9604475Z         for sentinel in ready:
2025-04-11T03:52:12.9604584Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9604690Z             process = self.processes[index]
2025-04-11T03:52:12.9604832Z             process.join()
2025-04-11T03:52:12.9604932Z             if process.exitcode != 0:
2025-04-11T03:52:12.9605019Z                 error_index = index
2025-04-11T03:52:12.9605100Z                 break
2025-04-11T03:52:12.9605171Z     
2025-04-11T03:52:12.9605264Z         # Return if there was no error.
2025-04-11T03:52:12.9605352Z         if error_index is None:
2025-04-11T03:52:12.9605488Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9605587Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9605659Z     
2025-04-11T03:52:12.9605804Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9605905Z         for process in self.processes:
2025-04-11T03:52:12.9605995Z             if process.is_alive():
2025-04-11T03:52:12.9606090Z                 process.terminate()
2025-04-11T03:52:12.9606175Z             process.join()
2025-04-11T03:52:12.9606252Z     
2025-04-11T03:52:12.9606391Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9606506Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9606615Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9606737Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9606825Z             if exitcode < 0:
2025-04-11T03:52:12.9606932Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9607040Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9607195Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9607372Z                     error_index=error_index,
2025-04-11T03:52:12.9607478Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9607568Z                     exit_code=exitcode,
2025-04-11T03:52:12.9607657Z                     signal_name=name,
2025-04-11T03:52:12.9607731Z                 )
2025-04-11T03:52:12.9607810Z             else:
2025-04-11T03:52:12.9607919Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9608082Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9608180Z                     error_index=error_index,
2025-04-11T03:52:12.9608344Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9608429Z                     exit_code=exitcode,
2025-04-11T03:52:12.9608506Z                 )
2025-04-11T03:52:12.9608575Z     
2025-04-11T03:52:12.9608710Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9608881Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9608970Z         msg += original_trace
2025-04-11T03:52:12.9609204Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9609369Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9609448Z E       
2025-04-11T03:52:12.9609575Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9609676Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9609973Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9610061Z E           fn(i, *args)
2025-04-11T03:52:12.9610318Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 53, in run_dist
2025-04-11T03:52:12.9610405Z E           exam_chunk_memory()
2025-04-11T03:52:12.9610667Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9610759Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9611013Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9611103Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9611377Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 21, in exam_chunk_memory
2025-04-11T03:52:12.9611552Z E           chunk_manager = ChunkManager(config)
2025-04-11T03:52:12.9611794Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T03:52:12.9612050Z E           self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.9612154Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9612440Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9612579Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9612747Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9612752Z 
2025-04-11T03:52:12.9613055Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9613215Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9613368Z [04/11/25 03:50:30] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9613494Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9613606Z                              :75 launch                                         
2025-04-11T03:52:12.9613742Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9613871Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9614132Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9614283Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9615178Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T03:52:12.9615349Z   return tensor.storage().size() == 0
2025-04-11T03:52:12.9615484Z ____________________________ test_chunk_function[1] ____________________________
2025-04-11T03:52:12.9615489Z 
2025-04-11T03:52:12.9615606Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T03:52:12.9616200Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9616265Z 
2025-04-11T03:52:12.9616369Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9616453Z         try_count = 0
2025-04-11T03:52:12.9616554Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9616639Z             max_try, int
2025-04-11T03:52:12.9616785Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9616859Z     
2025-04-11T03:52:12.9616970Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9617049Z             try:
2025-04-11T03:52:12.9617139Z                 try_count += 1
2025-04-11T03:52:12.9617228Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9617311Z                 return ret
2025-04-11T03:52:12.9617406Z             except exception_type as e:
2025-04-11T03:52:12.9617505Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9617694Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9617811Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9617957Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9618163Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9618251Z                     continue
2025-04-11T03:52:12.9618326Z                 else:
2025-04-11T03:52:12.9618542Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9618627Z >                   raise e
2025-04-11T03:52:12.9618632Z 
2025-04-11T03:52:12.9618724Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9618839Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9618972Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9619061Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9619233Z tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
2025-04-11T03:52:12.9619322Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9619427Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9619528Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9619790Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9619968Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9620257Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9620348Z     while not context.join():
2025-04-11T03:52:12.9620455Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9620521Z 
2025-04-11T03:52:12.9620719Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cf9840>
2025-04-11T03:52:12.9620797Z timeout = None
2025-04-11T03:52:12.9620803Z 
2025-04-11T03:52:12.9620900Z     def join(self, timeout=None):
2025-04-11T03:52:12.9621026Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9621104Z     
2025-04-11T03:52:12.9621249Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9621392Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9621555Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9621709Z         of the first process exiting.
2025-04-11T03:52:12.9621784Z     
2025-04-11T03:52:12.9621930Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9622071Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9622142Z     
2025-04-11T03:52:12.9622218Z         Args:
2025-04-11T03:52:12.9622359Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9622432Z         """
2025-04-11T03:52:12.9622629Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9622724Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9622807Z             return True
2025-04-11T03:52:12.9622882Z     
2025-04-11T03:52:12.9623014Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9623136Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9623228Z             self.sentinels.keys(),
2025-04-11T03:52:12.9623319Z             timeout=timeout,
2025-04-11T03:52:12.9623391Z         )
2025-04-11T03:52:12.9623461Z     
2025-04-11T03:52:12.9623549Z         error_index = None
2025-04-11T03:52:12.9623635Z         for sentinel in ready:
2025-04-11T03:52:12.9623744Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9623841Z             process = self.processes[index]
2025-04-11T03:52:12.9623927Z             process.join()
2025-04-11T03:52:12.9624025Z             if process.exitcode != 0:
2025-04-11T03:52:12.9624114Z                 error_index = index
2025-04-11T03:52:12.9624192Z                 break
2025-04-11T03:52:12.9624263Z     
2025-04-11T03:52:12.9624354Z         # Return if there was no error.
2025-04-11T03:52:12.9624499Z         if error_index is None:
2025-04-11T03:52:12.9624634Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9624737Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9624808Z     
2025-04-11T03:52:12.9624951Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9625051Z         for process in self.processes:
2025-04-11T03:52:12.9625137Z             if process.is_alive():
2025-04-11T03:52:12.9625235Z                 process.terminate()
2025-04-11T03:52:12.9625318Z             process.join()
2025-04-11T03:52:12.9625394Z     
2025-04-11T03:52:12.9625535Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9625653Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9625769Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9625887Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9625976Z             if exitcode < 0:
2025-04-11T03:52:12.9626083Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9626193Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9626340Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9626441Z                     error_index=error_index,
2025-04-11T03:52:12.9626550Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9626637Z                     exit_code=exitcode,
2025-04-11T03:52:12.9626726Z                     signal_name=name,
2025-04-11T03:52:12.9626799Z                 )
2025-04-11T03:52:12.9626935Z             else:
2025-04-11T03:52:12.9627045Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9627210Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9627313Z                     error_index=error_index,
2025-04-11T03:52:12.9627416Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9627510Z                     exit_code=exitcode,
2025-04-11T03:52:12.9627587Z                 )
2025-04-11T03:52:12.9627660Z     
2025-04-11T03:52:12.9627800Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9627972Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9628125Z         msg += original_trace
2025-04-11T03:52:12.9628299Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9628503Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9628583Z E       
2025-04-11T03:52:12.9628712Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9628815Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9629171Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9629257Z E           fn(i, *args)
2025-04-11T03:52:12.9629504Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T03:52:12.9629590Z E           exam_chunk_basic()
2025-04-11T03:52:12.9629847Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9629937Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9630192Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9630277Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9630529Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9630615Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9630721Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.9630982Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T03:52:12.9631132Z E           my_chunk = Chunk(
2025-04-11T03:52:12.9631369Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T03:52:12.9631569Z E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T03:52:12.9631686Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9631966Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9632106Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9632269Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9632274Z 
2025-04-11T03:52:12.9632576Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9632731Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9632887Z [04/11/25 03:50:34] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9633019Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9633126Z                              :75 launch                                         
2025-04-11T03:52:12.9633267Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9633391Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.9633586Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9633787Z ____________________________ test_chunk_function[2] ____________________________
2025-04-11T03:52:12.9633793Z 
2025-04-11T03:52:12.9633910Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T03:52:12.9634506Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9634514Z 
2025-04-11T03:52:12.9634615Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9634700Z         try_count = 0
2025-04-11T03:52:12.9634869Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9634955Z             max_try, int
2025-04-11T03:52:12.9635103Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9635173Z     
2025-04-11T03:52:12.9635287Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9635366Z             try:
2025-04-11T03:52:12.9635454Z                 try_count += 1
2025-04-11T03:52:12.9635545Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9635698Z                 return ret
2025-04-11T03:52:12.9635794Z             except exception_type as e:
2025-04-11T03:52:12.9635892Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9636084Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9636199Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9636347Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9636503Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9636587Z                     continue
2025-04-11T03:52:12.9636665Z                 else:
2025-04-11T03:52:12.9636889Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9637012Z >                   raise e
2025-04-11T03:52:12.9637019Z 
2025-04-11T03:52:12.9637118Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9637232Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9637426Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9637514Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9637682Z tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
2025-04-11T03:52:12.9637771Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9637875Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9637977Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9638238Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9638414Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9638701Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9638794Z     while not context.join():
2025-04-11T03:52:12.9638905Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9638910Z 
2025-04-11T03:52:12.9639114Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ecbfe80>
2025-04-11T03:52:12.9639194Z timeout = None
2025-04-11T03:52:12.9639199Z 
2025-04-11T03:52:12.9639293Z     def join(self, timeout=None):
2025-04-11T03:52:12.9639420Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9639498Z     
2025-04-11T03:52:12.9639644Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9639786Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9639947Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9640099Z         of the first process exiting.
2025-04-11T03:52:12.9640172Z     
2025-04-11T03:52:12.9640318Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9640455Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9640528Z     
2025-04-11T03:52:12.9640603Z         Args:
2025-04-11T03:52:12.9640747Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9640820Z         """
2025-04-11T03:52:12.9640961Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9641054Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9641201Z             return True
2025-04-11T03:52:12.9641276Z     
2025-04-11T03:52:12.9641409Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9641529Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9641620Z             self.sentinels.keys(),
2025-04-11T03:52:12.9641705Z             timeout=timeout,
2025-04-11T03:52:12.9641783Z         )
2025-04-11T03:52:12.9641853Z     
2025-04-11T03:52:12.9641940Z         error_index = None
2025-04-11T03:52:12.9642081Z         for sentinel in ready:
2025-04-11T03:52:12.9642189Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9642290Z             process = self.processes[index]
2025-04-11T03:52:12.9642375Z             process.join()
2025-04-11T03:52:12.9642473Z             if process.exitcode != 0:
2025-04-11T03:52:12.9642560Z                 error_index = index
2025-04-11T03:52:12.9642641Z                 break
2025-04-11T03:52:12.9642711Z     
2025-04-11T03:52:12.9642802Z         # Return if there was no error.
2025-04-11T03:52:12.9642894Z         if error_index is None:
2025-04-11T03:52:12.9643025Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9643121Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9643191Z     
2025-04-11T03:52:12.9643330Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9643431Z         for process in self.processes:
2025-04-11T03:52:12.9643518Z             if process.is_alive():
2025-04-11T03:52:12.9643615Z                 process.terminate()
2025-04-11T03:52:12.9643698Z             process.join()
2025-04-11T03:52:12.9643771Z     
2025-04-11T03:52:12.9643910Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9644079Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9644192Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9644313Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9644405Z             if exitcode < 0:
2025-04-11T03:52:12.9644512Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9644617Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9644768Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9644865Z                     error_index=error_index,
2025-04-11T03:52:12.9644970Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9645059Z                     exit_code=exitcode,
2025-04-11T03:52:12.9645149Z                     signal_name=name,
2025-04-11T03:52:12.9645222Z                 )
2025-04-11T03:52:12.9645295Z             else:
2025-04-11T03:52:12.9645403Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9645567Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9645663Z                     error_index=error_index,
2025-04-11T03:52:12.9645764Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9645855Z                     exit_code=exitcode,
2025-04-11T03:52:12.9645928Z                 )
2025-04-11T03:52:12.9646001Z     
2025-04-11T03:52:12.9646137Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9646304Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9646451Z         msg += original_trace
2025-04-11T03:52:12.9646626Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9646792Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9646871Z E       
2025-04-11T03:52:12.9647002Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9647103Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9647399Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9647484Z E           fn(i, *args)
2025-04-11T03:52:12.9647795Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T03:52:12.9647880Z E           exam_chunk_basic()
2025-04-11T03:52:12.9648144Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9648236Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9648491Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9648636Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9648894Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9648982Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9649086Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.9649344Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T03:52:12.9649434Z E           my_chunk = Chunk(
2025-04-11T03:52:12.9649674Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T03:52:12.9649876Z E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T03:52:12.9649989Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9650295Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9650433Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9650657Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9650662Z 
2025-04-11T03:52:12.9650966Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9651121Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9651277Z [04/11/25 03:50:38] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9651408Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9651513Z                              :75 launch                                         
2025-04-11T03:52:12.9651653Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9651776Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9651971Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9652112Z ____________________________ test_chunk_function[4] ____________________________
2025-04-11T03:52:12.9652116Z 
2025-04-11T03:52:12.9652230Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9652832Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9652840Z 
2025-04-11T03:52:12.9652941Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9653026Z         try_count = 0
2025-04-11T03:52:12.9653185Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9653269Z             max_try, int
2025-04-11T03:52:12.9653416Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9653489Z     
2025-04-11T03:52:12.9653605Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9653685Z             try:
2025-04-11T03:52:12.9653770Z                 try_count += 1
2025-04-11T03:52:12.9653862Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9653943Z                 return ret
2025-04-11T03:52:12.9654043Z             except exception_type as e:
2025-04-11T03:52:12.9654205Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9654391Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9654506Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9654651Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9654806Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9654943Z                     continue
2025-04-11T03:52:12.9655024Z                 else:
2025-04-11T03:52:12.9655241Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9655325Z >                   raise e
2025-04-11T03:52:12.9655330Z 
2025-04-11T03:52:12.9655423Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9655537Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9655671Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9655759Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9655931Z tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
2025-04-11T03:52:12.9656020Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9656122Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9656224Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9656481Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9656658Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9656940Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9657092Z     while not context.join():
2025-04-11T03:52:12.9657201Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9657205Z 
2025-04-11T03:52:12.9657407Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be4edb40>
2025-04-11T03:52:12.9657487Z timeout = None
2025-04-11T03:52:12.9657492Z 
2025-04-11T03:52:12.9657583Z     def join(self, timeout=None):
2025-04-11T03:52:12.9657707Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9657779Z     
2025-04-11T03:52:12.9657928Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9658071Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9658236Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9658330Z         of the first process exiting.
2025-04-11T03:52:12.9658404Z     
2025-04-11T03:52:12.9658547Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9658681Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9658754Z     
2025-04-11T03:52:12.9658827Z         Args:
2025-04-11T03:52:12.9658969Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9659041Z         """
2025-04-11T03:52:12.9659178Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9659274Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9659412Z             return True
2025-04-11T03:52:12.9659489Z     
2025-04-11T03:52:12.9659619Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9659744Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9659836Z             self.sentinels.keys(),
2025-04-11T03:52:12.9659920Z             timeout=timeout,
2025-04-11T03:52:12.9659997Z         )
2025-04-11T03:52:12.9660066Z     
2025-04-11T03:52:12.9660153Z         error_index = None
2025-04-11T03:52:12.9660236Z         for sentinel in ready:
2025-04-11T03:52:12.9660341Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9660441Z             process = self.processes[index]
2025-04-11T03:52:12.9660604Z             process.join()
2025-04-11T03:52:12.9660702Z             if process.exitcode != 0:
2025-04-11T03:52:12.9660790Z                 error_index = index
2025-04-11T03:52:12.9660870Z                 break
2025-04-11T03:52:12.9660940Z     
2025-04-11T03:52:12.9661032Z         # Return if there was no error.
2025-04-11T03:52:12.9661122Z         if error_index is None:
2025-04-11T03:52:12.9661254Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9661410Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9661480Z     
2025-04-11T03:52:12.9661618Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9661721Z         for process in self.processes:
2025-04-11T03:52:12.9661808Z             if process.is_alive():
2025-04-11T03:52:12.9661902Z                 process.terminate()
2025-04-11T03:52:12.9661984Z             process.join()
2025-04-11T03:52:12.9662053Z     
2025-04-11T03:52:12.9662200Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9662317Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9662429Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9662549Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9662636Z             if exitcode < 0:
2025-04-11T03:52:12.9662742Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9662846Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9663003Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9663101Z                     error_index=error_index,
2025-04-11T03:52:12.9663263Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9663351Z                     exit_code=exitcode,
2025-04-11T03:52:12.9663441Z                     signal_name=name,
2025-04-11T03:52:12.9663514Z                 )
2025-04-11T03:52:12.9663588Z             else:
2025-04-11T03:52:12.9663700Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9663864Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9663960Z                     error_index=error_index,
2025-04-11T03:52:12.9664058Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9664146Z                     exit_code=exitcode,
2025-04-11T03:52:12.9664221Z                 )
2025-04-11T03:52:12.9664294Z     
2025-04-11T03:52:12.9664426Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9664592Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9664684Z         msg += original_trace
2025-04-11T03:52:12.9664854Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9665017Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9665094Z E       
2025-04-11T03:52:12.9665221Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9665325Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9665621Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9665703Z E           fn(i, *args)
2025-04-11T03:52:12.9666010Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T03:52:12.9666097Z E           exam_chunk_basic()
2025-04-11T03:52:12.9666357Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9666449Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9666699Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9666786Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9667030Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9667184Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9667292Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.9667553Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T03:52:12.9667640Z E           my_chunk = Chunk(
2025-04-11T03:52:12.9667882Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T03:52:12.9668143Z E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T03:52:12.9668260Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9668597Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9668735Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9668901Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9668906Z 
2025-04-11T03:52:12.9669211Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9669365Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9669519Z [04/11/25 03:50:43] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9669650Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9669756Z                              :75 launch                                         
2025-04-11T03:52:12.9669984Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9670107Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9670305Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9670453Z ____________________________ test_grad_accumulation ____________________________
2025-04-11T03:52:12.9670457Z 
2025-04-11T03:52:12.9670550Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9671139Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9671149Z 
2025-04-11T03:52:12.9671251Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9671333Z         try_count = 0
2025-04-11T03:52:12.9671432Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9671518Z             max_try, int
2025-04-11T03:52:12.9671665Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9671736Z     
2025-04-11T03:52:12.9671850Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9671927Z             try:
2025-04-11T03:52:12.9672013Z                 try_count += 1
2025-04-11T03:52:12.9672109Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9672190Z                 return ret
2025-04-11T03:52:12.9672287Z             except exception_type as e:
2025-04-11T03:52:12.9672386Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9672638Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9672756Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9672908Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9673064Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9673148Z                     continue
2025-04-11T03:52:12.9673229Z                 else:
2025-04-11T03:52:12.9673447Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9673598Z >                   raise e
2025-04-11T03:52:12.9673603Z 
2025-04-11T03:52:12.9673698Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9673813Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9673946Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9674036Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9674221Z tests/test_zero/test_gemini/test_grad_accum.py:158: in test_grad_accumulation
2025-04-11T03:52:12.9674448Z     spawn(run_dist, 2)
2025-04-11T03:52:12.9674552Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9674654Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9674918Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9675101Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9675389Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9675487Z     while not context.join():
2025-04-11T03:52:12.9675595Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9675599Z 
2025-04-11T03:52:12.9675801Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ec751b0>
2025-04-11T03:52:12.9675880Z timeout = None
2025-04-11T03:52:12.9675885Z 
2025-04-11T03:52:12.9675978Z     def join(self, timeout=None):
2025-04-11T03:52:12.9676104Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9676175Z     
2025-04-11T03:52:12.9676326Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9676529Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9676695Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9676791Z         of the first process exiting.
2025-04-11T03:52:12.9676867Z     
2025-04-11T03:52:12.9677016Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9677154Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9677230Z     
2025-04-11T03:52:12.9677304Z         Args:
2025-04-11T03:52:12.9677446Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9677517Z         """
2025-04-11T03:52:12.9677659Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9677757Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9677836Z             return True
2025-04-11T03:52:12.9677910Z     
2025-04-11T03:52:12.9678038Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9678160Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9678251Z             self.sentinels.keys(),
2025-04-11T03:52:12.9678334Z             timeout=timeout,
2025-04-11T03:52:12.9678412Z         )
2025-04-11T03:52:12.9678483Z     
2025-04-11T03:52:12.9678570Z         error_index = None
2025-04-11T03:52:12.9678654Z         for sentinel in ready:
2025-04-11T03:52:12.9678761Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9678864Z             process = self.processes[index]
2025-04-11T03:52:12.9679005Z             process.join()
2025-04-11T03:52:12.9679101Z             if process.exitcode != 0:
2025-04-11T03:52:12.9679188Z                 error_index = index
2025-04-11T03:52:12.9679267Z                 break
2025-04-11T03:52:12.9679340Z     
2025-04-11T03:52:12.9679431Z         # Return if there was no error.
2025-04-11T03:52:12.9679519Z         if error_index is None:
2025-04-11T03:52:12.9679655Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9679756Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9679825Z     
2025-04-11T03:52:12.9679965Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9680127Z         for process in self.processes:
2025-04-11T03:52:12.9680214Z             if process.is_alive():
2025-04-11T03:52:12.9680309Z                 process.terminate()
2025-04-11T03:52:12.9680391Z             process.join()
2025-04-11T03:52:12.9680463Z     
2025-04-11T03:52:12.9680606Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9680722Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9680832Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9681016Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9681103Z             if exitcode < 0:
2025-04-11T03:52:12.9681211Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9681315Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9681468Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9681563Z                     error_index=error_index,
2025-04-11T03:52:12.9681670Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9681758Z                     exit_code=exitcode,
2025-04-11T03:52:12.9681844Z                     signal_name=name,
2025-04-11T03:52:12.9681921Z                 )
2025-04-11T03:52:12.9681995Z             else:
2025-04-11T03:52:12.9682103Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9682266Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9682365Z                     error_index=error_index,
2025-04-11T03:52:12.9682465Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9682550Z                     exit_code=exitcode,
2025-04-11T03:52:12.9682688Z                 )
2025-04-11T03:52:12.9682761Z     
2025-04-11T03:52:12.9682899Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9683071Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9683162Z         msg += original_trace
2025-04-11T03:52:12.9683340Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9683507Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9683587Z E       
2025-04-11T03:52:12.9683717Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9683823Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9684132Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9684218Z E           fn(i, *args)
2025-04-11T03:52:12.9684482Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 152, in run_dist
2025-04-11T03:52:12.9684576Z E           exam_gemini_grad_acc()
2025-04-11T03:52:12.9684844Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9684939Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9685199Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9685288Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9685542Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9685701Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9685807Z E         [Previous line repeated 4 more times]
2025-04-11T03:52:12.9686090Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 71, in exam_gemini_grad_acc
2025-04-11T03:52:12.9686194Z E           torch_model = model_builder().cuda()
2025-04-11T03:52:12.9686483Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9686582Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9686919Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9687043Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9687310Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9687404Z E           module._apply(fn)
2025-04-11T03:52:12.9687670Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9687819Z E           module._apply(fn)
2025-04-11T03:52:12.9688093Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9688195Z E           param_applied = fn(param)
2025-04-11T03:52:12.9688473Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9688589Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9688701Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9688988Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9689131Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9689297Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9689302Z 
2025-04-11T03:52:12.9689615Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9689766Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9689979Z [04/11/25 03:50:49] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9690108Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9690215Z                              :75 launch                                         
2025-04-11T03:52:12.9690357Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9690480Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9690676Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9690821Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9691952Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9692126Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9693235Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9693466Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9694161Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9694248Z   warnings.warn(
2025-04-11T03:52:12.9694935Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9695082Z   warnings.warn(
2025-04-11T03:52:12.9695905Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9696048Z   warnings.warn(
2025-04-11T03:52:12.9696854Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9696940Z   warnings.warn(
2025-04-11T03:52:12.9697737Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9697821Z   warnings.warn(
2025-04-11T03:52:12.9698618Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9698760Z   warnings.warn(
2025-04-11T03:52:12.9699553Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9699638Z   warnings.warn(
2025-04-11T03:52:12.9700433Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9700520Z   warnings.warn(
2025-04-11T03:52:12.9701396Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T03:52:12.9701504Z   return tensor.storage().size() == 0
2025-04-11T03:52:12.9702451Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9702690Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9702828Z ______________________________ test_grad_clip[1] _______________________________
2025-04-11T03:52:12.9702835Z 
2025-04-11T03:52:12.9702959Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T03:52:12.9703551Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9703557Z 
2025-04-11T03:52:12.9703728Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9703810Z         try_count = 0
2025-04-11T03:52:12.9703913Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9704001Z             max_try, int
2025-04-11T03:52:12.9704150Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9704230Z     
2025-04-11T03:52:12.9704343Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9704422Z             try:
2025-04-11T03:52:12.9704567Z                 try_count += 1
2025-04-11T03:52:12.9704660Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9704747Z                 return ret
2025-04-11T03:52:12.9704846Z             except exception_type as e:
2025-04-11T03:52:12.9704948Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9705132Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9705248Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9705398Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9705548Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9705634Z                     continue
2025-04-11T03:52:12.9705711Z                 else:
2025-04-11T03:52:12.9705937Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9706018Z >                   raise e
2025-04-11T03:52:12.9706024Z 
2025-04-11T03:52:12.9706119Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9706230Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9706421Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9706515Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9706679Z tests/test_zero/test_gemini/test_grad_clip.py:134: in test_grad_clip
2025-04-11T03:52:12.9706781Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9706882Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9706981Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9707244Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9707423Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9707711Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9707801Z     while not context.join():
2025-04-11T03:52:12.9707913Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9707919Z 
2025-04-11T03:52:12.9708119Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be4ef6a0>
2025-04-11T03:52:12.9708202Z timeout = None
2025-04-11T03:52:12.9708207Z 
2025-04-11T03:52:12.9708297Z     def join(self, timeout=None):
2025-04-11T03:52:12.9708462Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9708541Z     
2025-04-11T03:52:12.9708687Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9708835Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9709070Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9709163Z         of the first process exiting.
2025-04-11T03:52:12.9709240Z     
2025-04-11T03:52:12.9709387Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9709526Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9709598Z     
2025-04-11T03:52:12.9709675Z         Args:
2025-04-11T03:52:12.9709811Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9709883Z         """
2025-04-11T03:52:12.9710027Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9710188Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9710276Z             return True
2025-04-11T03:52:12.9710347Z     
2025-04-11T03:52:12.9710480Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9710600Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9710695Z             self.sentinels.keys(),
2025-04-11T03:52:12.9710785Z             timeout=timeout,
2025-04-11T03:52:12.9710857Z         )
2025-04-11T03:52:12.9710996Z     
2025-04-11T03:52:12.9711080Z         error_index = None
2025-04-11T03:52:12.9711167Z         for sentinel in ready:
2025-04-11T03:52:12.9711277Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9711379Z             process = self.processes[index]
2025-04-11T03:52:12.9711468Z             process.join()
2025-04-11T03:52:12.9711562Z             if process.exitcode != 0:
2025-04-11T03:52:12.9711648Z                 error_index = index
2025-04-11T03:52:12.9711729Z                 break
2025-04-11T03:52:12.9711800Z     
2025-04-11T03:52:12.9711896Z         # Return if there was no error.
2025-04-11T03:52:12.9711981Z         if error_index is None:
2025-04-11T03:52:12.9712116Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9712217Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9712288Z     
2025-04-11T03:52:12.9712432Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9712528Z         for process in self.processes:
2025-04-11T03:52:12.9712622Z             if process.is_alive():
2025-04-11T03:52:12.9712715Z                 process.terminate()
2025-04-11T03:52:12.9712802Z             process.join()
2025-04-11T03:52:12.9712954Z     
2025-04-11T03:52:12.9713096Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9713215Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9713324Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9713446Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9713533Z             if exitcode < 0:
2025-04-11T03:52:12.9713640Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9713749Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9713899Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9713998Z                     error_index=error_index,
2025-04-11T03:52:12.9714102Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9714189Z                     exit_code=exitcode,
2025-04-11T03:52:12.9714280Z                     signal_name=name,
2025-04-11T03:52:12.9714355Z                 )
2025-04-11T03:52:12.9714433Z             else:
2025-04-11T03:52:12.9714534Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9714698Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9714797Z                     error_index=error_index,
2025-04-11T03:52:12.9714899Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9714988Z                     exit_code=exitcode,
2025-04-11T03:52:12.9715060Z                 )
2025-04-11T03:52:12.9715137Z     
2025-04-11T03:52:12.9715266Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9715495Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9715587Z         msg += original_trace
2025-04-11T03:52:12.9715765Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9715930Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9716006Z E       
2025-04-11T03:52:12.9716140Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9716239Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9716536Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9716689Z E           fn(i, *args)
2025-04-11T03:52:12.9716944Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T03:52:12.9717041Z E           exam_grad_clipping()
2025-04-11T03:52:12.9717305Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9717398Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9717705Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9717793Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9718044Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9718130Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9718239Z E         [Previous line repeated 2 more times]
2025-04-11T03:52:12.9718508Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T03:52:12.9718614Z E           torch_model = model_builder().cuda()
2025-04-11T03:52:12.9718900Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9719000Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9719266Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9719386Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9719655Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9719798Z E           module._apply(fn)
2025-04-11T03:52:12.9720069Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9720159Z E           module._apply(fn)
2025-04-11T03:52:12.9720424Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9720524Z E           param_applied = fn(param)
2025-04-11T03:52:12.9720799Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9720923Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9721031Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9721317Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9721457Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9721623Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9721628Z 
2025-04-11T03:52:12.9721928Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9722084Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9722244Z [04/11/25 03:50:56] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9722374Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9722546Z                              :75 launch                                         
2025-04-11T03:52:12.9722685Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9722810Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.9723005Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9723155Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9724268Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9724508Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9725231Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9725314Z   warnings.warn(
2025-04-11T03:52:12.9726127Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9726208Z   warnings.warn(
2025-04-11T03:52:12.9727004Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9727087Z   warnings.warn(
2025-04-11T03:52:12.9727883Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9728020Z   warnings.warn(
2025-04-11T03:52:12.9728158Z ______________________________ test_grad_clip[2] _______________________________
2025-04-11T03:52:12.9728164Z 
2025-04-11T03:52:12.9728285Z args = (), kwargs = {'world_size': 2}, try_count = 1
2025-04-11T03:52:12.9728882Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9728889Z 
2025-04-11T03:52:12.9728993Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9729077Z         try_count = 0
2025-04-11T03:52:12.9729177Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9729262Z             max_try, int
2025-04-11T03:52:12.9729411Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9729482Z     
2025-04-11T03:52:12.9729597Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9729671Z             try:
2025-04-11T03:52:12.9729761Z                 try_count += 1
2025-04-11T03:52:12.9729853Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9729932Z                 return ret
2025-04-11T03:52:12.9730029Z             except exception_type as e:
2025-04-11T03:52:12.9730128Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9730377Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9730496Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9730643Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9730798Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9730882Z                     continue
2025-04-11T03:52:12.9730964Z                 else:
2025-04-11T03:52:12.9731180Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9731329Z >                   raise e
2025-04-11T03:52:12.9731334Z 
2025-04-11T03:52:12.9731429Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9731540Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9731677Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9731766Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9731935Z tests/test_zero/test_gemini/test_grad_clip.py:134: in test_grad_clip
2025-04-11T03:52:12.9732083Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9732194Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9732294Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9732549Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9732728Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9733013Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9733107Z     while not context.join():
2025-04-11T03:52:12.9733218Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9733222Z 
2025-04-11T03:52:12.9733421Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ecb3490>
2025-04-11T03:52:12.9733501Z timeout = None
2025-04-11T03:52:12.9733505Z 
2025-04-11T03:52:12.9733598Z     def join(self, timeout=None):
2025-04-11T03:52:12.9733728Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9733799Z     
2025-04-11T03:52:12.9733948Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9734151Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9734319Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9734413Z         of the first process exiting.
2025-04-11T03:52:12.9734487Z     
2025-04-11T03:52:12.9734636Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9734771Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9734846Z     
2025-04-11T03:52:12.9734919Z         Args:
2025-04-11T03:52:12.9735057Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9735133Z         """
2025-04-11T03:52:12.9735276Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9735379Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9735459Z             return True
2025-04-11T03:52:12.9735533Z     
2025-04-11T03:52:12.9735667Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9735784Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9735878Z             self.sentinels.keys(),
2025-04-11T03:52:12.9735961Z             timeout=timeout,
2025-04-11T03:52:12.9736036Z         )
2025-04-11T03:52:12.9736108Z     
2025-04-11T03:52:12.9736190Z         error_index = None
2025-04-11T03:52:12.9736278Z         for sentinel in ready:
2025-04-11T03:52:12.9736382Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9736484Z             process = self.processes[index]
2025-04-11T03:52:12.9736569Z             process.join()
2025-04-11T03:52:12.9736722Z             if process.exitcode != 0:
2025-04-11T03:52:12.9736812Z                 error_index = index
2025-04-11T03:52:12.9736888Z                 break
2025-04-11T03:52:12.9736964Z     
2025-04-11T03:52:12.9737057Z         # Return if there was no error.
2025-04-11T03:52:12.9737144Z         if error_index is None:
2025-04-11T03:52:12.9737282Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9737378Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9737455Z     
2025-04-11T03:52:12.9737644Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9737747Z         for process in self.processes:
2025-04-11T03:52:12.9737904Z             if process.is_alive():
2025-04-11T03:52:12.9738002Z                 process.terminate()
2025-04-11T03:52:12.9738090Z             process.join()
2025-04-11T03:52:12.9738160Z     
2025-04-11T03:52:12.9738306Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9738425Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9738535Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9738739Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9738823Z             if exitcode < 0:
2025-04-11T03:52:12.9738935Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9739043Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9739197Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9739293Z                     error_index=error_index,
2025-04-11T03:52:12.9739400Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9739487Z                     exit_code=exitcode,
2025-04-11T03:52:12.9739578Z                     signal_name=name,
2025-04-11T03:52:12.9739659Z                 )
2025-04-11T03:52:12.9739733Z             else:
2025-04-11T03:52:12.9739839Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9740003Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9740097Z                     error_index=error_index,
2025-04-11T03:52:12.9740201Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9740285Z                     exit_code=exitcode,
2025-04-11T03:52:12.9740425Z                 )
2025-04-11T03:52:12.9740498Z     
2025-04-11T03:52:12.9740634Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9740802Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9740891Z         msg += original_trace
2025-04-11T03:52:12.9741066Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9741233Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9741314Z E       
2025-04-11T03:52:12.9741439Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9741539Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9741837Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9741920Z E           fn(i, *args)
2025-04-11T03:52:12.9742175Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T03:52:12.9742268Z E           exam_grad_clipping()
2025-04-11T03:52:12.9742527Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9742614Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9742867Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9742961Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9743204Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9743359Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9743466Z E         [Previous line repeated 2 more times]
2025-04-11T03:52:12.9743744Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T03:52:12.9743846Z E           torch_model = model_builder().cuda()
2025-04-11T03:52:12.9744135Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9744236Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9744497Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9744680Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9744950Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9745042Z E           module._apply(fn)
2025-04-11T03:52:12.9745307Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9745454Z E           module._apply(fn)
2025-04-11T03:52:12.9745721Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9745817Z E           param_applied = fn(param)
2025-04-11T03:52:12.9746092Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9746208Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9746323Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9746610Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9746750Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9746913Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9746918Z 
2025-04-11T03:52:12.9747225Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9747381Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9747596Z [04/11/25 03:51:03] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9747732Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9747839Z                              :75 launch                                         
2025-04-11T03:52:12.9747980Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9748104Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9748299Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9748485Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9749613Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9749791Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9750890Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9751135Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9751833Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9751918Z   warnings.warn(
2025-04-11T03:52:12.9752604Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9752761Z   warnings.warn(
2025-04-11T03:52:12.9753593Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9753743Z   warnings.warn(
2025-04-11T03:52:12.9754550Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9754636Z   warnings.warn(
2025-04-11T03:52:12.9755435Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9755519Z   warnings.warn(
2025-04-11T03:52:12.9756321Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9756471Z   warnings.warn(
2025-04-11T03:52:12.9757267Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9757351Z   warnings.warn(
2025-04-11T03:52:12.9758166Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9758250Z   warnings.warn(
2025-04-11T03:52:12.9758554Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26619 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9759502Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9759678Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9759820Z ______________________________ test_inference[1] _______________________________
2025-04-11T03:52:12.9759825Z 
2025-04-11T03:52:12.9759942Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T03:52:12.9760597Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9760607Z 
2025-04-11T03:52:12.9760713Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9760794Z         try_count = 0
2025-04-11T03:52:12.9760899Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9760983Z             max_try, int
2025-04-11T03:52:12.9761133Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9761267Z     
2025-04-11T03:52:12.9761383Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9761457Z             try:
2025-04-11T03:52:12.9761542Z                 try_count += 1
2025-04-11T03:52:12.9761638Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9761721Z                 return ret
2025-04-11T03:52:12.9761824Z             except exception_type as e:
2025-04-11T03:52:12.9761925Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9762166Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9762286Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9762433Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9762593Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9762676Z                     continue
2025-04-11T03:52:12.9762758Z                 else:
2025-04-11T03:52:12.9762978Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9763059Z >                   raise e
2025-04-11T03:52:12.9763067Z 
2025-04-11T03:52:12.9763162Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9763275Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9763412Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9763502Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9763669Z tests/test_zero/test_gemini/test_inference.py:118: in test_inference
2025-04-11T03:52:12.9763817Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9763920Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9764027Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9764286Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9764466Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9764749Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9764840Z     while not context.join():
2025-04-11T03:52:12.9764949Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9764955Z 
2025-04-11T03:52:12.9765151Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfa290>
2025-04-11T03:52:12.9765235Z timeout = None
2025-04-11T03:52:12.9765240Z 
2025-04-11T03:52:12.9765329Z     def join(self, timeout=None):
2025-04-11T03:52:12.9765461Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9765531Z     
2025-04-11T03:52:12.9765678Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9765821Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9765983Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9766080Z         of the first process exiting.
2025-04-11T03:52:12.9766150Z     
2025-04-11T03:52:12.9766299Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9766437Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9766567Z     
2025-04-11T03:52:12.9766643Z         Args:
2025-04-11T03:52:12.9766777Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9766859Z         """
2025-04-11T03:52:12.9766996Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9767093Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9767176Z             return True
2025-04-11T03:52:12.9767246Z     
2025-04-11T03:52:12.9767379Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9767496Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9767664Z             self.sentinels.keys(),
2025-04-11T03:52:12.9767749Z             timeout=timeout,
2025-04-11T03:52:12.9767823Z         )
2025-04-11T03:52:12.9767895Z     
2025-04-11T03:52:12.9767980Z         error_index = None
2025-04-11T03:52:12.9768069Z         for sentinel in ready:
2025-04-11T03:52:12.9768179Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9768282Z             process = self.processes[index]
2025-04-11T03:52:12.9768365Z             process.join()
2025-04-11T03:52:12.9768522Z             if process.exitcode != 0:
2025-04-11T03:52:12.9768616Z                 error_index = index
2025-04-11T03:52:12.9768692Z                 break
2025-04-11T03:52:12.9768768Z     
2025-04-11T03:52:12.9768859Z         # Return if there was no error.
2025-04-11T03:52:12.9768944Z         if error_index is None:
2025-04-11T03:52:12.9769083Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9769181Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9769258Z     
2025-04-11T03:52:12.9769399Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9769499Z         for process in self.processes:
2025-04-11T03:52:12.9769588Z             if process.is_alive():
2025-04-11T03:52:12.9769677Z                 process.terminate()
2025-04-11T03:52:12.9769768Z             process.join()
2025-04-11T03:52:12.9769838Z     
2025-04-11T03:52:12.9769983Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9770099Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9770206Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9770386Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9770470Z             if exitcode < 0:
2025-04-11T03:52:12.9770581Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9770687Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9770842Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9770937Z                     error_index=error_index,
2025-04-11T03:52:12.9771037Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9771130Z                     exit_code=exitcode,
2025-04-11T03:52:12.9771217Z                     signal_name=name,
2025-04-11T03:52:12.9771291Z                 )
2025-04-11T03:52:12.9771364Z             else:
2025-04-11T03:52:12.9771469Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9771640Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9771733Z                     error_index=error_index,
2025-04-11T03:52:12.9771838Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9771926Z                     exit_code=exitcode,
2025-04-11T03:52:12.9772000Z                 )
2025-04-11T03:52:12.9772071Z     
2025-04-11T03:52:12.9772199Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9772376Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9772461Z         msg += original_trace
2025-04-11T03:52:12.9772635Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9772854Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9772931Z E       
2025-04-11T03:52:12.9773059Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9773160Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9773463Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9773547Z E           fn(i, *args)
2025-04-11T03:52:12.9773802Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T03:52:12.9773888Z E           exam_inference()
2025-04-11T03:52:12.9774205Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9774300Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9774547Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9774641Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9774885Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9775037Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9775298Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T03:52:12.9775404Z E           torch_model = model_builder().cuda()
2025-04-11T03:52:12.9775689Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9775789Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9776057Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9776177Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9776444Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9776533Z E           module._apply(fn)
2025-04-11T03:52:12.9776796Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9776884Z E           module._apply(fn)
2025-04-11T03:52:12.9777146Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9777300Z E           param_applied = fn(param)
2025-04-11T03:52:12.9777570Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9777692Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9777801Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9778094Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9778236Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9778397Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9778406Z 
2025-04-11T03:52:12.9778711Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9778867Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9779028Z [04/11/25 03:51:10] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9779156Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9779268Z                              :75 launch                                         
2025-04-11T03:52:12.9779403Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9779530Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.9779784Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9779930Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9781063Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9781298Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9782002Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9782086Z   warnings.warn(
2025-04-11T03:52:12.9782907Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9783049Z   warnings.warn(
2025-04-11T03:52:12.9783851Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9783933Z   warnings.warn(
2025-04-11T03:52:12.9784739Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9784820Z   warnings.warn(
2025-04-11T03:52:12.9784958Z ______________________________ test_inference[4] _______________________________
2025-04-11T03:52:12.9785017Z 
2025-04-11T03:52:12.9785135Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9785726Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9785733Z 
2025-04-11T03:52:12.9785836Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9785918Z         try_count = 0
2025-04-11T03:52:12.9786019Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9786102Z             max_try, int
2025-04-11T03:52:12.9786255Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9786325Z     
2025-04-11T03:52:12.9786440Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9786517Z             try:
2025-04-11T03:52:12.9786603Z                 try_count += 1
2025-04-11T03:52:12.9786698Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9786779Z                 return ret
2025-04-11T03:52:12.9786878Z             except exception_type as e:
2025-04-11T03:52:12.9786978Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9787168Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9787288Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9787431Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9787586Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9787726Z                     continue
2025-04-11T03:52:12.9787806Z                 else:
2025-04-11T03:52:12.9788024Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9788109Z >                   raise e
2025-04-11T03:52:12.9788114Z 
2025-04-11T03:52:12.9788211Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9788320Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9788484Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9788573Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9788906Z tests/test_zero/test_gemini/test_inference.py:118: in test_inference
2025-04-11T03:52:12.9788996Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9789101Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9789200Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9789460Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9789639Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9789989Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9790082Z     while not context.join():
2025-04-11T03:52:12.9790193Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9790198Z 
2025-04-11T03:52:12.9790400Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f688ec01e70>
2025-04-11T03:52:12.9790478Z timeout = None
2025-04-11T03:52:12.9790484Z 
2025-04-11T03:52:12.9790575Z     def join(self, timeout=None):
2025-04-11T03:52:12.9790704Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9790777Z     
2025-04-11T03:52:12.9790928Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9791075Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9791242Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9791338Z         of the first process exiting.
2025-04-11T03:52:12.9791408Z     
2025-04-11T03:52:12.9791560Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9791758Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9791834Z     
2025-04-11T03:52:12.9791909Z         Args:
2025-04-11T03:52:12.9792050Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9792125Z         """
2025-04-11T03:52:12.9792265Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9792361Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9792443Z             return True
2025-04-11T03:52:12.9792518Z     
2025-04-11T03:52:12.9792650Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9792770Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9792865Z             self.sentinels.keys(),
2025-04-11T03:52:12.9792949Z             timeout=timeout,
2025-04-11T03:52:12.9793023Z         )
2025-04-11T03:52:12.9793095Z     
2025-04-11T03:52:12.9793178Z         error_index = None
2025-04-11T03:52:12.9793270Z         for sentinel in ready:
2025-04-11T03:52:12.9793376Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9793478Z             process = self.processes[index]
2025-04-11T03:52:12.9793563Z             process.join()
2025-04-11T03:52:12.9793659Z             if process.exitcode != 0:
2025-04-11T03:52:12.9793749Z                 error_index = index
2025-04-11T03:52:12.9793825Z                 break
2025-04-11T03:52:12.9793899Z     
2025-04-11T03:52:12.9793991Z         # Return if there was no error.
2025-04-11T03:52:12.9794079Z         if error_index is None:
2025-04-11T03:52:12.9794214Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9794383Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9794458Z     
2025-04-11T03:52:12.9794600Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9794699Z         for process in self.processes:
2025-04-11T03:52:12.9794788Z             if process.is_alive():
2025-04-11T03:52:12.9794881Z                 process.terminate()
2025-04-11T03:52:12.9794967Z             process.join()
2025-04-11T03:52:12.9795036Z     
2025-04-11T03:52:12.9795180Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9795296Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9795472Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9795597Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9795685Z             if exitcode < 0:
2025-04-11T03:52:12.9795797Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9795908Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9796060Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9796212Z                     error_index=error_index,
2025-04-11T03:52:12.9796316Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9796406Z                     exit_code=exitcode,
2025-04-11T03:52:12.9796494Z                     signal_name=name,
2025-04-11T03:52:12.9796570Z                 )
2025-04-11T03:52:12.9796645Z             else:
2025-04-11T03:52:12.9796750Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9796911Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9797006Z                     error_index=error_index,
2025-04-11T03:52:12.9797110Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9797195Z                     exit_code=exitcode,
2025-04-11T03:52:12.9797271Z                 )
2025-04-11T03:52:12.9797345Z     
2025-04-11T03:52:12.9797480Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9797650Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9797739Z         msg += original_trace
2025-04-11T03:52:12.9797913Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9798133Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9798209Z E       
2025-04-11T03:52:12.9798335Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9798435Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9798735Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9798835Z E           fn(i, *args)
2025-04-11T03:52:12.9799093Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T03:52:12.9799180Z E           exam_inference()
2025-04-11T03:52:12.9799437Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9799527Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9799775Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9799868Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9800110Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9800200Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9800456Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T03:52:12.9800560Z E           torch_model = model_builder().cuda()
2025-04-11T03:52:12.9800841Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9801002Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9801267Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9801388Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9801658Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9801744Z E           module._apply(fn)
2025-04-11T03:52:12.9802012Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9802158Z E           module._apply(fn)
2025-04-11T03:52:12.9802426Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9802521Z E           param_applied = fn(param)
2025-04-11T03:52:12.9802792Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9802915Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9803077Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9803369Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9803509Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9803673Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9803678Z 
2025-04-11T03:52:12.9803978Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9804135Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9804293Z [04/11/25 03:51:18] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9804424Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9804535Z                              :75 launch                                         
2025-04-11T03:52:12.9804674Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9804802Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9805056Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9805203Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9806311Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9806487Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9807582Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9807754Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9808854Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9809078Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9810148Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9810393Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9811063Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9811153Z   warnings.warn(
2025-04-11T03:52:12.9811866Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9811952Z   warnings.warn(
2025-04-11T03:52:12.9812606Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9812691Z   warnings.warn(
2025-04-11T03:52:12.9813341Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9813425Z   warnings.warn(
2025-04-11T03:52:12.9814229Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9814367Z   warnings.warn(
2025-04-11T03:52:12.9815159Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9815240Z   warnings.warn(
2025-04-11T03:52:12.9816027Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9816108Z   warnings.warn(
2025-04-11T03:52:12.9816917Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9816998Z   warnings.warn(
2025-04-11T03:52:12.9817803Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9817946Z   warnings.warn(
2025-04-11T03:52:12.9818731Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9818811Z   warnings.warn(
2025-04-11T03:52:12.9819594Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9819739Z   warnings.warn(
2025-04-11T03:52:12.9820529Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9820669Z   warnings.warn(
2025-04-11T03:52:12.9821457Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9821540Z   warnings.warn(
2025-04-11T03:52:12.9822327Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9822413Z   warnings.warn(
2025-04-11T03:52:12.9823203Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9823292Z   warnings.warn(
2025-04-11T03:52:12.9823592Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9824448Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9824529Z   warnings.warn(
2025-04-11T03:52:12.9824823Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9825103Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9826043Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9826221Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9827140Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9827360Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9828269Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9828479Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9828690Z ________________________________ test_optim[4] _________________________________
2025-04-11T03:52:12.9828695Z 
2025-04-11T03:52:12.9828813Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9829406Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9829476Z 
2025-04-11T03:52:12.9829581Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9829667Z         try_count = 0
2025-04-11T03:52:12.9829768Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9829851Z             max_try, int
2025-04-11T03:52:12.9830002Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9830073Z     
2025-04-11T03:52:12.9830192Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9830269Z             try:
2025-04-11T03:52:12.9830357Z                 try_count += 1
2025-04-11T03:52:12.9830448Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9830529Z                 return ret
2025-04-11T03:52:12.9830630Z             except exception_type as e:
2025-04-11T03:52:12.9830730Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9830922Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9831042Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9831187Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9831408Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9831491Z                     continue
2025-04-11T03:52:12.9831575Z                 else:
2025-04-11T03:52:12.9831794Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9831881Z >                   raise e
2025-04-11T03:52:12.9831886Z 
2025-04-11T03:52:12.9831980Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9832093Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9832229Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9832318Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9832470Z tests/test_zero/test_gemini/test_optim.py:193: in test_optim
2025-04-11T03:52:12.9832565Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9832671Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9832771Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9833029Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9833210Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9833493Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9833589Z     while not context.join():
2025-04-11T03:52:12.9833699Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9833704Z 
2025-04-11T03:52:12.9833905Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfa290>
2025-04-11T03:52:12.9834042Z timeout = None
2025-04-11T03:52:12.9834047Z 
2025-04-11T03:52:12.9834143Z     def join(self, timeout=None):
2025-04-11T03:52:12.9834272Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9834344Z     
2025-04-11T03:52:12.9834494Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9834644Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9834812Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9834907Z         of the first process exiting.
2025-04-11T03:52:12.9835044Z     
2025-04-11T03:52:12.9835199Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9835335Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9835412Z     
2025-04-11T03:52:12.9835488Z         Args:
2025-04-11T03:52:12.9835636Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9835709Z         """
2025-04-11T03:52:12.9835850Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9836005Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9836085Z             return True
2025-04-11T03:52:12.9836159Z     
2025-04-11T03:52:12.9836293Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9836411Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9836509Z             self.sentinels.keys(),
2025-04-11T03:52:12.9836594Z             timeout=timeout,
2025-04-11T03:52:12.9836674Z         )
2025-04-11T03:52:12.9836743Z     
2025-04-11T03:52:12.9836825Z         error_index = None
2025-04-11T03:52:12.9836913Z         for sentinel in ready:
2025-04-11T03:52:12.9837024Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9837127Z             process = self.processes[index]
2025-04-11T03:52:12.9837214Z             process.join()
2025-04-11T03:52:12.9837314Z             if process.exitcode != 0:
2025-04-11T03:52:12.9837402Z                 error_index = index
2025-04-11T03:52:12.9837481Z                 break
2025-04-11T03:52:12.9837554Z     
2025-04-11T03:52:12.9837646Z         # Return if there was no error.
2025-04-11T03:52:12.9837734Z         if error_index is None:
2025-04-11T03:52:12.9837926Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9838024Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9838116Z     
2025-04-11T03:52:12.9838334Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9838468Z         for process in self.processes:
2025-04-11T03:52:12.9838562Z             if process.is_alive():
2025-04-11T03:52:12.9838655Z                 process.terminate()
2025-04-11T03:52:12.9838740Z             process.join()
2025-04-11T03:52:12.9838811Z     
2025-04-11T03:52:12.9838956Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9839075Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9839187Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9839311Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9839396Z             if exitcode < 0:
2025-04-11T03:52:12.9839508Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9839615Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9839766Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9839861Z                     error_index=error_index,
2025-04-11T03:52:12.9839968Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9840056Z                     exit_code=exitcode,
2025-04-11T03:52:12.9840142Z                     signal_name=name,
2025-04-11T03:52:12.9840220Z                 )
2025-04-11T03:52:12.9840296Z             else:
2025-04-11T03:52:12.9840456Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9840621Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9840715Z                     error_index=error_index,
2025-04-11T03:52:12.9840821Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9840908Z                     exit_code=exitcode,
2025-04-11T03:52:12.9840986Z                 )
2025-04-11T03:52:12.9841056Z     
2025-04-11T03:52:12.9841191Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9841359Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9841508Z         msg += original_trace
2025-04-11T03:52:12.9841688Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9841852Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9841930Z E       
2025-04-11T03:52:12.9842058Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9842163Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9842463Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9842610Z E           fn(i, *args)
2025-04-11T03:52:12.9842855Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 185, in run_dist
2025-04-11T03:52:12.9842944Z E           exam_model_step()
2025-04-11T03:52:12.9843201Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9843291Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9843543Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9843631Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9843875Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9843969Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9844076Z E         [Previous line repeated 2 more times]
2025-04-11T03:52:12.9844329Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 75, in exam_model_step
2025-04-11T03:52:12.9844430Z E           torch_model = model_builder().cuda()
2025-04-11T03:52:12.9844775Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:12.9844875Z E           return super().cuda(*args, **kwargs)
2025-04-11T03:52:12.9845144Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9845271Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9845539Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9845636Z E           module._apply(fn)
2025-04-11T03:52:12.9845899Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9845991Z E           module._apply(fn)
2025-04-11T03:52:12.9846256Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9846354Z E           param_applied = fn(param)
2025-04-11T03:52:12.9846627Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:12.9846744Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9846855Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9847140Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9847279Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9847500Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9847505Z 
2025-04-11T03:52:12.9847820Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9847977Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9848136Z [04/11/25 03:51:26] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9848266Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9848372Z                              :75 launch                                         
2025-04-11T03:52:12.9848576Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9848701Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9848900Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9849046Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9850168Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9850412Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9851513Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9851683Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9852788Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9853014Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9854104Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9854269Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9854951Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9855038Z   warnings.warn(
2025-04-11T03:52:12.9855721Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9855804Z   warnings.warn(
2025-04-11T03:52:12.9856536Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9856625Z   warnings.warn(
2025-04-11T03:52:12.9857293Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9857441Z   warnings.warn(
2025-04-11T03:52:12.9858267Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9858352Z   warnings.warn(
2025-04-11T03:52:12.9859157Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9859300Z   warnings.warn(
2025-04-11T03:52:12.9860117Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9860202Z   warnings.warn(
2025-04-11T03:52:12.9861012Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9861098Z   warnings.warn(
2025-04-11T03:52:12.9861906Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9862056Z   warnings.warn(
2025-04-11T03:52:12.9862838Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9862921Z   warnings.warn(
2025-04-11T03:52:12.9863702Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9863789Z   warnings.warn(
2025-04-11T03:52:12.9864576Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9864661Z   warnings.warn(
2025-04-11T03:52:12.9865454Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9865594Z   warnings.warn(
2025-04-11T03:52:12.9866384Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9866466Z   warnings.warn(
2025-04-11T03:52:12.9867263Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9867404Z   warnings.warn(
2025-04-11T03:52:12.9868199Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9868340Z   warnings.warn(
2025-04-11T03:52:12.9869318Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9869491Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9870415Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9870586Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9871495Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
2025-04-11T03:52:12.9871723Z   optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
2025-04-11T03:52:12.9871865Z ________________________________ test_search[1] ________________________________
2025-04-11T03:52:12.9871870Z 
2025-04-11T03:52:12.9871986Z args = (), kwargs = {'world_size': 1}, try_count = 1
2025-04-11T03:52:12.9872585Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9872594Z 
2025-04-11T03:52:12.9872697Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9872783Z         try_count = 0
2025-04-11T03:52:12.9872884Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9872970Z             max_try, int
2025-04-11T03:52:12.9873117Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9873187Z     
2025-04-11T03:52:12.9873304Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9873380Z             try:
2025-04-11T03:52:12.9873467Z                 try_count += 1
2025-04-11T03:52:12.9873560Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9873640Z                 return ret
2025-04-11T03:52:12.9873746Z             except exception_type as e:
2025-04-11T03:52:12.9873911Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9874100Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9874223Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9874373Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9874528Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9874612Z                     continue
2025-04-11T03:52:12.9874692Z                 else:
2025-04-11T03:52:12.9874912Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9875068Z >                   raise e
2025-04-11T03:52:12.9875073Z 
2025-04-11T03:52:12.9875168Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9875282Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9875415Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9875504Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9875655Z tests/test_zero/test_gemini/test_search.py:68: in test_search
2025-04-11T03:52:12.9875807Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9875913Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9876013Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9876275Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9876456Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9876749Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9876842Z     while not context.join():
2025-04-11T03:52:12.9876954Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9876958Z 
2025-04-11T03:52:12.9877159Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfa4d0>
2025-04-11T03:52:12.9877238Z timeout = None
2025-04-11T03:52:12.9877243Z 
2025-04-11T03:52:12.9877338Z     def join(self, timeout=None):
2025-04-11T03:52:12.9877464Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9877536Z     
2025-04-11T03:52:12.9877741Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9877886Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9878053Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9878146Z         of the first process exiting.
2025-04-11T03:52:12.9878222Z     
2025-04-11T03:52:12.9878368Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9878503Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9878576Z     
2025-04-11T03:52:12.9878650Z         Args:
2025-04-11T03:52:12.9878791Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9878865Z         """
2025-04-11T03:52:12.9879003Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9879101Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9879183Z             return True
2025-04-11T03:52:12.9879260Z     
2025-04-11T03:52:12.9879389Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9879508Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9879600Z             self.sentinels.keys(),
2025-04-11T03:52:12.9879683Z             timeout=timeout,
2025-04-11T03:52:12.9879760Z         )
2025-04-11T03:52:12.9879830Z     
2025-04-11T03:52:12.9879914Z         error_index = None
2025-04-11T03:52:12.9879999Z         for sentinel in ready:
2025-04-11T03:52:12.9880107Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9880210Z             process = self.processes[index]
2025-04-11T03:52:12.9880358Z             process.join()
2025-04-11T03:52:12.9880458Z             if process.exitcode != 0:
2025-04-11T03:52:12.9880548Z                 error_index = index
2025-04-11T03:52:12.9880625Z                 break
2025-04-11T03:52:12.9880699Z     
2025-04-11T03:52:12.9880789Z         # Return if there was no error.
2025-04-11T03:52:12.9880878Z         if error_index is None:
2025-04-11T03:52:12.9881012Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9881113Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9881185Z     
2025-04-11T03:52:12.9881325Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9881507Z         for process in self.processes:
2025-04-11T03:52:12.9881594Z             if process.is_alive():
2025-04-11T03:52:12.9881689Z                 process.terminate()
2025-04-11T03:52:12.9881773Z             process.join()
2025-04-11T03:52:12.9881843Z     
2025-04-11T03:52:12.9881988Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9882104Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9882273Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9882395Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9882482Z             if exitcode < 0:
2025-04-11T03:52:12.9882592Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9882698Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9882853Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9882949Z                     error_index=error_index,
2025-04-11T03:52:12.9883055Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9883145Z                     exit_code=exitcode,
2025-04-11T03:52:12.9883231Z                     signal_name=name,
2025-04-11T03:52:12.9883310Z                 )
2025-04-11T03:52:12.9883383Z             else:
2025-04-11T03:52:12.9883490Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9883656Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9883756Z                     error_index=error_index,
2025-04-11T03:52:12.9883857Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9883998Z                     exit_code=exitcode,
2025-04-11T03:52:12.9884077Z                 )
2025-04-11T03:52:12.9884147Z     
2025-04-11T03:52:12.9884282Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9884454Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9884543Z         msg += original_trace
2025-04-11T03:52:12.9884723Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9884883Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9884960Z E       
2025-04-11T03:52:12.9885085Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9885186Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9885490Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9885574Z E           fn(i, *args)
2025-04-11T03:52:12.9885824Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T03:52:12.9885913Z E           exam_chunk_manager()
2025-04-11T03:52:12.9886181Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T03:52:12.9886287Z E           chunk_manager = init_chunk_manager(
2025-04-11T03:52:12.9886560Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T03:52:12.9886646Z E           dist.barrier()
2025-04-11T03:52:12.9886952Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:12.9887125Z E           return func(*args, **kwargs)
2025-04-11T03:52:12.9887446Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T03:52:12.9887556Z E           work = default_pg.barrier(opts=opts)
2025-04-11T03:52:12.9887663Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9887950Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9888086Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9888311Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9888316Z 
2025-04-11T03:52:12.9888616Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9888771Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9888929Z [04/11/25 03:51:30] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9889113Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9889225Z                              :75 launch                                         
2025-04-11T03:52:12.9889362Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9889489Z                              environment is initialized, world size: 1          
2025-04-11T03:52:12.9889683Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9889815Z ________________________________ test_search[4] ________________________________
2025-04-11T03:52:12.9889823Z 
2025-04-11T03:52:12.9889937Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9890516Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9890529Z 
2025-04-11T03:52:12.9890630Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9890764Z         try_count = 0
2025-04-11T03:52:12.9890867Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9890949Z             max_try, int
2025-04-11T03:52:12.9891098Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9891168Z     
2025-04-11T03:52:12.9891283Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9891363Z             try:
2025-04-11T03:52:12.9891446Z                 try_count += 1
2025-04-11T03:52:12.9891540Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9891619Z                 return ret
2025-04-11T03:52:12.9891713Z             except exception_type as e:
2025-04-11T03:52:12.9891818Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9892004Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9892126Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9892271Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9892431Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9892511Z                     continue
2025-04-11T03:52:12.9892586Z                 else:
2025-04-11T03:52:12.9892806Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9892888Z >                   raise e
2025-04-11T03:52:12.9892892Z 
2025-04-11T03:52:12.9892989Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9893102Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9893296Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9893382Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9893530Z tests/test_zero/test_gemini/test_search.py:68: in test_search
2025-04-11T03:52:12.9893626Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9893728Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9893831Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9894089Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9894267Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9894617Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9894707Z     while not context.join():
2025-04-11T03:52:12.9894817Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9894823Z 
2025-04-11T03:52:12.9895021Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f6887cfa7d0>
2025-04-11T03:52:12.9895106Z timeout = None
2025-04-11T03:52:12.9895165Z 
2025-04-11T03:52:12.9895256Z     def join(self, timeout=None):
2025-04-11T03:52:12.9895387Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9895459Z     
2025-04-11T03:52:12.9895601Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9895750Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9895910Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9896011Z         of the first process exiting.
2025-04-11T03:52:12.9896080Z     
2025-04-11T03:52:12.9896230Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9896365Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9896437Z     
2025-04-11T03:52:12.9896515Z         Args:
2025-04-11T03:52:12.9896652Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9896729Z         """
2025-04-11T03:52:12.9896869Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9896966Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9897106Z             return True
2025-04-11T03:52:12.9897178Z     
2025-04-11T03:52:12.9897317Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9897434Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9897529Z             self.sentinels.keys(),
2025-04-11T03:52:12.9897615Z             timeout=timeout,
2025-04-11T03:52:12.9897688Z         )
2025-04-11T03:52:12.9897762Z     
2025-04-11T03:52:12.9897844Z         error_index = None
2025-04-11T03:52:12.9897932Z         for sentinel in ready:
2025-04-11T03:52:12.9898039Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9898141Z             process = self.processes[index]
2025-04-11T03:52:12.9898231Z             process.join()
2025-04-11T03:52:12.9898325Z             if process.exitcode != 0:
2025-04-11T03:52:12.9898418Z                 error_index = index
2025-04-11T03:52:12.9898496Z                 break
2025-04-11T03:52:12.9898571Z     
2025-04-11T03:52:12.9898663Z         # Return if there was no error.
2025-04-11T03:52:12.9898751Z         if error_index is None:
2025-04-11T03:52:12.9898886Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9898990Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9899064Z     
2025-04-11T03:52:12.9899205Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9899302Z         for process in self.processes:
2025-04-11T03:52:12.9899395Z             if process.is_alive():
2025-04-11T03:52:12.9899485Z                 process.terminate()
2025-04-11T03:52:12.9899576Z             process.join()
2025-04-11T03:52:12.9899705Z     
2025-04-11T03:52:12.9899846Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9899968Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9900080Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9900204Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9900290Z             if exitcode < 0:
2025-04-11T03:52:12.9900404Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9900510Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9900658Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9900822Z                     error_index=error_index,
2025-04-11T03:52:12.9900925Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9901018Z                     exit_code=exitcode,
2025-04-11T03:52:12.9901105Z                     signal_name=name,
2025-04-11T03:52:12.9901180Z                 )
2025-04-11T03:52:12.9901256Z             else:
2025-04-11T03:52:12.9901359Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9901529Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9901679Z                     error_index=error_index,
2025-04-11T03:52:12.9901784Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9901874Z                     exit_code=exitcode,
2025-04-11T03:52:12.9901946Z                 )
2025-04-11T03:52:12.9902019Z     
2025-04-11T03:52:12.9902149Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9902324Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9902413Z         msg += original_trace
2025-04-11T03:52:12.9902585Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9902747Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9902821Z E       
2025-04-11T03:52:12.9902951Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9903048Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9903352Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9903433Z E           fn(i, *args)
2025-04-11T03:52:12.9903737Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T03:52:12.9903827Z E           exam_chunk_manager()
2025-04-11T03:52:12.9904082Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T03:52:12.9904191Z E           chunk_manager = init_chunk_manager(
2025-04-11T03:52:12.9904451Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T03:52:12.9904537Z E           dist.barrier()
2025-04-11T03:52:12.9904832Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:12.9904930Z E           return func(*args, **kwargs)
2025-04-11T03:52:12.9905242Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T03:52:12.9905347Z E           work = default_pg.barrier(opts=opts)
2025-04-11T03:52:12.9905454Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9905732Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9905875Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9906035Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9906040Z 
2025-04-11T03:52:12.9906341Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9906648Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9906810Z [04/11/25 03:51:36] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9906942Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9907052Z                              :75 launch                                         
2025-04-11T03:52:12.9907194Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9907318Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9907582Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9907727Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9908021Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43909 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9908302Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43909 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9908678Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43909 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9908818Z _______________________________ test_zero_ddp[4] _______________________________
2025-04-11T03:52:12.9908823Z 
2025-04-11T03:52:12.9908938Z args = (), kwargs = {'world_size': 4}, try_count = 1
2025-04-11T03:52:12.9909524Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9909532Z 
2025-04-11T03:52:12.9909635Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9909720Z         try_count = 0
2025-04-11T03:52:12.9909827Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9909910Z             max_try, int
2025-04-11T03:52:12.9910058Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9910134Z     
2025-04-11T03:52:12.9910247Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9910393Z             try:
2025-04-11T03:52:12.9910482Z                 try_count += 1
2025-04-11T03:52:12.9910575Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9910658Z                 return ret
2025-04-11T03:52:12.9910754Z             except exception_type as e:
2025-04-11T03:52:12.9910856Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9911044Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9911160Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9911307Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9911461Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9911547Z                     continue
2025-04-11T03:52:12.9911623Z                 else:
2025-04-11T03:52:12.9911837Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9911925Z >                   raise e
2025-04-11T03:52:12.9911930Z 
2025-04-11T03:52:12.9912023Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9912139Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9912273Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9912365Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9912545Z tests/test_zero/test_gemini/test_zeroddp_state_dict.py:85: in test_zero_ddp
2025-04-11T03:52:12.9912635Z     spawn(run_dist, world_size)
2025-04-11T03:52:12.9912740Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9912914Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9913173Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9913352Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9913644Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9913735Z     while not context.join():
2025-04-11T03:52:12.9913845Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9913849Z 
2025-04-11T03:52:12.9914121Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be1e89a0>
2025-04-11T03:52:12.9914203Z timeout = None
2025-04-11T03:52:12.9914208Z 
2025-04-11T03:52:12.9914301Z     def join(self, timeout=None):
2025-04-11T03:52:12.9914425Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9914501Z     
2025-04-11T03:52:12.9914647Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9914791Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9915021Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9915116Z         of the first process exiting.
2025-04-11T03:52:12.9915195Z     
2025-04-11T03:52:12.9915345Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9915486Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9915561Z     
2025-04-11T03:52:12.9915640Z         Args:
2025-04-11T03:52:12.9915784Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9915860Z         """
2025-04-11T03:52:12.9916005Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9916101Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9916187Z             return True
2025-04-11T03:52:12.9916265Z     
2025-04-11T03:52:12.9916399Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9916525Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9916620Z             self.sentinels.keys(),
2025-04-11T03:52:12.9916706Z             timeout=timeout,
2025-04-11T03:52:12.9916840Z         )
2025-04-11T03:52:12.9916912Z     
2025-04-11T03:52:12.9916998Z         error_index = None
2025-04-11T03:52:12.9917083Z         for sentinel in ready:
2025-04-11T03:52:12.9917190Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9917287Z             process = self.processes[index]
2025-04-11T03:52:12.9917373Z             process.join()
2025-04-11T03:52:12.9917472Z             if process.exitcode != 0:
2025-04-11T03:52:12.9917559Z                 error_index = index
2025-04-11T03:52:12.9917637Z                 break
2025-04-11T03:52:12.9917707Z     
2025-04-11T03:52:12.9917799Z         # Return if there was no error.
2025-04-11T03:52:12.9917890Z         if error_index is None:
2025-04-11T03:52:12.9918024Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9918127Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9918197Z     
2025-04-11T03:52:12.9918337Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9918436Z         for process in self.processes:
2025-04-11T03:52:12.9918524Z             if process.is_alive():
2025-04-11T03:52:12.9918618Z                 process.terminate()
2025-04-11T03:52:12.9918700Z             process.join()
2025-04-11T03:52:12.9918776Z     
2025-04-11T03:52:12.9918918Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9919034Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9919146Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9919265Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9919410Z             if exitcode < 0:
2025-04-11T03:52:12.9919518Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9919625Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9919781Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9919876Z                     error_index=error_index,
2025-04-11T03:52:12.9919983Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9920070Z                     exit_code=exitcode,
2025-04-11T03:52:12.9920160Z                     signal_name=name,
2025-04-11T03:52:12.9920233Z                 )
2025-04-11T03:52:12.9920372Z             else:
2025-04-11T03:52:12.9920477Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9920643Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9920741Z                     error_index=error_index,
2025-04-11T03:52:12.9920842Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9920935Z                     exit_code=exitcode,
2025-04-11T03:52:12.9921007Z                 )
2025-04-11T03:52:12.9921139Z     
2025-04-11T03:52:12.9921276Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9921445Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9921539Z         msg += original_trace
2025-04-11T03:52:12.9921712Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9921875Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9921954Z E       
2025-04-11T03:52:12.9922082Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9922183Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9922483Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9922567Z E           fn(i, *args)
2025-04-11T03:52:12.9922848Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 78, in run_dist
2025-04-11T03:52:12.9922938Z E           exam_state_dict()
2025-04-11T03:52:12.9923202Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9923351Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9923607Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9923695Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9923939Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:12.9924025Z E           partial_func(**kwargs)
2025-04-11T03:52:12.9924129Z E         [Previous line repeated 1 more time]
2025-04-11T03:52:12.9924414Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 45, in exam_state_dict
2025-04-11T03:52:12.9924663Z E           model = GeminiDDP(model, config_dict, **placement_config, pin_memory=True, master_weights=master_weights)
2025-04-11T03:52:12.9924902Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 109, in __init__
2025-04-11T03:52:12.9925005Z E           self.chunk_manager = ChunkManager(
2025-04-11T03:52:12.9925247Z E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T03:52:12.9925491Z E           self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:12.9925602Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9925878Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9926014Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9926233Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9926238Z 
2025-04-11T03:52:12.9926545Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9926703Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9926861Z [04/11/25 03:51:45] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9926997Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9927105Z                              :75 launch                                         
2025-04-11T03:52:12.9927308Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9927435Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9927632Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9927780Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:12.9928903Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9929138Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9930243Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9930418Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9931517Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9931762Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9932855Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:12.9933023Z   deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:12.9933707Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9933795Z   warnings.warn(
2025-04-11T03:52:12.9934466Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9934549Z   warnings.warn(
2025-04-11T03:52:12.9935286Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9935373Z   warnings.warn(
2025-04-11T03:52:12.9936040Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:12.9936186Z   warnings.warn(
2025-04-11T03:52:12.9937022Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9937107Z   warnings.warn(
2025-04-11T03:52:12.9937916Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9938060Z   warnings.warn(
2025-04-11T03:52:12.9939013Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9939100Z   warnings.warn(
2025-04-11T03:52:12.9939893Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9939979Z   warnings.warn(
2025-04-11T03:52:12.9940765Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9940903Z   warnings.warn(
2025-04-11T03:52:12.9941700Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9941779Z   warnings.warn(
2025-04-11T03:52:12.9942582Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9942662Z   warnings.warn(
2025-04-11T03:52:12.9943461Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9943544Z   warnings.warn(
2025-04-11T03:52:12.9944339Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9944472Z   warnings.warn(
2025-04-11T03:52:12.9945290Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9945372Z   warnings.warn(
2025-04-11T03:52:12.9946182Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9946330Z   warnings.warn(
2025-04-11T03:52:12.9947127Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:12.9947266Z   warnings.warn(
2025-04-11T03:52:12.9947567Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30659 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9947854Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30659 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:12.9948780Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T03:52:12.9948889Z   return tensor.storage().size() == 0
2025-04-11T03:52:12.9949057Z Exception ignored in: <function GeminiDDP.__del__ at 0x7efd22975f30>
2025-04-11T03:52:12.9949160Z Traceback (most recent call last):
2025-04-11T03:52:12.9949394Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
2025-04-11T03:52:12.9949488Z     self.remove_hooks()
2025-04-11T03:52:12.9949727Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
2025-04-11T03:52:12.9949899Z     for p in self.module.parameters():
2025-04-11T03:52:12.9950203Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
2025-04-11T03:52:12.9950395Z     raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
2025-04-11T03:52:12.9950553Z AttributeError: 'GeminiDDP' object has no attribute 'module'
2025-04-11T03:52:12.9951442Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T03:52:12.9951549Z   return tensor.storage().size() == 0
2025-04-11T03:52:12.9952417Z /__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T03:52:12.9952516Z   return tensor.storage().size() == 0
2025-04-11T03:52:12.9952660Z _________________________________ test_comm_nd _________________________________
2025-04-11T03:52:12.9952664Z 
2025-04-11T03:52:12.9952757Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9953360Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9953423Z 
2025-04-11T03:52:12.9953532Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9953612Z         try_count = 0
2025-04-11T03:52:12.9953720Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9953804Z             max_try, int
2025-04-11T03:52:12.9953952Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9954027Z     
2025-04-11T03:52:12.9954141Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9954289Z             try:
2025-04-11T03:52:12.9954375Z                 try_count += 1
2025-04-11T03:52:12.9954470Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9954551Z                 return ret
2025-04-11T03:52:12.9954647Z             except exception_type as e:
2025-04-11T03:52:12.9954751Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9954942Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9955129Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9955275Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9955432Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9955518Z                     continue
2025-04-11T03:52:12.9955595Z                 else:
2025-04-11T03:52:12.9955818Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9955903Z >                   raise e
2025-04-11T03:52:12.9955907Z 
2025-04-11T03:52:12.9956006Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9956117Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9956252Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9956345Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9956506Z tests/test_zero/test_low_level/test_coll_nd.py:38: in test_comm_nd
2025-04-11T03:52:12.9956596Z     spawn(run_dist, 4)
2025-04-11T03:52:12.9956697Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9956872Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9957132Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9957309Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9957599Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9957691Z     while not context.join():
2025-04-11T03:52:12.9957803Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9957808Z 
2025-04-11T03:52:12.9958005Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f038ba90>
2025-04-11T03:52:12.9958087Z timeout = None
2025-04-11T03:52:12.9958092Z 
2025-04-11T03:52:12.9958185Z     def join(self, timeout=None):
2025-04-11T03:52:12.9958314Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9958385Z     
2025-04-11T03:52:12.9958532Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9958684Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9958844Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9958938Z         of the first process exiting.
2025-04-11T03:52:12.9959011Z     
2025-04-11T03:52:12.9959156Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9959294Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9959364Z     
2025-04-11T03:52:12.9959440Z         Args:
2025-04-11T03:52:12.9959639Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9959716Z         """
2025-04-11T03:52:12.9959854Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9959948Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9960032Z             return True
2025-04-11T03:52:12.9960104Z     
2025-04-11T03:52:12.9960238Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9960358Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9960449Z             self.sentinels.keys(),
2025-04-11T03:52:12.9960540Z             timeout=timeout,
2025-04-11T03:52:12.9960669Z         )
2025-04-11T03:52:12.9960742Z     
2025-04-11T03:52:12.9960827Z         error_index = None
2025-04-11T03:52:12.9960911Z         for sentinel in ready:
2025-04-11T03:52:12.9961020Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9961122Z             process = self.processes[index]
2025-04-11T03:52:12.9961214Z             process.join()
2025-04-11T03:52:12.9961306Z             if process.exitcode != 0:
2025-04-11T03:52:12.9961397Z                 error_index = index
2025-04-11T03:52:12.9961542Z                 break
2025-04-11T03:52:12.9961613Z     
2025-04-11T03:52:12.9961710Z         # Return if there was no error.
2025-04-11T03:52:12.9961795Z         if error_index is None:
2025-04-11T03:52:12.9961934Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9962033Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9962104Z     
2025-04-11T03:52:12.9962247Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9962345Z         for process in self.processes:
2025-04-11T03:52:12.9962435Z             if process.is_alive():
2025-04-11T03:52:12.9962525Z                 process.terminate()
2025-04-11T03:52:12.9962611Z             process.join()
2025-04-11T03:52:12.9962682Z     
2025-04-11T03:52:12.9962820Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9962942Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9963051Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9963181Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9963263Z             if exitcode < 0:
2025-04-11T03:52:12.9963504Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9963618Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9963766Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9963868Z                     error_index=error_index,
2025-04-11T03:52:12.9963977Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9964069Z                     exit_code=exitcode,
2025-04-11T03:52:12.9964156Z                     signal_name=name,
2025-04-11T03:52:12.9964229Z                 )
2025-04-11T03:52:12.9964309Z             else:
2025-04-11T03:52:12.9964416Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9964585Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9964681Z                     error_index=error_index,
2025-04-11T03:52:12.9964784Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9964878Z                     exit_code=exitcode,
2025-04-11T03:52:12.9964950Z                 )
2025-04-11T03:52:12.9965024Z     
2025-04-11T03:52:12.9965155Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9965329Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9965419Z         msg += original_trace
2025-04-11T03:52:12.9965594Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9965758Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9965831Z E       
2025-04-11T03:52:12.9965963Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9966117Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9966416Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9966505Z E           fn(i, *args)
2025-04-11T03:52:12.9966755Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 32, in run_dist
2025-04-11T03:52:12.9966850Z E           check_all_gather_2d()
2025-04-11T03:52:12.9967121Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 16, in check_all_gather_2d
2025-04-11T03:52:12.9967320Z E           tensor = torch.rand(128, device=get_current_device())
2025-04-11T03:52:12.9967426Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9967712Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9967852Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9968013Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9968075Z 
2025-04-11T03:52:12.9968387Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9968542Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9968704Z [04/11/25 03:51:50] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9968830Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9968941Z                              :75 launch                                         
2025-04-11T03:52:12.9969079Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9969204Z                              environment is initialized, world size: 4          
2025-04-11T03:52:12.9969404Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:12.9969542Z ____________________________ test_grad_accumulation ____________________________
2025-04-11T03:52:12.9969548Z 
2025-04-11T03:52:12.9969638Z     @pytest.mark.dist
2025-04-11T03:52:12.9969731Z     def test_grad_accumulation():
2025-04-11T03:52:12.9969876Z >       spawn(run_dist, 2)
2025-04-11T03:52:12.9969882Z 
2025-04-11T03:52:12.9970014Z tests/test_zero/test_low_level/test_grad_acc.py:146: 
2025-04-11T03:52:12.9970125Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9970228Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9970330Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9970588Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9970764Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9971054Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9971145Z     while not context.join():
2025-04-11T03:52:12.9971261Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9971265Z 
2025-04-11T03:52:12.9971460Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be1ebf40>
2025-04-11T03:52:12.9971543Z timeout = None
2025-04-11T03:52:12.9971551Z 
2025-04-11T03:52:12.9971641Z     def join(self, timeout=None):
2025-04-11T03:52:12.9971766Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9971845Z     
2025-04-11T03:52:12.9971989Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9972137Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9972299Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9972449Z         of the first process exiting.
2025-04-11T03:52:12.9972524Z     
2025-04-11T03:52:12.9972671Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9972811Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9972882Z     
2025-04-11T03:52:12.9972961Z         Args:
2025-04-11T03:52:12.9973101Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9973175Z         """
2025-04-11T03:52:12.9973319Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9973411Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9973562Z             return True
2025-04-11T03:52:12.9973633Z     
2025-04-11T03:52:12.9973765Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9973888Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9973980Z             self.sentinels.keys(),
2025-04-11T03:52:12.9974068Z             timeout=timeout,
2025-04-11T03:52:12.9974143Z         )
2025-04-11T03:52:12.9974213Z     
2025-04-11T03:52:12.9974304Z         error_index = None
2025-04-11T03:52:12.9974447Z         for sentinel in ready:
2025-04-11T03:52:12.9974558Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9974658Z             process = self.processes[index]
2025-04-11T03:52:12.9974750Z             process.join()
2025-04-11T03:52:12.9974846Z             if process.exitcode != 0:
2025-04-11T03:52:12.9974933Z                 error_index = index
2025-04-11T03:52:12.9975014Z                 break
2025-04-11T03:52:12.9975084Z     
2025-04-11T03:52:12.9975178Z         # Return if there was no error.
2025-04-11T03:52:12.9975265Z         if error_index is None:
2025-04-11T03:52:12.9975397Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9975499Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9975569Z     
2025-04-11T03:52:12.9975710Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9975807Z         for process in self.processes:
2025-04-11T03:52:12.9975894Z             if process.is_alive():
2025-04-11T03:52:12.9975990Z                 process.terminate()
2025-04-11T03:52:12.9976074Z             process.join()
2025-04-11T03:52:12.9976147Z     
2025-04-11T03:52:12.9976287Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9976468Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9976576Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9976699Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9976790Z             if exitcode < 0:
2025-04-11T03:52:12.9976895Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9977005Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9977154Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9977254Z                     error_index=error_index,
2025-04-11T03:52:12.9977355Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9977442Z                     exit_code=exitcode,
2025-04-11T03:52:12.9977537Z                     signal_name=name,
2025-04-11T03:52:12.9977612Z                 )
2025-04-11T03:52:12.9977691Z             else:
2025-04-11T03:52:12.9977795Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9977958Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9978054Z                     error_index=error_index,
2025-04-11T03:52:12.9978156Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9978246Z                     exit_code=exitcode,
2025-04-11T03:52:12.9978319Z                 )
2025-04-11T03:52:12.9978392Z     
2025-04-11T03:52:12.9978521Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9978690Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9978840Z         msg += original_trace
2025-04-11T03:52:12.9979012Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9979175Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9979249Z E       
2025-04-11T03:52:12.9979377Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9979478Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9979771Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9979857Z E           fn(i, *args)
2025-04-11T03:52:12.9980171Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 139, in run_dist
2025-04-11T03:52:12.9980270Z E           exam_zero_1_grad_acc(sync=True)
2025-04-11T03:52:12.9980545Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 86, in exam_zero_1_grad_acc
2025-04-11T03:52:12.9980650Z E           zero_model = zero_model.to(device)
2025-04-11T03:52:12.9980921Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:12.9981071Z E           return self._apply(convert)
2025-04-11T03:52:12.9981357Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9981446Z E           module._apply(fn)
2025-04-11T03:52:12.9981724Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9981822Z E           param_applied = fn(param)
2025-04-11T03:52:12.9982104Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:12.9982315Z E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:12.9982426Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:12.9982714Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:12.9982853Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:12.9983074Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:12.9983079Z 
2025-04-11T03:52:12.9983378Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:12.9983533Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:12.9983689Z [04/11/25 03:51:54] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:12.9983821Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:12.9983928Z                              :75 launch                                         
2025-04-11T03:52:12.9984067Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:12.9984196Z                              environment is initialized, world size: 2          
2025-04-11T03:52:12.9984326Z ________________________________ test_zero_1_2 _________________________________
2025-04-11T03:52:12.9984332Z 
2025-04-11T03:52:12.9984428Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:12.9985018Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:12.9985026Z 
2025-04-11T03:52:12.9985133Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:12.9985215Z         try_count = 0
2025-04-11T03:52:12.9985317Z         assert max_try is None or isinstance(
2025-04-11T03:52:12.9985399Z             max_try, int
2025-04-11T03:52:12.9985607Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:12.9985682Z     
2025-04-11T03:52:12.9985797Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:12.9985878Z             try:
2025-04-11T03:52:12.9985966Z                 try_count += 1
2025-04-11T03:52:12.9986056Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:12.9986145Z                 return ret
2025-04-11T03:52:12.9986240Z             except exception_type as e:
2025-04-11T03:52:12.9986344Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:12.9986526Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:12.9986717Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:12.9986862Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:12.9987012Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:12.9987103Z                     continue
2025-04-11T03:52:12.9987180Z                 else:
2025-04-11T03:52:12.9987405Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:12.9987544Z >                   raise e
2025-04-11T03:52:12.9987550Z 
2025-04-11T03:52:12.9987649Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:12.9987760Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9987891Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:12.9987982Z     ret = func(*args, **kwargs)
2025-04-11T03:52:12.9988149Z tests/test_zero/test_low_level/test_mem_leak.py:57: in test_zero_1_2
2025-04-11T03:52:12.9988234Z     spawn(run_dist, 2)
2025-04-11T03:52:12.9988335Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:12.9988479Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:12.9988740Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:12.9988917Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:12.9989210Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:12.9989301Z     while not context.join():
2025-04-11T03:52:12.9989483Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:12.9989488Z 
2025-04-11T03:52:12.9989683Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be1eb2b0>
2025-04-11T03:52:12.9989765Z timeout = None
2025-04-11T03:52:12.9989771Z 
2025-04-11T03:52:12.9989861Z     def join(self, timeout=None):
2025-04-11T03:52:12.9989985Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:12.9990060Z     
2025-04-11T03:52:12.9990204Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:12.9990350Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:12.9990510Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:12.9990607Z         of the first process exiting.
2025-04-11T03:52:12.9990678Z     
2025-04-11T03:52:12.9990822Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:12.9990962Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:12.9991031Z     
2025-04-11T03:52:12.9991107Z         Args:
2025-04-11T03:52:12.9991243Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:12.9991318Z         """
2025-04-11T03:52:12.9991459Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:12.9991553Z         if len(self.sentinels) == 0:
2025-04-11T03:52:12.9991637Z             return True
2025-04-11T03:52:12.9991709Z     
2025-04-11T03:52:12.9991844Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:12.9992027Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:12.9992118Z             self.sentinels.keys(),
2025-04-11T03:52:12.9992211Z             timeout=timeout,
2025-04-11T03:52:12.9992284Z         )
2025-04-11T03:52:12.9992356Z     
2025-04-11T03:52:12.9992439Z         error_index = None
2025-04-11T03:52:12.9992526Z         for sentinel in ready:
2025-04-11T03:52:12.9992635Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:12.9992734Z             process = self.processes[index]
2025-04-11T03:52:12.9992823Z             process.join()
2025-04-11T03:52:12.9992916Z             if process.exitcode != 0:
2025-04-11T03:52:12.9993073Z                 error_index = index
2025-04-11T03:52:12.9993155Z                 break
2025-04-11T03:52:12.9993225Z     
2025-04-11T03:52:12.9993320Z         # Return if there was no error.
2025-04-11T03:52:12.9993407Z         if error_index is None:
2025-04-11T03:52:12.9993549Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:12.9993648Z             return len(self.sentinels) == 0
2025-04-11T03:52:12.9993717Z     
2025-04-11T03:52:12.9993923Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:12.9994019Z         for process in self.processes:
2025-04-11T03:52:12.9994115Z             if process.is_alive():
2025-04-11T03:52:12.9994208Z                 process.terminate()
2025-04-11T03:52:12.9994291Z             process.join()
2025-04-11T03:52:12.9994365Z     
2025-04-11T03:52:12.9994505Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:12.9994624Z         failed_process = self.processes[error_index]
2025-04-11T03:52:12.9994733Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:12.9994856Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:12.9994942Z             if exitcode < 0:
2025-04-11T03:52:12.9995049Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:12.9995160Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9995310Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:12.9995410Z                     error_index=error_index,
2025-04-11T03:52:12.9995513Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9995600Z                     exit_code=exitcode,
2025-04-11T03:52:12.9995749Z                     signal_name=name,
2025-04-11T03:52:12.9995823Z                 )
2025-04-11T03:52:12.9995904Z             else:
2025-04-11T03:52:12.9996005Z                 raise ProcessExitedException(
2025-04-11T03:52:12.9996173Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:12.9996268Z                     error_index=error_index,
2025-04-11T03:52:12.9996369Z                     error_pid=failed_process.pid,
2025-04-11T03:52:12.9996460Z                     exit_code=exitcode,
2025-04-11T03:52:12.9996533Z                 )
2025-04-11T03:52:12.9996609Z     
2025-04-11T03:52:12.9996741Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:12.9996908Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:12.9997000Z         msg += original_trace
2025-04-11T03:52:12.9997174Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:12.9997342Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:12.9997416Z E       
2025-04-11T03:52:12.9997545Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:12.9997644Z E       Traceback (most recent call last):
2025-04-11T03:52:12.9997945Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:12.9998032Z E           fn(i, *args)
2025-04-11T03:52:12.9998281Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 51, in run_dist
2025-04-11T03:52:12.9998445Z E           exam_mem_leak(world_size=world_size)
2025-04-11T03:52:12.9998699Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 36, in exam_mem_leak
2025-04-11T03:52:12.9998801Z E           zero_model = MlpModel().cuda()
2025-04-11T03:52:12.9999064Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:12.9999184Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:12.9999457Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:12.9999604Z E           module._apply(fn)
2025-04-11T03:52:12.9999879Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:12.9999973Z E           param_applied = fn(param)
2025-04-11T03:52:13.0000255Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.0000377Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.0000543Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.0000827Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.0000966Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.0001131Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.0001136Z 
2025-04-11T03:52:13.0001437Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:13.0001595Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:13.0001750Z [04/11/25 03:51:59] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:13.0001884Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:13.0001989Z                              :75 launch                                         
2025-04-11T03:52:13.0002129Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:13.0002259Z                              environment is initialized, world size: 2          
2025-04-11T03:52:13.0002511Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:13.0002663Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:13.0002960Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:33730 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:13.0003095Z ________________________________ test_zero_1_2 _________________________________
2025-04-11T03:52:13.0003100Z 
2025-04-11T03:52:13.0003190Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:13.0003784Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:13.0003791Z 
2025-04-11T03:52:13.0003892Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:13.0003980Z         try_count = 0
2025-04-11T03:52:13.0004079Z         assert max_try is None or isinstance(
2025-04-11T03:52:13.0004160Z             max_try, int
2025-04-11T03:52:13.0004312Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:13.0004382Z     
2025-04-11T03:52:13.0004499Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:13.0004576Z             try:
2025-04-11T03:52:13.0004661Z                 try_count += 1
2025-04-11T03:52:13.0004756Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:13.0004836Z                 return ret
2025-04-11T03:52:13.0004989Z             except exception_type as e:
2025-04-11T03:52:13.0005089Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:13.0005277Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:13.0005397Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:13.0005542Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:13.0005703Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:13.0005783Z                     continue
2025-04-11T03:52:13.0005863Z                 else:
2025-04-11T03:52:13.0006148Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:13.0006236Z >                   raise e
2025-04-11T03:52:13.0006241Z 
2025-04-11T03:52:13.0006338Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:13.0006453Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:13.0006594Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:13.0006738Z     ret = func(*args, **kwargs)
2025-04-11T03:52:13.0006907Z tests/test_zero/test_low_level/test_zero1_2.py:224: in test_zero_1_2
2025-04-11T03:52:13.0006991Z     spawn(run_dist, 4)
2025-04-11T03:52:13.0007102Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:13.0007204Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:13.0007460Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:13.0007643Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:13.0007927Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:13.0008025Z     while not context.join():
2025-04-11T03:52:13.0008135Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:13.0008141Z 
2025-04-11T03:52:13.0008345Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68be1ebbb0>
2025-04-11T03:52:13.0008426Z timeout = None
2025-04-11T03:52:13.0008431Z 
2025-04-11T03:52:13.0008521Z     def join(self, timeout=None):
2025-04-11T03:52:13.0008648Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:13.0008776Z     
2025-04-11T03:52:13.0008927Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:13.0009071Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:13.0009236Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:13.0009333Z         of the first process exiting.
2025-04-11T03:52:13.0009405Z     
2025-04-11T03:52:13.0009555Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:13.0009692Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:13.0009766Z     
2025-04-11T03:52:13.0009842Z         Args:
2025-04-11T03:52:13.0009978Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:13.0010058Z         """
2025-04-11T03:52:13.0010197Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:13.0010298Z         if len(self.sentinels) == 0:
2025-04-11T03:52:13.0010380Z             return True
2025-04-11T03:52:13.0010455Z     
2025-04-11T03:52:13.0010584Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:13.0010702Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:13.0010797Z             self.sentinels.keys(),
2025-04-11T03:52:13.0010884Z             timeout=timeout,
2025-04-11T03:52:13.0010962Z         )
2025-04-11T03:52:13.0011033Z     
2025-04-11T03:52:13.0011116Z         error_index = None
2025-04-11T03:52:13.0011205Z         for sentinel in ready:
2025-04-11T03:52:13.0011310Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:13.0011487Z             process = self.processes[index]
2025-04-11T03:52:13.0011580Z             process.join()
2025-04-11T03:52:13.0011675Z             if process.exitcode != 0:
2025-04-11T03:52:13.0011771Z                 error_index = index
2025-04-11T03:52:13.0011847Z                 break
2025-04-11T03:52:13.0011921Z     
2025-04-11T03:52:13.0012013Z         # Return if there was no error.
2025-04-11T03:52:13.0012101Z         if error_index is None:
2025-04-11T03:52:13.0012234Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:13.0012332Z             return len(self.sentinels) == 0
2025-04-11T03:52:13.0012406Z     
2025-04-11T03:52:13.0012617Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:13.0012718Z         for process in self.processes:
2025-04-11T03:52:13.0012805Z             if process.is_alive():
2025-04-11T03:52:13.0012896Z                 process.terminate()
2025-04-11T03:52:13.0012983Z             process.join()
2025-04-11T03:52:13.0013055Z     
2025-04-11T03:52:13.0013202Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:13.0013378Z         failed_process = self.processes[error_index]
2025-04-11T03:52:13.0013488Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:13.0013611Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:13.0013695Z             if exitcode < 0:
2025-04-11T03:52:13.0013808Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:13.0013913Z                 raise ProcessExitedException(
2025-04-11T03:52:13.0014069Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:13.0014169Z                     error_index=error_index,
2025-04-11T03:52:13.0014271Z                     error_pid=failed_process.pid,
2025-04-11T03:52:13.0014366Z                     exit_code=exitcode,
2025-04-11T03:52:13.0014452Z                     signal_name=name,
2025-04-11T03:52:13.0014536Z                 )
2025-04-11T03:52:13.0014610Z             else:
2025-04-11T03:52:13.0014718Z                 raise ProcessExitedException(
2025-04-11T03:52:13.0014887Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:13.0014980Z                     error_index=error_index,
2025-04-11T03:52:13.0015223Z                     error_pid=failed_process.pid,
2025-04-11T03:52:13.0015311Z                     exit_code=exitcode,
2025-04-11T03:52:13.0015389Z                 )
2025-04-11T03:52:13.0015462Z     
2025-04-11T03:52:13.0015597Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:13.0015773Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:13.0015859Z         msg += original_trace
2025-04-11T03:52:13.0016037Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:13.0016200Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.0016278Z E       
2025-04-11T03:52:13.0016406Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:13.0016503Z E       Traceback (most recent call last):
2025-04-11T03:52:13.0016808Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.0016891Z E           fn(i, *args)
2025-04-11T03:52:13.0017146Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 217, in run_dist
2025-04-11T03:52:13.0017238Z E           exam_zero_1_torch_ddp()
2025-04-11T03:52:13.0017500Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.0017592Z E           partial_func(**kwargs)
2025-04-11T03:52:13.0017842Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.0017935Z E           partial_func(**kwargs)
2025-04-11T03:52:13.0018242Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.0018333Z E           partial_func(**kwargs)
2025-04-11T03:52:13.0018610Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 151, in exam_zero_1_torch_ddp
2025-04-11T03:52:13.0018727Z E           torch_model = MlpModel().cuda().to(dtype)
2025-04-11T03:52:13.0018993Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.0019111Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.0019440Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.0019528Z E           module._apply(fn)
2025-04-11T03:52:13.0019799Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.0019894Z E           param_applied = fn(param)
2025-04-11T03:52:13.0020169Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.0020360Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.0020471Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.0020753Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.0020890Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.0021055Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.0021061Z 
2025-04-11T03:52:13.0021361Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:13.0021516Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:13.0021669Z [04/11/25 03:52:05] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:13.0021799Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:13.0021908Z                              :75 launch                                         
2025-04-11T03:52:13.0022048Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:13.0022237Z                              environment is initialized, world size: 4          
2025-04-11T03:52:13.0022433Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:13.0022585Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:13.0022879Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:63920 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:13.0023014Z ________________________________ test_zero_ckpt ________________________________
2025-04-11T03:52:13.0023021Z 
2025-04-11T03:52:13.0023111Z args = (), kwargs = {}, try_count = 1
2025-04-11T03:52:13.0023697Z error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T03:52:13.0023706Z 
2025-04-11T03:52:13.0023807Z     def _run_until_success(*args, **kwargs):
2025-04-11T03:52:13.0023892Z         try_count = 0
2025-04-11T03:52:13.0023995Z         assert max_try is None or isinstance(
2025-04-11T03:52:13.0024077Z             max_try, int
2025-04-11T03:52:13.0024231Z         ), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T03:52:13.0024303Z     
2025-04-11T03:52:13.0024417Z         while max_try is None or try_count < max_try:
2025-04-11T03:52:13.0024494Z             try:
2025-04-11T03:52:13.0024579Z                 try_count += 1
2025-04-11T03:52:13.0024729Z                 ret = func(*args, **kwargs)
2025-04-11T03:52:13.0024810Z                 return ret
2025-04-11T03:52:13.0024911Z             except exception_type as e:
2025-04-11T03:52:13.0025013Z                 error_lines = str(e).split("\n")
2025-04-11T03:52:13.0025205Z                 if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
2025-04-11T03:52:13.0025325Z                     print("Exception is caught, retrying...")
2025-04-11T03:52:13.0025469Z                     # when pattern is not specified, we always skip the exception
2025-04-11T03:52:13.0025627Z                     # when pattern is specified, we only skip when pattern is matched
2025-04-11T03:52:13.0025771Z                     continue
2025-04-11T03:52:13.0025854Z                 else:
2025-04-11T03:52:13.0026074Z                     print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
2025-04-11T03:52:13.0026157Z >                   raise e
2025-04-11T03:52:13.0026164Z 
2025-04-11T03:52:13.0026259Z colossalai/testing/utils.py:144: 
2025-04-11T03:52:13.0026369Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:13.0026565Z colossalai/testing/utils.py:133: in _run_until_success
2025-04-11T03:52:13.0026653Z     ret = func(*args, **kwargs)
2025-04-11T03:52:13.0026829Z tests/test_zero/test_low_level/test_zero_ckpt.py:129: in test_zero_ckpt
2025-04-11T03:52:13.0026911Z     spawn(run_dist, 4)
2025-04-11T03:52:13.0027014Z colossalai/testing/utils.py:252: in spawn
2025-04-11T03:52:13.0027115Z     mp.spawn(wrapped_func, nprocs=nprocs)
2025-04-11T03:52:13.0027368Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
2025-04-11T03:52:13.0027549Z     return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
2025-04-11T03:52:13.0027833Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
2025-04-11T03:52:13.0027932Z     while not context.join():
2025-04-11T03:52:13.0028039Z _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
2025-04-11T03:52:13.0028045Z 
2025-04-11T03:52:13.0028246Z self = <torch.multiprocessing.spawn.ProcessContext object at 0x7f68f0389b10>
2025-04-11T03:52:13.0028324Z timeout = None
2025-04-11T03:52:13.0028382Z 
2025-04-11T03:52:13.0028523Z     def join(self, timeout=None):
2025-04-11T03:52:13.0028654Z         r"""Join one or more processes within spawn context.
2025-04-11T03:52:13.0028724Z     
2025-04-11T03:52:13.0028874Z         Attempt to join one or more processes in this spawn context.
2025-04-11T03:52:13.0029017Z         If one of them exited with a non-zero exit status, this function
2025-04-11T03:52:13.0029184Z         kills the remaining processes and raises an exception with the cause
2025-04-11T03:52:13.0029278Z         of the first process exiting.
2025-04-11T03:52:13.0029347Z     
2025-04-11T03:52:13.0029496Z         Returns ``True`` if all processes have been joined successfully,
2025-04-11T03:52:13.0029637Z         ``False`` if there are more processes that need to be joined.
2025-04-11T03:52:13.0029713Z     
2025-04-11T03:52:13.0029787Z         Args:
2025-04-11T03:52:13.0029925Z             timeout (float): Wait this long before giving up on waiting.
2025-04-11T03:52:13.0030004Z         """
2025-04-11T03:52:13.0030147Z         # Ensure this function can be called even when we're done.
2025-04-11T03:52:13.0030246Z         if len(self.sentinels) == 0:
2025-04-11T03:52:13.0030326Z             return True
2025-04-11T03:52:13.0030402Z     
2025-04-11T03:52:13.0030533Z         # Wait for any process to fail or all of them to succeed.
2025-04-11T03:52:13.0030651Z         ready = multiprocessing.connection.wait(
2025-04-11T03:52:13.0030747Z             self.sentinels.keys(),
2025-04-11T03:52:13.0030832Z             timeout=timeout,
2025-04-11T03:52:13.0030909Z         )
2025-04-11T03:52:13.0030980Z     
2025-04-11T03:52:13.0031062Z         error_index = None
2025-04-11T03:52:13.0031223Z         for sentinel in ready:
2025-04-11T03:52:13.0031331Z             index = self.sentinels.pop(sentinel)
2025-04-11T03:52:13.0031435Z             process = self.processes[index]
2025-04-11T03:52:13.0031521Z             process.join()
2025-04-11T03:52:13.0031619Z             if process.exitcode != 0:
2025-04-11T03:52:13.0031708Z                 error_index = index
2025-04-11T03:52:13.0031784Z                 break
2025-04-11T03:52:13.0031858Z     
2025-04-11T03:52:13.0031949Z         # Return if there was no error.
2025-04-11T03:52:13.0032038Z         if error_index is None:
2025-04-11T03:52:13.0032171Z             # Return whether or not all processes have been joined.
2025-04-11T03:52:13.0032338Z             return len(self.sentinels) == 0
2025-04-11T03:52:13.0032414Z     
2025-04-11T03:52:13.0032554Z         # Assume failure. Terminate processes that are still alive.
2025-04-11T03:52:13.0032656Z         for process in self.processes:
2025-04-11T03:52:13.0032745Z             if process.is_alive():
2025-04-11T03:52:13.0032837Z                 process.terminate()
2025-04-11T03:52:13.0032926Z             process.join()
2025-04-11T03:52:13.0033056Z     
2025-04-11T03:52:13.0033201Z         # There won't be an error on the queue if the process crashed.
2025-04-11T03:52:13.0033320Z         failed_process = self.processes[error_index]
2025-04-11T03:52:13.0033438Z         if self.error_queues[error_index].empty():
2025-04-11T03:52:13.0033562Z             exitcode = self.processes[error_index].exitcode
2025-04-11T03:52:13.0033652Z             if exitcode < 0:
2025-04-11T03:52:13.0033767Z                 name = signal.Signals(-exitcode).name
2025-04-11T03:52:13.0033876Z                 raise ProcessExitedException(
2025-04-11T03:52:13.0034028Z                     "process %d terminated with signal %s" % (error_index, name),
2025-04-11T03:52:13.0034123Z                     error_index=error_index,
2025-04-11T03:52:13.0034224Z                     error_pid=failed_process.pid,
2025-04-11T03:52:13.0034318Z                     exit_code=exitcode,
2025-04-11T03:52:13.0034407Z                     signal_name=name,
2025-04-11T03:52:13.0034485Z                 )
2025-04-11T03:52:13.0034561Z             else:
2025-04-11T03:52:13.0034666Z                 raise ProcessExitedException(
2025-04-11T03:52:13.0034828Z                     "process %d terminated with exit code %d" % (error_index, exitcode),
2025-04-11T03:52:13.0034997Z                     error_index=error_index,
2025-04-11T03:52:13.0035105Z                     error_pid=failed_process.pid,
2025-04-11T03:52:13.0035192Z                     exit_code=exitcode,
2025-04-11T03:52:13.0035269Z                 )
2025-04-11T03:52:13.0035343Z     
2025-04-11T03:52:13.0035476Z         original_trace = self.error_queues[error_index].get()
2025-04-11T03:52:13.0035647Z         msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
2025-04-11T03:52:13.0035735Z         msg += original_trace
2025-04-11T03:52:13.0035911Z >       raise ProcessRaisedException(msg, error_index, failed_process.pid)
2025-04-11T03:52:13.0036072Z E       torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.0036151Z E       
2025-04-11T03:52:13.0036277Z E       -- Process 0 terminated with the following error:
2025-04-11T03:52:13.0036373Z E       Traceback (most recent call last):
2025-04-11T03:52:13.0036674Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.0036756Z E           fn(i, *args)
2025-04-11T03:52:13.0037015Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 123, in run_dist
2025-04-11T03:52:13.0037112Z E           exam_zero_1_torch_ddp_ckpt()
2025-04-11T03:52:13.0037371Z E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.0037461Z E           partial_func(**kwargs)
2025-04-11T03:52:13.0037754Z E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 62, in exam_zero_1_torch_ddp_ckpt
2025-04-11T03:52:13.0037917Z E           torch_model = MlpModel().cuda()
2025-04-11T03:52:13.0038184Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.0038307Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.0038574Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.0038668Z E           module._apply(fn)
2025-04-11T03:52:13.0038932Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.0039093Z E           param_applied = fn(param)
2025-04-11T03:52:13.0039430Z E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.0039550Z E           return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.0039663Z E       RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.0039944Z E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.0040142Z E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.0040306Z E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.0040311Z 
2025-04-11T03:52:13.0040612Z /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
2025-04-11T03:52:13.0040761Z ----------------------------- Captured stdout call -----------------------------
2025-04-11T03:52:13.0040919Z [04/11/25 03:52:11] INFO     colossalai - colossalai - INFO:                    
2025-04-11T03:52:13.0041051Z                              /__w/ColossalAI/ColossalAI/colossalai/initialize.py
2025-04-11T03:52:13.0041157Z                              :75 launch                                         
2025-04-11T03:52:13.0041300Z                     INFO     colossalai - colossalai - INFO: Distributed        
2025-04-11T03:52:13.0041423Z                              environment is initialized, world size: 4          
2025-04-11T03:52:13.0041619Z Maximum number of attempts is reached or pattern is not matched, no more retrying...
2025-04-11T03:52:13.0041823Z ----------------------------- Captured stderr call -----------------------------
2025-04-11T03:52:13.0042121Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:37496 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:13.0042409Z [W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:37496 (errno: 99 - Cannot assign requested address).
2025-04-11T03:52:13.0042519Z =============================== warnings summary ===============================
2025-04-11T03:52:13.0042622Z colossalai/interface/model.py:45
2025-04-11T03:52:13.0042921Z   /__w/ColossalAI/ColossalAI/colossalai/interface/model.py:45: DeprecationWarning: invalid escape sequence '\S'
2025-04-11T03:52:13.0043139Z     to_return = {re.sub(f"lora_\S\.{adapter_name}\.(weight|bias)", "base_layer", k) for k in to_return}
2025-04-11T03:52:13.0043145Z 
2025-04-11T03:52:13.0043242Z colossalai/interface/model.py:45
2025-04-11T03:52:13.0043528Z   /__w/ColossalAI/ColossalAI/colossalai/interface/model.py:45: DeprecationWarning: invalid escape sequence '\.'
2025-04-11T03:52:13.0043733Z     to_return = {re.sub(f"lora_\S\.{adapter_name}\.(weight|bias)", "base_layer", k) for k in to_return}
2025-04-11T03:52:13.0043738Z 
2025-04-11T03:52:13.0043841Z colossalai/checkpoint_io/utils.py:862
2025-04-11T03:52:13.0044138Z   /__w/ColossalAI/ColossalAI/colossalai/checkpoint_io/utils.py:862: DeprecationWarning: invalid escape sequence '\.'
2025-04-11T03:52:13.0044247Z     reg = re.compile("(.*?).index((\..*)?).json")
2025-04-11T03:52:13.0044257Z 
2025-04-11T03:52:13.0044356Z colossalai/nn/optimizer/cpu_adam.py:12
2025-04-11T03:52:13.0044714Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/cpu_adam.py:12: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T03:52:13.0044795Z     """
2025-04-11T03:52:13.0044800Z 
2025-04-11T03:52:13.0044902Z colossalai/nn/optimizer/fused_adam.py:15
2025-04-11T03:52:13.0045206Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/fused_adam.py:15: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T03:52:13.0045300Z     """Implements Adam algorithm.
2025-04-11T03:52:13.0045305Z 
2025-04-11T03:52:13.0045408Z colossalai/nn/optimizer/hybrid_adam.py:12
2025-04-11T03:52:13.0045720Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/hybrid_adam.py:12: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T03:52:13.0045876Z     """Implements Adam algorithm.
2025-04-11T03:52:13.0045881Z 
2025-04-11T03:52:13.0046165Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34
2025-04-11T03:52:13.0047310Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
2025-04-11T03:52:13.0047546Z     deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T03:52:13.0047551Z 
2025-04-11T03:52:13.0047786Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896
2025-04-11T03:52:13.0047936Z tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T03:52:13.0048101Z tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T03:52:13.0048265Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.0048959Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
2025-04-11T03:52:13.0049053Z     warnings.warn(
2025-04-11T03:52:13.0049058Z 
2025-04-11T03:52:13.0049277Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T03:52:13.0049547Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T03:52:13.0049755Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T03:52:13.0049962Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T03:52:13.0050169Z ../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
2025-04-11T03:52:13.0050288Z tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T03:52:13.0050456Z tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T03:52:13.0050610Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.0051429Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
2025-04-11T03:52:13.0051517Z     warnings.warn(
2025-04-11T03:52:13.0051522Z 
2025-04-11T03:52:13.0051619Z <frozen importlib._bootstrap>:283
2025-04-11T03:52:13.0051758Z tests/test_config/test_load_config.py::test_load_config
2025-04-11T03:52:13.0051912Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.0052065Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.0052213Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.0052422Z tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.0052797Z   <frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead
2025-04-11T03:52:13.0052806Z 
2025-04-11T03:52:13.0052908Z colossalai/fx/profiler/dataflow.py:20
2025-04-11T03:52:13.0053215Z   /__w/ColossalAI/ColossalAI/colossalai/fx/profiler/dataflow.py:20: DeprecationWarning: invalid escape sequence '\_'
2025-04-11T03:52:13.0053292Z     """
2025-04-11T03:52:13.0053297Z 
2025-04-11T03:52:13.0053394Z colossalai/fx/profiler/dataflow.py:77
2025-04-11T03:52:13.0053756Z   /__w/ColossalAI/ColossalAI/colossalai/fx/profiler/dataflow.py:77: DeprecationWarning: invalid escape sequence '\_'
2025-04-11T03:52:13.0053932Z     """Analyze the autograd node dependencies and find out the memory usage.
2025-04-11T03:52:13.0053937Z 
2025-04-11T03:52:13.0054087Z colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py:31
2025-04-11T03:52:13.0054447Z   /__w/ColossalAI/ColossalAI/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py:31: DeprecationWarning: invalid escape sequence '\:'
2025-04-11T03:52:13.0054776Z     """A wrapper for optimizer. ``ShardedOptimizerV2`` and ``ShardedModelV2`` implement Zero Redundancy Optimizer (ZeRO).
2025-04-11T03:52:13.0054784Z 
2025-04-11T03:52:13.1416600Z colossalai/inference/utils.py:80
2025-04-11T03:52:13.1416957Z   /__w/ColossalAI/ColossalAI/colossalai/inference/utils.py:80: DeprecationWarning: invalid escape sequence '\.'
2025-04-11T03:52:13.1417097Z     reg = re.compile("(.*?).index((\..*)?).json")
2025-04-11T03:52:13.1417106Z 
2025-04-11T03:52:13.1417232Z colossalai/inference/executor/rpc_worker.py:188
2025-04-11T03:52:13.1417592Z   /__w/ColossalAI/ColossalAI/colossalai/inference/executor/rpc_worker.py:188: SyntaxWarning: "is" with a literal. Did you mean "=="?
2025-04-11T03:52:13.1417695Z     if arch is "BaichuanForCausalLM":
2025-04-11T03:52:13.1417700Z 
2025-04-11T03:52:13.1417856Z tests/test_infer/test_async_engine/test_async_engine.py:49
2025-04-11T03:52:13.1418572Z   /__w/ColossalAI/ColossalAI/tests/test_infer/test_async_engine/test_async_engine.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
2025-04-11T03:52:13.1418751Z     @pytest.mark.asyncio
2025-04-11T03:52:13.1418756Z 
2025-04-11T03:52:13.1418878Z tests/test_tensor/test_dtensor/test_dtensor.py:10
2025-04-11T03:52:13.1419469Z   /__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py:10: PytestCollectionWarning: cannot collect test class 'TestModel' because it has a __init__ constructor (from: tests/test_tensor/test_dtensor/test_dtensor.py)
2025-04-11T03:52:13.1419578Z     class TestModel(torch.nn.Module):
2025-04-11T03:52:13.1419582Z 
2025-04-11T03:52:13.1419703Z tests/test_zero/test_low_level/test_mem_leak.py:23
2025-04-11T03:52:13.1420349Z   /__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py:23: PytestCollectionWarning: cannot collect test class 'TestLowLevelZeroOptimizer' because it has a __init__ constructor (from: tests/test_zero/test_low_level/test_mem_leak.py)
2025-04-11T03:52:13.1420505Z     class TestLowLevelZeroOptimizer(LowLevelZeroOptimizer):
2025-04-11T03:52:13.1420512Z 
2025-04-11T03:52:13.1420633Z tests/test_booster/test_accelerator.py: 1 warning
2025-04-11T03:52:13.1420793Z tests/test_checkpoint_io/test_general_checkpoint_io.py: 1 warning
2025-04-11T03:52:13.1420972Z tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py: 1 warning
2025-04-11T03:52:13.1421127Z tests/test_checkpoint_io/test_safetensors_async_io.py: 1 warning
2025-04-11T03:52:13.1421230Z tests/test_fp8/test_fp8_cast.py: 1 warning
2025-04-11T03:52:13.1421437Z tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py: 2 warnings
2025-04-11T03:52:13.1421583Z tests/test_shardformer/test_flash_attention.py: 1 warning
2025-04-11T03:52:13.1421872Z tests/test_shardformer/test_with_torch_ddp.py: 1 warning
2025-04-11T03:52:13.1422089Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py: 1 warning
2025-04-11T03:52:13.1422316Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py: 1 warning
2025-04-11T03:52:13.1422533Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py: 1 warning
2025-04-11T03:52:13.1422689Z tests/test_shardformer/test_model/test_shard_bert.py: 1 warning
2025-04-11T03:52:13.1422838Z tests/test_shardformer/test_model/test_shard_blip2.py: 1 warning
2025-04-11T03:52:13.1422984Z tests/test_shardformer/test_model/test_shard_bloom.py: 1 warning
2025-04-11T03:52:13.1423220Z tests/test_shardformer/test_model/test_shard_chatglm2.py: 1 warning
2025-04-11T03:52:13.1423371Z tests/test_shardformer/test_model/test_shard_command.py: 1 warning
2025-04-11T03:52:13.1423538Z tests/test_shardformer/test_model/test_shard_falcon.py: 1 warning
2025-04-11T03:52:13.1423693Z tests/test_shardformer/test_model/test_shard_gpt2.py: 1 warning
2025-04-11T03:52:13.1423852Z tests/test_shardformer/test_model/test_shard_llama.py: 1 warning
2025-04-11T03:52:13.1424091Z tests/test_shardformer/test_model/test_shard_mistral.py: 1 warning
2025-04-11T03:52:13.1424238Z tests/test_shardformer/test_model/test_shard_opt.py: 1 warning
2025-04-11T03:52:13.1424391Z tests/test_shardformer/test_model/test_shard_qwen2.py: 1 warning
2025-04-11T03:52:13.1424535Z tests/test_shardformer/test_model/test_shard_sam.py: 1 warning
2025-04-11T03:52:13.1424680Z tests/test_shardformer/test_model/test_shard_t5.py: 1 warning
2025-04-11T03:52:13.1424820Z tests/test_shardformer/test_model/test_shard_vit.py: 1 warning
2025-04-11T03:52:13.1424978Z tests/test_shardformer/test_model/test_shard_whisper.py: 1 warning
2025-04-11T03:52:13.1425558Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:13.1425652Z     warnings.warn(
2025-04-11T03:52:13.1425657Z 
2025-04-11T03:52:13.1425775Z tests/test_booster/test_accelerator.py: 1 warning
2025-04-11T03:52:13.1425930Z tests/test_checkpoint_io/test_general_checkpoint_io.py: 1 warning
2025-04-11T03:52:13.1426108Z tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py: 1 warning
2025-04-11T03:52:13.1426312Z tests/test_checkpoint_io/test_safetensors_async_io.py: 1 warning
2025-04-11T03:52:13.1426420Z tests/test_fp8/test_fp8_cast.py: 1 warning
2025-04-11T03:52:13.1426606Z tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py: 2 warnings
2025-04-11T03:52:13.1426747Z tests/test_shardformer/test_flash_attention.py: 1 warning
2025-04-11T03:52:13.1426879Z tests/test_shardformer/test_with_torch_ddp.py: 1 warning
2025-04-11T03:52:13.1427104Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py: 1 warning
2025-04-11T03:52:13.1427337Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py: 1 warning
2025-04-11T03:52:13.1427558Z tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py: 1 warning
2025-04-11T03:52:13.1427713Z tests/test_shardformer/test_model/test_shard_bert.py: 1 warning
2025-04-11T03:52:13.1427858Z tests/test_shardformer/test_model/test_shard_blip2.py: 1 warning
2025-04-11T03:52:13.1428007Z tests/test_shardformer/test_model/test_shard_bloom.py: 1 warning
2025-04-11T03:52:13.1428169Z tests/test_shardformer/test_model/test_shard_chatglm2.py: 1 warning
2025-04-11T03:52:13.1428325Z tests/test_shardformer/test_model/test_shard_command.py: 1 warning
2025-04-11T03:52:13.1428564Z tests/test_shardformer/test_model/test_shard_falcon.py: 1 warning
2025-04-11T03:52:13.1428716Z tests/test_shardformer/test_model/test_shard_gpt2.py: 1 warning
2025-04-11T03:52:13.1428866Z tests/test_shardformer/test_model/test_shard_llama.py: 1 warning
2025-04-11T03:52:13.1429019Z tests/test_shardformer/test_model/test_shard_mistral.py: 1 warning
2025-04-11T03:52:13.1429250Z tests/test_shardformer/test_model/test_shard_opt.py: 1 warning
2025-04-11T03:52:13.1429395Z tests/test_shardformer/test_model/test_shard_qwen2.py: 1 warning
2025-04-11T03:52:13.1429537Z tests/test_shardformer/test_model/test_shard_sam.py: 1 warning
2025-04-11T03:52:13.1429684Z tests/test_shardformer/test_model/test_shard_t5.py: 1 warning
2025-04-11T03:52:13.1429827Z tests/test_shardformer/test_model/test_shard_vit.py: 1 warning
2025-04-11T03:52:13.1429992Z tests/test_shardformer/test_model/test_shard_whisper.py: 1 warning
2025-04-11T03:52:13.1430542Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
2025-04-11T03:52:13.1430706Z     warnings.warn(
2025-04-11T03:52:13.1430711Z 
2025-04-11T03:52:13.1430907Z tests/test_infer/test_async_engine/test_async_engine.py::test_new_requests_event
2025-04-11T03:52:13.1431389Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/_pytest/python.py:183: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
2025-04-11T03:52:13.1431632Z   You need to install a suitable plugin for your async framework, for example:
2025-04-11T03:52:13.1431714Z     - anyio
2025-04-11T03:52:13.1431820Z     - pytest-asyncio
2025-04-11T03:52:13.1431907Z     - pytest-tornasync
2025-04-11T03:52:13.1431996Z     - pytest-trio
2025-04-11T03:52:13.1432080Z     - pytest-twisted
2025-04-11T03:52:13.1432259Z     warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))
2025-04-11T03:52:13.1432269Z 
2025-04-11T03:52:13.1432533Z tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
2025-04-11T03:52:13.1433420Z   /__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
2025-04-11T03:52:13.1433528Z     numel += p.storage().size()
2025-04-11T03:52:13.1433533Z 
2025-04-11T03:52:13.1433702Z tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
2025-04-11T03:52:13.1434945Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
2025-04-11T03:52:13.1435153Z     warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
2025-04-11T03:52:13.1435166Z 
2025-04-11T03:52:13.1435336Z tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
2025-04-11T03:52:13.1435797Z   /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
2025-04-11T03:52:13.1435966Z     warnings.warn("To get the last learning rate computed by the scheduler, "
2025-04-11T03:52:13.1435973Z 
2025-04-11T03:52:13.1436154Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2025-04-11T03:52:13.1436282Z ============================== slowest durations ===============================
2025-04-11T03:52:13.1436477Z 16.61s call     tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.1436682Z 15.30s call     tests/test_infer/test_continuous_batching.py::test_continuous_batching
2025-04-11T03:52:13.1436888Z 12.61s call     tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter
2025-04-11T03:52:13.1437158Z 10.71s call     tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4]
2025-04-11T03:52:13.1437351Z 9.38s call     tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin
2025-04-11T03:52:13.1437500Z 8.94s call     tests/test_optimizer/test_dist_came.py::test_dist_came
2025-04-11T03:52:13.1437672Z 8.61s call     tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin
2025-04-11T03:52:13.1437841Z 8.47s call     tests/test_zero/test_gemini/test_inference.py::test_inference[4]
2025-04-11T03:52:13.1438047Z 8.47s call     tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO
2025-04-11T03:52:13.1438305Z 8.34s call     tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4]
2025-04-11T03:52:13.1438463Z 8.11s call     tests/test_zero/test_gemini/test_optim.py::test_optim[4]
2025-04-11T03:52:13.1438647Z 8.04s call     tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4]
2025-04-11T03:52:13.1438974Z 7.74s call     tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4]
2025-04-11T03:52:13.1439129Z 7.65s call     tests/test_optimizer/test_dist_lamb.py::test_dist_lamb
2025-04-11T03:52:13.1439393Z 7.65s call     tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin
2025-04-11T03:52:13.1439568Z 7.64s call     tests/test_optimizer/test_dist_galore.py::test_dist_galore
2025-04-11T03:52:13.1439773Z 7.58s call     tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2]
2025-04-11T03:52:13.1439946Z 7.43s call     tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2]
2025-04-11T03:52:13.1440118Z 7.30s call     tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor
2025-04-11T03:52:13.1440332Z 7.25s call     tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin
2025-04-11T03:52:13.1440477Z 7.11s call     tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp
2025-04-11T03:52:13.1440704Z 6.73s call     tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin
2025-04-11T03:52:13.1440983Z 6.52s call     tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2]
2025-04-11T03:52:13.1441137Z 6.48s call     tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all
2025-04-11T03:52:13.1441345Z 6.46s call     tests/test_lora/test_lora.py::test_torch_ddp_lora
2025-04-11T03:52:13.1441489Z 6.43s call     tests/test_infer/test_streamingllm.py::test_engine
2025-04-11T03:52:13.1441676Z 6.42s call     tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation
2025-04-11T03:52:13.1441840Z 6.32s call     tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single
2025-04-11T03:52:13.1442005Z 6.24s call     tests/test_zero/test_gemini/test_inference.py::test_inference[1]
2025-04-11T03:52:13.1442153Z 6.19s call     tests/test_fp8/test_fp8_allreduce.py::test_all_reduce
2025-04-11T03:52:13.1442307Z 6.19s call     tests/test_zero/test_gemini/test_search.py::test_search[4]
2025-04-11T03:52:13.1442504Z 6.15s call     tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4]
2025-04-11T03:52:13.1442677Z 6.06s call     tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4]
2025-04-11T03:52:13.1442845Z 5.99s call     tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1]
2025-04-11T03:52:13.1442991Z 5.91s call     tests/test_fp8/test_fp8_allgather.py::test_all_gather
2025-04-11T03:52:13.1443153Z 5.90s call     tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt
2025-04-11T03:52:13.1443333Z 5.89s call     tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring
2025-04-11T03:52:13.1443521Z 5.86s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6]
2025-04-11T03:52:13.1443707Z 5.83s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4]
2025-04-11T03:52:13.1443891Z 5.81s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4]
2025-04-11T03:52:13.1444125Z 5.79s call     tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2
2025-04-11T03:52:13.1444299Z 5.78s call     tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single
2025-04-11T03:52:13.1444508Z 5.76s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12]
2025-04-11T03:52:13.1444671Z 5.69s call     tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter
2025-04-11T03:52:13.1444826Z 5.40s call     tests/test_device/test_init_logical_pg.py::test_logical_pg
2025-04-11T03:52:13.1444962Z 5.35s call     tests/test_infer/test_drafter.py::test_spec_dec
2025-04-11T03:52:13.1445233Z 5.08s call     tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader
2025-04-11T03:52:13.1445392Z 5.04s call     tests/test_tensor/test_comm_spec_apply.py::test_comm_spec
2025-04-11T03:52:13.1445578Z 5.01s call     tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm
2025-04-11T03:52:13.1445748Z 4.97s call     tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2
2025-04-11T03:52:13.1446067Z 4.96s call     tests/test_cluster/test_process_group_mesh.py::test_process_group_mesh
2025-04-11T03:52:13.1446382Z 4.96s call     tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy
2025-04-11T03:52:13.1446564Z 4.93s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4]
2025-04-11T03:52:13.1446761Z 4.93s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12]
2025-04-11T03:52:13.1446949Z 4.92s call     tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d
2025-04-11T03:52:13.1447227Z 4.91s call     tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention
2025-04-11T03:52:13.1447445Z 4.90s call     tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T03:52:13.1447613Z 4.89s call     tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec
2025-04-11T03:52:13.1447792Z 4.88s call     tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp
2025-04-11T03:52:13.1447963Z 4.87s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4]
2025-04-11T03:52:13.1448150Z 4.85s call     tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager
2025-04-11T03:52:13.1448374Z 4.81s call     tests/test_tensor/test_shape_consistency_apply.py::test_apply
2025-04-11T03:52:13.1448540Z 4.80s call     tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd
2025-04-11T03:52:13.1448733Z 4.71s call     tests/test_device/test_device_mesh.py::test_device_mesh_from_process_group
2025-04-11T03:52:13.1448874Z 4.71s call     tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T03:52:13.1449099Z 4.70s call     tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO
2025-04-11T03:52:13.1449283Z 4.65s call     tests/test_cluster/test_device_mesh_manager.py::test_device_mesh_manager
2025-04-11T03:52:13.1449493Z 4.58s call     tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt
2025-04-11T03:52:13.1449664Z 4.45s call     tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T03:52:13.1449849Z 4.28s call     tests/test_infer/test_config_and_struct.py::test_config_and_inference
2025-04-11T03:52:13.1450008Z 4.23s call     tests/test_tensor/test_padded_tensor.py::test_padded_tensor
2025-04-11T03:52:13.1450211Z 4.18s call     tests/test_infer/test_request_handler.py::test_running_list_and_request_handler
2025-04-11T03:52:13.1450386Z 4.08s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2]
2025-04-11T03:52:13.1450565Z 4.07s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6]
2025-04-11T03:52:13.1450748Z 4.07s call     tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation
2025-04-11T03:52:13.1450984Z 4.07s call     tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding
2025-04-11T03:52:13.1451225Z 4.07s call     tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2]
2025-04-11T03:52:13.1451407Z 4.05s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4]
2025-04-11T03:52:13.1451625Z 4.04s call     tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T03:52:13.1451802Z 4.00s call     tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn
2025-04-11T03:52:13.1451969Z 3.97s call     tests/test_shardformer/test_layer/test_linear_1d.py::test_linear
2025-04-11T03:52:13.1452148Z 3.97s call     tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p
2025-04-11T03:52:13.1452467Z 3.93s call     tests/test_infer/test_kvcache_manager.py::test_cache_manager
2025-04-11T03:52:13.1452677Z 3.90s call     tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor
2025-04-11T03:52:13.1452906Z 3.89s call     tests/test_shardformer/test_layer/test_dropout.py::test_dropout
2025-04-11T03:52:13.1453092Z 3.88s call     tests/test_zero/test_gemini/test_search.py::test_search[1]
2025-04-11T03:52:13.1453264Z 3.69s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1]
2025-04-11T03:52:13.1453625Z 0.78s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1453907Z 0.76s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
2025-04-11T03:52:13.1454164Z 0.68s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-True]
2025-04-11T03:52:13.1454419Z 0.64s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-False]
2025-04-11T03:52:13.1454665Z 0.59s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False]
2025-04-11T03:52:13.1454820Z 0.51s setup    tests/test_infer/test_drafter.py::test_drafter[5]
2025-04-11T03:52:13.1455108Z 0.47s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False]
2025-04-11T03:52:13.1455432Z 0.37s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32]
2025-04-11T03:52:13.1455751Z 0.35s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True]
2025-04-11T03:52:13.1456021Z 0.35s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True]
2025-04-11T03:52:13.1456270Z 0.32s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False]
2025-04-11T03:52:13.1456563Z 0.32s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1456804Z 0.31s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True]
2025-04-11T03:52:13.1457096Z 0.30s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1457385Z 0.28s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1457542Z 0.28s call     tests/test_shardformer/test_with_torch_ddp.py::test_gpt2
2025-04-11T03:52:13.1457754Z 0.27s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False]
2025-04-11T03:52:13.1457955Z 0.26s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True]
2025-04-11T03:52:13.1458238Z 0.22s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2]
2025-04-11T03:52:13.1458406Z 0.22s setup    tests/test_infer/test_kvcache_manager.py::test_logical_blocks
2025-04-11T03:52:13.1458691Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3]
2025-04-11T03:52:13.1459022Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0]
2025-04-11T03:52:13.1459296Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device1]
2025-04-11T03:52:13.1459571Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3]
2025-04-11T03:52:13.1459845Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0]
2025-04-11T03:52:13.1460179Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2]
2025-04-11T03:52:13.1460331Z 0.21s setup    tests/test_fp8/test_fp8_allreduce.py::test_all_reduce
2025-04-11T03:52:13.1460598Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2]
2025-04-11T03:52:13.1460873Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4]
2025-04-11T03:52:13.1461205Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3]
2025-04-11T03:52:13.1461484Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4]
2025-04-11T03:52:13.1461753Z 0.21s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4]
2025-04-11T03:52:13.1462028Z 0.20s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4]
2025-04-11T03:52:13.1462170Z 0.20s call     tests/test_fp8/test_fp8_hook.py::test_fp8_hook
2025-04-11T03:52:13.1462332Z 0.20s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True]
2025-04-11T03:52:13.1462471Z 0.20s setup    tests/test_fp8/test_fp8_cast.py::test_fp8_cast
2025-04-11T03:52:13.1462637Z 0.19s setup    tests/test_infer/test_kvcache_manager.py::test_cache_manager
2025-04-11T03:52:13.1462928Z 0.18s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1463276Z 0.18s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1463558Z 0.18s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1463850Z 0.17s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1464110Z 0.17s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7]
2025-04-11T03:52:13.1464400Z 0.17s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1464618Z 0.17s setup    tests/test_infer/test_async_engine/test_async_engine.py::test_new_requests_event
2025-04-11T03:52:13.1464945Z 0.16s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32]
2025-04-11T03:52:13.1465234Z 0.16s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1465486Z 0.16s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4]
2025-04-11T03:52:13.1465752Z 0.16s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device1]
2025-04-11T03:52:13.1466000Z 0.16s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32]
2025-04-11T03:52:13.1466318Z 0.16s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7]
2025-04-11T03:52:13.1466579Z 0.16s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device1]
2025-04-11T03:52:13.1466770Z 0.15s setup    tests/test_tensor/test_padded_tensor.py::test_padded_tensor
2025-04-11T03:52:13.1466918Z 0.15s setup    tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp
2025-04-11T03:52:13.1467172Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4]
2025-04-11T03:52:13.1467423Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4]
2025-04-11T03:52:13.1467748Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention
2025-04-11T03:52:13.1468095Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32]
2025-04-11T03:52:13.1468478Z 0.15s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0]
2025-04-11T03:52:13.1468754Z 0.15s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True]
2025-04-11T03:52:13.1469059Z 0.15s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1469317Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4]
2025-04-11T03:52:13.1469604Z 0.15s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1469946Z 0.15s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7]
2025-04-11T03:52:13.1470218Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3]
2025-04-11T03:52:13.1470505Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device1]
2025-04-11T03:52:13.1470779Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3]
2025-04-11T03:52:13.1470921Z 0.15s setup    tests/test_fp8/test_fp8_hook.py::test_fp8_hook
2025-04-11T03:52:13.1471253Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3]
2025-04-11T03:52:13.1471403Z 0.15s setup    tests/test_infer/test_drafter.py::test_spec_dec
2025-04-11T03:52:13.1471688Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3]
2025-04-11T03:52:13.1471947Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7]
2025-04-11T03:52:13.1472192Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4]
2025-04-11T03:52:13.1472464Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3]
2025-04-11T03:52:13.1472758Z 0.15s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1472912Z 0.15s setup    tests/test_fp8/test_fp8_allgather.py::test_all_gather
2025-04-11T03:52:13.1473183Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3]
2025-04-11T03:52:13.1473442Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32]
2025-04-11T03:52:13.1473657Z 0.15s setup    tests/test_infer/test_request_handler.py::test_running_list_and_request_handler
2025-04-11T03:52:13.1473903Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4]
2025-04-11T03:52:13.1474135Z 0.15s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True]
2025-04-11T03:52:13.1474289Z 0.15s setup    tests/test_infer/test_streamingllm.py::test_engine
2025-04-11T03:52:13.1474545Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4]
2025-04-11T03:52:13.1474820Z 0.15s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4]
2025-04-11T03:52:13.1475068Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4]
2025-04-11T03:52:13.1475391Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32]
2025-04-11T03:52:13.1475647Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4]
2025-04-11T03:52:13.1475896Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32]
2025-04-11T03:52:13.1476066Z 0.15s setup    tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter
2025-04-11T03:52:13.1476373Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32]
2025-04-11T03:52:13.1476643Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0]
2025-04-11T03:52:13.1476908Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7]
2025-04-11T03:52:13.1477225Z 0.15s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32]
2025-04-11T03:52:13.1477480Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32]
2025-04-11T03:52:13.1477730Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7]
2025-04-11T03:52:13.1477980Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7]
2025-04-11T03:52:13.1478242Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4]
2025-04-11T03:52:13.1478493Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4]
2025-04-11T03:52:13.1478812Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7]
2025-04-11T03:52:13.1479073Z 0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device1]
2025-04-11T03:52:13.1479319Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4]
2025-04-11T03:52:13.1479597Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7]
2025-04-11T03:52:13.1479807Z 0.15s setup    tests/test_infer/test_async_engine/test_request_tracer.py::test_request_tracer
2025-04-11T03:52:13.1480075Z 0.15s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32]
2025-04-11T03:52:13.1480319Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7]
2025-04-11T03:52:13.1480589Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device1]
2025-04-11T03:52:13.1480850Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4]
2025-04-11T03:52:13.1481158Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1481335Z 0.14s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False]
2025-04-11T03:52:13.1481596Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32]
2025-04-11T03:52:13.1481861Z 0.14s setup    tests/test_infer/test_continuous_batching.py::test_continuous_batching
2025-04-11T03:52:13.1482037Z 0.14s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False]
2025-04-11T03:52:13.1482325Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1482596Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0]
2025-04-11T03:52:13.1482892Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1483236Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1483515Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention
2025-04-11T03:52:13.1483804Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1484152Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1484434Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1484725Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1485012Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1485307Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1485604Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1485876Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0]
2025-04-11T03:52:13.1486134Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7]
2025-04-11T03:52:13.1486477Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0]
2025-04-11T03:52:13.1486770Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1487022Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32]
2025-04-11T03:52:13.1487295Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2]
2025-04-11T03:52:13.1487549Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32]
2025-04-11T03:52:13.1487775Z 0.14s setup    tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4]
2025-04-11T03:52:13.1488047Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0]
2025-04-11T03:52:13.1488323Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2]
2025-04-11T03:52:13.1488588Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4]
2025-04-11T03:52:13.1488862Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4]
2025-04-11T03:52:13.1489133Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0]
2025-04-11T03:52:13.1489447Z 0.14s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7]
2025-04-11T03:52:13.1489715Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4]
2025-04-11T03:52:13.1489976Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4]
2025-04-11T03:52:13.1490252Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4]
2025-04-11T03:52:13.1490513Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2]
2025-04-11T03:52:13.1490894Z 0.14s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7]
2025-04-11T03:52:13.1491158Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4]
2025-04-11T03:52:13.1491424Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2]
2025-04-11T03:52:13.1491886Z 0.14s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7]
2025-04-11T03:52:13.1492146Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2]
2025-04-11T03:52:13.1492303Z 0.14s setup    tests/test_infer/test_batch_bucket.py::test_bucket
2025-04-11T03:52:13.1492560Z 0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2]
2025-04-11T03:52:13.1492754Z 0.14s setup    tests/test_infer/test_config_and_struct.py::test_config_and_inference
2025-04-11T03:52:13.1493040Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1493358Z 0.14s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32]
2025-04-11T03:52:13.1493644Z 0.14s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1493914Z 0.14s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4]
2025-04-11T03:52:13.1494276Z 0.14s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7]
2025-04-11T03:52:13.1494569Z 0.13s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1494790Z 0.13s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2]
2025-04-11T03:52:13.1495068Z 0.13s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0]
2025-04-11T03:52:13.1495541Z 0.13s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1495826Z 0.13s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1496136Z 0.13s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7]
2025-04-11T03:52:13.1496416Z 0.13s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1496611Z 0.13s setup    tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single
2025-04-11T03:52:13.1496830Z 0.13s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4]
2025-04-11T03:52:13.1497084Z 0.13s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32]
2025-04-11T03:52:13.1497450Z 0.13s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32]
2025-04-11T03:52:13.1497723Z 0.13s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device1]
2025-04-11T03:52:13.1497906Z 0.13s setup    tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0]
2025-04-11T03:52:13.1498066Z 0.13s setup    tests/test_zero/test_gemini/test_search.py::test_search[4]
2025-04-11T03:52:13.1498352Z 0.13s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1498702Z 0.13s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1498878Z 0.13s setup    tests/test_shardformer/test_layer/test_dropout.py::test_dropout
2025-04-11T03:52:13.1499126Z 0.13s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32]
2025-04-11T03:52:13.1499330Z 0.13s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12]
2025-04-11T03:52:13.1499575Z 0.12s setup    tests/test_shardformer/test_flash_attention.py::test_flash_attn_func
2025-04-11T03:52:13.1499808Z 0.12s setup    tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention
2025-04-11T03:52:13.1500058Z 0.12s setup    tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO
2025-04-11T03:52:13.1500275Z 0.12s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8]
2025-04-11T03:52:13.1500475Z 0.12s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4]
2025-04-11T03:52:13.1500730Z 0.12s setup    tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py::test_t5_pipeline_distribution
2025-04-11T03:52:13.1500991Z 0.12s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4]
2025-04-11T03:52:13.1501279Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1501560Z 0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False]
2025-04-11T03:52:13.1501780Z 0.12s setup    tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd
2025-04-11T03:52:13.1501968Z 0.12s setup    tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation
2025-04-11T03:52:13.1502121Z 0.12s setup    tests/test_optimizer/test_dist_lamb.py::test_dist_lamb
2025-04-11T03:52:13.1502310Z 0.12s setup    tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
2025-04-11T03:52:13.1502488Z 0.12s setup    tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2]
2025-04-11T03:52:13.1502655Z 0.12s setup    tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1]
2025-04-11T03:52:13.1502850Z 0.12s setup    tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager
2025-04-11T03:52:13.1503146Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1503320Z 0.12s setup    tests/test_zero/test_gemini/test_inference.py::test_inference[4]
2025-04-11T03:52:13.1503489Z 0.12s setup    tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2
2025-04-11T03:52:13.1503646Z 0.12s setup    tests/test_optimizer/test_dist_came.py::test_dist_came
2025-04-11T03:52:13.1503806Z 0.12s setup    tests/test_optimizer/test_dist_galore.py::test_dist_galore
2025-04-11T03:52:13.1503981Z 0.12s setup    tests/test_shardformer/test_model/test_shard_bert.py::test_bert
2025-04-11T03:52:13.1504179Z 0.12s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12]
2025-04-11T03:52:13.1504357Z 0.12s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4]
2025-04-11T03:52:13.1504635Z 0.12s setup    tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T03:52:13.1504802Z 0.12s setup    tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt
2025-04-11T03:52:13.1505041Z 0.12s setup    tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding
2025-04-11T03:52:13.1505223Z 0.12s setup    tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn
2025-04-11T03:52:13.1505411Z 0.12s setup    tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring
2025-04-11T03:52:13.1505588Z 0.12s setup    tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon
2025-04-11T03:52:13.1505817Z 0.12s setup    tests/test_shardformer/test_layer/test_linear_1d.py::test_linear
2025-04-11T03:52:13.1506014Z 0.12s setup    tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4]
2025-04-11T03:52:13.1506254Z 0.12s setup    tests/test_tensor/test_dtensor/test_dtensor_sharding_spec.py::test_dtensor_sharding_spec
2025-04-11T03:52:13.1506442Z 0.12s setup    tests/test_tensor/test_shape_consistency.py::test_one_step_transform
2025-04-11T03:52:13.1506680Z 0.12s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4]
2025-04-11T03:52:13.1506863Z 0.12s setup    tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2]
2025-04-11T03:52:13.1507020Z 0.12s setup    tests/test_zero/test_gemini/test_search.py::test_search[1]
2025-04-11T03:52:13.1507185Z 0.12s setup    tests/test_zero/test_gemini/test_inference.py::test_inference[1]
2025-04-11T03:52:13.1507350Z 0.12s setup    tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor
2025-04-11T03:52:13.1507525Z 0.12s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1]
2025-04-11T03:52:13.1507703Z 0.12s setup    tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm
2025-04-11T03:52:13.1507873Z 0.12s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2]
2025-04-11T03:52:13.1508166Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1508328Z 0.12s setup    tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2
2025-04-11T03:52:13.1508552Z 0.12s setup    tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation
2025-04-11T03:52:13.1508901Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2]
2025-04-11T03:52:13.1509066Z 0.12s setup    tests/test_tensor/test_sharding_spec.py::test_sharding_spec
2025-04-11T03:52:13.1509340Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2]
2025-04-11T03:52:13.1509523Z 0.12s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4]
2025-04-11T03:52:13.1509689Z 0.12s setup    tests/test_zero/test_gemini/test_optim.py::test_optim[4]
2025-04-11T03:52:13.1509911Z 0.12s setup    tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv
2025-04-11T03:52:13.1510098Z 0.12s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6]
2025-04-11T03:52:13.1510275Z 0.12s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6]
2025-04-11T03:52:13.1510543Z 0.12s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4]
2025-04-11T03:52:13.1510775Z 0.12s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_get_batch_size
2025-04-11T03:52:13.1510964Z 0.12s setup    tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d
2025-04-11T03:52:13.1511144Z 0.12s setup    tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel
2025-04-11T03:52:13.1511396Z 0.12s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7]
2025-04-11T03:52:13.1511671Z 0.12s setup    tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin
2025-04-11T03:52:13.1511958Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1512248Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1512491Z 0.12s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False]
2025-04-11T03:52:13.1512774Z 0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True]
2025-04-11T03:52:13.1513113Z 0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2]
2025-04-11T03:52:13.1513352Z 0.12s setup    tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4]
2025-04-11T03:52:13.1513517Z 0.12s setup    tests/test_moe/test_kernel.py::test_moe_kernel[data_type0]
2025-04-11T03:52:13.1513805Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1514155Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1514439Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1514725Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1515008Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1515290Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1515572Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1515843Z 0.12s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm
2025-04-11T03:52:13.1516022Z 0.12s setup    tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
2025-04-11T03:52:13.1516326Z 0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-False]
2025-04-11T03:52:13.1516617Z 0.12s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1516895Z 0.11s call     tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4]
2025-04-11T03:52:13.1517050Z 0.11s setup    tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all
2025-04-11T03:52:13.1517331Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1517603Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device1]
2025-04-11T03:52:13.1517827Z 0.11s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16]
2025-04-11T03:52:13.1518118Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1518365Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True]
2025-04-11T03:52:13.1518651Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1518828Z 0.11s setup    tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor
2025-04-11T03:52:13.1519096Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device1]
2025-04-11T03:52:13.1519433Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3]
2025-04-11T03:52:13.1519628Z 0.11s setup    tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO
2025-04-11T03:52:13.1519898Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0]
2025-04-11T03:52:13.1520084Z 0.11s setup    tests/test_cluster/test_process_group_mesh.py::test_process_group_mesh
2025-04-11T03:52:13.1520255Z 0.11s setup    tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single
2025-04-11T03:52:13.1520470Z 0.11s setup    tests/test_config/test_load_config.py::test_load_config
2025-04-11T03:52:13.1520633Z 0.11s setup    tests/test_device/test_init_logical_pg.py::test_logical_pg
2025-04-11T03:52:13.1520872Z 0.11s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False]
2025-04-11T03:52:13.1521064Z 0.11s setup    tests/test_cluster/test_device_mesh_manager.py::test_device_mesh_manager
2025-04-11T03:52:13.1521434Z 0.11s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7]
2025-04-11T03:52:13.1521645Z 0.11s setup    tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt
2025-04-11T03:52:13.1521857Z 0.11s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True]
2025-04-11T03:52:13.1522122Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device1]
2025-04-11T03:52:13.1522377Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False]
2025-04-11T03:52:13.1522681Z 0.11s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7]
2025-04-11T03:52:13.1522963Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3]
2025-04-11T03:52:13.1523234Z 0.11s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7]
2025-04-11T03:52:13.1523497Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
2025-04-11T03:52:13.1523831Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4]
2025-04-11T03:52:13.1524102Z 0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3]
2025-04-11T03:52:13.1524339Z 0.11s setup    tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin
2025-04-11T03:52:13.1524612Z 0.11s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32]
2025-04-11T03:52:13.1524833Z 0.11s setup    tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2]
2025-04-11T03:52:13.1525087Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False]
2025-04-11T03:52:13.1525289Z 0.11s setup    tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin
2025-04-11T03:52:13.1525538Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True]
2025-04-11T03:52:13.1525752Z 0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint
2025-04-11T03:52:13.1526047Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1526200Z 0.11s setup    tests/test_device/test_device_mesh.py::test_device_mesh
2025-04-11T03:52:13.1526418Z 0.11s setup    tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin
2025-04-11T03:52:13.1526784Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1527075Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1527284Z 0.11s setup    tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader
2025-04-11T03:52:13.1527598Z 0.11s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32]
2025-04-11T03:52:13.1527802Z 0.11s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True]
2025-04-11T03:52:13.1528116Z 0.11s call     tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention
2025-04-11T03:52:13.1528413Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1528700Z 0.11s call     tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4]
2025-04-11T03:52:13.1529042Z 0.11s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1529347Z 0.11s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32]
2025-04-11T03:52:13.1529565Z 0.10s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False]
2025-04-11T03:52:13.1529783Z 0.10s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8]
2025-04-11T03:52:13.1530061Z 0.10s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4]
2025-04-11T03:52:13.1530221Z 0.10s setup    tests/test_shardformer/test_with_torch_ddp.py::test_gpt2
2025-04-11T03:52:13.1530397Z 0.10s setup    tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2
2025-04-11T03:52:13.1530554Z 0.10s setup    tests/test_booster/test_accelerator.py::test_accelerator
2025-04-11T03:52:13.1530792Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False]
2025-04-11T03:52:13.1531042Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32]
2025-04-11T03:52:13.1531301Z 0.10s setup    tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4]
2025-04-11T03:52:13.1531590Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1531876Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1532103Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16]
2025-04-11T03:52:13.1532416Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7]
2025-04-11T03:52:13.1532656Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False]
2025-04-11T03:52:13.1532944Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4]
2025-04-11T03:52:13.1533136Z 0.10s setup    tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2
2025-04-11T03:52:13.1533393Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4]
2025-04-11T03:52:13.1533637Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True]
2025-04-11T03:52:13.1533879Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True]
2025-04-11T03:52:13.1534180Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1534528Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1534716Z 0.10s setup    tests/test_shardformer/test_model/test_shard_llama.py::test_llama
2025-04-11T03:52:13.1534896Z 0.10s setup    tests/test_shardformer/test_model/test_shard_vit.py::test_vit
2025-04-11T03:52:13.1535195Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1535357Z 0.10s setup    tests/test_tensor/test_comm_spec_apply.py::test_comm_spec
2025-04-11T03:52:13.1535595Z 0.10s setup    tests/test_shardformer/test_model/test_shard_sam.py::test_sam
2025-04-11T03:52:13.1535887Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1536151Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4]
2025-04-11T03:52:13.1536395Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2]
2025-04-11T03:52:13.1536706Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False]
2025-04-11T03:52:13.1536904Z 0.10s setup    tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper
2025-04-11T03:52:13.1537101Z 0.10s setup    tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral
2025-04-11T03:52:13.1537283Z 0.10s setup    tests/test_shardformer/test_model/test_shard_t5.py::test_t5
2025-04-11T03:52:13.1537523Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False]
2025-04-11T03:52:13.1537757Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False]
2025-04-11T03:52:13.1537961Z 0.10s setup    tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4]
2025-04-11T03:52:13.1538190Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T03:52:13.1538365Z 0.10s call     tests/test_moe/test_kernel.py::test_moe_kernel[data_type0]
2025-04-11T03:52:13.1538660Z 0.10s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1538911Z 0.10s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4]
2025-04-11T03:52:13.1539232Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1539508Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4]
2025-04-11T03:52:13.1539752Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False]
2025-04-11T03:52:13.1539932Z 0.10s call     tests/test_shardformer/test_shard_utils.py::test_release_layer
2025-04-11T03:52:13.1540204Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4]
2025-04-11T03:52:13.1540458Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32]
2025-04-11T03:52:13.1540749Z 0.10s setup    tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4]
2025-04-11T03:52:13.1540983Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True]
2025-04-11T03:52:13.1541252Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4]
2025-04-11T03:52:13.1541485Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False]
2025-04-11T03:52:13.1541731Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4]
2025-04-11T03:52:13.1542105Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7]
2025-04-11T03:52:13.1542360Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7]
2025-04-11T03:52:13.1542599Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False]
2025-04-11T03:52:13.1542833Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True]
2025-04-11T03:52:13.1543153Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32]
2025-04-11T03:52:13.1543386Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True]
2025-04-11T03:52:13.1543674Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1543906Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True]
2025-04-11T03:52:13.1544231Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False]
2025-04-11T03:52:13.1544386Z 0.10s call     tests/test_infer/test_batch_bucket.py::test_bucket
2025-04-11T03:52:13.1544637Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32]
2025-04-11T03:52:13.1544880Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T03:52:13.1545010Z 0.10s call     tests/test_lazy/test_ops.py::test_lazy_ops
2025-04-11T03:52:13.1545301Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7]
2025-04-11T03:52:13.1545535Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False]
2025-04-11T03:52:13.1545834Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1546091Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7]
2025-04-11T03:52:13.1546384Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True]
2025-04-11T03:52:13.1546624Z 0.10s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False]
2025-04-11T03:52:13.1546926Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1547121Z 0.10s setup    tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom
2025-04-11T03:52:13.1547375Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4]
2025-04-11T03:52:13.1547685Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32]
2025-04-11T03:52:13.1547862Z 0.10s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False]
2025-04-11T03:52:13.1548133Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4]
2025-04-11T03:52:13.1548375Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4]
2025-04-11T03:52:13.1548602Z 0.10s setup    tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2
2025-04-11T03:52:13.1548837Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True]
2025-04-11T03:52:13.1549060Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True]
2025-04-11T03:52:13.1549319Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32]
2025-04-11T03:52:13.1549683Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4]
2025-04-11T03:52:13.1549922Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True]
2025-04-11T03:52:13.1550171Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32]
2025-04-11T03:52:13.1550424Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7]
2025-04-11T03:52:13.1550748Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4]
2025-04-11T03:52:13.1551034Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32]
2025-04-11T03:52:13.1551281Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7]
2025-04-11T03:52:13.1551526Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7]
2025-04-11T03:52:13.1551848Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7]
2025-04-11T03:52:13.1552080Z 0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True]
2025-04-11T03:52:13.1552334Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4]
2025-04-11T03:52:13.1552653Z 0.10s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7]
2025-04-11T03:52:13.1552834Z 0.10s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False]
2025-04-11T03:52:13.1553117Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7]
2025-04-11T03:52:13.1553403Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7]
2025-04-11T03:52:13.1553695Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1554062Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1554312Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32]
2025-04-11T03:52:13.1554626Z 0.10s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32]
2025-04-11T03:52:13.1554850Z 0.10s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load
2025-04-11T03:52:13.1555138Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1555387Z 0.10s setup    tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4]
2025-04-11T03:52:13.1555679Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1555938Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7]
2025-04-11T03:52:13.1556224Z 0.10s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1556476Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7]
2025-04-11T03:52:13.1556763Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1557054Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1557360Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7]
2025-04-11T03:52:13.1557527Z 0.10s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True]
2025-04-11T03:52:13.1557774Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4]
2025-04-11T03:52:13.1557932Z 0.10s call     tests/test_moe/test_kernel.py::test_moe_kernel[data_type1]
2025-04-11T03:52:13.1558114Z 0.10s call     tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0]
2025-04-11T03:52:13.1574526Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True]
2025-04-11T03:52:13.1574921Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True]
2025-04-11T03:52:13.1575244Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1575519Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7]
2025-04-11T03:52:13.1575898Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32]
2025-04-11T03:52:13.1576209Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32]
2025-04-11T03:52:13.1576465Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7]
2025-04-11T03:52:13.1576727Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7]
2025-04-11T03:52:13.1577027Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1577191Z 0.10s call     tests/test_fp8/test_fp8_cast.py::test_fp8_cast
2025-04-11T03:52:13.1577445Z 0.10s call     tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4]
2025-04-11T03:52:13.1577704Z 0.10s call     tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4]
2025-04-11T03:52:13.1578000Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1578355Z 0.10s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1578564Z 0.10s call     tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm
2025-04-11T03:52:13.1578843Z 0.10s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm
2025-04-11T03:52:13.1579096Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4]
2025-04-11T03:52:13.1579390Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1579680Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1579811Z 0.10s setup    tests/test_lazy/test_ops.py::test_lazy_ops
2025-04-11T03:52:13.1580099Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1580384Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1580672Z 0.10s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4]
2025-04-11T03:52:13.1580911Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T03:52:13.1581278Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1581471Z 0.10s setup    tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p
2025-04-11T03:52:13.1581753Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1582040Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1582233Z 0.10s setup    tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm
2025-04-11T03:52:13.1582585Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T03:52:13.1582865Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1583173Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7]
2025-04-11T03:52:13.1583455Z 0.10s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1583750Z 0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True]
2025-04-11T03:52:13.1583941Z 0.10s setup    tests/test_shardformer/test_model/test_shard_command.py::test_command
2025-04-11T03:52:13.1584177Z 0.10s call     tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4]
2025-04-11T03:52:13.1584433Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32]
2025-04-11T03:52:13.1584715Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1584992Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7]
2025-04-11T03:52:13.1585244Z 0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32]
2025-04-11T03:52:13.1585421Z 0.10s setup    tests/test_shardformer/test_shard_utils.py::test_release_layer
2025-04-11T03:52:13.1585779Z 0.10s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7]
2025-04-11T03:52:13.1586069Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1586349Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1586638Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1586941Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32]
2025-04-11T03:52:13.1587253Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32]
2025-04-11T03:52:13.1587483Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2]
2025-04-11T03:52:13.1587773Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1588057Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1588333Z 0.10s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7]
2025-04-11T03:52:13.1588675Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1589033Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1589319Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1589603Z 0.10s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1589903Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-16-7]
2025-04-11T03:52:13.1590125Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2]
2025-04-11T03:52:13.1590477Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1590762Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1590929Z 0.10s setup    tests/test_moe/test_kernel.py::test_moe_kernel[data_type1]
2025-04-11T03:52:13.1591271Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1591554Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1591837Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1592118Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1592427Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7]
2025-04-11T03:52:13.1592709Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1592995Z 0.10s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1593298Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32]
2025-04-11T03:52:13.1593655Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1593926Z 0.10s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4]
2025-04-11T03:52:13.1594210Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1594427Z 0.10s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True]
2025-04-11T03:52:13.1594725Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7]
2025-04-11T03:52:13.1595014Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1595191Z 0.10s call     tests/test_shardformer/test_model/test_shard_bert.py::test_bert
2025-04-11T03:52:13.1595487Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1595776Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1596069Z 0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1596365Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7]
2025-04-11T03:52:13.1596717Z 0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7]
2025-04-11T03:52:13.1597013Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1597251Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True]
2025-04-11T03:52:13.1597545Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1597782Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False]
2025-04-11T03:52:13.1598132Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1598412Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1598719Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7]
2025-04-11T03:52:13.1599068Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7]
2025-04-11T03:52:13.1599311Z 0.09s call     tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2]
2025-04-11T03:52:13.1599596Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1599887Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1600123Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False]
2025-04-11T03:52:13.1600308Z 0.09s setup    tests/test_tensor/test_shape_consistency.py::test_shape_consistency
2025-04-11T03:52:13.1600553Z 0.09s setup    tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py::test_t5_pipeline_layers
2025-04-11T03:52:13.1600844Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1601135Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1601431Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False]
2025-04-11T03:52:13.1601674Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2]
2025-04-11T03:52:13.1601980Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32]
2025-04-11T03:52:13.1602270Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1602556Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1602845Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1603130Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32]
2025-04-11T03:52:13.1603406Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32]
2025-04-11T03:52:13.1603635Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16]
2025-04-11T03:52:13.1603809Z 0.09s setup    tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec
2025-04-11T03:52:13.1604100Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1604455Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7]
2025-04-11T03:52:13.1604752Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7]
2025-04-11T03:52:13.1605039Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1605261Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4]
2025-04-11T03:52:13.1605608Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1605891Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1606193Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7]
2025-04-11T03:52:13.1606478Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1606763Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2]
2025-04-11T03:52:13.1607053Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1607341Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1607644Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32]
2025-04-11T03:52:13.1607915Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32]
2025-04-11T03:52:13.1608224Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32]
2025-04-11T03:52:13.1608504Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7]
2025-04-11T03:52:13.1608791Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1609154Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32]
2025-04-11T03:52:13.1609376Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8]
2025-04-11T03:52:13.1609609Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True]
2025-04-11T03:52:13.1609832Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4]
2025-04-11T03:52:13.1610133Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7]
2025-04-11T03:52:13.1610361Z 0.09s setup    tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy
2025-04-11T03:52:13.1610577Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8]
2025-04-11T03:52:13.1610868Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1611162Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7]
2025-04-11T03:52:13.1611385Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8]
2025-04-11T03:52:13.1611613Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.0-False]
2025-04-11T03:52:13.1611992Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32]
2025-04-11T03:52:13.1612218Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16]
2025-04-11T03:52:13.1612519Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32]
2025-04-11T03:52:13.1612829Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32]
2025-04-11T03:52:13.1613127Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32]
2025-04-11T03:52:13.1613345Z 0.09s setup    tests/test_lora/test_lora.py::test_torch_ddp_lora
2025-04-11T03:52:13.1613630Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1613902Z 0.09s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm
2025-04-11T03:52:13.1614087Z 0.09s call     tests/test_shardformer/test_flash_attention.py::test_flash_attn_func
2025-04-11T03:52:13.1614447Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7]
2025-04-11T03:52:13.1614747Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7]
2025-04-11T03:52:13.1615042Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32]
2025-04-11T03:52:13.1615353Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32]
2025-04-11T03:52:13.1615648Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7]
2025-04-11T03:52:13.1615942Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1616242Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7]
2025-04-11T03:52:13.1616617Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1616914Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32]
2025-04-11T03:52:13.1617095Z 0.09s setup    tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4]
2025-04-11T03:52:13.1617267Z 0.09s setup    tests/test_tensor/test_shape_consistency_apply.py::test_apply
2025-04-11T03:52:13.1617554Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1617857Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32]
2025-04-11T03:52:13.1618096Z 0.09s call     tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2]
2025-04-11T03:52:13.1618394Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1618614Z 0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4]
2025-04-11T03:52:13.1618891Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7]
2025-04-11T03:52:13.1619182Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1619484Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7]
2025-04-11T03:52:13.1619829Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1620129Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7]
2025-04-11T03:52:13.1620415Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1620704Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1621077Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32]
2025-04-11T03:52:13.1621368Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1621655Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1621988Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32]
2025-04-11T03:52:13.1622276Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1622551Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7]
2025-04-11T03:52:13.1622794Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False]
2025-04-11T03:52:13.1623077Z 0.09s setup    tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py::test_whisper_pipeline_distribution
2025-04-11T03:52:13.1623370Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1623655Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1623946Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1624287Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1624574Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1624759Z 0.09s call     tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon
2025-04-11T03:52:13.1625044Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1625332Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1625635Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7]
2025-04-11T03:52:13.1625928Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1626209Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1626495Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1626777Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1627072Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1627431Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32]
2025-04-11T03:52:13.1627668Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.1-False]
2025-04-11T03:52:13.1627950Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1628231Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1628553Z 0.09s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0]
2025-04-11T03:52:13.1629023Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1629309Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1629596Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1629930Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7]
2025-04-11T03:52:13.1630204Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7]
2025-04-11T03:52:13.1630494Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1630780Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1631073Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1631375Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32]
2025-04-11T03:52:13.1631668Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1631899Z 0.09s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_get_micro_batch
2025-04-11T03:52:13.1632259Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1632529Z 0.09s setup    tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py::test_whisper_pipeline_layers
2025-04-11T03:52:13.1632816Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1633109Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1633398Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1633695Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1633982Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1634222Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True]
2025-04-11T03:52:13.1634522Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7]
2025-04-11T03:52:13.1634798Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32]
2025-04-11T03:52:13.1635031Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True]
2025-04-11T03:52:13.1635384Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1635674Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1635958Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1636263Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32]
2025-04-11T03:52:13.1636608Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1636901Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1637170Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7]
2025-04-11T03:52:13.1637459Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1637822Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7]
2025-04-11T03:52:13.1638113Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1638300Z 0.09s call     tests/test_shardformer/test_model/test_shard_command.py::test_command
2025-04-11T03:52:13.1638574Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32]
2025-04-11T03:52:13.1638881Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32]
2025-04-11T03:52:13.1639188Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32]
2025-04-11T03:52:13.1639481Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1639759Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1640139Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32]
2025-04-11T03:52:13.1640436Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32]
2025-04-11T03:52:13.1640724Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1641024Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7]
2025-04-11T03:52:13.1641333Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32]
2025-04-11T03:52:13.1641632Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32]
2025-04-11T03:52:13.1641936Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7]
2025-04-11T03:52:13.1642212Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7]
2025-04-11T03:52:13.1642493Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1642770Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32]
2025-04-11T03:52:13.1643135Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32]
2025-04-11T03:52:13.1643439Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7]
2025-04-11T03:52:13.1643725Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1644014Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1644247Z 0.09s setup    tests/test_infer/test_models/test_custom_model.py::test_model
2025-04-11T03:52:13.1644534Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1644816Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1645103Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1645451Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1645743Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1646043Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32]
2025-04-11T03:52:13.1646343Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7]
2025-04-11T03:52:13.1646632Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1646920Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1647227Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7]
2025-04-11T03:52:13.1647510Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32]
2025-04-11T03:52:13.1647843Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7]
2025-04-11T03:52:13.1648158Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7]
2025-04-11T03:52:13.1648657Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7]
2025-04-11T03:52:13.1648963Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32]
2025-04-11T03:52:13.1649260Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7]
2025-04-11T03:52:13.1649539Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32]
2025-04-11T03:52:13.1649830Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1650125Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1650426Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32]
2025-04-11T03:52:13.1650719Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1651058Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32]
2025-04-11T03:52:13.1651350Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1651650Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7]
2025-04-11T03:52:13.1651952Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7]
2025-04-11T03:52:13.1652246Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7]
2025-04-11T03:52:13.1652587Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7]
2025-04-11T03:52:13.1652798Z 0.09s setup    tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter
2025-04-11T03:52:13.1653085Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1653445Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7]
2025-04-11T03:52:13.1653742Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7]
2025-04-11T03:52:13.1654027Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1654315Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1654621Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32]
2025-04-11T03:52:13.1654903Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1655197Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1655498Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7]
2025-04-11T03:52:13.1655827Z 0.09s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm
2025-04-11T03:52:13.1656124Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7]
2025-04-11T03:52:13.1656394Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7]
2025-04-11T03:52:13.1656678Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1656981Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7]
2025-04-11T03:52:13.1657272Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1657543Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32]
2025-04-11T03:52:13.1657843Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32]
2025-04-11T03:52:13.1658144Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32]
2025-04-11T03:52:13.1658434Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1658772Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32]
2025-04-11T03:52:13.1659055Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1659334Z 0.09s setup    tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4]
2025-04-11T03:52:13.1659618Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1659899Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1660261Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32]
2025-04-11T03:52:13.1660569Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32]
2025-04-11T03:52:13.1660856Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1661189Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32]
2025-04-11T03:52:13.1661484Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-32-32]
2025-04-11T03:52:13.1661758Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32]
2025-04-11T03:52:13.1662039Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1662246Z 0.09s setup    tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm
2025-04-11T03:52:13.1662513Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7]
2025-04-11T03:52:13.1662816Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7]
2025-04-11T03:52:13.1663111Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32]
2025-04-11T03:52:13.1663459Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7]
2025-04-11T03:52:13.1663761Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32]
2025-04-11T03:52:13.1664058Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7]
2025-04-11T03:52:13.1664335Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32]
2025-04-11T03:52:13.1664633Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32]
2025-04-11T03:52:13.1664923Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1665206Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1665480Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7]
2025-04-11T03:52:13.1665756Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1665996Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False]
2025-04-11T03:52:13.1666280Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1666622Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7]
2025-04-11T03:52:13.1666906Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1667187Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1667461Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7]
2025-04-11T03:52:13.1667707Z 0.09s call     tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel
2025-04-11T03:52:13.1667991Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1668260Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32]
2025-04-11T03:52:13.1668602Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1669020Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32]
2025-04-11T03:52:13.1669262Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False]
2025-04-11T03:52:13.1669549Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1669836Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1670113Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1670410Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32]
2025-04-11T03:52:13.1670690Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32]
2025-04-11T03:52:13.1670976Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1671316Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32]
2025-04-11T03:52:13.1671599Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1671887Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1672185Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7]
2025-04-11T03:52:13.1672499Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32]
2025-04-11T03:52:13.1672787Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1673079Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1673359Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1673663Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32]
2025-04-11T03:52:13.1673948Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1674311Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1674583Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7]
2025-04-11T03:52:13.1674872Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1675177Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-32-32]
2025-04-11T03:52:13.1675461Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1675819Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1676102Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1676379Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32]
2025-04-11T03:52:13.1676719Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1676994Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7]
2025-04-11T03:52:13.1677262Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32]
2025-04-11T03:52:13.1677546Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1677855Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32]
2025-04-11T03:52:13.1678136Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1678440Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-32-7]
2025-04-11T03:52:13.1678730Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1679094Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7]
2025-04-11T03:52:13.1679380Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1679689Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-16-32]
2025-04-11T03:52:13.1679974Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1680265Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1680573Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-16-32]
2025-04-11T03:52:13.1680862Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1681169Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-32-32]
2025-04-11T03:52:13.1681467Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-32-32]
2025-04-11T03:52:13.1681757Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1682114Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-32-32]
2025-04-11T03:52:13.1682391Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7]
2025-04-11T03:52:13.1682563Z 0.09s call     tests/test_shardformer/test_model/test_shard_sam.py::test_sam
2025-04-11T03:52:13.1682849Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1683131Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1683484Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1683764Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1684043Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32]
2025-04-11T03:52:13.1684386Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1684559Z 0.09s call     tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2
2025-04-11T03:52:13.1684862Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-16-32]
2025-04-11T03:52:13.1685148Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1685440Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1685729Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1686016Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1686298Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1686606Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-16-7]
2025-04-11T03:52:13.1686944Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1687251Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32]
2025-04-11T03:52:13.1687541Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1687813Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32]
2025-04-11T03:52:13.1688123Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-16-32]
2025-04-11T03:52:13.1688408Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1688703Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1688989Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1689275Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1689557Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1689900Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1690202Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-16-32]
2025-04-11T03:52:13.1690504Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-32-32]
2025-04-11T03:52:13.1690804Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-16-7]
2025-04-11T03:52:13.1691161Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-16-7]
2025-04-11T03:52:13.1691442Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1691725Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1692025Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-16-32]
2025-04-11T03:52:13.1692398Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-32-7]
2025-04-11T03:52:13.1692681Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7]
2025-04-11T03:52:13.1692986Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32]
2025-04-11T03:52:13.1693288Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-16-7]
2025-04-11T03:52:13.1693571Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1693859Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1694161Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-32-32]
2025-04-11T03:52:13.1694525Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-16-32]
2025-04-11T03:52:13.1694805Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1695104Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-32-7]
2025-04-11T03:52:13.1695392Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1695675Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1695960Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1696245Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1696551Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7]
2025-04-11T03:52:13.1696822Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7]
2025-04-11T03:52:13.1697121Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-16-7]
2025-04-11T03:52:13.1697415Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-32-32]
2025-04-11T03:52:13.1697763Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1698066Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-16-32]
2025-04-11T03:52:13.1698359Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1698590Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.0-True]
2025-04-11T03:52:13.1698974Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32]
2025-04-11T03:52:13.1699269Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1699506Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T03:52:13.1699800Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1700154Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-32-7]
2025-04-11T03:52:13.1700468Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32]
2025-04-11T03:52:13.1700631Z 0.09s call     tests/test_booster/test_accelerator.py::test_accelerator
2025-04-11T03:52:13.1700927Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1701229Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-16-7]
2025-04-11T03:52:13.1701527Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-32-7]
2025-04-11T03:52:13.1701815Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1702110Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-32-7]
2025-04-11T03:52:13.1702469Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-32-7]
2025-04-11T03:52:13.1702690Z 0.09s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_merge_batch
2025-04-11T03:52:13.1702973Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32]
2025-04-11T03:52:13.1703259Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1703446Z 0.09s setup    tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp
2025-04-11T03:52:13.1703748Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-32-7]
2025-04-11T03:52:13.1704021Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7]
2025-04-11T03:52:13.1704302Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7]
2025-04-11T03:52:13.1704604Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-16-7]
2025-04-11T03:52:13.1704888Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1705178Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1705523Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1705790Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32]
2025-04-11T03:52:13.1706076Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1706298Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16]
2025-04-11T03:52:13.1706578Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1706917Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1707203Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1707485Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1707830Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1708024Z 0.09s call     tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm
2025-04-11T03:52:13.1708309Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1708634Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16]
2025-04-11T03:52:13.1708868Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.1-True]
2025-04-11T03:52:13.1709106Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.1-False]
2025-04-11T03:52:13.1709387Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1709666Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32]
2025-04-11T03:52:13.1710010Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32]
2025-04-11T03:52:13.1710247Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.0-False]
2025-04-11T03:52:13.1710530Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1710824Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1711057Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T03:52:13.1711343Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1711634Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1711813Z 0.09s call     tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2
2025-04-11T03:52:13.1711995Z 0.09s call     tests/test_shardformer/test_model/test_shard_llama.py::test_llama
2025-04-11T03:52:13.1712289Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32]
2025-04-11T03:52:13.1712519Z 0.09s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False]
2025-04-11T03:52:13.1712692Z 0.09s call     tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom
2025-04-11T03:52:13.1713024Z 0.09s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm
2025-04-11T03:52:13.1713306Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1713569Z 0.09s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm
2025-04-11T03:52:13.1713875Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7]
2025-04-11T03:52:13.1714158Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1714499Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7]
2025-04-11T03:52:13.1714768Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7]
2025-04-11T03:52:13.1715057Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7]
2025-04-11T03:52:13.1715321Z 0.09s call     tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral
2025-04-11T03:52:13.1715601Z 0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7]
2025-04-11T03:52:13.1715869Z 0.09s call     tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention
2025-04-11T03:52:13.1716160Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1716446Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1716732Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1716998Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7]
2025-04-11T03:52:13.1717280Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1717621Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1717905Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1718194Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1718475Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1718767Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1718987Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16]
2025-04-11T03:52:13.1719294Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7]
2025-04-11T03:52:13.1719579Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1719867Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1720103Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.1-True]
2025-04-11T03:52:13.1720387Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1720749Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1720937Z 0.09s call     tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper
2025-04-11T03:52:13.1721228Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1721509Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1721801Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1722131Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7]
2025-04-11T03:52:13.1722422Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1722707Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1723035Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7]
2025-04-11T03:52:13.1723320Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1723601Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1723888Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1724173Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1724414Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.0-True]
2025-04-11T03:52:13.1724648Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.0-False]
2025-04-11T03:52:13.1724950Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1725285Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1725575Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1725879Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7]
2025-04-11T03:52:13.1726171Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1726455Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1726729Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32]
2025-04-11T03:52:13.1727000Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32]
2025-04-11T03:52:13.1727279Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7]
2025-04-11T03:52:13.1727572Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1727860Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1728132Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7]
2025-04-11T03:52:13.1728488Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1728778Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1729062Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1729296Z 0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.1-True]
2025-04-11T03:52:13.1729467Z 0.09s call     tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2
2025-04-11T03:52:13.1729804Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7]
2025-04-11T03:52:13.1729970Z 0.09s call     tests/test_shardformer/test_model/test_shard_vit.py::test_vit
2025-04-11T03:52:13.1730252Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1730522Z 0.09s call     tests/test_shardformer/test_model/test_shard_t5.py::test_t5
2025-04-11T03:52:13.1730821Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32]
2025-04-11T03:52:13.1731115Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1731379Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32]
2025-04-11T03:52:13.1731599Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4]
2025-04-11T03:52:13.1731880Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1732165Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1732431Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7]
2025-04-11T03:52:13.1732709Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7]
2025-04-11T03:52:13.1733047Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1733324Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1733609Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1733892Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1734185Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1734465Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1734753Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1735032Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7]
2025-04-11T03:52:13.1735322Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1735623Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7]
2025-04-11T03:52:13.1735975Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7]
2025-04-11T03:52:13.1736258Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1736537Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1736806Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7]
2025-04-11T03:52:13.1737107Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7]
2025-04-11T03:52:13.1737454Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1737731Z 0.09s setup    tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2]
2025-04-11T03:52:13.1738016Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1738352Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1738634Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1738903Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32]
2025-04-11T03:52:13.1739191Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1739459Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7]
2025-04-11T03:52:13.1739745Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1740027Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1740312Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1740635Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7]
2025-04-11T03:52:13.1740933Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32]
2025-04-11T03:52:13.1741236Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32]
2025-04-11T03:52:13.1741516Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1741802Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1742084Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1742360Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32]
2025-04-11T03:52:13.1742642Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16]
2025-04-11T03:52:13.1742943Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32]
2025-04-11T03:52:13.1743221Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1743487Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7]
2025-04-11T03:52:13.1743839Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1744108Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7]
2025-04-11T03:52:13.1744395Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1744672Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1745011Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1745291Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1745595Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7]
2025-04-11T03:52:13.1745919Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32]
2025-04-11T03:52:13.1746201Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1746483Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1746763Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1747038Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1747313Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7]
2025-04-11T03:52:13.1747596Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1747862Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32]
2025-04-11T03:52:13.1748188Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32]
2025-04-11T03:52:13.1748463Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4]
2025-04-11T03:52:13.1748778Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7]
2025-04-11T03:52:13.1749264Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32]
2025-04-11T03:52:13.1749658Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16]
2025-04-11T03:52:13.1749948Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1750224Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32]
2025-04-11T03:52:13.1750495Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32]
2025-04-11T03:52:13.1750800Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32]
2025-04-11T03:52:13.1751076Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32]
2025-04-11T03:52:13.1751340Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32]
2025-04-11T03:52:13.1751796Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1752091Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1752401Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32]
2025-04-11T03:52:13.1752667Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7]
2025-04-11T03:52:13.1753023Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1753241Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2]
2025-04-11T03:52:13.1753508Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32]
2025-04-11T03:52:13.1753773Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7]
2025-04-11T03:52:13.1754113Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1754388Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7]
2025-04-11T03:52:13.1754664Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16]
2025-04-11T03:52:13.1754932Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32]
2025-04-11T03:52:13.1755215Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1755523Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7]
2025-04-11T03:52:13.1755801Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1756074Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32]
2025-04-11T03:52:13.1756417Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7]
2025-04-11T03:52:13.1756696Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32]
2025-04-11T03:52:13.1756916Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2]
2025-04-11T03:52:13.1757186Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32]
2025-04-11T03:52:13.1757469Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16]
2025-04-11T03:52:13.1757754Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1758055Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7]
2025-04-11T03:52:13.1758359Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32]
2025-04-11T03:52:13.1758549Z 0.09s setup    tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
2025-04-11T03:52:13.1758856Z 0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32]
2025-04-11T03:52:13.1759129Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32]
2025-04-11T03:52:13.1759454Z 0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7]
2025-04-11T03:52:13.1759748Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16]
2025-04-11T03:52:13.1760047Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7]
2025-04-11T03:52:13.1760328Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16]
2025-04-11T03:52:13.1760680Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32]
2025-04-11T03:52:13.1760962Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1761245Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1761465Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4]
2025-04-11T03:52:13.1761831Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32]
2025-04-11T03:52:13.1762114Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7]
2025-04-11T03:52:13.1762398Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7]
2025-04-11T03:52:13.1762616Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8]
2025-04-11T03:52:13.1762833Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16]
2025-04-11T03:52:13.1763113Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7]
2025-04-11T03:52:13.1763415Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7]
2025-04-11T03:52:13.1763705Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16]
2025-04-11T03:52:13.1763989Z 0.09s setup    tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO
2025-04-11T03:52:13.1764269Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16]
2025-04-11T03:52:13.1764569Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7]
2025-04-11T03:52:13.1764787Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8]
2025-04-11T03:52:13.1765062Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7]
2025-04-11T03:52:13.1765351Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1765629Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1765937Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32]
2025-04-11T03:52:13.1766149Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2]
2025-04-11T03:52:13.1766367Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4]
2025-04-11T03:52:13.1766645Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7]
2025-04-11T03:52:13.1767002Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7]
2025-04-11T03:52:13.1767300Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7]
2025-04-11T03:52:13.1767592Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7]
2025-04-11T03:52:13.1767878Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1768177Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7]
2025-04-11T03:52:13.1768472Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2]
2025-04-11T03:52:13.1768687Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16]
2025-04-11T03:52:13.1768987Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7]
2025-04-11T03:52:13.1769326Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7]
2025-04-11T03:52:13.1769610Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1769911Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32]
2025-04-11T03:52:13.1770200Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1770410Z 0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8]
2025-04-11T03:52:13.1770701Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7]
2025-04-11T03:52:13.1770989Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16]
2025-04-11T03:52:13.1771285Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32]
2025-04-11T03:52:13.1771636Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32]
2025-04-11T03:52:13.1771933Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32]
2025-04-11T03:52:13.1772216Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1772498Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-32-7]
2025-04-11T03:52:13.1772780Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16]
2025-04-11T03:52:13.1773079Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7]
2025-04-11T03:52:13.1773392Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32]
2025-04-11T03:52:13.1773675Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1773973Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7]
2025-04-11T03:52:13.1774272Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7]
2025-04-11T03:52:13.1774573Z 0.09s call     tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO
2025-04-11T03:52:13.1774869Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32]
2025-04-11T03:52:13.1775145Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1775454Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32]
2025-04-11T03:52:13.1775753Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32]
2025-04-11T03:52:13.1776104Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1776408Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7]
2025-04-11T03:52:13.1776689Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1777047Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32]
2025-04-11T03:52:13.1777350Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32]
2025-04-11T03:52:13.1777645Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32]
2025-04-11T03:52:13.1777931Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1778210Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7]
2025-04-11T03:52:13.1778493Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16]
2025-04-11T03:52:13.1778714Z 0.09s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint
2025-04-11T03:52:13.1779011Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7]
2025-04-11T03:52:13.1779414Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1779696Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1779999Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32]
2025-04-11T03:52:13.1780281Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16]
2025-04-11T03:52:13.1780567Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-16-16]
2025-04-11T03:52:13.1780846Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16]
2025-04-11T03:52:13.1781149Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32]
2025-04-11T03:52:13.1781432Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7]
2025-04-11T03:52:13.1781709Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-16-7]
2025-04-11T03:52:13.1782011Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7]
2025-04-11T03:52:13.1782288Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-16-7]
2025-04-11T03:52:13.1782649Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7]
2025-04-11T03:52:13.1782938Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-32-16]
2025-04-11T03:52:13.1783226Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1783533Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32]
2025-04-11T03:52:13.1783897Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32]
2025-04-11T03:52:13.1784179Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7]
2025-04-11T03:52:13.1784482Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32]
2025-04-11T03:52:13.1784833Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1785133Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32]
2025-04-11T03:52:13.1785416Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7]
2025-04-11T03:52:13.1785698Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-32-7]
2025-04-11T03:52:13.1785982Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-32-16]
2025-04-11T03:52:13.1786283Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7]
2025-04-11T03:52:13.1786588Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32]
2025-04-11T03:52:13.1786889Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7]
2025-04-11T03:52:13.1787231Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7]
2025-04-11T03:52:13.1787529Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7]
2025-04-11T03:52:13.1787829Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7]
2025-04-11T03:52:13.1788109Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-16-16]
2025-04-11T03:52:13.1788448Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32]
2025-04-11T03:52:13.1788750Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7]
2025-04-11T03:52:13.1789035Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16]
2025-04-11T03:52:13.1789335Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7]
2025-04-11T03:52:13.1789636Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32]
2025-04-11T03:52:13.1789923Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-16-16]
2025-04-11T03:52:13.1790276Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1790567Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-32-7]
2025-04-11T03:52:13.1790851Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1791151Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32]
2025-04-11T03:52:13.1791504Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-16-7]
2025-04-11T03:52:13.1791790Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7]
2025-04-11T03:52:13.1792071Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-16-7]
2025-04-11T03:52:13.1792376Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32]
2025-04-11T03:52:13.1792729Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-16-16]
2025-04-11T03:52:13.1793013Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-32-16]
2025-04-11T03:52:13.1793295Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-16-7]
2025-04-11T03:52:13.1793594Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32]
2025-04-11T03:52:13.1793876Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-32-7]
2025-04-11T03:52:13.1794176Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7]
2025-04-11T03:52:13.1794478Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7]
2025-04-11T03:52:13.1794821Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16]
2025-04-11T03:52:13.1795105Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-32-7]
2025-04-11T03:52:13.1795400Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7]
2025-04-11T03:52:13.1795699Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32]
2025-04-11T03:52:13.1795981Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-32-16]
2025-04-11T03:52:13.1796850Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-32-16]
2025-04-11T03:52:13.1797133Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1797413Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-16-16]
2025-04-11T03:52:13.1797695Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-16-16]
2025-04-11T03:52:13.1797974Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-32-7]
2025-04-11T03:52:13.1798258Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-32-7]
2025-04-11T03:52:13.1798594Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-32-7]
2025-04-11T03:52:13.1798879Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7]
2025-04-11T03:52:13.1799180Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32]
2025-04-11T03:52:13.1799470Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-32-16]
2025-04-11T03:52:13.1799828Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7]
2025-04-11T03:52:13.1800109Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16]
2025-04-11T03:52:13.1800385Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7]
2025-04-11T03:52:13.1800682Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7]
2025-04-11T03:52:13.1801021Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16]
2025-04-11T03:52:13.1801303Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-32-16]
2025-04-11T03:52:13.1801584Z 0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7]
2025-04-11T03:52:13.1801882Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7]
2025-04-11T03:52:13.1802181Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32]
2025-04-11T03:52:13.1802468Z 0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-16-16]
2025-04-11T03:52:13.1802770Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32]
2025-04-11T03:52:13.1803126Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32]
2025-04-11T03:52:13.1803426Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32]
2025-04-11T03:52:13.1803724Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7]
2025-04-11T03:52:13.1804024Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32]
2025-04-11T03:52:13.1804322Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7]
2025-04-11T03:52:13.1804624Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7]
2025-04-11T03:52:13.1804923Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7]
2025-04-11T03:52:13.1805218Z 0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7]
2025-04-11T03:52:13.1805479Z 0.08s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-True]
2025-04-11T03:52:13.1805660Z 0.08s setup    tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin
2025-04-11T03:52:13.1805851Z 0.08s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load
2025-04-11T03:52:13.1806108Z 0.08s setup    tests/test_device/test_device_mesh.py::test_device_mesh_from_process_group
2025-04-11T03:52:13.1806343Z 0.03s call     tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.1-False]
2025-04-11T03:52:13.1806573Z 0.01s call     tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.0-True]
2025-04-11T03:52:13.1806581Z 
2025-04-11T03:52:13.1806748Z (1095 durations < 0.005s hidden.  Use -vv to show these durations.)
2025-04-11T03:52:13.1806877Z =========================== short test summary info ============================
2025-04-11T03:52:13.1807131Z FAILED tests/test_booster/test_accelerator.py::test_accelerator - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1807486Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1807620Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1807789Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1808118Z FAILED tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1808179Z 
2025-04-11T03:52:13.1808312Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1808414Z Traceback (most recent call last):
2025-04-11T03:52:13.1808725Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1808809Z     fn(i, *args)
2025-04-11T03:52:13.1809070Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 271, in run_dist
2025-04-11T03:52:13.1809182Z     check_3d_plugin(early_stop=early_stop)
2025-04-11T03:52:13.1809436Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1809534Z     partial_func(**kwargs)
2025-04-11T03:52:13.1809803Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 104, in check_3d_plugin
2025-04-11T03:52:13.1809965Z     err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:13.1810182Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1810280Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1810594Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1810698Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1810984Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1811089Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1811197Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1811484Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1811623Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1811790Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1812163Z FAILED tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1812169Z 
2025-04-11T03:52:13.1812297Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1812394Z Traceback (most recent call last):
2025-04-11T03:52:13.1812686Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1812768Z     fn(i, *args)
2025-04-11T03:52:13.1813033Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 89, in run_dist
2025-04-11T03:52:13.1813138Z     check_dataloader_sharding()
2025-04-11T03:52:13.1813445Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 69, in check_dataloader_sharding
2025-04-11T03:52:13.1813617Z     batch = next(iter(train_dataloader))[0].cuda()
2025-04-11T03:52:13.1813716Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1814000Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1814133Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1814296Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1814638Z FAILED tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1814718Z 
2025-04-11T03:52:13.1814837Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1814937Z Traceback (most recent call last):
2025-04-11T03:52:13.1815218Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1815300Z     fn(i, *args)
2025-04-11T03:52:13.1815566Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 167, in run_dist
2025-04-11T03:52:13.1815738Z     check_gemini_plugin(early_stop=early_stop)
2025-04-11T03:52:13.1815990Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1816081Z     partial_func(**kwargs)
2025-04-11T03:52:13.1816332Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1816419Z     partial_func(**kwargs)
2025-04-11T03:52:13.1816666Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1816755Z     partial_func(**kwargs)
2025-04-11T03:52:13.1816861Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.1817151Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 149, in check_gemini_plugin
2025-04-11T03:52:13.1817352Z     err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn, zero_size, tp_size)
2025-04-11T03:52:13.1817569Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1817666Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1817928Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1818083Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1818362Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1818460Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1818558Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1818836Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1818965Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1819130Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1819511Z FAILED tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1819518Z 
2025-04-11T03:52:13.1819639Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1819732Z Traceback (most recent call last):
2025-04-11T03:52:13.1820019Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1820097Z     fn(i, *args)
2025-04-11T03:52:13.1820379Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 135, in run_dist
2025-04-11T03:52:13.1820508Z     check_low_level_zero_plugin(early_stop=early_stop)
2025-04-11T03:52:13.1820753Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1820843Z     partial_func(**kwargs)
2025-04-11T03:52:13.1821219Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 84, in check_low_level_zero_plugin
2025-04-11T03:52:13.1821369Z     err = run_fn(stage, model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:13.1821580Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1821676Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1821933Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1822026Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1822353Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1822450Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1822550Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1822821Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1822951Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1823166Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1823526Z FAILED tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1823533Z 
2025-04-11T03:52:13.1823654Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1823745Z Traceback (most recent call last):
2025-04-11T03:52:13.1824031Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1824109Z     fn(i, *args)
2025-04-11T03:52:13.1824378Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 113, in run_dist
2025-04-11T03:52:13.1824471Z     check_torch_ddp_plugin()
2025-04-11T03:52:13.1824774Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 52, in check_torch_ddp_plugin
2025-04-11T03:52:13.1824897Z     run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:13.1825110Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1825208Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1825515Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1825614Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1825878Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1825975Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1826074Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1826346Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1826480Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1826636Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1827003Z FAILED tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1827007Z 
2025-04-11T03:52:13.1827122Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1827218Z Traceback (most recent call last):
2025-04-11T03:52:13.1827493Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1827569Z     fn(i, *args)
2025-04-11T03:52:13.1827842Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 77, in run_dist
2025-04-11T03:52:13.1827933Z     check_torch_fsdp_plugin()
2025-04-11T03:52:13.1828243Z   File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 70, in check_torch_fsdp_plugin
2025-04-11T03:52:13.1828483Z     run_fn(model_fn, data_gen_fn, output_transform_fn)
2025-04-11T03:52:13.1828700Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1828797Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1829044Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1829147Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1829414Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1829514Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1829684Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1829963Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1830091Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1830250Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1830618Z FAILED tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1830684Z 
2025-04-11T03:52:13.1830800Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1830901Z Traceback (most recent call last):
2025-04-11T03:52:13.1831180Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1831260Z     fn(i, *args)
2025-04-11T03:52:13.1831521Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_checkpoint_io.py", line 212, in run_dist
2025-04-11T03:52:13.1831607Z     exam_state_dict()
2025-04-11T03:52:13.1831823Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1831915Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1832166Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1832261Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1832533Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1832629Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1832784Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1833066Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1833197Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1833364Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1833737Z FAILED tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1833743Z 
2025-04-11T03:52:13.1833863Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1833957Z Traceback (most recent call last):
2025-04-11T03:52:13.1834245Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1834323Z     fn(i, *args)
2025-04-11T03:52:13.1834601Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_torch_compability.py", line 167, in run_dist
2025-04-11T03:52:13.1834704Z     exam_torch_load_from_gemini()
2025-04-11T03:52:13.1834915Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1835018Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1835274Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1835373Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1835639Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1835795Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1835896Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1836167Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1836300Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1836455Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1836772Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1837048Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1837235Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1837387Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1837733Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1838005Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1838184Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1838339Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1838688Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1838962Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1839088Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1839238Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1839584Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1839852Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1839988Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1840135Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1840539Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1840806Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1840937Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1841085Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1841456Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1841730Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1841854Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1842006Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1842374Z FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1842643Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1842767Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1842921Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1843308Z FAILED tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1843389Z 
2025-04-11T03:52:13.1843510Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1843606Z Traceback (most recent call last):
2025-04-11T03:52:13.1843887Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1843971Z     fn(i, *args)
2025-04-11T03:52:13.1844281Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py", line 148, in run_dist
2025-04-11T03:52:13.1844368Z     exam_state_dict()
2025-04-11T03:52:13.1844619Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1844770Z     partial_func(**kwargs)
2025-04-11T03:52:13.1845019Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1845106Z     partial_func(**kwargs)
2025-04-11T03:52:13.1845362Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1845448Z     partial_func(**kwargs)
2025-04-11T03:52:13.1845608Z   [Previous line repeated 3 more times]
2025-04-11T03:52:13.1845824Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1845920Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1846173Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1846267Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1846540Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1846639Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1846740Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1847014Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1847148Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1847303Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1847654Z FAILED tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1847983Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1848107Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1848262Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1848688Z FAILED tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1848694Z 
2025-04-11T03:52:13.1848810Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1848904Z Traceback (most recent call last):
2025-04-11T03:52:13.1849188Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1849271Z     fn(i, *args)
2025-04-11T03:52:13.1849628Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py", line 72, in run_dist
2025-04-11T03:52:13.1849728Z     exam_from_pretrained()
2025-04-11T03:52:13.1849940Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1850039Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1850287Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1850387Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1850656Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1850750Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1850905Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1851174Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1851302Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1851451Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1851758Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1852027Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1852210Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1852365Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1852662Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1852932Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1853111Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1853266Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1853562Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1853832Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1853955Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1854102Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1854400Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1854668Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1854794Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1854941Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1855216Z FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1855540Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1855662Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1855813Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1856192Z FAILED tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1856197Z 
2025-04-11T03:52:13.1856318Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1856412Z Traceback (most recent call last):
2025-04-11T03:52:13.1856697Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1856776Z     fn(i, *args)
2025-04-11T03:52:13.1857041Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 76, in run_dist
2025-04-11T03:52:13.1857136Z     check_torch_ddp_checkpointIO()
2025-04-11T03:52:13.1857388Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1857479Z     partial_func(**kwargs)
2025-04-11T03:52:13.1857724Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1857817Z     partial_func(**kwargs)
2025-04-11T03:52:13.1858059Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1858205Z     partial_func(**kwargs)
2025-04-11T03:52:13.1858304Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.1858629Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 26, in check_torch_ddp_checkpointIO
2025-04-11T03:52:13.1858884Z     model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)
2025-04-11T03:52:13.1859092Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T03:52:13.1859288Z     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T03:52:13.1859617Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_ddp_plugin.py", line 283, in configure
2025-04-11T03:52:13.1859725Z     model = model.to(get_current_device())
2025-04-11T03:52:13.1859987Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:13.1860085Z     return self._apply(convert)
2025-04-11T03:52:13.1860356Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.1860500Z     module._apply(fn)
2025-04-11T03:52:13.1860765Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.1860859Z     param_applied = fn(param)
2025-04-11T03:52:13.1861131Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:13.1861345Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:13.1861452Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1861728Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1861856Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1862017Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1862389Z FAILED tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1862396Z 
2025-04-11T03:52:13.1862513Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1862664Z Traceback (most recent call last):
2025-04-11T03:52:13.1862945Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1863022Z     fn(i, *args)
2025-04-11T03:52:13.1863295Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 156, in run_dist
2025-04-11T03:52:13.1863392Z     check_torch_fsdp_ckpt()
2025-04-11T03:52:13.1863638Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1863725Z     partial_func(**kwargs)
2025-04-11T03:52:13.1864033Z   File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 53, in check_torch_fsdp_ckpt
2025-04-11T03:52:13.1864233Z     fsdp_model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion)
2025-04-11T03:52:13.1864433Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
2025-04-11T03:52:13.1864622Z     model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
2025-04-11T03:52:13.1864887Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 533, in configure
2025-04-11T03:52:13.1865110Z     fsdp_model = TorchFSDPModel(model, device_id=torch.cuda.current_device(), **self.fsdp_kwargs)
2025-04-11T03:52:13.1865379Z   File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 438, in __init__
2025-04-11T03:52:13.1865492Z     self.module = FSDP(module, *args, **kwargs)
2025-04-11T03:52:13.1865860Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 503, in __init__
2025-04-11T03:52:13.1866023Z     _init_param_handle_from_module(
2025-04-11T03:52:13.1866402Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 568, in _init_param_handle_from_module
2025-04-11T03:52:13.1866492Z     _move_module_to_device(
2025-04-11T03:52:13.1866841Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 956, in _move_module_to_device
2025-04-11T03:52:13.1867023Z     _move_states_to_device(params_to_move, bufs_to_move, device_from_device_id)
2025-04-11T03:52:13.1867425Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 986, in _move_states_to_device
2025-04-11T03:52:13.1867544Z     param.data = param.to(device_from_device_id)
2025-04-11T03:52:13.1867643Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1867924Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1868055Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1868274Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1868650Z FAILED tests/test_device/test_init_logical_pg.py::test_logical_pg - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1868656Z 
2025-04-11T03:52:13.1868773Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1868873Z Traceback (most recent call last):
2025-04-11T03:52:13.1869149Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1869231Z     fn(i, *args)
2025-04-11T03:52:13.1869466Z   File "/__w/ColossalAI/ColossalAI/tests/test_device/test_init_logical_pg.py", line 17, in check_layer
2025-04-11T03:52:13.1869588Z     tensor_to_check = torch.tensor([2, 2, 2, 2]).cuda()
2025-04-11T03:52:13.1869687Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1869955Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1870089Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1870243Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1870735Z FAILED tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1870740Z 
2025-04-11T03:52:13.1870852Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1870948Z Traceback (most recent call last):
2025-04-11T03:52:13.1871228Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1871308Z     fn(i, *args)
2025-04-11T03:52:13.1871537Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_all_to_all_single.py", line 67, in run_dist
2025-04-11T03:52:13.1871623Z     check_all2all()
2025-04-11T03:52:13.1871843Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1871942Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1872200Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1872298Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1872563Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1872663Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1872760Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1873038Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1873166Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1873321Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1873669Z FAILED tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1873675Z 
2025-04-11T03:52:13.1873794Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1873885Z Traceback (most recent call last):
2025-04-11T03:52:13.1874165Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1874249Z     fn(i, *args)
2025-04-11T03:52:13.1874470Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all.py", line 31, in run_dist
2025-04-11T03:52:13.1874557Z     check_4gpu()
2025-04-11T03:52:13.1874839Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1874937Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1875188Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1875284Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1875554Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1875710Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1875806Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1876083Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1876218Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1876376Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1876703Z FAILED tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1876710Z 
2025-04-11T03:52:13.1876822Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1876913Z Traceback (most recent call last):
2025-04-11T03:52:13.1877204Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1877279Z     fn(i, *args)
2025-04-11T03:52:13.1877520Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all_single.py", line 29, in run_dist
2025-04-11T03:52:13.1877598Z     check_4gpu()
2025-04-11T03:52:13.1877867Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1877963Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1878211Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1878309Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1878572Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1878669Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1878762Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1879033Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1879164Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1879320Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1879615Z FAILED tests/test_fp8/test_fp8_allgather.py::test_all_gather - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1879621Z 
2025-04-11T03:52:13.1879731Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1879825Z Traceback (most recent call last):
2025-04-11T03:52:13.1880099Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1880180Z     fn(i, *args)
2025-04-11T03:52:13.1880395Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allgather.py", line 37, in run_dist
2025-04-11T03:52:13.1880473Z     check_4gpu()
2025-04-11T03:52:13.1880683Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1880830Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1881086Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1881179Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1881448Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1881545Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1881640Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1881921Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1882110Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1882267Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1882567Z FAILED tests/test_fp8/test_fp8_allreduce.py::test_all_reduce - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1882573Z 
2025-04-11T03:52:13.1882688Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1882837Z Traceback (most recent call last):
2025-04-11T03:52:13.1883117Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1883196Z     fn(i, *args)
2025-04-11T03:52:13.1883412Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allreduce.py", line 47, in run_dist
2025-04-11T03:52:13.1883495Z     check_4gpu()
2025-04-11T03:52:13.1883740Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1883832Z     partial_func(**kwargs)
2025-04-11T03:52:13.1884043Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1884134Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1884388Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1884482Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1884752Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1884845Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1884999Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1885271Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1885399Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1885557Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1885780Z FAILED tests/test_fp8/test_fp8_cast.py::test_fp8_cast - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1886056Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1886182Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1886336Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1886623Z FAILED tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1886628Z 
2025-04-11T03:52:13.1886744Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1886835Z Traceback (most recent call last):
2025-04-11T03:52:13.1887112Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1887195Z     fn(i, *args)
2025-04-11T03:52:13.1887430Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_fsdp_comm_hook.py", line 95, in demo_basic
2025-04-11T03:52:13.1887508Z     run_model()
2025-04-11T03:52:13.1887720Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1887818Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1888122Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1888218Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1888500Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1888600Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1888706Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1888979Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1889113Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1889323Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1889543Z FAILED tests/test_fp8/test_fp8_hook.py::test_fp8_hook - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1889817Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1889947Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1890163Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1890419Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1890695Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1890818Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1890965Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1891227Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1891493Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1891629Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1891779Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1892043Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1892307Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1892495Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1892644Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1892897Z FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1893169Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1893291Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1893447Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1893768Z FAILED tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1893774Z 
2025-04-11T03:52:13.1893897Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1893993Z Traceback (most recent call last):
2025-04-11T03:52:13.1894301Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1894394Z     fn(i, *args)
2025-04-11T03:52:13.1894625Z   File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_reduce_scatter.py", line 36, in run_dist
2025-04-11T03:52:13.1894711Z     check_4gpu()
2025-04-11T03:52:13.1894923Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.1895022Z     get_accelerator().synchronize()
2025-04-11T03:52:13.1895270Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.1895420Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.1895694Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.1895791Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.1895896Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1896170Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1896306Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1896459Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1896761Z FAILED tests/test_infer/test_batch_bucket.py::test_bucket - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1897029Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1897155Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1897310Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1897725Z FAILED tests/test_infer/test_continuous_batching.py::test_continuous_batching - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1897732Z 
2025-04-11T03:52:13.1897851Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1897943Z Traceback (most recent call last):
2025-04-11T03:52:13.1898225Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1898305Z     fn(i, *args)
2025-04-11T03:52:13.1898539Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 61, in run_dist
2025-04-11T03:52:13.1898634Z     check_inference_engine()
2025-04-11T03:52:13.1898882Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1898978Z     partial_func(**kwargs)
2025-04-11T03:52:13.1899223Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1899312Z     partial_func(**kwargs)
2025-04-11T03:52:13.1899554Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1899691Z     partial_func(**kwargs)
2025-04-11T03:52:13.1899797Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.1900081Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 39, in check_inference_engine
2025-04-11T03:52:13.1900249Z     model = LlamaForCausalLM(LlamaConfig(num_hidden_layers=2)).cuda()
2025-04-11T03:52:13.1900536Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.1900642Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.1900908Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.1901026Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.1901303Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.1901387Z     module._apply(fn)
2025-04-11T03:52:13.1901659Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.1901743Z     module._apply(fn)
2025-04-11T03:52:13.1902013Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.1902108Z     param_applied = fn(param)
2025-04-11T03:52:13.1902373Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.1902492Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.1902590Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1902930Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1903058Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1903217Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1903457Z FAILED tests/test_infer/test_drafter.py::test_drafter[5] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1903732Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1903859Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1904071Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1904300Z FAILED tests/test_infer/test_drafter.py::test_spec_dec - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1904567Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1904698Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1904847Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1905224Z FAILED tests/test_infer/test_kvcache_manager.py::test_cache_manager - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1905231Z 
2025-04-11T03:52:13.1905345Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1905441Z Traceback (most recent call last):
2025-04-11T03:52:13.1905718Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1905797Z     fn(i, *args)
2025-04-11T03:52:13.1906030Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 168, in run_dist
2025-04-11T03:52:13.1906118Z     check_cache_manager()
2025-04-11T03:52:13.1906367Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.1906452Z     partial_func(**kwargs)
2025-04-11T03:52:13.1906701Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 89, in check_cache_manager
2025-04-11T03:52:13.1906862Z     cache_manager = KVCacheManager(inference_config, model_config)
2025-04-11T03:52:13.1907123Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T03:52:13.1907346Z     self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T03:52:13.1907637Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T03:52:13.1907858Z     k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T03:52:13.1907959Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1908234Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1908363Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1908569Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1908944Z FAILED tests/test_infer/test_request_handler.py::test_running_list_and_request_handler - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1908951Z 
2025-04-11T03:52:13.1909065Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1909165Z Traceback (most recent call last):
2025-04-11T03:52:13.1909455Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1909540Z     fn(i, *args)
2025-04-11T03:52:13.1909769Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 95, in run_dist
2025-04-11T03:52:13.1909860Z     check_request_handler()
2025-04-11T03:52:13.1910118Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 70, in check_request_handler
2025-04-11T03:52:13.1910351Z     request_handler = RequestHandler(inference_config, model_config)
2025-04-11T03:52:13.1910613Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 160, in __init__
2025-04-11T03:52:13.1910705Z     self._init_cache(model_config)
2025-04-11T03:52:13.1910971Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 222, in _init_cache
2025-04-11T03:52:13.1911149Z     self.cache_manager = KVCacheManager(self.inference_config, model_config)
2025-04-11T03:52:13.1911413Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
2025-04-11T03:52:13.1911640Z     self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
2025-04-11T03:52:13.1911933Z   File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
2025-04-11T03:52:13.1912153Z     k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
2025-04-11T03:52:13.1912256Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1912531Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1912720Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1912881Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1913178Z FAILED tests/test_infer/test_streamingllm.py::test_engine - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.1913182Z 
2025-04-11T03:52:13.1913303Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.1913398Z Traceback (most recent call last):
2025-04-11T03:52:13.1913679Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.1913761Z     fn(i, *args)
2025-04-11T03:52:13.1913984Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 107, in run_dist
2025-04-11T03:52:13.1914085Z     ret[rank] = func_to_run(**kwargs)
2025-04-11T03:52:13.1914335Z   File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 39, in check_streamingllm
2025-04-11T03:52:13.1914415Z     ).cuda()
2025-04-11T03:52:13.1914694Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.1914864Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.1915126Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.1915234Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.1915500Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.1915582Z     module._apply(fn)
2025-04-11T03:52:13.1915844Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.1915927Z     module._apply(fn)
2025-04-11T03:52:13.1916181Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.1916281Z     param_applied = fn(param)
2025-04-11T03:52:13.1916545Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.1916658Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.1916757Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1917034Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1917162Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1917314Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1917684Z FAILED tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1918018Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1918152Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1918307Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1918683Z FAILED tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1918956Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1919152Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1919305Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1919642Z FAILED tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1919918Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1920105Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1920257Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1920582Z FAILED tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1920853Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1920979Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1921135Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1921471Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1921735Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1921862Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1922014Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1922407Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1922678Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1922804Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1922955Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1923292Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1923565Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1923688Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1923843Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1924173Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1924447Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1924571Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1924726Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1925060Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1925383Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1925508Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1925654Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1925994Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1926261Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1926464Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1926612Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1926950Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1927220Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1927403Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1927551Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1927881Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1928154Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1928277Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1928431Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1928765Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1929036Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1929158Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1929307Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1929641Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1929963Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1930089Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1930238Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1930572Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1930835Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1930958Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1931105Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1931441Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1931714Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1931838Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1931991Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1932319Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1932590Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1932766Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1932916Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1933246Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1933515Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1933642Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1933855Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1934199Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1934470Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1934597Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1934800Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1935138Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1935408Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1935530Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1935681Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1936014Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1936285Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1936408Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1936561Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1936899Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1937222Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1937344Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1937495Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1937836Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1938100Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1938229Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1938379Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1938716Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1938983Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1939108Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1939257Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1939592Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1939861Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1940036Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1940191Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1940524Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1940797Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1940917Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1941129Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1941461Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1941727Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1941854Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1942061Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1942404Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1942674Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1942799Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1942948Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1943283Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1943551Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1943673Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1943826Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1944155Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1944481Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1944603Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1944755Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1945092Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1945361Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1945485Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1945631Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1945970Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1946237Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1946361Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1946508Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1946843Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1947109Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1947294Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1947441Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1947785Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1948058Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1948178Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1948331Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1948766Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1949036Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1949159Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1949310Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1949716Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1950000Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1950159Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1950309Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1950657Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1950923Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1951049Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1951198Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1951541Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1951871Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1951996Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1952148Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1952484Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1952756Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1952881Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1953033Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1953370Z FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1953643Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1953764Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1953912Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1954227Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1954489Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1954695Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1954844Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1955164Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1955434Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1955561Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1955711Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1956084Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1956357Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1956479Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1956631Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1956940Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1957266Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1957388Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1957540Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1957847Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1958116Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1958243Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1958392Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1958702Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1958968Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1959155Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1959303Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1959611Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1959887Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1960009Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1960163Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1960479Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1960750Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1960872Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1961023Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1961329Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1961597Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1961723Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1961870Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1962234Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1962500Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1962628Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1962775Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1963089Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1963416Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1963539Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1963689Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1963999Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1964325Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1964446Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1964598Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1964904Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1965172Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1965296Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1965443Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1965752Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1966019Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1966147Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1966293Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1966661Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1966931Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1967060Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1967210Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1967522Z FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1967790Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1967915Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1968065Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1968432Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1968703Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1968827Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1968977Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1969336Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1969658Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1969783Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1969930Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1970286Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1970549Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1970742Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1970889Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1971237Z FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1971506Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1971687Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1971837Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1972167Z FAILED tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1972439Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1972563Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1972712Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1973037Z FAILED tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1973309Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1973434Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1973581Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1974046Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1974318Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1974445Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1974594Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1975000Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1975268Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1975394Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1975540Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1975939Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1976215Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1976339Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1976495Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1976895Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1977219Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1977341Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1977492Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1977890Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1978156Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1978348Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1978496Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1978905Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1979171Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1979361Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1979511Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1979911Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1980180Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1980306Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1980459Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1980858Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1981133Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1981255Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1981472Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1981862Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1982134Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1982254Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1982402Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1982798Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1983064Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1983190Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1983340Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1983731Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1983998Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1984124Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1984272Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1984726Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1985001Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1985128Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1985278Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1985675Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1986006Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1986128Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1986280Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1986673Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1987105Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1987229Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1987376Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1987769Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1988036Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1988164Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1988314Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1988745Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1989014Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1989207Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1989354Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1989746Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1990019Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1990141Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1990294Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1990691Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1990963Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1991085Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1991236Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1991634Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1991901Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1992026Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1992244Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1992650Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1992917Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1993046Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1993194Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1993664Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1993935Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1994059Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1994209Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1994699Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1994980Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1995103Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1995254Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1995658Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1995932Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1996057Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1996208Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1996630Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1996956Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1997085Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1997233Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1997635Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1997905Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1998032Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1998180Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1998579Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1998852Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1998978Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.1999129Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.1999531Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.1999805Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.1999986Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2000138Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2000539Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2000815Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2000937Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2001145Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2001546Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2001814Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2001940Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2002144Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2002548Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2002817Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2002941Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2003089Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2003485Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2003760Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2003884Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2004035Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2004436Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2004767Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2004893Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2005042Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2005436Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2005705Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2005834Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2005985Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2006387Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2006655Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2006781Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2006929Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2007321Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2007642Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2007766Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2007916Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2008312Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2008585Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2008771Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2008923Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2009314Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2009590Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2009784Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2009932Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2010330Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2010597Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2010724Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2010869Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2011265Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2011536Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2011663Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2011808Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2012256Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2012527Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2012652Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2012804Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2013196Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2013467Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2013592Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2013744Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2014133Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2014406Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2014529Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2014677Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2015072Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2015401Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2015528Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2015677Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2016075Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2016407Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2016532Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2016680Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2017072Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2017394Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2017517Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2017672Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2018066Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2018336Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2018457Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2018607Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2019005Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2019275Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2019454Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2019600Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2019996Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2020265Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2020389Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2020536Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2020936Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2021205Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2021330Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2021481Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2021878Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2022151Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2022272Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2022485Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2022879Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2023152Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2023273Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2023424Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2023821Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2024145Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2024271Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2024421Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2024819Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2025144Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2025272Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2025420Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2025818Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2026093Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2026214Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2026368Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2026763Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2027036Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2027263Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2027412Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2027809Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2028084Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2028207Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2028353Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2028804Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2029074Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2029201Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2029348Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2029753Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2030019Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2030224Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2030372Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2030767Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2031037Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2031159Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2031307Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2031768Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2032042Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2032164Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2032318Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2032777Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2033046Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2033175Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2033324Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2033729Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2034000Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2034126Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2034276Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2034673Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2035024Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2035146Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2035297Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2035694Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2035967Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2036089Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2036241Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2036631Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2036901Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2037022Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2037170Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2037566Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2037903Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2038028Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2038178Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2038576Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2038846Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2038972Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2039195Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2039593Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2039866Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2040046Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2040197Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2040594Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2040868Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2040992Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2041142Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2041540Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2041808Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2041937Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2042087Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2042542Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2042812Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2042943Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2043092Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2043494Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2043761Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2043886Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2044033Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2044434Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2044710Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2044834Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2044987Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2045383Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2045711Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2045834Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2045985Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2046380Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2046648Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2046837Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2046988Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2047392Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2047666Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2047850Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2048003Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2048404Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2048671Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2048799Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2048952Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2049355Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2049630Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2049752Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2049962Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2050365Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2050691Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2050819Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2050967Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2051373Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2051644Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2051771Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2051920Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2052317Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2052586Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2052713Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2052861Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2053318Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2053592Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2053717Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2053869Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2054267Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2054599Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2054720Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2054871Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2055272Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2055591Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2055718Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2055865Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2056265Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2056537Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2056664Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2056812Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2057219Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2057488Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2057671Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2057818Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2058216Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2058489Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2058610Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2058765Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2059169Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2059441Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2059565Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2059715Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2060115Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2060384Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2060565Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2060711Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2061114Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2061386Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2061511Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2061655Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2062130Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2062401Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2062527Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2062679Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2063132Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2063405Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2063528Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2063677Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2064079Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2064351Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2064474Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2064621Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2065030Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2065356Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2065481Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2065627Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2066030Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2066297Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2066424Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2066572Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2066977Z FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2067249Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2067370Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2067522Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2067909Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2068186Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2068377Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2068574Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2068957Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2069228Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2069352Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2069577Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2069965Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2070234Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2070363Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2070572Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2070958Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2071225Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2071351Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2071502Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2071881Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2072156Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2072278Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2072432Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2072815Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2073149Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2073271Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2073422Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2073803Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2074072Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2074195Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2074344Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2074732Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2074995Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2075121Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2075269Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2075647Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2075985Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2076110Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2076263Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2076642Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2076914Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2077099Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2077250Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2077627Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2077901Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2078078Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2078224Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2078609Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2078876Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2079004Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2079149Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2079530Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2079799Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2079927Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2080075Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2080517Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2080792Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2080918Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2081068Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2081443Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2081718Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2081844Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2081996Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2082380Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2082652Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2082782Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2082928Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2083316Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2083641Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2083769Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2083919Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2084307Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2084582Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2084768Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2084917Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2085297Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2085573Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2085747Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2085898Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2086284Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2086555Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2086679Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2086827Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2087210Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2087478Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2087606Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2087807Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2088207Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2088478Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2088605Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2088755Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2089140Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2089416Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2089539Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2089697Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2090088Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2090363Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2090486Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2090638Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2091023Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2091347Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2091471Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2091624Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2092010Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2092333Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2092457Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2092604Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2092985Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2093308Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2093433Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2093584Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2093969Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2094244Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2094365Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2094519Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2094901Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2095172Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2095293Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2095506Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2095897Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2096166Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2096291Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2096436Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2096824Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2097093Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2097218Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2097366Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2097752Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2098022Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2098141Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2098293Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2098732Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2099006Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2099129Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2099277Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2099654Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2099989Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2100112Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2100259Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2100642Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2100963Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2101091Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2101237Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2101614Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2101885Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2102011Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2102159Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2102539Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2102810Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2102993Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2103145Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2103524Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2103795Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2103918Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2104070Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2104446Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2104716Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2104843Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2104990Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2105372Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2105641Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2105766Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2105971Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2106359Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2106629Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2106753Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2106907Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2107288Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2107709Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2107832Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2107985Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2108361Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2108724Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2108848Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2108998Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2109380Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2109650Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2109777Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2109927Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2110307Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2110576Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2110770Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2110918Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2111299Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2111571Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2111694Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2111848Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2112221Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2112496Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2112617Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2112767Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2113148Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2113414Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2113603Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2113749Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2114137Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2114408Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2114533Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2114681Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2115135Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2115402Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2115524Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2115678Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2116122Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2116398Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2116518Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2116670Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2117054Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2117324Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2117447Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2117595Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2117982Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2118307Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2118435Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2118584Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2118983Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2119251Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2119382Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2119528Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2119910Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2120183Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2120305Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2120457Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2120844Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2121117Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2121295Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2121443Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2121822Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2122094Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2122215Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2122361Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2122810Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2123075Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2123204Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2123419Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2123805Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2124076Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2124204Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2124357Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2124742Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2125015Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2125139Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2125292Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2125678Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2126005Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2126130Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2126282Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2126668Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2126935Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2127061Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2127209Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2127597Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2127863Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2127990Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2128141Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2128528Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2128795Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2128976Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2129127Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2129507Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2129781Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2129903Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2130121Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2130503Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2130776Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2130901Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2131105Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2131488Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2131758Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2131884Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2132032Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2132409Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2132677Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2132809Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2132955Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2133333Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2133662Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2133790Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2133941Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2134323Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2134591Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2134712Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2134862Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2135237Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2135500Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2135627Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2135773Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2136159Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2136499Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2136628Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2136779Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2137163Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2137435Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2137637Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2137788Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2138160Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2138431Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2138609Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2138761Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2139140Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2139414Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2139539Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2139687Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2140070Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2140342Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2140471Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2140619Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2141056Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2141327Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2141455Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2141602Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2141985Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2142260Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2142384Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2142537Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2142917Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2143192Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2143316Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2143465Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2143846Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2144173Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2144297Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2144445Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2144829Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2145094Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2145282Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2145432Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2145818Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2146093Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2146272Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2146424Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2146800Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2147069Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2147193Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2147344Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2147725Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2147999Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2148124Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2148328Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2148758Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2149022Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2149150Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2149297Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2149688Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2149957Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2150080Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2150227Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2150608Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2150876Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2150996Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2151155Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2151640Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2151914Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2152036Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2152187Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2152566Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2152920Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2153046Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2153195Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2153587Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2153915Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2154041Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2154189Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2154573Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2154843Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2154967Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2155117Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2155505Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2155779Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2155967Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2156119Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2156506Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2156784Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2156905Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2157056Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2157445Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2157714Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2157840Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2157991Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2158374Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2158645Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2158771Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2158920Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2159366Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2159638Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2159768Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2159917Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2160299Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2160634Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2160754Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2160903Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2161277Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2161603Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2161728Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2161875Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2162260Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2162530Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2162658Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2162806Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2163185Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2163453Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2163631Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2163777Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2164153Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2164422Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2164544Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2164696Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2165073Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2165344Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2165468Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2165622Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2165996Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2166265Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2166386Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2166593Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2166976Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2167244Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2167371Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2167518Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2167895Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2168223Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2168347Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2168497Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2168870Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2169197Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2169321Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2169475Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2169850Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2170126Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2170247Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2170400Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2170773Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2171040Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2171223Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2171370Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2171756Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2172024Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2172151Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2172298Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2172684Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2172955Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2173075Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2173227Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2173603Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2173877Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2174053Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2174201Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2174580Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2174851Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2174972Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2175118Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2175561Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2175831Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2175959Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2176106Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2176549Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2176819Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2176943Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2177091Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2177470Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2177742Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2177865Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2178016Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2178399Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2178718Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2178838Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2178990Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2179370Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2179638Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2179765Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2179913Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2180301Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2180566Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2180690Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2180836Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2181215Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2181483Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2181678Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2181830Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2182214Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2182487Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2182608Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2182818Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2183196Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2183470Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2183593Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2183796Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2184179Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2184447Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2184572Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2184723Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2185104Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2185372Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2185498Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2185647Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2186023Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2186347Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2186468Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2186623Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2187001Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2187270Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2187394Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2187545Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2187926Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2188196Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2188318Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2188503Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2188886Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2189215Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2189340Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2189489Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2189874Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2190143Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2190336Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2190484Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2190865Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2191138Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2191320Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2191472Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2191854Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2192128Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2192250Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2192402Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2192781Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2193049Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2193177Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2193325Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2193772Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2194043Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2194170Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2194316Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2194707Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2194979Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2195104Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2195256Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2195642Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2195914Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2196038Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2196189Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2196571Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2196897Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2197021Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2197167Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2197552Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2197819Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2198008Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2198155Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2198541Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2198810Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2198991Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2199141Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2199523Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2199795Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2199918Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2200067Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2200445Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2200721Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2200843Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2200994Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2201438Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2201711Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2201841Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2201990Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2202382Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2202655Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2202783Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2202935Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2203327Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2203595Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2203722Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2203870Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2204247Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2204570Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2204693Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2204848Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2205232Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2205503Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2205684Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2205831Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2206217Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2206591Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2206721Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2206870Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2207256Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2207525Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2207651Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2207798Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2208182Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2208451Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2208571Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2208809Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2209199Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2209472Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2209594Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2209744Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2210134Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2210407Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2210532Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2210678Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2211079Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2211347Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2211472Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2211620Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2212067Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2212338Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2212468Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2212618Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2213012Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2213345Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2213467Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2213616Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2213999Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2214324Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2214447Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2214598Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2214984Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2215256Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2215384Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2215531Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2215922Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2216188Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2216373Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2216521Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2216914Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2217184Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2217305Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2217461Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2217848Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2218121Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2218244Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2218399Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2218791Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2219062Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2219184Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2219387Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2219773Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2220040Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2220168Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2220314Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2220705Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2221033Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2221157Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2221311Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2221695Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2222021Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2222146Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2222299Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2222679Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2222952Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2223072Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2223227Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2223606Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2223874Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2224055Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2224201Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2224593Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2224857Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2224985Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2225136Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2225523Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2225794Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2225924Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2226075Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2226464Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2226740Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2227006Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2227155Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2227539Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2227811Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2227933Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2228085Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2228567Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2228834Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2228962Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2229109Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2229559Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2229833Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2229959Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2230107Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2230495Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2230763Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2230886Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2231040Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2231423Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2231755Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2231878Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2232030Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2232414Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2232687Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2232811Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2232962Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2233348Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2233616Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2233744Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2233894Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2234284Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2234553Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2234740Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2234892Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2235276Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2235551Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2235671Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2235893Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2236274Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2236546Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2236666Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2236885Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2237269Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2237539Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2237668Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2237820Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2238207Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2238475Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2238600Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2238749Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2239139Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2239461Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2239582Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2239734Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2240120Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2240391Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2240513Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2240671Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2241053Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2241322Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2241444Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2241592Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2241984Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2242313Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2242442Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2242590Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2242978Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2243249Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2243437Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2243584Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2243979Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2244255Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2244435Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2244590Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2244983Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2245262Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2245388Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2245541Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2245929Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2246204Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2246328Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2246477Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2246923Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2247193Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2247321Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2247468Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2247855Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2248124Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2248251Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2248397Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2248786Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2249056Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2249181Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2249334Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2249722Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2250057Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2250182Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2250332Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2250711Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2250972Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2251156Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2251305Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2251697Z FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2252020Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2252204Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2252351Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2252721Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2252990Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2253115Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2253266Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2253625Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2253898Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2254021Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2254173Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2254586Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2254856Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2254979Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2255128Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2255490Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2255760Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2255890Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2256041Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2256402Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2256671Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2256801Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2256952Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2257302Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2257627Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2257750Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2257903Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2258260Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2258530Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2258714Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2258865Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2259225Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2259492Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2259676Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2259827Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2260191Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2260460Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2260587Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2260734Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2261100Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2261369Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2261493Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2261702Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2262055Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2262324Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2262448Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2262598Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2262953Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2263221Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2263342Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2263490Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2263844Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2264105Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2264232Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2264376Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2264733Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2265055Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2265183Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2265334Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2265687Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2265959Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2266152Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2266304Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2266658Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2266988Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2267109Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2267261Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2267612Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2267882Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2268011Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2268160Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2268560Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2268832Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2268959Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2269179Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2269538Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2269806Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2269926Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2270078Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2270437Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2270708Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2270831Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2270983Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2271339Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2271606Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2271726Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2271872Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2272307Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2272577Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2272705Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2272858Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2273217Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2273558Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2273685Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2273836Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2274199Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2274537Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2274658Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2274813Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2275167Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2275439Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2275565Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2275717Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2276075Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2276344Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2276471Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2276672Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2277035Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2277302Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2277425Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2277573Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2277940Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2278207Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2278328Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2278482Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2278835Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2279107Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2279227Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2279380Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2279799Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2280076Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2280199Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2280351Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2280716Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2281049Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2281178Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2281326Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2281696Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2282029Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2282157Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2282305Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2282662Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2282934Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2283059Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2283212Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2283577Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2283849Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2284033Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2284182Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2284540Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2284812Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2284936Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2285083Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2285455Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2285727Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2285854Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2286003Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2286366Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2286637Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2286764Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2286913Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2287328Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2287604Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2287728Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2287882Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2288237Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2288564Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2288686Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2288836Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2289197Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2289517Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2289645Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2289790Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2290146Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2290413Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2290540Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2290688Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2291045Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2291315Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2291493Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2291645Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2292007Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2292280Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2292403Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2292559Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2292925Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2293200Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2293324Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2293476Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2293845Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2294113Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2294238Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2294385Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2294820Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2295091Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2295219Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2295368Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2295731Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2296061Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2296182Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2296336Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2296698Z FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2297021Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2297145Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2297294Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2297593Z FAILED tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2297859Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2297987Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2298135Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2298514Z FAILED tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2298778Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2298967Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2299115Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2299491Z FAILED tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2299763Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2299885Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2300039Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2300371Z FAILED tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2300641Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2300764Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2300914Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2301187Z FAILED tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2301457Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2301580Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2301730Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2301999Z FAILED tests/test_lazy/test_ops.py::test_lazy_ops - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2302264Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2302395Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2302545Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2302846Z FAILED tests/test_lora/test_lora.py::test_torch_ddp_lora - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2302851Z 
2025-04-11T03:52:13.2302969Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2303133Z Traceback (most recent call last):
2025-04-11T03:52:13.2303438Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2303521Z     fn(i, *args)
2025-04-11T03:52:13.2303734Z   File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 103, in run_dist
2025-04-11T03:52:13.2303820Z     run_lora_test()
2025-04-11T03:52:13.2304034Z   File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 98, in run_lora_test
2025-04-11T03:52:13.2304269Z     check_fwd_bwd(model_fn, data_gen_fn, output_transform_fn, loss_fn, task_type)
2025-04-11T03:52:13.2304486Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.2304589Z     get_accelerator().synchronize()
2025-04-11T03:52:13.2304844Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.2304947Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.2305221Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.2305326Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.2305425Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2305697Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2305829Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2305984Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2306237Z FAILED tests/test_moe/test_kernel.py::test_moe_kernel[data_type0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2306563Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2306694Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2306845Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2307094Z FAILED tests/test_moe/test_kernel.py::test_moe_kernel[data_type1] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2307364Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2307491Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2307644Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2307971Z FAILED tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2307978Z 
2025-04-11T03:52:13.2308100Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2308194Z Traceback (most recent call last):
2025-04-11T03:52:13.2308525Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2308606Z     fn(i, *args)
2025-04-11T03:52:13.2308831Z   File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 165, in run_dist
2025-04-11T03:52:13.2308924Z     check_moe_checkpoint()
2025-04-11T03:52:13.2309177Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2309339Z     partial_func(**kwargs)
2025-04-11T03:52:13.2309592Z   File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 101, in check_moe_checkpoint
2025-04-11T03:52:13.2309734Z     dist.broadcast_object_list(broadcast_objects, src=0)
2025-04-11T03:52:13.2310024Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:13.2310120Z     return func(*args, **kwargs)
2025-04-11T03:52:13.2310474Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in broadcast_object_list
2025-04-11T03:52:13.2310755Z     tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T03:52:13.2311088Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in <listcomp>
2025-04-11T03:52:13.2311293Z     tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
2025-04-11T03:52:13.2311639Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2115, in _object_to_tensor
2025-04-11T03:52:13.2311838Z     byte_tensor = torch.ByteTensor(byte_storage).to(device)
2025-04-11T03:52:13.2311945Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2312224Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2312353Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2312510Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2312856Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2313131Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2313256Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2313410Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2313748Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2314087Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2314209Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2314356Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2314693Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2314961Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2315089Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2315235Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2315568Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2315841Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2315967Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2316113Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2316439Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2316711Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2316833Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2317048Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2317368Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2317636Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2317760Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2317911Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2318234Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2318615Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2318748Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2318902Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2319234Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2319557Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2319685Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2319832Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2320158Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2320427Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2320547Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2320703Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2321023Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2321297Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2321489Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2321645Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2321976Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2322259Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2322385Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2322538Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2322866Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2323141Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2323276Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2323425Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2323761Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2324038Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2324171Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2324325Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2324708Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2324983Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2325109Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2325261Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2325590Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2325924Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2326045Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2326194Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2326521Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2326846Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2326974Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2327121Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2327450Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2327716Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2327843Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2327990Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2328313Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2328586Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2328765Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2328916Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2329241Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2329518Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2329642Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2329794Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2330126Z FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2330404Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2330529Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2330678Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2331070Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2331339Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2331466Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2331613Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2332050Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2332320Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2332446Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2332598Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2332979Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2333311Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2333432Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2333585Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2333962Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2334296Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2334423Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2334579Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2334956Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2335235Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2335364Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2335512Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2335892Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2336168Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2336357Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2336506Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2336878Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2337151Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2337274Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2337426Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2337802Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2338075Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2338200Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2338351Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2338724Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2338997Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2339121Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2339268Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2339700Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2339971Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2340100Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2340248Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2340629Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2341017Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2341142Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2341290Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2341662Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2341991Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2342118Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2342269Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2342639Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2342915Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2343040Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2343190Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2343557Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2343826Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2344080Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2344226Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2344600Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2344868Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2344997Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2345143Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2345518Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2345787Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2345910Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2346061Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2346440Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2346712Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2346835Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2346985Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2347409Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2347683Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2347808Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2347955Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2348336Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2348712Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2348840Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2348987Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2349367Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2349711Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2349838Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2349985Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2350353Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2350624Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2350743Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2350895Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2351256Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2351525Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2351712Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2351864Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2352233Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2352559Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2352688Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2352840Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2353214Z FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2353484Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2353610Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2353759Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2354093Z FAILED tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2354100Z 
2025-04-11T03:52:13.2354220Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2354319Z Traceback (most recent call last):
2025-04-11T03:52:13.2354610Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2354755Z     fn(i, *args)
2025-04-11T03:52:13.2355004Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 459, in run_dist
2025-04-11T03:52:13.2355100Z     exam_dist_adafactor_base()
2025-04-11T03:52:13.2355358Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2355448Z     partial_func(**kwargs)
2025-04-11T03:52:13.2355697Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2355790Z     partial_func(**kwargs)
2025-04-11T03:52:13.2356073Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 111, in exam_dist_adafactor_base
2025-04-11T03:52:13.2356317Z     model_col = nn.Linear(H, W).to(local_rank)  # Col parallel weight
2025-04-11T03:52:13.2356581Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:13.2356681Z     return self._apply(convert)
2025-04-11T03:52:13.2356951Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2357098Z     param_applied = fn(param)
2025-04-11T03:52:13.2357372Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:13.2357585Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:13.2357693Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2357967Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2358100Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2358256Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2358564Z FAILED tests/test_optimizer/test_dist_came.py::test_dist_came - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2358571Z 
2025-04-11T03:52:13.2358691Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2358788Z Traceback (most recent call last):
2025-04-11T03:52:13.2359071Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2359207Z     fn(i, *args)
2025-04-11T03:52:13.2359443Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 349, in run_dist
2025-04-11T03:52:13.2359586Z     exam_bert_test_on_lowlevelzero_plugin()  # err in TODO layer
2025-04-11T03:52:13.2359841Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2359930Z     partial_func(**kwargs)
2025-04-11T03:52:13.2360225Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 206, in exam_bert_test_on_lowlevelzero_plugin
2025-04-11T03:52:13.2360453Z     ) = build_model_from_low_level_zero_plugin(model_fn, loss_fn, test_config, CAME, DistributedCAME)
2025-04-11T03:52:13.2360767Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 188, in build_model_from_low_level_zero_plugin
2025-04-11T03:52:13.2360867Z     org_model = org_model.cuda()
2025-04-11T03:52:13.2361146Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2361250Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2361511Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2361632Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2361898Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2361984Z     module._apply(fn)
2025-04-11T03:52:13.2362253Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2362393Z     module._apply(fn)
2025-04-11T03:52:13.2362658Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2362754Z     param_applied = fn(param)
2025-04-11T03:52:13.2363023Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2363137Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2363239Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2363520Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2363712Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2363870Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2364188Z FAILED tests/test_optimizer/test_dist_galore.py::test_dist_galore - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2364194Z 
2025-04-11T03:52:13.2364315Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2364463Z Traceback (most recent call last):
2025-04-11T03:52:13.2364740Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2364825Z     fn(i, *args)
2025-04-11T03:52:13.2365076Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_galore.py", line 291, in check_dist_galore
2025-04-11T03:52:13.2365163Z     dist.barrier()
2025-04-11T03:52:13.2365447Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:13.2365543Z     return func(*args, **kwargs)
2025-04-11T03:52:13.2365849Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T03:52:13.2365948Z     work = default_pg.barrier(opts=opts)
2025-04-11T03:52:13.2366052Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2366329Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2366463Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2366616Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2366978Z FAILED tests/test_optimizer/test_dist_lamb.py::test_dist_lamb - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2366983Z 
2025-04-11T03:52:13.2367099Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2367200Z Traceback (most recent call last):
2025-04-11T03:52:13.2367478Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2367555Z     fn(i, *args)
2025-04-11T03:52:13.2367805Z   File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_lamb.py", line 263, in check_dist_lamb
2025-04-11T03:52:13.2367892Z     run_dist_lamb_basic()
2025-04-11T03:52:13.2368152Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2368240Z     partial_func(**kwargs)
2025-04-11T03:52:13.2368495Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2368583Z     partial_func(**kwargs)
2025-04-11T03:52:13.2368825Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2368915Z     partial_func(**kwargs)
2025-04-11T03:52:13.2369131Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
2025-04-11T03:52:13.2369232Z     get_accelerator().synchronize()
2025-04-11T03:52:13.2369485Z   File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
2025-04-11T03:52:13.2369584Z     torch.cuda.synchronize(device)
2025-04-11T03:52:13.2369909Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
2025-04-11T03:52:13.2370007Z     return torch._C._cuda_synchronize()
2025-04-11T03:52:13.2370112Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2370386Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2370520Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2370676Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2371011Z FAILED tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2371080Z 
2025-04-11T03:52:13.2371195Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2371291Z Traceback (most recent call last):
2025-04-11T03:52:13.2371570Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2371652Z     fn(i, *args)
2025-04-11T03:52:13.2371897Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 73, in run_dist
2025-04-11T03:52:13.2372045Z     check_p2p_communication()
2025-04-11T03:52:13.2372336Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 21, in check_p2p_communication
2025-04-11T03:52:13.2372503Z     tensor = torch.ones(1, device=get_accelerator().get_current_device())
2025-04-11T03:52:13.2372603Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2372879Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2373008Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2373164Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2373508Z FAILED tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2373515Z 
2025-04-11T03:52:13.2373634Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2373728Z Traceback (most recent call last):
2025-04-11T03:52:13.2374009Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2374143Z     fn(i, *args)
2025-04-11T03:52:13.2374373Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 68, in run_dist
2025-04-11T03:52:13.2374465Z     check_stage_manager()
2025-04-11T03:52:13.2374716Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 56, in check_stage_manager
2025-04-11T03:52:13.2374814Z     dist.barrier(group=group)
2025-04-11T03:52:13.2375105Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:13.2375198Z     return func(*args, **kwargs)
2025-04-11T03:52:13.2375507Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3441, in barrier
2025-04-11T03:52:13.2375609Z     work = group.barrier(opts=opts)
2025-04-11T03:52:13.2375713Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2375984Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2376117Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2376270Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2376614Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2376620Z 
2025-04-11T03:52:13.2376735Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2376827Z Traceback (most recent call last):
2025-04-11T03:52:13.2377105Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2377239Z     fn(i, *args)
2025-04-11T03:52:13.2377502Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:13.2377599Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2377861Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2377973Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2378235Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2378403Z     module._apply(fn)
2025-04-11T03:52:13.2378665Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2378753Z     module._apply(fn)
2025-04-11T03:52:13.2379013Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2379108Z     param_applied = fn(param)
2025-04-11T03:52:13.2379379Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2379549Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2379649Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2379924Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2380057Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2380209Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2380559Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2380564Z 
2025-04-11T03:52:13.2380675Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2380771Z Traceback (most recent call last):
2025-04-11T03:52:13.2381044Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2381122Z     fn(i, *args)
2025-04-11T03:52:13.2381379Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:13.2381471Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2381789Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2381896Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2382157Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2382241Z     module._apply(fn)
2025-04-11T03:52:13.2382500Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2382582Z     module._apply(fn)
2025-04-11T03:52:13.2382838Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2382935Z     param_applied = fn(param)
2025-04-11T03:52:13.2383200Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2383312Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2383410Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2383684Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2383815Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2383973Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2384320Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2384325Z 
2025-04-11T03:52:13.2384437Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2384591Z Traceback (most recent call last):
2025-04-11T03:52:13.2384867Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2384951Z     fn(i, *args)
2025-04-11T03:52:13.2385207Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:13.2385299Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2385567Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2385671Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2386000Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2386081Z     module._apply(fn)
2025-04-11T03:52:13.2386343Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2386426Z     module._apply(fn)
2025-04-11T03:52:13.2386684Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2386836Z     param_applied = fn(param)
2025-04-11T03:52:13.2387102Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2387214Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2387312Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2387593Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2387725Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2387880Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2388225Z FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2388232Z 
2025-04-11T03:52:13.2388345Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2388492Z Traceback (most recent call last):
2025-04-11T03:52:13.2388773Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2388852Z     fn(i, *args)
2025-04-11T03:52:13.2389175Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
2025-04-11T03:52:13.2389267Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2389522Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2389627Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2389890Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2389970Z     module._apply(fn)
2025-04-11T03:52:13.2390232Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2390316Z     module._apply(fn)
2025-04-11T03:52:13.2390578Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2390668Z     param_applied = fn(param)
2025-04-11T03:52:13.2390935Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2391045Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2391142Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2391415Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2391547Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2391703Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2392027Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2392099Z 
2025-04-11T03:52:13.2392219Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2392318Z Traceback (most recent call last):
2025-04-11T03:52:13.2392594Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2392678Z     fn(i, *args)
2025-04-11T03:52:13.2392939Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:13.2393046Z     examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:13.2393376Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:13.2393468Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2393734Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2393842Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2394105Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2394251Z     module._apply(fn)
2025-04-11T03:52:13.2394514Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2394597Z     module._apply(fn)
2025-04-11T03:52:13.2394853Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2394947Z     param_applied = fn(param)
2025-04-11T03:52:13.2395208Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2395319Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2395417Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2395691Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2395819Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2395975Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2396298Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2396358Z 
2025-04-11T03:52:13.2396475Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2396575Z Traceback (most recent call last):
2025-04-11T03:52:13.2396849Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2396933Z     fn(i, *args)
2025-04-11T03:52:13.2397185Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:13.2397292Z     examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:13.2397551Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:13.2397646Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2397911Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2398017Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2398286Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2398371Z     module._apply(fn)
2025-04-11T03:52:13.2398635Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2398721Z     module._apply(fn)
2025-04-11T03:52:13.2398979Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2399073Z     param_applied = fn(param)
2025-04-11T03:52:13.2399338Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2399501Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2399599Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2399878Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2400007Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2400162Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2400497Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2400565Z 
2025-04-11T03:52:13.2400680Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2400779Z Traceback (most recent call last):
2025-04-11T03:52:13.2401055Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2401136Z     fn(i, *args)
2025-04-11T03:52:13.2401391Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:13.2401559Z     examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:13.2401817Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:13.2401913Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2402175Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2402283Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2402551Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2402636Z     module._apply(fn)
2025-04-11T03:52:13.2402902Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2402987Z     module._apply(fn)
2025-04-11T03:52:13.2403251Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2403349Z     param_applied = fn(param)
2025-04-11T03:52:13.2403617Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2403782Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2403880Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2404159Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2404288Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2404447Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2404779Z FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2404786Z 
2025-04-11T03:52:13.2404899Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2405003Z Traceback (most recent call last):
2025-04-11T03:52:13.2405279Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2405362Z     fn(i, *args)
2025-04-11T03:52:13.2405618Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
2025-04-11T03:52:13.2405723Z     examine_pp(num_microbatch, batch_size)
2025-04-11T03:52:13.2405979Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
2025-04-11T03:52:13.2406072Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2406334Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2406437Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2406695Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2406843Z     module._apply(fn)
2025-04-11T03:52:13.2407110Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2407193Z     module._apply(fn)
2025-04-11T03:52:13.2407457Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2407554Z     param_applied = fn(param)
2025-04-11T03:52:13.2407818Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2408050Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2408148Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2408429Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2408560Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2408717Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2409058Z FAILED tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2409116Z 
2025-04-11T03:52:13.2409233Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2409334Z Traceback (most recent call last):
2025-04-11T03:52:13.2409612Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2409691Z     fn(i, *args)
2025-04-11T03:52:13.2409964Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 1070, in run_dist
2025-04-11T03:52:13.2410065Z     run_with_booster_moehybridplugin()
2025-04-11T03:52:13.2410322Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2410408Z     partial_func(**kwargs)
2025-04-11T03:52:13.2410755Z   File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 788, in run_with_booster_moehybridplugin
2025-04-11T03:52:13.2410883Z     torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T03:52:13.2411166Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2411322Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2411578Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2411691Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2411954Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2412042Z     module._apply(fn)
2025-04-11T03:52:13.2412304Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2412398Z     param_applied = fn(param)
2025-04-11T03:52:13.2412665Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2412774Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2412870Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2413144Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2413275Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2413430Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2413724Z FAILED tests/test_shardformer/test_flash_attention.py::test_flash_attn_func - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2413997Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2414130Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2414338Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2414595Z FAILED tests/test_shardformer/test_shard_utils.py::test_release_layer - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2414867Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2414995Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2415146Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2415392Z FAILED tests/test_shardformer/test_with_torch_ddp.py::test_gpt2 - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2415723Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2415846Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2415998Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2416358Z FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2416685Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2416812Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2416960Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2417326Z FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2417594Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2417719Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2417868Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2418228Z FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2418496Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2418671Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2418822Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2419200Z FAILED tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2419207Z 
2025-04-11T03:52:13.2419330Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2419426Z Traceback (most recent call last):
2025-04-11T03:52:13.2419708Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2419788Z     fn(i, *args)
2025-04-11T03:52:13.2420111Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dist_crossentropy.py", line 20, in check_dist_crossentropy
2025-04-11T03:52:13.2420250Z     pred = torch.randn(2, 4, 8, requires_grad=True).cuda()
2025-04-11T03:52:13.2420346Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2420623Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2420747Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2420902Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2421227Z FAILED tests/test_shardformer/test_layer/test_dropout.py::test_dropout - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2421232Z 
2025-04-11T03:52:13.2421351Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2421446Z Traceback (most recent call last):
2025-04-11T03:52:13.2421784Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2421867Z     fn(i, *args)
2025-04-11T03:52:13.2422120Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 60, in run_dist
2025-04-11T03:52:13.2422222Z     check_dropout_parallel_input()
2025-04-11T03:52:13.2422527Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 12, in check_dropout_parallel_input
2025-04-11T03:52:13.2422751Z     dropout_1d = DropoutForParallelInput.from_native_module(dropout, process_group=None)
2025-04-11T03:52:13.2423086Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 42, in from_native_module
2025-04-11T03:52:13.2423301Z     return DropoutForParallelInput(p=p, inplace=inplace, process_group=process_group)
2025-04-11T03:52:13.2423547Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 31, in __init__
2025-04-11T03:52:13.2423751Z     self.randomizer = create_randomizer_with_offset(seed, process_group=process_group)
2025-04-11T03:52:13.2424111Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 318, in create_randomizer_with_offset
2025-04-11T03:52:13.2424299Z     is_synchronized = Randomizer.is_randomizer_index_synchronized(process_group)
2025-04-11T03:52:13.2424607Z   File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 258, in is_randomizer_index_synchronized
2025-04-11T03:52:13.2424842Z     index_tensor = torch.tensor(index, dtype=torch.int32, device=get_accelerator().get_current_device())
2025-04-11T03:52:13.2424948Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2425223Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2425351Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2425510Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2425865Z FAILED tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2425871Z 
2025-04-11T03:52:13.2425995Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2426088Z Traceback (most recent call last):
2025-04-11T03:52:13.2426430Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2426507Z     fn(i, *args)
2025-04-11T03:52:13.2426765Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 47, in run_dist
2025-04-11T03:52:13.2426856Z     check_embedding_1d()
2025-04-11T03:52:13.2427104Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2427197Z     partial_func(**kwargs)
2025-04-11T03:52:13.2427475Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 18, in check_embedding_1d
2025-04-11T03:52:13.2427588Z     embedding = nn.Embedding(32, 128).cuda()
2025-04-11T03:52:13.2427852Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2427962Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2428227Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2428320Z     param_applied = fn(param)
2025-04-11T03:52:13.2428648Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2428758Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2428865Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2429140Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2429359Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2429516Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2429892Z FAILED tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2429904Z 
2025-04-11T03:52:13.2430018Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2430111Z Traceback (most recent call last):
2025-04-11T03:52:13.2430393Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2430539Z     fn(i, *args)
2025-04-11T03:52:13.2430846Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 204, in run_dist
2025-04-11T03:52:13.2430941Z     check_gpt2_qkv_fused_linear_1d()
2025-04-11T03:52:13.2431197Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2431286Z     partial_func(**kwargs)
2025-04-11T03:52:13.2431538Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2431690Z     partial_func(**kwargs)
2025-04-11T03:52:13.2432046Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 194, in check_gpt2_qkv_fused_linear_1d
2025-04-11T03:52:13.2432184Z     check_linear_conv_1d_col(lazy_init, seq_parallel_mode)
2025-04-11T03:52:13.2432516Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 47, in check_linear_conv_1d_col
2025-04-11T03:52:13.2432614Z     linear = Conv1D(192, 48).cuda()
2025-04-11T03:52:13.2432875Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2432981Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2433256Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2433346Z     param_applied = fn(param)
2025-04-11T03:52:13.2433626Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2433733Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2433912Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2434188Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2434317Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2434478Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2434820Z FAILED tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2434825Z 
2025-04-11T03:52:13.2434943Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2435039Z Traceback (most recent call last):
2025-04-11T03:52:13.2435321Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2435400Z     fn(i, *args)
2025-04-11T03:52:13.2435655Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 45, in run_dist
2025-04-11T03:52:13.2435738Z     check_layernorm()
2025-04-11T03:52:13.2435986Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2436076Z     partial_func(**kwargs)
2025-04-11T03:52:13.2436351Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 17, in check_layernorm
2025-04-11T03:52:13.2436457Z     norm = nn.LayerNorm(128, 0.00001).cuda()
2025-04-11T03:52:13.2436715Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2436886Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2437153Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2437246Z     param_applied = fn(param)
2025-04-11T03:52:13.2437520Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2437628Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2437729Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2438002Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2438197Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2438355Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2438686Z FAILED tests/test_shardformer/test_layer/test_linear_1d.py::test_linear - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2438698Z 
2025-04-11T03:52:13.2438811Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2438906Z Traceback (most recent call last):
2025-04-11T03:52:13.2439247Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2439327Z     fn(i, *args)
2025-04-11T03:52:13.2439614Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 279, in check_dist_linear
2025-04-11T03:52:13.2439698Z     run_dist_linear_test()
2025-04-11T03:52:13.2439944Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2440035Z     partial_func(**kwargs)
2025-04-11T03:52:13.2440279Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2440371Z     partial_func(**kwargs)
2025-04-11T03:52:13.2440609Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2440700Z     partial_func(**kwargs)
2025-04-11T03:52:13.2440990Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 270, in run_dist_linear_test
2025-04-11T03:52:13.2441141Z     check_linear_1d_col(lazy_init, seq_parallel_mode, overlap)
2025-04-11T03:52:13.2441489Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T03:52:13.2441583Z     linear = nn.Linear(32, 128).cuda()
2025-04-11T03:52:13.2441857Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2441967Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2442238Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2442327Z     param_applied = fn(param)
2025-04-11T03:52:13.2442602Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2442708Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2442809Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2443091Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2443220Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2443383Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2443750Z FAILED tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2443757Z 
2025-04-11T03:52:13.2443883Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2443977Z Traceback (most recent call last):
2025-04-11T03:52:13.2444255Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2444390Z     fn(i, *args)
2025-04-11T03:52:13.2444673Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 158, in run_dist
2025-04-11T03:52:13.2444768Z     check_linear_1d_col()
2025-04-11T03:52:13.2445017Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2445109Z     partial_func(**kwargs)
2025-04-11T03:52:13.2445418Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 21, in check_linear_1d_col
2025-04-11T03:52:13.2445572Z     linear = nn.Linear(8, 80).cuda()
2025-04-11T03:52:13.2445835Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2445941Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2446203Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2446294Z     param_applied = fn(param)
2025-04-11T03:52:13.2446562Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2446722Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2446825Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2447103Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2447232Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2447394Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2447726Z FAILED tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2447731Z 
2025-04-11T03:52:13.2447851Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2447946Z Traceback (most recent call last):
2025-04-11T03:52:13.2448223Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2448302Z     fn(i, *args)
2025-04-11T03:52:13.2448586Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 169, in launch_single_ring
2025-04-11T03:52:13.2448734Z     check_packed_seq()
2025-04-11T03:52:13.2448985Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2449077Z     partial_func(**kwargs)
2025-04-11T03:52:13.2449323Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2449413Z     partial_func(**kwargs)
2025-04-11T03:52:13.2449656Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2449738Z     partial_func(**kwargs)
2025-04-11T03:52:13.2449844Z   [Previous line repeated 2 more times]
2025-04-11T03:52:13.2450122Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 94, in check_packed_seq
2025-04-11T03:52:13.2450297Z     padding_mask = torch.ones((bs, seqlen), dtype=torch.int, device=device)
2025-04-11T03:52:13.2450394Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2450672Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2450799Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2450951Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2451296Z FAILED tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2451300Z 
2025-04-11T03:52:13.2451415Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2451512Z Traceback (most recent call last):
2025-04-11T03:52:13.2451847Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2451932Z     fn(i, *args)
2025-04-11T03:52:13.2452215Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 175, in launch_double_ring
2025-04-11T03:52:13.2452318Z     check_ring_attn(inner_ring_size=2)
2025-04-11T03:52:13.2452566Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2452651Z     partial_func(**kwargs)
2025-04-11T03:52:13.2452903Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2453080Z     partial_func(**kwargs)
2025-04-11T03:52:13.2453329Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2453412Z     partial_func(**kwargs)
2025-04-11T03:52:13.2453515Z   [Previous line repeated 2 more times]
2025-04-11T03:52:13.2453791Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 36, in check_ring_attn
2025-04-11T03:52:13.2454055Z     qkv = torch.randn(bs, seq_len, 3, nheads, d, device=device, dtype=dtype, requires_grad=True)
2025-04-11T03:52:13.2454156Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2454435Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2454571Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2454728Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2455123Z FAILED tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2455128Z 
2025-04-11T03:52:13.2455241Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2455335Z Traceback (most recent call last):
2025-04-11T03:52:13.2455633Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2455712Z     fn(i, *args)
2025-04-11T03:52:13.2456030Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 169, in check_all2all_attn
2025-04-11T03:52:13.2456175Z     run_seq_parallel_attn()
2025-04-11T03:52:13.2456422Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2456505Z     partial_func(**kwargs)
2025-04-11T03:52:13.2456744Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2456833Z     partial_func(**kwargs)
2025-04-11T03:52:13.2457073Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2457162Z     partial_func(**kwargs)
2025-04-11T03:52:13.2457258Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.2457583Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 164, in run_seq_parallel_attn
2025-04-11T03:52:13.2457732Z     seq_parallel_attn(seq_len, hidden_dim, head_num, batch_size)
2025-04-11T03:52:13.2458034Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 101, in seq_parallel_attn
2025-04-11T03:52:13.2458172Z     x = torch.randn(batch_size, seq_len, hidden_dim).cuda()
2025-04-11T03:52:13.2458269Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2458541Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2458671Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2458827Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2459216Z FAILED tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2459274Z 
2025-04-11T03:52:13.2459395Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2459489Z Traceback (most recent call last):
2025-04-11T03:52:13.2459770Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2459854Z     fn(i, *args)
2025-04-11T03:52:13.2460158Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 49, in run_dist
2025-04-11T03:52:13.2460261Z     check_vocab_embedding_1d()
2025-04-11T03:52:13.2460568Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2460662Z     partial_func(**kwargs)
2025-04-11T03:52:13.2461004Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 18, in check_vocab_embedding_1d
2025-04-11T03:52:13.2461116Z     embedding = nn.Embedding(128, 32).to("cuda")
2025-04-11T03:52:13.2461379Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:13.2461610Z     return self._apply(convert)
2025-04-11T03:52:13.2461881Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2461975Z     param_applied = fn(param)
2025-04-11T03:52:13.2462245Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:13.2462457Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:13.2462559Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2462840Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2462969Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2463130Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2463395Z FAILED tests/test_shardformer/test_model/test_shard_bert.py::test_bert - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2463675Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2463857Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2464017Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2464283Z FAILED tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2 - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2464553Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2464683Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2464834Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2465105Z FAILED tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2465376Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2465508Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2465658Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2465941Z FAILED tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2466212Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2466338Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2466489Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2466822Z FAILED tests/test_shardformer/test_model/test_shard_command.py::test_command - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2467093Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2467218Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2467373Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2467723Z FAILED tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2467728Z 
2025-04-11T03:52:13.2467848Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2467997Z Traceback (most recent call last):
2025-04-11T03:52:13.2468276Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2468358Z     fn(i, *args)
2025-04-11T03:52:13.2468679Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 216, in check_deepseek
2025-04-11T03:52:13.2468773Z     run_deepseek_test()
2025-04-11T03:52:13.2469090Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2469177Z     partial_func(**kwargs)
2025-04-11T03:52:13.2469477Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 187, in run_deepseek_test
2025-04-11T03:52:13.2469567Z     run_deepseek_commom(config)
2025-04-11T03:52:13.2469869Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 77, in run_deepseek_commom
2025-04-11T03:52:13.2470069Z     torch_model = AutoModel.from_config(config, trust_remote_code=True).cuda().to(dtype)
2025-04-11T03:52:13.2470350Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2470448Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2470712Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2470824Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2471088Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2471242Z     module._apply(fn)
2025-04-11T03:52:13.2471503Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2471600Z     param_applied = fn(param)
2025-04-11T03:52:13.2471863Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2471975Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2472072Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2472350Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2472484Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2472637Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2473013Z FAILED tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2473020Z 
2025-04-11T03:52:13.2473136Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2473232Z Traceback (most recent call last):
2025-04-11T03:52:13.2473514Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2473597Z     fn(i, *args)
2025-04-11T03:52:13.2473908Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 93, in check_deepseek_v3
2025-04-11T03:52:13.2473996Z     run_deepseek_v3_test()
2025-04-11T03:52:13.2474250Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2474396Z     partial_func(**kwargs)
2025-04-11T03:52:13.2474713Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 82, in run_deepseek_v3_test
2025-04-11T03:52:13.2474806Z     check_forward_backward(
2025-04-11T03:52:13.2475124Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 29, in check_forward_backward
2025-04-11T03:52:13.2475391Z     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
2025-04-11T03:52:13.2475687Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 138, in build_model_from_hybrid_plugin
2025-04-11T03:52:13.2475862Z     org_model = org_model.cuda()
2025-04-11T03:52:13.2476145Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2476252Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2476513Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2476681Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2476948Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2477041Z     module._apply(fn)
2025-04-11T03:52:13.2477303Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2477387Z     module._apply(fn)
2025-04-11T03:52:13.2477653Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2477749Z     param_applied = fn(param)
2025-04-11T03:52:13.2478019Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2478124Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2478227Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2478501Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2478629Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2478846Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2479123Z FAILED tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2479396Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2479524Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2479682Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2479942Z FAILED tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2 - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2480210Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2480342Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2480489Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2480759Z FAILED tests/test_shardformer/test_model/test_shard_llama.py::test_llama - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2481023Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2481152Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2481300Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2481581Z FAILED tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2481843Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2482021Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2482174Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2482513Z FAILED tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2482520Z 
2025-04-11T03:52:13.2482637Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2482731Z Traceback (most recent call last):
2025-04-11T03:52:13.2483009Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2483142Z     fn(i, *args)
2025-04-11T03:52:13.2483433Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 210, in check_mixtral
2025-04-11T03:52:13.2483517Z     run_mixtral_test()
2025-04-11T03:52:13.2483771Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2483860Z     partial_func(**kwargs)
2025-04-11T03:52:13.2484205Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 180, in run_mixtral_test
2025-04-11T03:52:13.2484302Z     run_mixtral_commom(config)
2025-04-11T03:52:13.2484592Z   File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 70, in run_mixtral_commom
2025-04-11T03:52:13.2484720Z     torch_model = MixtralModel(config).to(dtype).cuda()
2025-04-11T03:52:13.2484997Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2485095Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2485354Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2485463Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2485727Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2485810Z     module._apply(fn)
2025-04-11T03:52:13.2486070Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2486214Z     param_applied = fn(param)
2025-04-11T03:52:13.2486479Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2486587Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2486684Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2486964Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2487092Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2487250Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2487522Z FAILED tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2487793Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2487921Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2488073Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2488341Z FAILED tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2 - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2488606Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2488734Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2488882Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2489137Z FAILED tests/test_shardformer/test_model/test_shard_sam.py::test_sam - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2489456Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2489582Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2489736Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2489987Z FAILED tests/test_shardformer/test_model/test_shard_t5.py::test_t5 - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2490257Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2490454Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2490607Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2490867Z FAILED tests/test_shardformer/test_model/test_shard_vit.py::test_vit - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2491142Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2491321Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2491471Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2491757Z FAILED tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper - RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2492025Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2492152Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2492302Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2492609Z FAILED tests/test_tensor/test_comm_spec_apply.py::test_comm_spec - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2492614Z 
2025-04-11T03:52:13.2492729Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2492827Z Traceback (most recent call last):
2025-04-11T03:52:13.2493108Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2493188Z     fn(i, *args)
2025-04-11T03:52:13.2493431Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 191, in check_comm
2025-04-11T03:52:13.2493586Z     check_all_gather(device_mesh, rank)
2025-04-11T03:52:13.2493841Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 16, in check_all_gather
2025-04-11T03:52:13.2493961Z     sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T03:52:13.2494061Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2494336Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2494460Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2494617Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2494934Z FAILED tests/test_tensor/test_padded_tensor.py::test_padded_tensor - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2494941Z 
2025-04-11T03:52:13.2495060Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2495156Z Traceback (most recent call last):
2025-04-11T03:52:13.2495439Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2495518Z     fn(i, *args)
2025-04-11T03:52:13.2495766Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_padded_tensor.py", line 14, in check_padded_tensor
2025-04-11T03:52:13.2495887Z     original_tensor = torch.rand(32, 64).to("cuda")
2025-04-11T03:52:13.2495983Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2496261Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2496442Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2496598Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2496922Z FAILED tests/test_tensor/test_shape_consistency_apply.py::test_apply - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2496929Z 
2025-04-11T03:52:13.2497051Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2497145Z Traceback (most recent call last):
2025-04-11T03:52:13.2497424Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2497629Z     fn(i, *args)
2025-04-11T03:52:13.2497892Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_shape_consistency_apply.py", line 41, in check_apply
2025-04-11T03:52:13.2498069Z     tensor_to_comm = torch.cat((sharded_tensor_0, sharded_tensor_1), 1).cuda()
2025-04-11T03:52:13.2498167Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2498443Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2498626Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2498777Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2499100Z FAILED tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2499107Z 
2025-04-11T03:52:13.2499220Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2499318Z Traceback (most recent call last):
2025-04-11T03:52:13.2499596Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2499676Z     fn(i, *args)
2025-04-11T03:52:13.2499931Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 140, in check_comm
2025-04-11T03:52:13.2500041Z     check_all_gather(process_group_dict, rank)
2025-04-11T03:52:13.2500309Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 15, in check_all_gather
2025-04-11T03:52:13.2500427Z     sharded_tensor_to_comm = torch.ones(2, 2).cuda()
2025-04-11T03:52:13.2500531Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2500800Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2501006Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2501159Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2501483Z FAILED tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2501492Z 
2025-04-11T03:52:13.2501605Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2501698Z Traceback (most recent call last):
2025-04-11T03:52:13.2501981Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2502060Z     fn(i, *args)
2025-04-11T03:52:13.2502318Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py", line 25, in check_dtensor
2025-04-11T03:52:13.2502418Z     test_model = TestModel(8, 8).to("cuda")
2025-04-11T03:52:13.2502682Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:13.2502776Z     return self._apply(convert)
2025-04-11T03:52:13.2503036Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2503128Z     module._apply(fn)
2025-04-11T03:52:13.2503388Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2503485Z     param_applied = fn(param)
2025-04-11T03:52:13.2503750Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:13.2504023Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:13.2504122Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2504394Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2504528Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2504682Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2505052Z FAILED tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2505124Z 
2025-04-11T03:52:13.2505238Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2505338Z Traceback (most recent call last):
2025-04-11T03:52:13.2505613Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2505698Z     fn(i, *args)
2025-04-11T03:52:13.2506026Z   File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_layout_converter.py", line 162, in check_layout_converting_apply
2025-04-11T03:52:13.2506206Z     original_tensor = torch.rand(global_shape).cuda()
2025-04-11T03:52:13.2506310Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2506582Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2506713Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2506864Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2507203Z FAILED tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2507208Z 
2025-04-11T03:52:13.2507321Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2507416Z Traceback (most recent call last):
2025-04-11T03:52:13.2507695Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2507775Z     fn(i, *args)
2025-04-11T03:52:13.2508025Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 53, in run_dist
2025-04-11T03:52:13.2508164Z     exam_chunk_memory()
2025-04-11T03:52:13.2508466Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2508554Z     partial_func(**kwargs)
2025-04-11T03:52:13.2508803Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2508899Z     partial_func(**kwargs)
2025-04-11T03:52:13.2509166Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 21, in exam_chunk_memory
2025-04-11T03:52:13.2509274Z     chunk_manager = ChunkManager(config)
2025-04-11T03:52:13.2509515Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T03:52:13.2509762Z     self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:13.2509861Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2510129Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2510262Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2510417Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2510751Z FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2510756Z 
2025-04-11T03:52:13.2510869Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2510964Z Traceback (most recent call last):
2025-04-11T03:52:13.2511243Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2511402Z     fn(i, *args)
2025-04-11T03:52:13.2511640Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T03:52:13.2511723Z     exam_chunk_basic()
2025-04-11T03:52:13.2511978Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2512064Z     partial_func(**kwargs)
2025-04-11T03:52:13.2512315Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2512462Z     partial_func(**kwargs)
2025-04-11T03:52:13.2512713Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2512797Z     partial_func(**kwargs)
2025-04-11T03:52:13.2512897Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.2513159Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T03:52:13.2513244Z     my_chunk = Chunk(
2025-04-11T03:52:13.2513480Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T03:52:13.2513739Z     self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T03:52:13.2513847Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2514120Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2514250Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2514412Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2514743Z FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2514748Z 
2025-04-11T03:52:13.2514869Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2514964Z Traceback (most recent call last):
2025-04-11T03:52:13.2515245Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2515327Z     fn(i, *args)
2025-04-11T03:52:13.2515562Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T03:52:13.2515715Z     exam_chunk_basic()
2025-04-11T03:52:13.2515959Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2516050Z     partial_func(**kwargs)
2025-04-11T03:52:13.2516291Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2516381Z     partial_func(**kwargs)
2025-04-11T03:52:13.2516617Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2516698Z     partial_func(**kwargs)
2025-04-11T03:52:13.2516802Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.2517052Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T03:52:13.2517143Z     my_chunk = Chunk(
2025-04-11T03:52:13.2517371Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T03:52:13.2517572Z     self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T03:52:13.2517671Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2517941Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2518074Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2518228Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2518561Z FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2518631Z 
2025-04-11T03:52:13.2518748Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2518849Z Traceback (most recent call last):
2025-04-11T03:52:13.2519131Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2519215Z     fn(i, *args)
2025-04-11T03:52:13.2519449Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
2025-04-11T03:52:13.2519532Z     exam_chunk_basic()
2025-04-11T03:52:13.2519776Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2519923Z     partial_func(**kwargs)
2025-04-11T03:52:13.2520165Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2520249Z     partial_func(**kwargs)
2025-04-11T03:52:13.2520493Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2520580Z     partial_func(**kwargs)
2025-04-11T03:52:13.2520677Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.2520993Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
2025-04-11T03:52:13.2521075Z     my_chunk = Chunk(
2025-04-11T03:52:13.2521308Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
2025-04-11T03:52:13.2521505Z     self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
2025-04-11T03:52:13.2521606Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2521885Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2522014Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2522176Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2522522Z FAILED tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2522528Z 
2025-04-11T03:52:13.2522645Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2522740Z Traceback (most recent call last):
2025-04-11T03:52:13.2523081Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2523160Z     fn(i, *args)
2025-04-11T03:52:13.2523402Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 152, in run_dist
2025-04-11T03:52:13.2523491Z     exam_gemini_grad_acc()
2025-04-11T03:52:13.2523729Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2523816Z     partial_func(**kwargs)
2025-04-11T03:52:13.2524054Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2524144Z     partial_func(**kwargs)
2025-04-11T03:52:13.2524382Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2524466Z     partial_func(**kwargs)
2025-04-11T03:52:13.2524568Z   [Previous line repeated 4 more times]
2025-04-11T03:52:13.2524839Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 71, in exam_gemini_grad_acc
2025-04-11T03:52:13.2524942Z     torch_model = model_builder().cuda()
2025-04-11T03:52:13.2525217Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2525321Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2525579Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2525690Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2525957Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2526100Z     module._apply(fn)
2025-04-11T03:52:13.2526376Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2526458Z     module._apply(fn)
2025-04-11T03:52:13.2526722Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2526818Z     param_applied = fn(param)
2025-04-11T03:52:13.2527087Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2527261Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2527358Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2527638Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2527767Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2527930Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2528247Z FAILED tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2528305Z 
2025-04-11T03:52:13.2528432Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2528528Z Traceback (most recent call last):
2025-04-11T03:52:13.2528800Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2528883Z     fn(i, *args)
2025-04-11T03:52:13.2529120Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T03:52:13.2529216Z     exam_grad_clipping()
2025-04-11T03:52:13.2529459Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2529545Z     partial_func(**kwargs)
2025-04-11T03:52:13.2529787Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2529869Z     partial_func(**kwargs)
2025-04-11T03:52:13.2530114Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2530197Z     partial_func(**kwargs)
2025-04-11T03:52:13.2530352Z   [Previous line repeated 2 more times]
2025-04-11T03:52:13.2530616Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T03:52:13.2530714Z     torch_model = model_builder().cuda()
2025-04-11T03:52:13.2530992Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2531088Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2531351Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2531459Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2531726Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2531808Z     module._apply(fn)
2025-04-11T03:52:13.2532071Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2532154Z     module._apply(fn)
2025-04-11T03:52:13.2532412Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2532509Z     param_applied = fn(param)
2025-04-11T03:52:13.2532775Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2532885Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2532984Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2533261Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2533447Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2533602Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2533920Z FAILED tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2533927Z 
2025-04-11T03:52:13.2534042Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2534143Z Traceback (most recent call last):
2025-04-11T03:52:13.2534418Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2534561Z     fn(i, *args)
2025-04-11T03:52:13.2534806Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
2025-04-11T03:52:13.2534899Z     exam_grad_clipping()
2025-04-11T03:52:13.2535143Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2535228Z     partial_func(**kwargs)
2025-04-11T03:52:13.2535476Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2535620Z     partial_func(**kwargs)
2025-04-11T03:52:13.2535868Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2535954Z     partial_func(**kwargs)
2025-04-11T03:52:13.2536051Z   [Previous line repeated 2 more times]
2025-04-11T03:52:13.2536313Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
2025-04-11T03:52:13.2536408Z     torch_model = model_builder().cuda()
2025-04-11T03:52:13.2536685Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2536779Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2537037Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2537143Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2537417Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2537500Z     module._apply(fn)
2025-04-11T03:52:13.2537814Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2537902Z     module._apply(fn)
2025-04-11T03:52:13.2538159Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2538255Z     param_applied = fn(param)
2025-04-11T03:52:13.2538516Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2538625Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2538725Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2539001Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2539139Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2539296Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2539618Z FAILED tests/test_zero/test_gemini/test_inference.py::test_inference[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2539625Z 
2025-04-11T03:52:13.2539737Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2539836Z Traceback (most recent call last):
2025-04-11T03:52:13.2540115Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2540193Z     fn(i, *args)
2025-04-11T03:52:13.2540439Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T03:52:13.2540523Z     exam_inference()
2025-04-11T03:52:13.2540823Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2540911Z     partial_func(**kwargs)
2025-04-11T03:52:13.2541160Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2541249Z     partial_func(**kwargs)
2025-04-11T03:52:13.2541491Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2541582Z     partial_func(**kwargs)
2025-04-11T03:52:13.2541837Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T03:52:13.2541994Z     torch_model = model_builder().cuda()
2025-04-11T03:52:13.2542269Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2542365Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2542628Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2542731Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2543062Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2543145Z     module._apply(fn)
2025-04-11T03:52:13.2543411Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2543494Z     module._apply(fn)
2025-04-11T03:52:13.2543762Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2543859Z     param_applied = fn(param)
2025-04-11T03:52:13.2544132Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2544244Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2544343Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2544631Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2544767Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2544933Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2545321Z FAILED tests/test_zero/test_gemini/test_inference.py::test_inference[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2545327Z 
2025-04-11T03:52:13.2545448Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2545543Z Traceback (most recent call last):
2025-04-11T03:52:13.2545821Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2545906Z     fn(i, *args)
2025-04-11T03:52:13.2546146Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
2025-04-11T03:52:13.2546233Z     exam_inference()
2025-04-11T03:52:13.2546477Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2546571Z     partial_func(**kwargs)
2025-04-11T03:52:13.2546811Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2546898Z     partial_func(**kwargs)
2025-04-11T03:52:13.2547140Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2547222Z     partial_func(**kwargs)
2025-04-11T03:52:13.2547475Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
2025-04-11T03:52:13.2547572Z     torch_model = model_builder().cuda()
2025-04-11T03:52:13.2547847Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2547941Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2548255Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2548365Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2548670Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2548758Z     module._apply(fn)
2025-04-11T03:52:13.2549012Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2549096Z     module._apply(fn)
2025-04-11T03:52:13.2549353Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2549513Z     param_applied = fn(param)
2025-04-11T03:52:13.2549781Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2549886Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2549992Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2550269Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2550465Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2550623Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2550925Z FAILED tests/test_zero/test_gemini/test_optim.py::test_optim[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2550937Z 
2025-04-11T03:52:13.2551052Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2551147Z Traceback (most recent call last):
2025-04-11T03:52:13.2551430Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2551507Z     fn(i, *args)
2025-04-11T03:52:13.2551746Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 185, in run_dist
2025-04-11T03:52:13.2551830Z     exam_model_step()
2025-04-11T03:52:13.2552078Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2552171Z     partial_func(**kwargs)
2025-04-11T03:52:13.2552415Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2552564Z     partial_func(**kwargs)
2025-04-11T03:52:13.2552805Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2552896Z     partial_func(**kwargs)
2025-04-11T03:52:13.2552993Z   [Previous line repeated 2 more times]
2025-04-11T03:52:13.2553235Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 75, in exam_model_step
2025-04-11T03:52:13.2553337Z     torch_model = model_builder().cuda()
2025-04-11T03:52:13.2553660Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
2025-04-11T03:52:13.2553765Z     return super().cuda(*args, **kwargs)
2025-04-11T03:52:13.2554021Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2554132Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2554394Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2554479Z     module._apply(fn)
2025-04-11T03:52:13.2554741Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2554820Z     module._apply(fn)
2025-04-11T03:52:13.2555083Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2555174Z     param_applied = fn(param)
2025-04-11T03:52:13.2555447Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2555619Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2555722Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2556000Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2556129Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2556294Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2556603Z FAILED tests/test_zero/test_gemini/test_search.py::test_search[1] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2556607Z 
2025-04-11T03:52:13.2556726Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2556885Z Traceback (most recent call last):
2025-04-11T03:52:13.2557165Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2557243Z     fn(i, *args)
2025-04-11T03:52:13.2557475Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T03:52:13.2557570Z     exam_chunk_manager()
2025-04-11T03:52:13.2557884Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T03:52:13.2557982Z     chunk_manager = init_chunk_manager(
2025-04-11T03:52:13.2558248Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T03:52:13.2558335Z     dist.barrier()
2025-04-11T03:52:13.2558625Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:13.2558719Z     return func(*args, **kwargs)
2025-04-11T03:52:13.2559035Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T03:52:13.2559134Z     work = default_pg.barrier(opts=opts)
2025-04-11T03:52:13.2559236Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2559511Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2559648Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2559802Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2560106Z FAILED tests/test_zero/test_gemini/test_search.py::test_search[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2560173Z 
2025-04-11T03:52:13.2560289Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2560382Z Traceback (most recent call last):
2025-04-11T03:52:13.2560668Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2560745Z     fn(i, *args)
2025-04-11T03:52:13.2560980Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
2025-04-11T03:52:13.2561067Z     exam_chunk_manager()
2025-04-11T03:52:13.2561329Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
2025-04-11T03:52:13.2561425Z     chunk_manager = init_chunk_manager(
2025-04-11T03:52:13.2561685Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
2025-04-11T03:52:13.2561772Z     dist.barrier()
2025-04-11T03:52:13.2562063Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
2025-04-11T03:52:13.2562158Z     return func(*args, **kwargs)
2025-04-11T03:52:13.2562466Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
2025-04-11T03:52:13.2562568Z     work = default_pg.barrier(opts=opts)
2025-04-11T03:52:13.2562665Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2562938Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2563125Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2563283Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2563627Z FAILED tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4] - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2563634Z 
2025-04-11T03:52:13.2563747Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2563850Z Traceback (most recent call last):
2025-04-11T03:52:13.2564132Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2564279Z     fn(i, *args)
2025-04-11T03:52:13.2564543Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 78, in run_dist
2025-04-11T03:52:13.2564626Z     exam_state_dict()
2025-04-11T03:52:13.2564881Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2564969Z     partial_func(**kwargs)
2025-04-11T03:52:13.2565219Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2565364Z     partial_func(**kwargs)
2025-04-11T03:52:13.2565606Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2565694Z     partial_func(**kwargs)
2025-04-11T03:52:13.2565793Z   [Previous line repeated 1 more time]
2025-04-11T03:52:13.2566075Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 45, in exam_state_dict
2025-04-11T03:52:13.2566324Z     model = GeminiDDP(model, config_dict, **placement_config, pin_memory=True, master_weights=master_weights)
2025-04-11T03:52:13.2566559Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 109, in __init__
2025-04-11T03:52:13.2566657Z     self.chunk_manager = ChunkManager(
2025-04-11T03:52:13.2566899Z   File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
2025-04-11T03:52:13.2567150Z     self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
2025-04-11T03:52:13.2567250Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2567534Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2567720Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2567877Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2568179Z FAILED tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2568186Z 
2025-04-11T03:52:13.2568304Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2568399Z Traceback (most recent call last):
2025-04-11T03:52:13.2568678Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2568763Z     fn(i, *args)
2025-04-11T03:52:13.2569002Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 32, in run_dist
2025-04-11T03:52:13.2569094Z     check_all_gather_2d()
2025-04-11T03:52:13.2569356Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 16, in check_all_gather_2d
2025-04-11T03:52:13.2569490Z     tensor = torch.rand(128, device=get_current_device())
2025-04-11T03:52:13.2569588Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2569857Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2569990Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2570143Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2570487Z FAILED tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2570545Z 
2025-04-11T03:52:13.2570659Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2570757Z Traceback (most recent call last):
2025-04-11T03:52:13.2571034Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2571120Z     fn(i, *args)
2025-04-11T03:52:13.2571362Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 139, in run_dist
2025-04-11T03:52:13.2571455Z     exam_zero_1_grad_acc(sync=True)
2025-04-11T03:52:13.2571790Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 86, in exam_zero_1_grad_acc
2025-04-11T03:52:13.2571886Z     zero_model = zero_model.to(device)
2025-04-11T03:52:13.2572148Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
2025-04-11T03:52:13.2572244Z     return self._apply(convert)
2025-04-11T03:52:13.2572512Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2572679Z     module._apply(fn)
2025-04-11T03:52:13.2572940Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2573037Z     param_applied = fn(param)
2025-04-11T03:52:13.2573306Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
2025-04-11T03:52:13.2573520Z     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
2025-04-11T03:52:13.2573622Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2573906Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2574038Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2574195Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2574523Z FAILED tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2 - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2574531Z 
2025-04-11T03:52:13.2574645Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2574801Z Traceback (most recent call last):
2025-04-11T03:52:13.2575077Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2575160Z     fn(i, *args)
2025-04-11T03:52:13.2575408Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 51, in run_dist
2025-04-11T03:52:13.2575515Z     exam_mem_leak(world_size=world_size)
2025-04-11T03:52:13.2575773Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 36, in exam_mem_leak
2025-04-11T03:52:13.2575864Z     zero_model = MlpModel().cuda()
2025-04-11T03:52:13.2576130Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2576238Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2576509Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2576591Z     module._apply(fn)
2025-04-11T03:52:13.2576858Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2576949Z     param_applied = fn(param)
2025-04-11T03:52:13.2577211Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2577326Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2577421Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2577700Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2577886Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2578050Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2578368Z FAILED tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2 - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2578374Z 
2025-04-11T03:52:13.2578489Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2578589Z Traceback (most recent call last):
2025-04-11T03:52:13.2578864Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2578945Z     fn(i, *args)
2025-04-11T03:52:13.2579325Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 217, in run_dist
2025-04-11T03:52:13.2579418Z     exam_zero_1_torch_ddp()
2025-04-11T03:52:13.2579667Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2579751Z     partial_func(**kwargs)
2025-04-11T03:52:13.2579999Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2580142Z     partial_func(**kwargs)
2025-04-11T03:52:13.2580392Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2580480Z     partial_func(**kwargs)
2025-04-11T03:52:13.2580757Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 151, in exam_zero_1_torch_ddp
2025-04-11T03:52:13.2580866Z     torch_model = MlpModel().cuda().to(dtype)
2025-04-11T03:52:13.2581125Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2581238Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2581498Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2581587Z     module._apply(fn)
2025-04-11T03:52:13.2581843Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2581938Z     param_applied = fn(param)
2025-04-11T03:52:13.2582201Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2582371Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2582472Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2582747Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2582884Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2583040Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2583362Z FAILED tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt - torch.multiprocessing.spawn.ProcessRaisedException: 
2025-04-11T03:52:13.2583369Z 
2025-04-11T03:52:13.2583484Z -- Process 0 terminated with the following error:
2025-04-11T03:52:13.2583581Z Traceback (most recent call last):
2025-04-11T03:52:13.2583856Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
2025-04-11T03:52:13.2583934Z     fn(i, *args)
2025-04-11T03:52:13.2584183Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 123, in run_dist
2025-04-11T03:52:13.2584279Z     exam_zero_1_torch_ddp_ckpt()
2025-04-11T03:52:13.2584528Z   File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
2025-04-11T03:52:13.2584617Z     partial_func(**kwargs)
2025-04-11T03:52:13.2584908Z   File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 62, in exam_zero_1_torch_ddp_ckpt
2025-04-11T03:52:13.2585004Z     torch_model = MlpModel().cuda()
2025-04-11T03:52:13.2585259Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
2025-04-11T03:52:13.2585432Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2585692Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
2025-04-11T03:52:13.2585782Z     module._apply(fn)
2025-04-11T03:52:13.2586041Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
2025-04-11T03:52:13.2586137Z     param_applied = fn(param)
2025-04-11T03:52:13.2586399Z   File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
2025-04-11T03:52:13.2586565Z     return self._apply(lambda t: t.cuda(device))
2025-04-11T03:52:13.2586669Z RuntimeError: CUDA error: out of memory
2025-04-11T03:52:13.2586947Z CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
2025-04-11T03:52:13.2587083Z For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
2025-04-11T03:52:13.2587245Z Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T03:52:13.2587442Z = 573 failed, 74 passed, 195 skipped, 23 deselected, 91 warnings in 673.91s (0:11:13) =
2025-04-11T03:52:13.9056282Z ##[error]Process completed with exit code 1.
